
@article{abdelfattahLeastRecentlyFive2012,
  title = {Least {{Recently Plus Five Least Frequently Replacement Policy}} ({{LR}}+{{5LF}})},
  author = {AbdelFattah, Adwan and Samra, Aiman Abu},
  date = {2012},
  journaltitle = {The International Arab Journal of Information Technology},
  volume = {9},
  pages = {6},
  abstract = {In this paper, we present a new block replacement policy in which we proposed a new efficient algorithm for combining two important policies Least Recently Used (LRU) and Least Frequently Used (LFU). The implementation of the proposed policy is simple. It requires limited calculations to determine the victim block. We proposed our models to implement LRU and LFU policies. The new policy gives each block in cache two weighing values corresponding to LRU and LFU policies. Then a simple algorithm is used to get the overall value for each block. A comprehensive comparison is made between our Policy and LRU, First In First Out (FIFO), V-WAY, and Combined LRU and LFU (CRF) policies. Experimental results show that the LR+5LF replacement policy significantly reduces the number of cache misses. We modified simple scalar simulator version 3 under Linux Ubuntu 9.04 and we used speccpu2000 benchmark to simulate this policy. The results of simulations showed, that giving higher weighing to LFU policy gives this policy best performance characteristics over other policies. Substantial improvement on miss rate was achieved on instruction level 1 cache and at level 2 cache memory.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8VMXNYT5\\AbdelFattah and Samra - 2012 - Least Recently Plus Five Least Frequently Replacem.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{abramsCachingProxiesLimitations1995,
  title = {Caching {{Proxies}}: {{Limitations}} and {{Potentials}}},
  booktitle = {Proceedings {{Fourth International World Wide Web Conference}}},
  author = {Abrams, Marc and Standridge, Charles R. and Abdulla, Ghaleb and Williams, Stephen and Fox, Edward A.},
  date = {1995},
  volume = {1},
  publisher = {{O'Reilly}},
  location = {{Boston, Mass.}},
  url = {https://www.w3.org/Conferences/WWW4/Papers/155/},
  urldate = {2020-01-04},
  eventtitle = {Fourth {{International World Wide Web Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BDZASLFE\\155.html},
  isbn = {1-56592-169-0}
}

@article{acklandSinglechip6billion16b2000,
  title = {A Single-Chip, 1.6-Billion, 16-b {{MAC}}/s Multiprocessor {{DSP}}},
  author = {Ackland, B. and Anesko, A. and Brinthaupt, D. and Daubert, S.J. and Kalavade, A. and Knobloch, J. and Micca, E. and Moturi, M. and Nicol, C.J. and O'Neill, J.H. and Othmer, J. and Sackinger, E. and Singh, K.J. and Sweet, J. and Terman, C.J. and Williams, J.},
  date = {2000-03},
  journaltitle = {IEEE Journal of Solid-State Circuits},
  volume = {35},
  pages = {412--424},
  issn = {1558-173X},
  doi = {10.1109/4.826824},
  abstract = {An MIMD multiprocessor digital signal-processing (DSP) chip containing four 64-b processing elements (PE's) interconnected by a 128-b pipelined split transaction bus (STBus) is presented. Each PE contains a 32-b RISC core with DSP enhancements and a 64-b single-instruction, multiple-data vector coprocessor with four 16-b MAC/s and a vector reduction unit. PEs are connected to the STBus through reconfigurable dual-ported snooping L1 cache memories that support shared memory multiprocessing using a modified-MESI data coherency protocol. High-bandwidth data transfers between system memory and on-chip caches are managed in a pipelined memory controller that supports multiple outstanding transactions. An embedded RTOS dynamically schedules multiple tasks onto the PEs. Process synchronization is achieved using cached semaphores. The 200-mm/sup 2/, 0.25-/spl mu/m CMOS chip operates at 100 MHz and dissipates 4 W from a 3.3-V supply.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PZKMNKUA\\Ackland et al. - 2000 - A single-chip, 1.6-billion, 16-b MACs multiproces.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RWREQ87P\\826824.html},
  keywords = {0.25 mum,100 MHz,16-b MAC/s,3.3 V,32 bit,4 W,64 bit,64-b single-instruction,Cache memory,cache storage,cached semaphores,CMOS chip,CMOS digital integrated circuits,Control systems,Coprocessors,Digital signal processing,digital signal processing chips,Digital signal processing chips,digital signal-processing chip,DSP,DSP enhancement,dual-ported snooping L1 cache memories,embedded RTOS,high-bandwidth data transfer,Memory management,MIMD multiprocessor,modified-MESI data coherency protocol,multiple outstanding transactions,multiple-data vector coprocessor,multiprocessor interconnection networks,on-chip caches,parallel architectures,pipeline processing,pipelined memory controller,pipelined split transaction bus,Protocols,reconfigurable architectures,reduced instruction set computing,Reduced instruction set computing,RISC core,shared memory multiprocessing,Signal processing,synchronisation,synchronization,System-on-a-chip,vector processor systems,vector reduction},
  number = {3}
}

@article{aguilarCoherenceReplacementProtocolWeb2006,
  title = {A {{Coherence}}-{{Replacement Protocol For Web Proxy Cache Systems}}},
  author = {Aguilar, J. and Leiss, E. L.},
  date = {2006-01-01},
  journaltitle = {International Journal of Computers and Applications},
  volume = {28},
  pages = {12--18},
  issn = {1206-212X},
  doi = {10.1080/1206212X.2006.11441783},
  url = {https://doi.org/10.1080/1206212X.2006.11441783},
  urldate = {2019-08-05},
  abstract = {As World Wide Web usage has grown dramatically in recent years, so has the recognition that web caches (especially proxy caches) will have an important role in reducing server loads, client request latencies, and network traffic. In this paper, we propose an adaptive cache coherence-replacement scheme for web proxy cache systems that is based on several criteria about the system and applications, with the objective of optimizing the distributed cache system performance. Our coherence-replacement scheme assigns a replacement priority value to each cache block according to a set of criteria for deciding which block to remove. The goal is to provide an effective utilization of the distributed cache memory and a good application performance.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PJVR3TZ5\\Aguilar and Leiss - 2006 - A Coherence-Replacement Protocol For Web Proxy Cac.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6LJTW773\\1206212X.2006.html},
  keywords = {coherency techniques,replacement techniques,Web caching},
  number = {1}
}

@article{aguilarGeneralAdaptiveCache2004,
  title = {A {{General Adaptive Cache Coherency}}-{{Replacement Protocol}} for {{Web Proxy Cache Systems}}},
  author = {Aguilar, Jose and Leiss, Ernst},
  date = {2004},
  journaltitle = {Computación y Sistemas},
  volume = {8},
  pages = {1--14},
  url = {http://www.scielo.org.mx/pdf/cys/v8n1/v8n1a2.pdf},
  urldate = {2020-01-02},
  abstract = {As World Wide Web usage has grown dramatically in recent years, so has grown the recognition that Web caches (especially proxy caches) will have an important role in reducing server loads, client request latencies, and network traffic. In this paper, we propose an adaptive cache coherence-replacement scheme for web proxy cache systems that is based on several criteria about the system and applications, with the objective of optimizing the distributed cache system performance. Our coherence-replacement scheme assigns a replacement priority value to each cache block according to a set of criteria to decide which block to remove. The goal is to provide an effective utilization of the distributed cache memory and a good application performance.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZPCHRQUQ\\Aguilar and Leiss - 2001 - A General Adaptive Cache Coherency-Replacement Sch.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{aimtongkhamNovelWebCaching2016,
  title = {A Novel Web Caching Scheme Using Hybrid Least Frequently Used and Support Vector Machine},
  booktitle = {2016 13th {{International Joint Conference}} on {{Computer Science}} and {{Software Engineering}} ({{JCSSE}})},
  author = {Aimtongkham, P. and So-In, C. and Sanguanpong, S.},
  date = {2016-07},
  pages = {1--6},
  doi = {10.1109/JCSSE.2016.7748932},
  abstract = {The gargantuan uses of web access in various types of applications, such as text, image, audio, and video, across the globe have caused the limitation for service providers to optimally make use of Internet infrastructure. The advance of web proxy/caching has recently been in place to mitigate this phenomenon using the concept of locality and proximity. There exist some traditional caching schemes, such as FIFO, LFU, LRU, and Size, but with key limitations on the precision. On the other hands, soft computing has recently been investigated due to its advantage of high precision. Thus, this paper proposes a novel caching method by integrating these twos. SVM was first used for classification, to divide the caching probability - to be replaced or else. Then, LFU was applied for the actual replacement given new web objects (if cache full); and these are Hybrid LFU-SVM. Its performance is practically confirmed from our intensive evaluation against SVM-LRU and its traditional schemes like LFU and LRU in order of 14\% to 52.3\% and 18\% to 63.2\%, for hit and byte hit rate, respectively, using a standard NLANR dataset.},
  eventtitle = {2016 13th {{International Joint Conference}} on {{Computer Science}} and {{Software Engineering}} ({{JCSSE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R6UPCWHS\\Aimtongkham et al. - 2016 - A novel web caching scheme using hybrid least freq.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HKWFKRWW\\7748932.html},
  keywords = {Artificial neural networks,cache storage,caching probability,FIFO,Fuzzy logic,Hybrid Caching,hybrid least frequently used,hybrid LFU-SVM,information retrieval,Internet,Internet infrastructure,Kernel,Least Frequency Used,locality concept,NLANR dataset,pattern classification,proximity concept,Proxy,Radio frequency,Replacement,service providers,Soft Computing,support vector machine,Support Vector Machine,support vector machines,Support vector machines,SVM-LRU,Training,Web access,Web Caching,Web caching scheme,Web objects,Web proxy,Web Proxy}
}

@inproceedings{akessonClassificationAnalysisPredictable2010,
  title = {Classification and {{Analysis}} of {{Predictable Memory Patterns}}},
  booktitle = {2010 {{IEEE}} 16th {{International Conference}} on {{Embedded}} and {{Real}}-{{Time Computing Systems}} and {{Applications}}},
  author = {Akesson, B. and Jr, W. Hayes and Goossens, K.},
  date = {2010-08},
  pages = {367--376},
  doi = {10.1109/RTCSA.2010.35},
  abstract = {The verification complexity of real-time requirements in embedded systems grows exponentially with the number of applications, as resource sharing prevents independent verification using simulation-based approaches. Formal verification is a promising alternative, although its applicability is limited to systems with predictable hardware and software. SDRAM memories are common examples of essential hardware components with unpredictable timing behavior, typically preventing use of formal approaches. A predictable SDRAM controller has been proposed that provides guarantees on bandwidth and latency by dynamically scheduling memory patterns, which are statically computed sequences of SDRAM commands. However, the proposed patterns become increasingly inefficient as memories become faster, making them unsuitable for DDR3 SDRAM. This paper extends the memory pattern concept in two ways. Firstly, we introduce a burst count parameter that enables patterns to have multiple SDRAM bursts per bank, which is required for DDR3 memories to be used efficiently. Secondly, we present a classification of memory pattern sets into four categories based on the combination of patterns that cause worst-case bandwidth and latency to be provided. Bounds on bandwidth and latency are derived that apply to all pattern types and burst counts, as opposed to the single case covered by earlier work. Experimental results show that these extensions are required to support the most efficient pattern sets for many use-cases. We also demonstrate that the burst count parameter increases efficiency in presence of large requests and enables a wider range of real-time requirements to be satisfied.},
  eventtitle = {2010 {{IEEE}} 16th {{International Conference}} on {{Embedded}} and {{Real}}-{{Time Computing Systems}} and {{Applications}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KGXWW4BB\\Akesson et al. - 2010 - Classification and Analysis of Predictable Memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZBWX45L8\\5591843.html},
  keywords = {Bandwidth,burst count,classification,Clocks,DDR3 SDRAM,DRAM chips,dynamic scheduling,dynamically scheduling memory patterns,embedded systems,formal verification,memory controller,Memory management,memory patterns,predictability,predictable hardware,predictable memory patterns,predictable software,Real time systems,real-time requirements,SDRAM,SDRAM memories,Switches,timing,Timing,timing behavior}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004,
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  booktitle = {Proceedings of the 42nd Annual {{Southeast}} Regional Conference on   - {{ACM}}-{{SE}} 42},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  date = {2004},
  pages = {267},
  publisher = {{ACM Press}},
  location = {{Huntsville, Alabama}},
  doi = {10.1145/986537.986601},
  url = {http://portal.acm.org/citation.cfm?doid=986537.986601},
  urldate = {2019-07-24},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  eventtitle = {The 42nd Annual {{Southeast}} Regional Conference},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9A3TBW67\\Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf},
  isbn = {978-1-58113-870-2},
  langid = {english}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004a,
  title = {Performance {{Evaluation}} of {{Cache Replacement Policies}} for the {{SPEC CPU2000 Benchmark Suite}}},
  booktitle = {Proceedings of the {{42Nd Annual Southeast Regional Conference}}},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  date = {2004},
  pages = {267--272},
  publisher = {{ACM}},
  location = {{Huntsville, Alabama}},
  doi = {10.1145/986537.986601},
  url = {http://doi.acm.org/10.1145/986537.986601},
  urldate = {2019-08-05},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions.Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50\%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I66AQ9LF\\Al-Zoubi et al. - 2004 - Performance Evaluation of Cache Replacement Polici.pdf},
  isbn = {978-1-58113-870-2},
  keywords = {cache memory,performance evaluation,replacement policy},
  series = {{{ACM}}-{{SE}} 42}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004b,
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  date = {2004},
  pages = {267},
  publisher = {{ACM Press}},
  doi = {10.1145/986537.986601},
  url = {http://portal.acm.org/citation.cfm?doid=986537.986601},
  urldate = {2019-08-06},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VPWMJPZB\\Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf},
  isbn = {978-1-58113-870-2},
  langid = {english}
}

@article{albericioExploitingReuseLocality2013,
  title = {Exploiting {{Reuse Locality}} on {{Inclusive Shared Last}}-Level {{Caches}}},
  author = {Albericio, Jorge and Ibáñez, Pablo and Viñals, Víctor and Llabería, Jose María},
  date = {2013-01},
  journaltitle = {ACM Trans. Archit. Code Optim.},
  volume = {9},
  pages = {38:1--38:19},
  issn = {1544-3566},
  doi = {10.1145/2400682.2400697},
  url = {http://doi.acm.org/10.1145/2400682.2400697},
  urldate = {2019-09-12},
  abstract = {Optimization of the replacement policy used for Shared Last-Level Cache (SLLC) management in a Chip-MultiProcessor (CMP) is critical for avoiding off-chip accesses. Temporal locality, while being exploited by first levels of private cache memories, is only slightly exhibited by the stream of references arriving at the SLLC. Thus, traditional replacement algorithms based on recency are bad choices for governing SLLC replacement. Recent proposals involve SLLC replacement policies that attempt to exploit reuse either by segmenting the replacement list or improving the rereference interval prediction. On the other hand, inclusive SLLCs are commonplace in the CMP market, but the interaction between replacement policy and the enforcement of inclusion has barely been discussed. After analyzing that interaction, this article introduces two simple replacement policies exploiting reuse locality and targeting inclusive SLLCs: Least Recently Reused (LRR) and Not Recently Reused (NRR). NRR has the same implementation cost as NRU, and LRR only adds one bit per line to the LRU cost. After considering reuse locality and its interaction with the invalidations induced by inclusion, the proposals are evaluated by simulating multiprogrammed workloads in an 8-core system with two private cache levels and an SLLC. LRR outperforms LRU by 4.5\% (performing better in 97 out of 100 mixes) and NRR outperforms NRU by 4.2\% (performing better in 99 out of 100 mixes). We also show that our mechanisms outperform rereference interval prediction, a recently proposed SLLC replacement policy and that similar conclusions can be drawn by varying the associativity or the SLLC size.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G8LAASED\\Albericio et al. - 2013 - Exploiting Reuse Locality on Inclusive Shared Last.pdf},
  keywords = {Replacement policy,shared resources management},
  number = {4}
}

@inproceedings{alexanderDistributedPrefetchbufferCache1996,
  title = {Distributed Prefetch-Buffer/Cache Design for High Performance Memory Systems},
  booktitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Alexander, T. and Kedem, G.},
  date = {1996-02},
  pages = {254--263},
  doi = {10.1109/HPCA.1996.501191},
  abstract = {Microprocessor execution speeds are improving at a rate of 50\%-80\% per year while DRAM access times are improving at a much lower rate of 5\%-10\% per year. Computer systems are rapidly approaching the point at which overall system performance is determined not by the speed of the CPU but by the memory system speed. We present a high performance memory system architecture that overcomes the growing speed disparity between high performance microprocessors and current generation DRAMs. A novel prediction and prefetching technique is combined with a distributed cache architecture to build a high performance memory system. We use a table based prediction scheme with a prediction cache to prefetch data from the on-chip DRAM array to an on-chip SRAM prefetch buffer. By prefetching data we are able to hide the large latency associated with DRAM access and cycle times. Our experiments show that with a small (32 KB) prediction cache we can get an effective main memory access time that is close to the access time of larger secondary caches.},
  eventtitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DUGPXIEJ\\Alexander and Kedem - 1996 - Distributed prefetch-buffercache design for high .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2W8RQXMC\\501191.html},
  keywords = {Bandwidth,cache design,cache storage,Clocks,Computer science,Delay,distributed cache architecture,distributed prefetch buffer design,DRAM access times,Gears,Hardware,high performance memory systems,memory architecture,memory system architecture,memory system speed,Microprocessors,Prefetching,prefetching technique,Random access memory,SRAM,System performance,table based prediction scheme}
}

@inproceedings{alghazoSFLRUCacheReplacement2004,
  title = {{{SF}}-{{LRU}} Cache Replacement Algorithm},
  booktitle = {Records of the 2004 {{International Workshop}} on {{Memory Technology}}, {{Design}} and {{Testing}}, 2004.},
  author = {Alghazo, J. and Akaaboune, A. and Botros, N.},
  date = {2004-08},
  pages = {19--24},
  doi = {10.1109/MTDT.2004.1327979},
  abstract = {In this paper we propose a replacement algorithm, SF-LRU (second chance-frequency - least recently used) that combines the LRU (least recently used) and the LFU (least frequently used) using the second chance concept. A comprehensive comparison is made between our algorithm and both LRU and LFU algorithms. Experimental results show that the SF-LRU significantly reduces the number of cache misses compared the other two algorithms. Simulation results show that our algorithm can provide a maximum value of approximately 6.3\% improvement in the miss ratio over the LRU algorithm in data cache and approximately 9.3\% improvement in miss ratio in instruction cache. This performance improvement is attributed to the fact that our algorithm provides a second chance to the block that may be deleted according to LRU's rules. This is done by comparing the frequency of the block with the block next to it in the set.},
  eventtitle = {Records of the 2004 {{International Workshop}} on {{Memory Technology}}, {{Design}} and {{Testing}}, 2004.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QAY5LIZV\\Alghazo et al. - 2004 - SF-LRU cache replacement algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HU2EYP6D\\1327979.html},
  keywords = {Application software,Bridges,Cache memory,cache misses reduction,cache replacement algorithm,cache storage,Clocks,Costs,data cache,Delay,Energy consumption,Frequency,History,instruction cache,LFU,low power cache,low-power electronics,LRU rules,miss ratio improvement,performance improvement,second chance concept,second chance-frequency - least recently used,SF-LRU,System performance}
}

@inproceedings{altmanNovelMethodologyUsing1993,
  title = {A {{Novel Methodology}} Using {{Genetic Algorithms}} for the {{Design}} of {{Caches}} and {{Cache Replacement Policy}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Genetic Algorithms}}},
  author = {Altman, Erik R and Agarwal, Vinod K and Gao, Guang R},
  date = {1993},
  pages = {392--399},
  publisher = {{Morgan Kaufmann Publishers}},
  eventtitle = {5th {{International Conference}} on {{Genetic Algorithms}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\339B4AMH\\Altman et al. - A Novel Methodology using Genetic Algorithms for t.pdf},
  isbn = {1-55860-299-2},
  langid = {english}
}

@article{altmeyerStaticProbabilisticTiming2015,
  title = {Static Probabilistic Timing Analysis for Real-Time Systems Using Random Replacement Caches},
  author = {Altmeyer, Sebastian and Cucu-Grosjean, Liliana and Davis, Robert I.},
  date = {2015-01-01},
  journaltitle = {Real-Time Systems},
  shortjournal = {Real-Time Syst},
  volume = {51},
  pages = {77--123},
  issn = {1573-1383},
  doi = {10.1007/s11241-014-9218-4},
  url = {https://doi.org/10.1007/s11241-014-9218-4},
  urldate = {2019-08-05},
  abstract = {In this paper, we investigate static probabilistic timing analysis (SPTA) for single processor real-time systems that use a cache with an evict-on-miss random replacement policy. We show that previously published formulae for the probability of a cache hit can produce results that are optimistic and unsound when used to compute probabilistic worst-case execution time (pWCET) distributions. We investigate the correctness, optimality, and precision of different approaches to SPTA for random replacement caches. We prove that one of the previously published formulae for the probability of a cache hit is optimal with respect to the limited information (reuse distance and cache associativity) that it uses. We derive an alternative formulation that makes use of additional information in the form of the number of distinct memory blocks accessed (the stack distance). This provides a complementary lower bound that can be used together with previously published formula to obtain more accurate analysis. We improve upon this joint approach by using extra information about cache contention. To investigate the precision of various approaches to SPTA, we introduce a simple exhaustive method that computes a precise pWCET distribution, albeit at the cost of exponential complexity. We integrate this precise approach, applied to small numbers of frequently accessed memory blocks, with imprecise analysis of other memory blocks, to form a combined approach that improves precision, without significantly increasing complexity. The performance of the various approaches are compared on benchmark programs. We also make comparisons against deterministic analysis of the least recently used replacement policy.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7B73XNLS\\Altmeyer et al. - 2015 - Static probabilistic timing analysis for real-time.pdf},
  keywords = {Random cache replacement,Static probabilistic timing analysis,Timing verification,WCET analysis},
  langid = {english},
  number = {1}
}

@inproceedings{anandkumarHybridCacheReplacement2014,
  title = {A Hybrid Cache Replacement Policy for Heterogeneous Multi-Cores},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  author = {AnandKumar, K. M. and S, A. and Ganesh, D. and Christy, M. S.},
  date = {2014-09},
  pages = {594--599},
  doi = {10.1109/ICACCI.2014.6968209},
  abstract = {Future generation computer architectures are endeavoring to achieve high performance without compromise on energy efficiency. In a multiprocessor system, cache miss degrades the performance as the miss penalty scales by an exponential factor across a shared memory system when compared to general purpose processors. This instigates the need for an efficient cache replacement scheme to cater to the data needs of underlying functional units in case of a cache miss. Minimal cache miss improves resource utilization and reduces data movement across the core which in turn contributes to a high performance and lesser power dissipation. Existing replacement policies has several issues when implemented in a heterogeneous multi-core system. The commonly used LRU replacement policy does not offer optimal performance for applications with high dependencies. Motivated by the limitations of the existing algorithms, we propose a hybrid cache replacement policy which combines Least Recently Used (LRU) and Least Frequently Used (LFU) replacement policies. Each cache block has two weighing values corresponding to LRU and LFU policies and a cumulative weight is calculated using these two values. Conducting simulations over wide range of cache sizes and associativity, we show that our proposed approach has shown increased cache hit to miss ratio when compared with LRU and other conventional cache replacement policies.},
  eventtitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JQI5WCDH\\AnandKumar et al. - 2014 - A hybrid cache replacement policy for heterogeneou.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\T8CJAVC8\\6968209.html},
  keywords = {cache miss,Cache Replacement,cache storage,data movement,energy efficiency,future generation computer architecture,heterogeneous multicore system,heterogeneous multicores,hybrid cache replacement policy,least frequently used replacement policy,least recently used replacement policy,LFU policy,Libraries,LRU replacement policy,multi-core,multiprocessor system,performance evaluation,power dissipation,resource allocation,resource utilization,shared memory system,shared memory systems,Time-frequency analysis,weighing values}
}

@inproceedings{ariACMEAdaptiveCaching2002,
  title = {{{ACME}}: {{Adaptive Caching Using Multiple Experts}}},
  shorttitle = {{{ACME}}},
  booktitle = {Distributed {{Data}} \& {{Structures}} 4, {{Records}} of the 4th {{International Meeting}} ({{WDAS}} 2002), {{Paris}}, {{France}}, {{March}} 20-23, 2002},
  author = {Ari, Ismail and Amer, Ahmed and Gramacy, Robert B. and Miller, Ethan L. and Brandt, Scott A. and Long, Darrell D. E.},
  editor = {Litwin, Witold and Lévy, Gérard},
  date = {2002},
  volume = {14},
  pages = {143--158},
  publisher = {{Carleton Scientific}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LYINEFII\\Ari et al. - ACME Adaptive Caching Using Multiple Experts.pdf},
  series = {Proceedings in {{Informatics}}}
}

@article{arlittEvaluatingContentManagement2000,
  title = {Evaluating Content Management Techniques for {{Web}} Proxy Caches},
  author = {Arlitt, Martin and Cherkasova, Ludmila and Dilley, John and Friedrich, Rich and Jin, Tai},
  date = {2000-03-01},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  volume = {27},
  pages = {3--11},
  issn = {01635999},
  doi = {10.1145/346000.346003},
  url = {http://portal.acm.org/citation.cfm?doid=346000.346003},
  urldate = {2019-11-28},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W34D6IUK\\Arlitt et al. - 2000 - Evaluating content management techniques for Web p.pdf},
  langid = {english},
  number = {4}
}

@inproceedings{armejachTidyCacheImproving2015,
  title = {Tidy {{Cache}}: {{Improving Data Placement}} in {{Die}}-{{Stacked DRAM Caches}}},
  shorttitle = {Tidy {{Cache}}},
  booktitle = {2015 27th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  author = {Armejach, A. and Cristal, A. and Unsal, O. S.},
  date = {2015-10},
  pages = {65--73},
  doi = {10.1109/SBAC-PAD.2015.23},
  abstract = {Die-stacked DRAM caches are likely to become available in mainstream chips in the near future. DRAM caches are typically used as a last level shared cache behind the traditional hierarchy of on-chip SRAM caches. However, its internal organization differs from traditional caches as it is based on DRAM technology that provides significantly diverse access latencies depending on the state of its internal structures. Accesses that hit in the row-buffer require only one DRAM command and are significantly faster than those that require closing the row-buffer to load a new row to read from. Prior work has focused on maximizing row-buffer locality while maintaining high cache hit ratios. However, past designs do not consider performance problems that may arise due to interleaved accesses from different applications that compete for the shared DRAM resources, nor the different access patterns and locality characteristics that each of these applications may have. In this paper, we first identify performance pathologies that are specific to DRAM caches which arise due to the interference caused by interleaved accesses from multiple cores. We then propose Tidy Cache, a novel DRAM cache design that is able to ameliorate these performance pathologies by dynamically adapting the replacement policy for demanded data. Our performance evaluation results show that our design outperforms the state-of-the-art by 9.2\% for multi-programmed SPEC workloads and by 16.7\% for a set of TPC-H queries, mainly due to significantly better cache miss ratios.},
  eventtitle = {2015 27th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UFPC3JNF\\Armejach et al. - 2015 - Tidy Cache Improving Data Placement in Die-Stacke.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5ZEB2AQ8\\7379835.html},
  keywords = {3D stacking,access latencies,Bandwidth,cache,cache miss ratios,cache storage,data placement improvement,data replacement policy,die-stacked DRAM caches,DRAM,DRAM cache design,DRAM chips,DRAM command,Interference,interleaved access,internal structures,multiprogrammed SPEC workloads,on-chip SRAM caches,Organizations,Pathology,performance evaluation,Proposals,Random access memory,row-buffer,shared DRAM resources,SRAM chips,System-on-chip,Tidy Cache,TPC-H queries}
}

@inproceedings{aroraCompositeDataPrefetcher2014,
  title = {A Composite Data {{Prefetcher}} Framework for Multilevel Caches},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  author = {Arora, H. and Banerjee, S. and Davina, V.},
  date = {2014-09},
  pages = {1827--1833},
  doi = {10.1109/ICACCI.2014.6968442},
  abstract = {The increasing difference between the Processor speed and the DRAM performance have led to the assertive need to hide memory latency and reduce memory access time. It is noticed that the Processor remains stalled on memory references. Data Prefetching is a technique that fetches that next instruction's data parallel to the current instruction execution in a typical Processor-Cache-DRAM system. A Prefetcher anticipates a cache miss that might take place in the next instruction and fetches the data before the actual memory reference. The goal of prefetching is to reduce as many cache misses as possible. In this paper we present a detailed summary of the different prefetching techniques, and implement a composite prefetcher prototype that employs the techniques of Sequential, Stride and Distance Prefetching.},
  eventtitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HHVXUMNB\\Arora et al. - 2014 - A composite data Prefetcher framework for multilev.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZQXL5GK\\6968442.html},
  keywords = {Arbitrary Stride Prefetching,Average Memory Access Time,cache miss,cache storage,composite data prefetcher framework,data prefetching,distance prefetching,Distance Prefetching,DRAM chips,DRAM performance,dynamic random access memory,Dynamic Read Only Memory,Educational institutions,Global History Buffer,Hardware,History,Markov processes,memory access time,memory latency,multilevel cache,Prefetching,processor speed,processor-cache-DRAM system,Random access memory,sequential prefetching,storage management,stride prefetching}
}

@article{backaschRuntimeVerificationMulticore2013,
  title = {Runtime {{Verification}} for {{Multicore SoC}} with {{High}}-Quality {{Trace Data}}},
  author = {Backasch, Rico and Hochberger, Christian and Weiss, Alexander and Leucker, Martin and Lasslop, Richard},
  date = {2013-04},
  journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {18},
  pages = {18:1--18:26},
  issn = {1084-4309},
  doi = {10.1145/2442087.2442089},
  url = {http://doi.acm.org/10.1145/2442087.2442089},
  urldate = {2017-03-07},
  abstract = {Multicore System-on-Chip (SoC) implementations of embedded systems are becoming very popular. In these systems it is possible to spread out computations over many cores. On one hand this leads to better energy efficiency if clock frequencies and core voltages are reduced. On the other hand this delivers very high performance to the software developer and thus enables complex software systems to be implemented. Unfortunately, debugging and validation of these systems becomes extremely difficult. Various technological approaches try to solve this dilemma. In this contribution we will show a new approach to observe multi-core SoCs and make their internal operations visible to external analysis tools. Also, we show that runtime verification can be employed to analyze and validate these internal operations while the system operates in its normal environment. The combination of these two approaches delivers unprecedented options to the developer to understand and verify system behavior even in complex multicore SoCs.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DFIGAC8K\\Backasch et al. - 2013 - Runtime Verification for Multicore SoC with High-q.pdf},
  keywords = {Embedded system,Multicore SoC,runtime verification,synchronisation,test driven development,trace data},
  number = {2}
}

@inproceedings{bansalCARClockAdaptive2004,
  title = {{{CAR}}: {{Clock}} with {{Adaptive Replacement}}},
  shorttitle = {{{CAR}}},
  booktitle = {Proceedings of the 3rd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  author = {Bansal, Sorav and Modha, Dharmendra S.},
  date = {2004},
  pages = {187--200},
  publisher = {{USENIX Association}},
  location = {{San Francisco, CA}},
  url = {http://dl.acm.org/citation.cfm?id=1096673.1096699},
  urldate = {2019-09-16},
  abstract = {CLOCK is a classical cache replacement policy dating back to 1968 that was proposed as a low-complexity approximation to LRU. On every cache hit, the policy LRU needs to move the accessed item to the most recently used position, at which point, to ensure consistency and correctness, it serializes cache hits behind a single global lock. CLOCK eliminates this lock contention, and, hence, can support high concurrency and high throughput environments such as virtual memory (for example, Multics, UNIX, BSD, AIX) and databases (for example, DB2). Unfortunately, CLOCK is still plagued by disadvantages of LRU such as disregard for "frequency", susceptibility to scans, and low performance.As our main contribution, we propose a simple and elegant new algorithm, namely, CLOCK with Adaptive Replacement (CAR), that has several advantages over CLOCK: (i) it is scan-resistant; (ii) it is self-tuning and it adaptively and dynamically captures the "recency" and "frequency" features of a workload; (iii) it uses essentially the same primitives as CLOCK, and, hence, is low-complexity and amenable to a high-concurrency implementation; and (iv) it outperforms CLOCK across a wide-range of cache sizes and workloads. The algorithm CAR is inspired by the Adaptive Replacement Cache (ARC) algorithm, and inherits virtually all advantages of ARC including its high performance, but does not serialize cache hits behind a single global lock. As our second contribution, we introduce another novel algorithm, namely, CAR with Temporal filtering (CART), that has all the advantages of CAR, but, in addition, uses a certain temporal filter to distill pages with long-term utility from those with only short-term utility.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9UNXPD7I\\Bansal and Modha - CAR Clock with Adaptive Replacement.pdf},
  series = {{{FAST}} '04}
}

@article{beladyAnomalySpacetimeCharacteristics1969,
  title = {An Anomaly in Space-Time Characteristics of Certain Programs Running in a Paging Machine},
  author = {Belady, L. A. and Nelson, R. A. and Shedler, G. S.},
  date = {1969-06-01},
  journaltitle = {Communications of the ACM},
  volume = {12},
  pages = {349--353},
  issn = {00010782},
  doi = {10.1145/363011.363155},
  url = {http://portal.acm.org/citation.cfm?doid=363011.363155},
  urldate = {2020-01-05},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3LYVC4U5\\Belady et al. - 1969 - An anomaly in space-time characteristics of certai.pdf},
  number = {6}
}

@article{beladyStudyReplacementAlgorithms1966,
  title = {A Study of Replacement Algorithms for a Virtual-Storage Computer},
  author = {Belady, L. A.},
  date = {1966},
  journaltitle = {IBM Systems Journal},
  volume = {5},
  pages = {78--101},
  issn = {0018-8670},
  doi = {10.1147/sj.52.0078},
  abstract = {One of the basic limitations of a digital computer is the size of its available memory.1In most cases, it is neither feasible nor economical for a user to insist that every problem program fit into memory. The number of words of information in a program often exceeds the number of cells (i.e., word locations) in memory. The only way to solve this problem is to assign more than one program word to a cell. Since a cell can hold only one word at a time, extra words assigned to the cell must be held in external storage. Conventionally, overlay techniques are employed to exchange memory words and external-storage words whenever needed; this, of course, places an additional planning and coding burden on the programmer. For several reasons, it would be advantageous to rid the programmer of this function by providing him with a “virtual” memory larger than his program. An approach that permits him to use a sufficiently large address range can accomplish this objective, assuming that means are provided for automatic execution of the memory-overlay functions.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YP53KECD\\Belady - 1966 - A study of replacement algorithms for a virtual-st.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EGMDU6VS\\5388441.html},
  number = {2}
}

@inproceedings{bieniaPARSECBenchmarkSuite2008,
  title = {The {{PARSEC}} Benchmark Suite: Characterization and Architectural Implications},
  shorttitle = {The {{PARSEC}} Benchmark Suite},
  booktitle = {Proceedings of the 17th International Conference on {{Parallel}} Architectures and Compilation Techniques - {{PACT}} '08},
  author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
  date = {2008},
  pages = {72},
  publisher = {{ACM Press}},
  location = {{Toronto, Ontario, Canada}},
  doi = {10.1145/1454115.1454128},
  url = {http://portal.acm.org/citation.cfm?doid=1454115.1454128},
  urldate = {2019-07-25},
  abstract = {This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on highperformance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic. The benchmark suite has been made available to the public.},
  eventtitle = {The 17th International Conference},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\77XP977J\\Bienia et al. - 2008 - The PARSEC benchmark suite characterization and a.pdf},
  isbn = {978-1-60558-282-5},
  langid = {english}
}

@article{bolandPredictingPrecludingProblems1994,
  title = {Predicting and Precluding Problems with Memory Latency},
  author = {Boland, K. and Dollas, A.},
  date = {1994-08},
  journaltitle = {IEEE Micro},
  volume = {14},
  pages = {59--67},
  issn = {0272-1732},
  doi = {10.1109/40.296166},
  abstract = {By examining the rate at which successive generations of processor and DRAM cycle times have been diverging over time, we can track the latency problem of computer memory systems. Our research survey starts with the fundamentals of single-level caches and moves to the need for multilevel cache hierarchies. We look at some of the current techniques for boosting cache performance, especially compiler-based methods for code restructuring and instruction and data prefetching. These two areas will likely yield improvements for a much larger domain of applications in the future.{$<>$}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3R5U6GDQ\\Boland and Dollas - 1994 - Predicting and precluding problems with memory lat.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JA6WJ3MG\\296166.html},
  keywords = {Application software,Boosting,buffer storage,Clocks,code restructuring,compiler-based methods,Content addressable storage,data prefetching,Delay,DRAM chips,DRAM cycle times,instruction prefetching,memory architecture,memory latency,Microcomputers,Microprocessors,multilevel cache hierarchies,Prefetching,Random access memory,single-level caches,Throughput},
  number = {4}
}

@inproceedings{caoPostSiliconTraceAnalysis2017,
  title = {A {{Post}}-{{Silicon Trace Analysis Approach}} for {{System}}-on-{{Chip Protocol Debug}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Cao, Y. and Zheng, H. and Palombo, H. and Ray, S. and Yang, J.},
  date = {2017-11},
  pages = {177--184},
  doi = {10.1109/ICCD.2017.35},
  abstract = {Reconstructing system-level behavior from silicon traces is a critical problem in post-silicon validation of System-on-Chip designs. Current industrial practice in this area is primarily manual, depending on collaborative insights of the architects, designers, and validators. This paper presents a trace analysis approach that exploits architectural models of system-level protocols to reconstruct design behavior from partially observed silicon traces in the presence of ambiguous and noisy data. The output of the approach is a set of all potential interpretations of a system's internal execution abstracted to system-level protocols. To support the trace analysis approach, a companion trace signal selection framework guided by system-level protocols is also presented, and its impacts on the complexity and accuracy of the analysis approach are discussed. That approach and the framework have been evaluated on a multi-core System-on-Chip prototype that implements a set of common industrial system-level protocols.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Design}} ({{ICCD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CC4BEFJK\\Cao et al. - 2017 - A Post-Silicon Trace Analysis Approach for System-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HEQE2UEE\\8119208.html},
  keywords = {companion trace signal selection framework,Computer bugs,computer debugging,formal verification,industrial system-level protocols,integrated circuit design,integrated circuit testing,IP networks,Observability,partially observed silicon traces,post-silicon trace analysis approach,post-silicon validation,protocols,Protocols,reconstructing system-level behavior,silicon,Silicon,software architecture,system-on-chip,System-on-chip,system-on-chip designs,system-on-chip protocol debug,system-on-chip prototype,Universal Serial Bus}
}

@inproceedings{carterImpulseBuildingSmarter1999,
  title = {Impulse: Building a Smarter Memory Controller},
  shorttitle = {Impulse},
  booktitle = {Proceedings {{Fifth International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Carter, J. and Hsieh, W. and Stoller, L. and Swanson, M. and {Lixin Zhang} and Brunvand, E. and Davis, A. and {Chen-Chi Kuo} and Kuramkote, R. and Parker, M. and Schaelicke, L. and Tateyama, T.},
  date = {1999-01},
  pages = {70--79},
  doi = {10.1109/HPCA.1999.744334},
  abstract = {Impulse is a new memory system architecture that adds two important features to a traditional memory controller. First, Impulse supports application-specific optimizations through configurable physical address remapping. By remapping physical addresses, applications control how their data is accessed and cached, improving their cache and bus utilization. Second, Impulse supports prefetching at the memory controller, which can hide much of the latency of DRAM accesses. In this paper we describe the design of the Impulse architecture, and show how an Impulse memory system can be used to improve the performance of memory-bound programs. For the NAS conjugate gradient benchmark, Impulse improves performance by 67\%. Because it requires no modification to processor, cache, or bus designs, Impulse can be adopted in conventional systems. In addition to scientific applications, we expect that Impulse will benefit regularly strided memory-bound applications of commercial importance, such as database and multimedia programs.},
  eventtitle = {Proceedings {{Fifth International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YBBYXM6M\\Carter et al. - 1999 - Impulse building a smarter memory controller.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8HLH9ZUF\\744334.html},
  keywords = {application-specific optimization,Bandwidth,bus design,cache design,cache storage,Cities and towns,Computer science,configurable physical address remapping,conjugate gradient methods,data access,data caching,database management systems,database programs,Databases,Delay,DRAM access latency hiding,Electronic switching systems,Impulse memory system architecture,memory architecture,memory controller,memory-bound program performance,Microprocessors,multimedia computing,multimedia programs,NAS conjugate gradient benchmark,performance,prefetching,Prefetching,processor design,Random access memory,scientific applications,Sparse matrices}
}

@inproceedings{chamskiTracebasedRuntimeAnalysis2010,
  title = {Trace-Based Runtime Analysis of Embedded Real-Time Systems},
  booktitle = {Proceedings of the 17th {{International Conference Mixed Design}} of {{Integrated Circuits}} and {{Systems}} - {{MIXDES}} 2010},
  author = {Chamski, Z. and Borzęcki, M. and Świercz, B.},
  date = {2010-06},
  pages = {117--120},
  abstract = {Execution tracing is one of the key techniques for analyzing and validating the operation of embedded products. After reviewing several approaches to the runtime behavior analysis of embedded systems, we present the experience gained in developing a range of high-bandwidth communications devices combining multiple wireless and wired link technologies. In particular, all cases studies are based on actual product development.},
  eventtitle = {Proceedings of the 17th {{International Conference Mixed Design}} of {{Integrated Circuits}} and {{Systems}} - {{MIXDES}} 2010},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9B7DX9RL\\Chamski et al. - 2010 - Trace-based runtime analysis of embedded real-time.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RBTRBMBM\\5551285.html},
  keywords = {code optimization,embedded devices,embedded product,embedded real time system,embedded systems,execution tracing,Hardware,high bandwidth communication device,Kernel,Linux,monitoring,Monitoring,multiple wireless technology,Probes,product development,profiling,Program processors,real time,system monitoring,systems analysis,telecommunication links,trace based runtime analysis,tracing,wired link technology}
}

@inproceedings{changAdaptiveBufferCache2016,
  title = {An {{Adaptive Buffer Cache Management Scheme}}},
  booktitle = {2016 {{International Computer Symposium}} ({{ICS}})},
  author = {Chang, H. and Chiang, C. and Yu, Y.},
  date = {2016-12},
  pages = {124--127},
  doi = {10.1109/ICS.2016.0033},
  abstract = {Previous cache replacement algorithms utilize the access history information to make replacement decisions. However, they fail to deliver utmost performance since the history information exploited is incomplete. Motivated by the limitations of existing algorithms, this paper proposes a novel replacement scheme, called the Pattern-assisted Adaptive Recency Caching (PARC). PARC simultaneously utilizes the history information of recency, frequency, and access patterns to estimate the locality strength and to select the victim block. Specifically, PARC exploits the reference regularities exhibited in past behaviors, including looping or sequential references, to actively and rapidly adapt the recency and frequency information of blocks so as to exactly distill blocks with long-term utility from those with only short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache replacement algorithm is similar, PARC always outperforms the least recently used (LRU) scheme and other existing cache replacement algorithms.},
  eventtitle = {2016 {{International Computer Symposium}} ({{ICS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4SBWTBUH\\Chang et al. - 2016 - An Adaptive Buffer Cache Management Scheme.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PL67HAC7\\7858455.html},
  keywords = {adaptive buffer cache management scheme,buffer cache,cache replacement algorithm,cache replacement algorithms,cache storage,Classification algorithms,Disk drives,file systems,History,memory management,Optimized production technology,page cache,pattern-assisted adaptive recency caching,Prediction algorithms,Radiation detectors,random workloads,replacement algorithms,Robustness}
}

@report{changLRUWWWProxy1999,
  title = {The {{LRU}}*{{WWW}} Proxy Cache Document Replacement Algorithm},
  author = {Chang, Chung-yi and McGregor, Anthony James and Holmes, Geoffrey},
  date = {1999},
  url = {https://hdl.handle.net/10289/1038},
  abstract = {Obtaining good performance from WWW proxy caches is critically dependent on the document replacement policy used by the proxy. This paper validates the work of other authors by reproducing their studies of proxy cache document replacement algorithms. From this basis a cross-trace study is mounted. This demonstrates that the performance of most document replacement algorithms is dependent on the type of workload that they are presented with. Finally we propose a new algorithm, LRU*, that consistently performs well across all our traces.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DZRX9ZZZ\\uow-cs-wp-1999-09.pdf},
  keywords = {caching,document replacement algorithm,Machine learning,WWW},
  langid = {english},
  note = {99/09},
  series = {Computer {{Science Working Papers}}},
  type = {Working Paper}
}

@article{changPARCNovelOS2018,
  title = {{{PARC}}: {{A}} Novel {{OS}} Cache Manager},
  shorttitle = {{{PARC}}},
  author = {Chang, Hsung-Pin and Chiang, Cheng-Pang},
  date = {2018},
  journaltitle = {Software: Practice and Experience},
  volume = {48},
  pages = {2193--2222},
  issn = {1097-024X},
  doi = {10.1002/spe.2633},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2633},
  urldate = {2019-01-30},
  abstract = {To boost input-output performance, operating systems employ a kernel-managed caching space called the buffer cache or page cache. Given the limited size of a buffer cache, an effective cache manager is required to decide which blocks should be evicted from the cache. Previous cache managers use historical information to make replacement decisions. However, existing approaches are unable to maximize performance since they rely on limited historical information. Motivated by the limitations of existing solutions, this paper proposes a novel manager called the Pattern-assisted Adaptive Recency Caching (PARC) manager. PARC simultaneously uses the historical information of recency, frequency, and access patterns to estimate the locality strengths of blocks and, upon a cache miss, evicts the block with the least strength. Specifically, PARC exploits the reference regularities exhibited in past input-output behaviors to actively and rapidly adapt the recency and frequency information of blocks so as to precisely distinguish blocks with long- and short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache manager is similar, PARC always outperforms existing solutions.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6AILZJZF\\Chang and Chiang - 2018 - PARC A novel OS cache manager.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7ZJDPIJI\\spe.html},
  keywords = {buffer cache,cache manager,operating systems,page cache,replacement algorithms},
  langid = {english},
  number = {12}
}

@inproceedings{changReevaluatingLatencyClaims2013,
  title = {Reevaluating the Latency Claims of {{3D}} Stacked Memories},
  booktitle = {2013 18th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP}}-{{DAC}})},
  author = {Chang, D. W. and Byun, G. and Kim, H. and Ahn, M. and Ryu, S. and Kim, N. S. and Schulte, M.},
  date = {2013-01},
  pages = {657--662},
  doi = {10.1109/ASPDAC.2013.6509675},
  abstract = {In recent years, 3D technology has been a popular area of study that has allowed researchers to explore a number of novel computer architectures. One of the more popular topics is that of integrating 3D main memory dies below the computing die and connecting them with through-silicon vias (TSVs). This is assumed to reduce off-chip main memory access latencies by roughly 45\% to 60\%. Our detailed circuit-level models, however, demonstrate that this latency reduction from the TSVs is significantly less. In this paper, we present these models, compare 2D and 3D main memory latencies, and show that the reduction in latency from using 3D main memory to be no more than 2.4 ns. We also show that although the wider I/O bus width enabled by using TSVs increases performance, it may do so with an increase in power consumption. Although TSVs consume less power per bit transfer than off-chip metal interconnects (11.2 times less power per bit transfer), TSVs typically use considerably more bits and may result in a net increase in power due to the large number of bits in the memory I/O bus. Our analysis shows that although a 3D memory hierarchy exploiting a wider memory bus can increase performance, this performance increase may not justify the net increase in power consumption.},
  eventtitle = {2013 18th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP}}-{{DAC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2F9MCVTP\\Chang et al. - 2013 - Reevaluating the latency claims of 3D stacked memo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3VDJGLZ8\\6509675.html},
  keywords = {3D stacked memories,Benchmark testing,computer architectures,DRAM chips,Integrated circuit interconnections,latency claims,Memory management,power consumption,Random access memory,Solid modeling,Three-dimensional displays,three-dimensional integrated circuits,through-silicon vias,Through-silicon vias}
}

@inproceedings{chatterjeeManagingDRAMLatency2014,
  title = {Managing {{DRAM Latency Divergence}} in {{Irregular GPGPU Applications}}},
  booktitle = {{{SC}} '14: {{Proceedings}} of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Chatterjee, N. and O'Connor, M. and Loh, G. H. and Jayasena, N. and Balasubramonia, R.},
  date = {2014-11},
  pages = {128--139},
  doi = {10.1109/SC.2014.16},
  abstract = {Memory controllers in modern GPUs aggressively reorder requests for high bandwidth usage, often interleaving requests from different warps. This leads to high variance in the latency of different requests issued by the threads of a warp. Since a warp in a SIMT architecture can proceed only when all of its memory requests are returned by memory, such latency divergence causes significant slowdown when running irregular GPGPU applications. To solve this issue, we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps. We further reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels. Finally we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization. Our combined scheme yields a 10.1\% performance improvement for irregular GPGPU workloads relative to a throughput-optimized GPU memory controller.},
  eventtitle = {{{SC}} '14: {{Proceedings}} of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\576J8EEP\\Chatterjee et al. - 2014 - Managing DRAM Latency Divergence in Irregular GPGP.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NUT6FVPY\\7012998.html},
  keywords = {average memory stall latency,Bandwidth,bandwidth utilization,DRAM chips,DRAM system,graphics processing units,Graphics processing units,high bandwidth usage,independent memory channels,Instruction sets,interleaving requests,interwarp interference,irregular GPGPU applications,latency divergence,memory controllers,Memory management,memory requests,memory scheduling mechanisms,memory scheduling policy,Parallel processing,Random access memory,scheduling decisions,SIMT architecture,storage management chips,throughput-optimized GPU memory controller}
}

@inproceedings{chaudhuriPseudoLIFOFoundationNew2009a,
  title = {Pseudo-{{LIFO}}: The Foundation of a New Family of Replacement Policies for Last-Level Caches},
  shorttitle = {Pseudo-{{LIFO}}},
  author = {Chaudhuri, Mainak},
  date = {2009},
  pages = {401},
  publisher = {{ACM Press}},
  doi = {10.1145/1669112.1669164},
  url = {http://portal.acm.org/citation.cfm?doid=1669112.1669164},
  urldate = {2020-03-01},
  abstract = {Cache blocks often exhibit a small number of uses during their life time in the last-level cache. Past research has exploited this property in two different ways. First, replacement policies have been designed to evict dead blocks early and retain the potentially live blocks. Second, dynamic insertion policies attempt to victimize single-use blocks (dead on fill) as early as possible, thereby leaving most of the working set undisturbed in the cache. However, we observe that as the last-level cache grows in capacity and associativity, the traditional dead block prediction-based replacement policy loses effectiveness because often the LRU block itself is dead leading to an LRU replacement decision. The benefit of dynamic insertion policies is also small in a large class of applications that exhibit a significant number of cache blocks with small, yet more than one, uses.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3TI5CXEM\\Chaudhuri - 2009 - Pseudo-LIFO the foundation of a new family of rep.pdf},
  isbn = {978-1-60558-798-1},
  langid = {english}
}

@inproceedings{chegeniDesignHighSpeed2007,
  title = {Design of a {{High Speed}}, {{Low Latency}} and {{Low Power Consumption DRAM Using}} Two-Transistor {{Cell Structure}}},
  booktitle = {2007 14th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  author = {Chegeni, A. and Hadidi, K. and Khoei, A.},
  date = {2007-12},
  pages = {1167--1170},
  doi = {10.1109/ICECS.2007.4511203},
  abstract = {This paper presents a new structure of DRAM, using two-transistor cell. The most important advantages of this structure are: a) High speed read, write and refresh operation b) low data access latency c) low power consumption compared to other structures d) each write/refresh operation can be carried out just in one cycle and e) no need to special process and compatible with standard digital process.},
  eventtitle = {2007 14th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7EYIFN9L\\Chegeni et al. - 2007 - Design of a High Speed, Low Latency and Low Power .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DZNH26XH\\4511203.html},
  keywords = {Capacitance,Circuits,data access latency,Delay,Distributed power generation,DRAM,DRAM chips,Energy consumption,Laboratories,Microelectronics,Microprocessors,power consumption,Random access memory,two-transistor cell structure,Voltage,write-refresh operation}
}

@inproceedings{chenMALRUMisspenaltyAware2017,
  title = {{{MALRU}}: {{Miss}}-Penalty Aware {{LRU}}-Based Cache Replacement for Hybrid Memory Systems},
  shorttitle = {{{MALRU}}},
  booktitle = {Design, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}}), 2017},
  author = {Chen, D. and Jin, H. and Liao, X. and Liu, H. and Guo, R. and Liu, D.},
  date = {2017-03},
  pages = {1086--1091},
  doi = {10.23919/DATE.2017.7927151},
  abstract = {Current DRAM based memory systems face the scalability challenges in terms of storage density, power, and cost. Hybrid memory architecture composed of emerging Non-Volatile Memory (NVM) and DRAM is a promising approach to large-capacity and energy-efficient main memory. However, hybrid memory systems pose a new challenge to on-chip cache management due to the asymmetrical penalty of memory access to DRAM and NVM in case of cache misses. Cache hit rate is no longer an effective metric for evaluating memory access performance in hybrid memory systems. Current cache replacement policies that aim to improve cache hit rate are not efficient either. In this paper, we take into account the asymmetry of cache miss penalty on DRAM and NVM, and advocate a more general metric, Average Memory Access Time (AMAT), to evaluate the performance of hybrid memories. We propose a miss penalty-aware LRU-based (MALRU) cache replacement policy for hybrid memory systems. MALRU is aware of the source (DRAM or NVM) of missing blocks and prevents high-latency NVM blocks as well as low-latency DRAM blocks with good temporal locality from being evicted. Experimental results show that MALRU improves system performance against LRU and the state-of-the-art HAP policy by up to 20.4\% and 11.7\% (11.1\% and 5.7\% on average), respectively.},
  eventtitle = {Design, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}}), 2017},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LBGSBKZV\\Chen et al. - 2017 - MALRU Miss-penalty aware LRU-based cache replacem.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VVAR3IGB\\7927151.html},
  keywords = {AMAT metric,average memory access time metric,Benchmark testing,cache miss penalty asymmetry,cache storage,DRAM based memory systems,DRAM chips,hybrid memory architecture,hybrid memory systems,Hybrid power systems,large-capacity energy-efficient main-memory,Layout,MALRU cache replacement policy,Mathematical model,Measurement,memory architecture,miss-penalty aware LRU-based cache replacement,nonvolatile memory,Nonvolatile memory,NVM,on-chip cache management,performance evaluation,Random access memory,temporal locality}
}

@inproceedings{chenSSARCShortSightedAdaptive2009,
  title = {{{SSARC}}: {{The Short}}-{{Sighted Adaptive Replacement Cache}}},
  shorttitle = {{{SSARC}}},
  booktitle = {2009 11th {{IEEE International Conference}} on {{High Performance Computing}} and {{Communications}}},
  author = {Chen, Z. and Xiao, N. and Liu, F. and Zhao, Y.},
  date = {2009-06},
  pages = {551--556},
  doi = {10.1109/HPCC.2009.82},
  abstract = {As the performance gap between disks and processors continues to increase, dozens of cache replacement policies come up to handle the problem. Unfortunately, most of the policies are static. Nimrod Megiddo etc put forward a low overhead adaptive policy called ARC. It outperforms most of the static policies in most situations. But, ARC adapts itself to the workloads by the feedback of the missed pages. It hasn 't carried out the adaption before missed pages are discovered. We propose a high performance adaptive replacement policy. It adapts itself to the workloads by the feedback of the hit pages, so, it is more sensitive to the changes of the workloads than ARC. As the policy stares at the tails of the queues regardless of other pages, we name the policy as short-sighted adaptive replacement policy. The ARC usually regrets for the missed pages and wishes to rescue the neighborhood of them. However, SSARC endeavors to protect the would-be-reused pages from being replaced aggressively. So, it outperforms ARC in most situations. We compared SSARC with LRU, 2Q and ARC. The trace-driven experiments represent that SSARC gains higher performance.},
  eventtitle = {2009 11th {{IEEE International Conference}} on {{High Performance Computing}} and {{Communications}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NVRT2KCP\\Chen et al. - 2009 - SSARC The Short-Sighted Adaptive Replacement Cach.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9ZX3IEXM\\5167043.html},
  keywords = {adaptive,Cache,cache replacement policies,cache storage,Computer science,Delay,Feedback,High performance computing,low overhead adaptive policy,Performance gain,Protection,short-sighted adaptive replacement cache,short-sighted adaptive replacement policy,SSARC,storge,Tail}
}

@article{chhedaMemorySystemsOverview,
  title = {Memory {{Systems}}: {{Overview}} and {{Trends}}},
  author = {Chheda, Saurabh and Chittamuru, Jeevan Kumar and Moritz, Csaba Andras},
  pages = {12},
  abstract = {Computer pioneers have correctly predicted that programmers would want unlimited amounts of memory. An economical solution to this desire is the implementation of a Memory Hierarchical System, which takes advantage of locality and cost/performance of memory technologies. As time has gone by, the technology has progressed, bringing about various changes in the way memory systems are built. Memory systems must be flexible enough to accommodate various levels of memory hierarchies, and must be able to emulate an environment with unlimited amount of memory. For more than two decades the main emphasis of memory system designers has been achieving high performance. However, recent market trends and application requirements suggest that other design goals such as low-power, predictability, and flexibility/reconfigurability are becoming equally important to consider. This paper gives a comprehensive overview of memory systems with the objective to give any reader a broad overview. Emphasis is put on the various components of a typical memory system of present-day systems and emerging new memory system architecture trends. We focus on emerging memory technologies, system architectures, compiler technology, which are likely to shape the computer industry in the future.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NISUVJUW\\Chheda et al. - Memory Systems Overview and Trends.pdf},
  langid = {english}
}

@inproceedings{chienLowLatencyMemory2007,
  title = {A {{Low Latency Memory Controller}} for {{Video Coding Systems}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Chien, C. and Wang, C. and Lin, C. and Hsieh, T. and Chu, Y. and Guo, J.},
  date = {2007-07},
  pages = {1211--1214},
  doi = {10.1109/ICME.2007.4284874},
  abstract = {The dynamic memory controller plays an important role in system-on-a-chip (SoC) designs to provide enough memory bandwidth through external memory for DSP and multimedia processing. However, the overhead cycles in accessing the data located in external memory have much influence on the SoC performance. In this paper, we propose a low latency memory controller with AHB interface to reduce the overhead cycles for the SDR memory access in the SoC designs. Through the pre-calculated addresses of impending transfers, two memory control schemes, i.e. Burst terminates Burst (BTB) and Anticipative Row Activation (ARA), are used to reduce the latency of SDR memory access. The experimental results show that the proposed memory controller reduces the memory bandwidth by 33\% in a typical MPEG-4 video decoding system.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LFPHEKY6\\Chien et al. - 2007 - A Low Latency Memory Controller for Video Coding S.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3AHHJWJJ\\4284874.html},
  keywords = {anticipative row activation,Bandwidth,burst terminates burst,Control systems,Costs,Decoding,Delay,DRAM chips,dynamic memory controller,low latency memory controller,MPEG 4 Standard,MPEG-4 video decoding system,Multimedia systems,Scheduling,SoC design,SRAM chips,System-on-a-chip,system-on-chip,video coding,Video coding,video coding systems}
}

@inproceedings{choiMultipleCloneRow2015,
  title = {Multiple {{Clone Row DRAM}}: {{A}} Low Latency and Area Optimized {{DRAM}}},
  shorttitle = {Multiple {{Clone Row DRAM}}},
  booktitle = {2015 {{ACM}}/{{IEEE}} 42nd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Choi, J. and Shin, W. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, L.},
  date = {2015-06},
  pages = {223--234},
  doi = {10.1145/2749469.2750402},
  abstract = {Several previous works have changed DRAM bank structure to reduce memory access latency and have shown performance improvement. However, changes in the area-optimized DRAM bank can incur large area-overhead. To solve this problem, we propose Multiple Clone Row DRAM (MCR-DRAM), which uses existing DRAM bank structure without any modification.},
  eventtitle = {2015 {{ACM}}/{{IEEE}} 42nd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ATDZYEJF\\Choi et al. - 2015 - Multiple Clone Row DRAM A low latency and area op.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DEIKM2ER\\7284068.html},
  keywords = {area optimized DRAM,bank structure,DRAM chips,low latency DRAM,MCR-DRAM,memory access latency,multiple clone row DRAM}
}

@article{chooDIGDegreeInterreference2006,
  title = {{{DIG}}: {{Degree}} of Inter-Reference Gap for a Dynamic Buffer Cache Management},
  shorttitle = {{{DIG}}},
  author = {Choo, Hyunseung and Lee, Young Jae and Yoo, Seong-Moo},
  date = {2006-04-22},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  volume = {176},
  pages = {1032--1044},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2005.01.018},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025505000551},
  urldate = {2019-09-12},
  abstract = {The effectiveness of the buffer cache replacement is critical to the performance of I/O systems. In this paper, we propose a degree of inter-reference gap (DIG) based block replacement scheme. This scheme keeps the simplicity of the least recently used (LRU) scheme and does not depend on the detection of access regularities. The proposed scheme is based on the low inter-reference recency set (LIRS) scheme, which is currently known to be very effective. However, the proposed scheme employs several history information items whereas the LIRS scheme uses only one history information item. The overhead of the proposed scheme is almost negligible. To evaluate the performance of the proposed scheme, the comprehensive trace-driven computer simulation is used in general access patterns. Our simulation results show that the cache hit ratio (CHR) in the proposed scheme is improved as much as 65.3\% (with an average of 26.6\%) compared to the LRU for the same workloads, and up to 6\% compared to the LIRS in multi3 trace.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TP762LZU\\Choo et al. - 2006 - DIG Degree of inter-reference gap for a dynamic b.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6VZZ28PM\\S0020025505000551.html},
  keywords = {Buffer cache management,Cache hit ratio,Degree of inter-reference gap,Least recently used,Low inter-reference recency set},
  number = {8}
}

@inproceedings{choTamingKillerMicrosecond2018,
  title = {Taming the {{Killer Microsecond}}},
  booktitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Cho, S. and Suresh, A. and Palit, T. and Ferdman, M. and Honarmand, N.},
  date = {2018-10},
  pages = {627--640},
  doi = {10.1109/MICRO.2018.00057},
  abstract = {Modern applications require access to vast datasets at low latencies. Emerging memory technologies can enable faster access to significantly larger volumes of data than what is possible today. However, these memory technologies have a significant caveat: their random access latency falls in a range that cannot be effectively hidden using current hardware and software latency-hiding techniques-namely, the microsecond range. Finding the root cause of this "Killer Microsecond" problem, is the subject of this work. Our goal is to answer the critical question of why existing hardware and software cannot hide microsecond-level latencies, and whether drastic changes to existing platforms are necessary to utilize microsecond-latency devices effectively. We use an FPGA-based microsecond-latency device emulator, a carefully-crafted microbenchmark, and three open-source data-intensive applications to show that existing systems are indeed incapable of effectively hiding such latencies. However, after uncovering the root causes of the problem, we show that simple changes to existing systems are sufficient to support microsecond-latency devices. In particular, we show that by replacing on-demand memory accesses with prefetch requests followed by fast user-mode context switches (to increase access-level parallelism) and enlarging hardware queues that track in-flight accesses (to accommodate many parallel accesses), conventional architectures can effectively hide microsecond-level latencies, and approach the performance of DRAM-based implementations of the same applications. In other words, we show that successful usage of microsecond-level devices is not predicated on drastically new hardware and software architectures.},
  eventtitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\T6D97TH7\\Cho et al. - 2018 - Taming the Killer Microsecond.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9NEN53JX\\8574574.html},
  keywords = {access-level parallelism,cache storage,Data intensive applications,DRAM chips,DRAM-based implementations,Emerging storage,field programmable gate arrays,FPGA,FPGA-based microsecond-latency device emulator,Hardware,hardware architectures,Kernel,killer microsecond problem,Killer microseconds,memory architecture,memory technologies,microsecond range,microsecond-level devices,microsecond-level latencies,on-demand memory accesses,open-source data-intensive applications,performance evaluation,Performance evaluation,Prefetching,Random access memory,Servers,software architecture,software architectures,software latency-hiding techniques,storage management,track in-flight accesses}
}

@inproceedings{cilkuImprovingPerformanceSinglePath2017,
  title = {Improving {{Performance}} of {{Single}}-{{Path Code}} through a {{Time}}-{{Predictable Memory Hierarchy}}},
  booktitle = {2017 {{IEEE}} 20th {{International Symposium}} on {{Real}}-{{Time Distributed Computing}} ({{ISORC}})},
  author = {Cilku, B. and Puffitsch, W. and Prokesch, D. and Schoeberl, M. and Puschner, P.},
  date = {2017-05},
  pages = {76--83},
  doi = {10.1109/ISORC.2017.17},
  abstract = {Deriving the Worst-Case Execution Time (WCET) of a task is a challenging process, especially for processor architectures that use caches, out-of-order pipelines, and speculative execution. Despite existing contributions to WCET analysis for these complex architectures, there are open problems. The single-path code generation overcomes these problems by generating time-predictable code that has a single execution trace. However, the simplicity of this approach comes at the cost of longer execution times. This paper addresses performance improvements for single-path code. We propose a time-predictable memory hierarchy with a prefetcher that exploits the predictability of execution traces in single-path code to speed up code execution. The new memory hierarchy reduces both the cache-miss penalty time and the cache-miss rate on the instruction cache. The benefit of the approach is demonstrated through benchmarks that are executed on an FPGA implementation.},
  eventtitle = {2017 {{IEEE}} 20th {{International Symposium}} on {{Real}}-{{Time Distributed Computing}} ({{ISORC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZXRZLXGQ\\Cilku et al. - 2017 - Improving Performance of Single-Path Code through .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R7Q7ARXP\\7964873.html},
  keywords = {cache storage,cache-miss penalty time,cache-miss rate,Computer architecture,FPGA,Hardware,Pipelines,prefetching,Prefetching,Process control,program compilers,Real-time systems,single-path code,single-path code generation,time-predictable memory hierarchy,Time-predictable memory hierarchy,WCET analysis,worst-case execution time}
}

@inproceedings{cilkuTimePredictableInstructionCacheArchitecture2015,
  title = {A {{Time}}-{{Predictable Instruction}}-{{Cache Architecture}} That {{Uses Prefetching}} and {{Cache Locking}}},
  booktitle = {2015 {{IEEE International Symposium}} on {{Object}}/{{Component}}/{{Service}}-{{Oriented Real}}-{{Time Distributed Computing Workshops}}},
  author = {Cilku, Bekim and Prokesch, Daniel and Puschner, Peter},
  date = {2015-04},
  pages = {74--79},
  publisher = {{IEEE}},
  location = {{Auckland, New Zealand}},
  doi = {10.1109/ISORCW.2015.58},
  url = {http://ieeexplore.ieee.org/document/7160126/},
  urldate = {2019-01-30},
  eventtitle = {2015 {{IEEE International Symposium}} on {{Object}}/{{Component}}/{{Service}}-{{Oriented Real}}-{{Time Distributed Computing Workshops}} ({{ISORCW}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G8M4Y82U\\Cilku et al. - 2015 - A Time-Predictable Instruction-Cache Architecture .pdf},
  isbn = {978-1-4673-7709-6}
}

@article{consuegraAnalyzingAdaptiveCache2015,
  title = {Analyzing {{Adaptive Cache Replacement Strategies}}},
  author = {Consuegra, Mario E. and Martinez, Wendy A. and Narasimhan, Giri and Rangaswami, Raju and Shao, Leo and Vietri, Giuseppe},
  date = {2015-03-26},
  url = {http://arxiv.org/abs/1503.07624},
  urldate = {2019-09-12},
  abstract = {Adaptive Replacement Cache (ARC) and CLOCK with Adaptive Replacement (CAR) are state-of-the- art "adaptive" cache replacement algorithms invented to improve on the shortcomings of classical cache replacement policies such as LRU, LFU and CLOCK. By separating out items that have been accessed only once and items that have been accessed more frequently, both ARC and CAR are able to control the harmful effect of single-access items flooding the cache and pushing out more frequently accessed items. Both ARC and CAR have been shown to outperform their classical and popular counterparts in practice. Both algorithms are complex, yet popular. Even though they can be treated as online algorithms with an "adaptive" twist, a theoretical proof of the competitiveness of ARC and CAR remained unsolved for over a decade. We show that the competitiveness ratio of CAR (and ARC) has a lower bound of N + 1 (where N is the size of the cache) and an upper bound of 18N (4N for ARC). If the size of cache offered to ARC or CAR is larger than the one provided to OPT, then we show improved competitiveness ratios. The important implication of the above results are that no "pathological" worst-case request sequences exist that could deteriorate the performance of ARC and CAR by more than a constant factor as compared to LRU.},
  archivePrefix = {arXiv},
  eprint = {1503.07624},
  eprinttype = {arxiv},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\N3EN9VJN\\Consuegra et al. - 2015 - Analyzing Adaptive Cache Replacement Strategies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RGS6Z6ZU\\1503.html},
  keywords = {Computer Science - Data Structures and Algorithms},
  primaryClass = {cs}
}

@incollection{corbatoPagingExperimentMultics1969,
  title = {A {{Paging Experiment}} with the {{Multics System}}},
  booktitle = {In Honor of {{Philip M}}. {{Morse}}},
  author = {Corbató, Fernando J.},
  editor = {Feshbach, Herman and Ingard, K. Uno and Morse, Philip M.},
  date = {1969},
  pages = {217--228},
  publisher = {{M.I.T. Press}},
  location = {{Cambridge}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F7MW59JS\\paging-experiment.pdf},
  isbn = {978-0-262-06028-8},
  keywords = {Morse; Philip M,Philip McCord,Physics}
}

@misc{CoreSightBaseSystem18,
  title = {{{CoreSight Base System Architecture}}},
  date = {0018-07-23},
  publisher = {{ARM}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JN5U2XCE\\CoreSight Base System Architecture.pdf},
  langid = {english}
}

@article{cuiNewHybridApproach2003,
  title = {A {{New Hybrid Approach}} to {{Exploit Localities}}: {{LRFU}} with {{Adaptive Prefetching}}},
  shorttitle = {A {{New Hybrid Approach}} to {{Exploit Localities}}},
  author = {Cui, Jike and Samadzadeh, Mansur. H.},
  date = {2003-12},
  journaltitle = {SIGMETRICS Perform. Eval. Rev.},
  volume = {31},
  pages = {37--43},
  issn = {0163-5999},
  doi = {10.1145/974036.974041},
  url = {http://doi.acm.org/10.1145/974036.974041},
  urldate = {2019-09-16},
  abstract = {This paper reviewed a number of existing methods to exploit the spatial and temporal locality commonly existing in programs, and provided detailed analysis and testing of adaptive prefetching (a method designed to utilize spatial locality) and the least recently and frequently used (LRFU) method (a method designed to utilize temporal locality). The two methods were combined in this work in terms of their exploitation of locality. The comparative studies of the methods were done using real traces, and hit rate was used as an evaluation measure.Results showed that by using adaptive prefetching, the hit rate improved significantly by an average of 11.7\% over the hit rate of LRU in the traces and cache configurations used. It also showed that LRFU consistently gives higher hit rates than LRU, but not by much in the trace files and cache configurations tested. And the X value (a controllable parameter which determines the Weights given to recency and frequency) has to be in a certain range, which is usually narrow, in order to get the best performance for hit rate. Compared to adaptive prefetching and LRU, the hybrid approach of combining adaptive prefetching and LRFU gave a consistently higher hit rate also. But, affected by the performance of LRFU, the improvement in the hit rate by the combination was low.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VVBD52LY\\Cui and Samadzadeh - 2003 - A New Hybrid Approach to Exploit Localities LRFU .pdf},
  number = {3}
}

@inproceedings{cuppuPerformanceComparisonContemporary1999,
  title = {A Performance Comparison of Contemporary {{DRAM}} Architectures},
  booktitle = {Proceedings of the 26th {{International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{99CB36367}})},
  author = {Cuppu, V. and Jacob, B. and Davis, B. and Mudge, T.},
  date = {1999-05},
  pages = {222--233},
  doi = {10.1109/ISCA.1999.765953},
  abstract = {In response to the growing gap between memory access time and processor speed, DRAM manufacturers have created several new DRAM architectures. This paper presents a simulation-based performance study of a representative group, each evaluated in a small system organization. These small-system organizations correspond to workstation-class computers and use on the order of IO DRAM chips. The study covers Fast Page Mode, Extended Data Out, Synchronous, Enhanced Synchronous, Synchronous Link, Rambus, and Direct Rambus designs. Our simulations reveal several things: (a) current advanced DRAM technologies are attacking the memory bandwidth problem but not the latency problem; (b) bus transmission speed will soon become a primary factor limiting memory-system performance; (c) the post-12 address stream still contains significant locality, though it varies from application to application; and (d) as we move to wider buses, row access time becomes more prominent, making it important to investigate techniques to exploit the available locality to decrease access time.},
  eventtitle = {Proceedings of the 26th {{International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{99CB36367}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\96RK28Z9\\Cuppu et al. - 1999 - A performance comparison of contemporary DRAM arch.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NKYR8DCJ\\765953.html},
  keywords = {Bandwidth,Capacitors,Computer architecture,contemporary DRAM architectures,Costs,Delay,digital simulation,DRAM chips,Jacobian matrices,memory access time,memory-system performance,Out of order,parallel architectures,performance comparison,performance evaluation,Pins,processor speed,Random access memory,simulation-based performance study,simulations,small-system organizations,Time measurement,workstation-class computers}
}

@thesis{damienStudyDifferentCache2007,
  title = {Study of {{Different Cache Line Replacement Algorithms}} in {{Embedded Systems}}},
  author = {Damien, Gille},
  date = {2007},
  institution = {{KTH Royal Institute of Technology}},
  location = {{Stockholm}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.217.3594&rep=rep1&type=pdf},
  abstract = {The increasing speed gap between processors and memories underlines the criticalness of cache memories. The strong area and power consumption constraints of general purpose embedded systems limit the size of cache memories. With the development of embedded systems provided with an operative system, these constraints are even stronger. The selection of an efficient replacement policy thus appears as critical. The Least Recently Used (LRU) strategy performs well on most memory patterns but this performance is obtained at the expense of the hardware requirements and of the power consumption. Consequently, new algorithms have been developed and this work is devoted to the evaluation of their performance. The implementation of a cache simulator allowed us to carry out a detailed investigation of the behaviour of the policies, which among others demonstrated the occurrence of Belady’s anomaly for a pseudo-LRU replacement algorithm, PLRUm. The replacement strategies that emerged from this study were then integrated in the ARM11 MPCore processor and their performance results were compared with the cache simulator ones. Our results show that the MRU-based pseudo-LRU replacement policy (PLRUm) approximates the LRU algorithm very closely and can even outperform it with low hardware and power consumption},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\MVYMQV9B\\Damien et al. - 2007 - Company Industrial supervisor.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UTW98MYD\\summary.html},
  type = {Masters Thesis}
}

@inproceedings{dasArbitrationCacheReplacements2016,
  title = {An Arbitration on Cache Replacements Based on Frequency — {{Recency}} Product Values},
  booktitle = {2016 {{International Conference}} on {{VLSI Systems}}, {{Architectures}}, {{Technology}} and {{Applications}} ({{VLSI}}-{{SATA}})},
  author = {Das, S. and Banerjee, A.},
  date = {2016-01},
  pages = {1--6},
  doi = {10.1109/VLSI-SATA.2016.7593031},
  abstract = {Evolving an efficient cache replacement policy has been a challenge since the last few decades. LRU (Least Recently Used) and LFU (Least Frequently Used) cache replacement techniques and a variety of their combinations were the most sought after. This paper proposes a new combination of the LRU and LFU in such a style that the time and complexity to replace moves below the current benchmarks. Here, a frequency-recency product value is computed which dictates the cache replacement arbitration. It out performs the existing methods by a significant reduction in computational overhead.},
  eventtitle = {2016 {{International Conference}} on {{VLSI Systems}}, {{Architectures}}, {{Technology}} and {{Applications}} ({{VLSI}}-{{SATA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\62NDP6DC\\Das and Banerjee - 2016 - An arbitration on cache replacements based on freq.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H22QFACA\\7593031.html},
  keywords = {Cache memory,cache storage,Complexity theory,frequency,Indexes,least frequently used cache replacement techniques,least recently used cache replacement techniques,LFRU,LFU,Loading,LRU,minima,Organizations,Radiation detectors,recency,replacement policy,set-associative mapping,Very large scale integration}
}

@inproceedings{dasLatencyAwareBlock2017,
  title = {Latency {{Aware Block Replacement}} for {{L1 Caches}} in {{Chip Multiprocessor}}},
  booktitle = {2017 {{IEEE Computer Society Annual Symposium}} on {{VLSI}} ({{ISVLSI}})},
  author = {Das, Shirshendu and Kapoor, Hemangee K.},
  date = {2017-07},
  pages = {182--187},
  issn = {2159-3477},
  doi = {10.1109/ISVLSI.2017.40},
  abstract = {Performance of chip multiprocessors (CMPs) heavily depends on the efficiency of the caches. There is still a large gap between implemented replacement policies and the theoretical optimal policy. Among other factors, replacement policies play a major role in deciding the memory access time as they directly affect the miss-rate. In a CMP environment, the cost incurred by a miss in the higher level cache needs to take into account the communication latency over the on-chip network as well as power consumption.There are existing replacement policies which use this misscost rather than miss-count as an optimisation factor. These existing policies targeting the optimisation of miss-cost are mainly designed for the Last-Level Caches (LLC). Due to their significant hardware overheads, these policies are not directly applicable for the L1 caches. In this paper, we propose a new replacement policy, LA-LRU, for L1 caches which uses communication latency as a factor in deciding the victim. Experimental evaluation on full system simulation shows 7.4\% improvement in Average Memory Access Time (AMAT) which leads to 7\% improvement in Cycles Per Instruction (CPI). Reduction in on-chip communication also helps in improving the on-chip network power consumption by 14.8\%.},
  eventtitle = {2017 {{IEEE Computer Society Annual Symposium}} on {{VLSI}} ({{ISVLSI}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G2KYKJ3X\\Das and Kapoor - 2017 - Latency Aware Block Replacement for L1 Caches in C.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\IN5EVC53\\7987516.html},
  keywords = {AMAT,average memory access time,Benchmark testing,Cache Architectures,cache efficiency,cache storage,chip multiprocessor,CMP,communication latency,CPI,cycles per instruction,Face,Hardware,L1 cache,LA-LRU replacement policy,last-level cache,latency aware block replacement,memory access time,multiprocessing systems,NUCA,Optimized production technology,Power demand,Replacement Policy,System-on-chip,Systems simulation}
}

@inproceedings{dasRandomLRUReplacementPolicy2013,
  title = {Random-{{LRU}}: {{A Replacement Policy}} for {{Chip Multiprocessors}}},
  shorttitle = {Random-{{LRU}}},
  booktitle = {{{VLSI Design}} and {{Test}}},
  author = {Das, Shirshendu and Polavarapu, Nagaraju and Halwe, Prateek D. and Kapoor, Hemangee K.},
  editor = {Gaur, Manoj Singh and Zwolinski, Mark and Laxmi, Vijay and Boolchandani, Dharmendra and Sing, Virendra and Sing, Adit D.},
  date = {2013},
  pages = {204--213},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-42024-5_25},
  abstract = {As the number of cores and associativity of the last level cache (LLC) on a Chip Multi-processor increases, the role of replacement policies becomes more vital. Though, pure least recently used (LRU) policy has some issues it has been generally believed that some versions of LRU policy performs better than the other policies. Therefore, a lot of work has been proposed to improve the performance of LRU-based policies. However, it has been shown that the true LRU imposes additional complexity and area overheads when implemented on high associative LLCs. Most of the LRU based works are more motivated towards the performance improvement than the reduction of area and hardware overhead of true LRU scheme. In this paper we proposed an LRU based cache replacement policy especially for the LLC to improve the performance of LRU as well as to reduce the area and hardware cost of pure LRU by more than a half. We use a combination of random and LRU replacement policy for each cache set. Instead of using LRU policy for the entire set we use it only for some number of ways within the set. Experiments conducted on a full-system simulator shows 36\% and 11\% improvements over miss rate and CPI respectively.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ASDI8YX6\\Das et al. - 2013 - Random-LRU A Replacement Policy for Chip Multipro.pdf},
  isbn = {978-3-642-42024-5},
  keywords = {LRU,NUCA,Pseudo-LRU,Random-LRU,Tiled CMP},
  langid = {english},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@inproceedings{dasRandomLRUReplacementPolicy2013a,
  title = {Random-{{LRU}}: {{A Replacement Policy}} for {{Chip Multiprocessors}}},
  shorttitle = {Random-{{LRU}}},
  booktitle = {{{VLSI Design}} and {{Test}}},
  author = {Das, Shirshendu and Polavarapu, Nagaraju and Halwe, Prateek D. and Kapoor, Hemangee K.},
  editor = {Gaur, Manoj Singh and Zwolinski, Mark and Laxmi, Vijay and Boolchandani, Dharmendra and Sing, Virendra and Sing, Adit D.},
  date = {2013},
  pages = {204--213},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-42024-5_25},
  abstract = {As the number of cores and associativity of the last level cache (LLC) on a Chip Multi-processor increases, the role of replacement policies becomes more vital. Though, pure least recently used (LRU) policy has some issues it has been generally believed that some versions of LRU policy performs better than the other policies. Therefore, a lot of work has been proposed to improve the performance of LRU-based policies. However, it has been shown that the true LRU imposes additional complexity and area overheads when implemented on high associative LLCs. Most of the LRU based works are more motivated towards the performance improvement than the reduction of area and hardware overhead of true LRU scheme. In this paper we proposed an LRU based cache replacement policy especially for the LLC to improve the performance of LRU as well as to reduce the area and hardware cost of pure LRU by more than a half. We use a combination of random and LRU replacement policy for each cache set. Instead of using LRU policy for the entire set we use it only for some number of ways within the set. Experiments conducted on a full-system simulator shows 36\% and 11\% improvements over miss rate and CPI respectively.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BVBS7NC3\\Das et al. - 2013 - Random-LRU A Replacement Policy for Chip Multipro.pdf},
  isbn = {978-3-642-42024-5},
  keywords = {LRU,NUCA,Pseudo-LRU,Random-LRU,Tiled CMP},
  langid = {english},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@inproceedings{dasRoleCacheReplacement2019,
  title = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}: {{A Survey}}},
  shorttitle = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}},
  booktitle = {Communication, {{Networks}} and {{Computing}}},
  author = {Das, Purnendu},
  editor = {Verma, Shekhar and Tomar, Ranjeet Singh and Chaurasia, Brijesh Kumar and Singh, Vrijendra and Abawajy, Jemal},
  date = {2019},
  pages = {400--410},
  publisher = {{Springer Singapore}},
  abstract = {Cache replacement policies play important roles in efficiently processing the current big data applications. The performance of any high performance computing system is highly depending on the performance of its cache memory. A better replacement policy allows the important blocks to be placed nearer to the core. Hence reduces the overall execution latency and gives better computational efficiency. There are different replacement policies exits. The main difference among these policies is how to select the victim block from the cache such that it can be replaced with another newly fetched block. Non-optimal replacement policy may remove important blocks from the cache when some less important (dead) blocks also present in the cache. Proposing better replacement policy for cache memory is a major research area from last three decades. The most widely used replacement policies used for classical cache memories are Least Recently Used Policy (LRU), Random Replacement Policy or Pseudo-LRU. As the technology advances the technology of cache memory is also changing. For efficient processing of big data based applications today’s computer having high performance computing ability requires larger cache memory. Such larger cache memory makes the task of replacement policies more challenging. In this paper we have done a survey about the innovations done in cache replacement policies to support the efficient processing of big data based applications.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FD9N4ZE3\\Das - 2019 - Role of Cache Replacement Policies in High Perform.pdf},
  isbn = {9789811323720},
  keywords = {Cache memory,Efficient data processing,Multicore-system,Replacement policies},
  langid = {english},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@inproceedings{deckerOnlineAnalysisDebug2018,
  title = {Online Analysis of Debug Trace Data for Embedded Systems},
  booktitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Decker, N. and Dreyer, B. and Gottschling, P. and Hochberger, C. and Lange, A. and Leucker, M. and Scheffel, T. and Wegener, S. and Weiss, A.},
  date = {2018-03},
  pages = {851--856},
  doi = {10.23919/DATE.2018.8342124},
  abstract = {Modern multi-core Systems-on-Chip (SoC) provide very high computational power. On the downside, they are hard to debug and it is often very difficult to understand what is going on in these chips because of the limited observability inside the SoC. Chip manufacturers try to compensate this difficulty by providing highly compressed trace data from the individual cores. In the past, the common way to deal with this data was storing it for later offline analysis, which severely limits the time span that can be observed. In this contribution, we present an FPGA-based solution that is able to process the trace data in real-time, enabling continuous observation of the state of a core. Moreover, we discuss applications enabled by this technology.},
  eventtitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WWBGHZM7\\Decker et al. - 2018 - Online analysis of debug trace data for embedded s.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W5G8AMFU\\8342124.html},
  keywords = {Bandwidth,Chip manufacturers,continuous observation,debug trace data,embedded systems,field programmable gate arrays,Field programmable gate arrays,FPGA-based solution,Hardware,high computational power,highly compressed trace data,individual cores,Instruments,modern multicore Systems-on-Chip,Monitoring,multiprocessing systems,offline analysis,online analysis,program debugging,Runtime,SoC,Software,system-on-chip,time span}
}

@article{delshadtehraniNileProgrammableMonitoring2018,
  title = {Nile: {{A Programmable Monitoring Coprocessor}}},
  shorttitle = {Nile},
  author = {Delshadtehrani, L. and Eldridge, S. and Canakci, S. and Egele, M. and Joshi, A.},
  date = {2018-01},
  journaltitle = {IEEE Computer Architecture Letters},
  volume = {17},
  pages = {92--95},
  issn = {1556-6056},
  doi = {10.1109/LCA.2017.2784416},
  abstract = {Researchers widely employ hardware performance counters (HPCs) as well as debugging and profiling tools in processors for monitoring different events such as cache hits, cache misses, and branch prediction statistics during the execution of programs. The collected information can be used for power, performance, and thermal management of the system as well as detecting anomalies or malicious behavior in the software. However, monitoring new or complex events using HPCs and existing tools is a challenging task because HPCs only provide a fixed pool of raw events to monitor. To address this challenge, we propose the implementation of a programmable hardware monitor in a complete system framework including the hardware monitor architecture and its interface with an in-order single-issue RISC-V processor as well as an operating system. As a proof of concept, we demonstrate how to programmatically implement a shadow stack using our hardware monitor and how the programmed shadow stack detects stack buffer overflow attacks. Our hardware monitor design incurs a 26 percent power overhead and a 15 percent area overhead over an unmodified RISC-V processor. Our programmed shadow stack has less than 3 percent performance overhead in the worst case.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5CLZ8NEA\\Delshadtehrani et al. - 2018 - Nile A Programmable Monitoring Coprocessor.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\634735H9\\8219379.html},
  keywords = {branch prediction statistics,cache hits,cache misses,cache storage,complete system framework,complex events,coprocessors,Coprocessors,debugging,fixed pool,Hardware,Hardware coprocessor,hardware monitor architecture,hardware monitor design,hardware performance counters,HPCs,Linux,malicious behavior,Monitoring,Nile,operating system,operating systems (computers),Pattern matching,performance evaluation,performance overhead,power overhead,profiling tools,Program processors,programmable hardware,programmable hardware monitor,programmable monitoring coprocessor,programmed shadow stack,raw events,reduced instruction set computing,Rockets,security,shadow stack,single-issue RISC-V processor,stack buffer overflow attack,stack buffer overflow attacks,thermal management,unmodified RISC-V processor},
  number = {1}
}

@inproceedings{denningThrashingItsCauses1968,
  title = {Thrashing: Its Causes and Prevention},
  shorttitle = {Thrashing},
  author = {Denning, Peter J.},
  date = {1968},
  pages = {915},
  publisher = {{ACM Press}},
  doi = {10.1145/1476589.1476705},
  url = {http://portal.acm.org/citation.cfm?doid=1476589.1476705},
  urldate = {2020-01-10},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2BYK7LVK\\Denning - 1968 - Thrashing its causes and prevention.pdf},
  langid = {english}
}

@article{denningWorkingSetModel1968,
  title = {The Working Set Model for Program Behavior},
  author = {Denning, Peter J.},
  date = {1968-05-01},
  journaltitle = {Communications of the ACM},
  volume = {11},
  pages = {323--333},
  issn = {00010782},
  doi = {10.1145/363095.363141},
  url = {http://portal.acm.org/citation.cfm?doid=363095.363141},
  urldate = {2020-01-10},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XANGYT6N\\Denning - The Working Set Model for Program Behavior.pdf},
  number = {5}
}

@inproceedings{desaiProcessVariationAware2012,
  title = {Process Variation Aware {{DRAM}} Design Using Block Based Adaptive Body Biasing Algorithm},
  booktitle = {Thirteenth {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  author = {Desai, S. and Roy, S. and Chakraborty, K.},
  date = {2012-03},
  pages = {255--261},
  doi = {10.1109/ISQED.2012.6187503},
  abstract = {Large dense structures like DRAMs are particularly susceptible to process variation, which can lead to variable latencies in different memory arrays. However, very little work exists on variation studies in the DRAM. This is due to the fact that DRAMs were traditionally placed off-chip and their latency changes due to process variation did not impact the overall processor performance. However, emerging technology trends like three dimensional integration, use of sophisticated memory controllers and continued scaling of technology nodes, substantially reduces DRAM access latency. Hence future technology nodes will see widespread adoption of embedded DRAMs. This makes process variation a critical upcoming challenge in DRAMs that must be addressed in current and forthcoming technology generations. In this paper, we present techniques for modeling the effect of random as well as spatial variation in large DRAM array structures. We use sensitivity based gate level process variation models combined with statistical timing analysis to estimate the impact of process variation on the DRAM performance and leakage power. We also propose a simulated annealing based Vth assignment algorithm using adaptive body biasing to improve the yield of DRAM structures. Applying our algorithm on a 1GB DRAM array, we report an average of 10.3\% improvement in the DRAM yield. To the best of our knowledge, ours is the first technique to model the impact of process variation on large scale DRAM arrays.},
  eventtitle = {Thirteenth {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XLSSMPGY\\Desai et al. - 2012 - Process variation aware DRAM design using block ba.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3KL3ADMV\\6187503.html},
  keywords = {Adaptive body bias,Algorithm design and analysis,block based adaptive body biasing algorithm,Correlation,Delay,DRAM,DRAM chips,Integrated circuit modeling,Logic gates,Mathematical model,memory arrays,memory controllers,Process Variation,process variation aware DRAM design,Random access memory,sensitivity based gate level process variation models,simulated annealing,simulated annealing based Vth assignment algorithm,statistical analysis,statistical timing analysis}
}

@inproceedings{deutschMassiveSignalTracing2014,
  title = {Massive Signal Tracing Using On-Chip {{DRAM}} for in-System Silicon Debug},
  booktitle = {2014 {{International Test Conference}}},
  author = {Deutsch, S. and Chakrabarty, K.},
  date = {2014-10},
  pages = {1--10},
  doi = {10.1109/TEST.2014.7035363},
  abstract = {Silicon debug is a major challenge due to continuously increasing design complexity. Traditional debug methods using signal tracing suffer from the limited capacity of on-chip trace buffers that only allow for signal observation during a short time window. We propose a low-cost debug architecture for massive signal tracing in ICs that integrate fast DRAM, such as 2D-ICs with embedded DRAM or 3D-stacked ICs with wide-I/O DRAM dies. The key idea is to use available on-chip DRAM for trace-data storage, which results in a significant increase of the observation window compared to traditional methods that use trace buffers. During a debug session, the entire observation window is divided into intervals and a signature is calculated for each observed interval using a multiple-input signature register. At run time, intervals containing erroneous bits are identified by comparing their signature with pre-calculated “golden” signatures that are stored in the DRAM a priori. Only failing intervals including their time stamp are stored into DRAM, which allows for a more efficient use of the memory, resulting in a larger observation window. The proposed method does not require multiple iterations or intermediate processing steps, hence it can be used during functional testing with minimum time overhead associated with the upload of golden signatures and the download of stored debug data to external equipment. We have created a Verilog RTL model for the proposed architecture, synthesized it using a 45 nm CMOS library, and verified its functionality by simulation. The results show that the observation window can be increased by orders of magnitude compared to prior work at comparable hardware cost.},
  eventtitle = {2014 {{International Test Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDASUK38\\Deutsch and Chakrabarty - 2014 - Massive signal tracing using on-chip DRAM for in-s.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZGZSZ7T\\7035363.html},
  keywords = {2D-IC,3D-stacked IC,Bandwidth,buffer circuits,Buffer storage,Clocks,CMOS,CMOS integrated circuits,DRAM chips,elemental semiconductors,hardware description languages,in-system silicon debug,logic testing,multiple-input signature register,on-chip DRAM,on-chip trace buffers t,Random access memory,Registers,Si,signal tracing,silicon,Silicon,size 45 nm,System-on-chip,trace-data storage,Verilog RTL model}
}

@article{devilleLowcostUsagebasedReplacement1990,
  title = {A Low-Cost Usage-Based Replacement Algorithm for Cache Memories},
  author = {Deville, Yannick},
  date = {1990-12-02},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  volume = {18},
  pages = {52--58},
  issn = {01635964},
  doi = {10.1145/121973.121979},
  url = {http://portal.acm.org/citation.cfm?doid=121973.121979},
  urldate = {2020-01-05},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RWRDHF8Z\\Deville - 1990 - A low-cost usage-based replacement algorithm for c.pdf},
  langid = {english},
  number = {4}
}

@inproceedings{diasCacheDesignAssessment2016,
  title = {A {{Cache Design Assessment Approach}} for {{Embedded Real}}-{{Time Systems Based}} on {{Execution Time Measurement}}},
  booktitle = {2016 {{VI Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  author = {Dias, D. and Lima, G. and Barros, E.},
  date = {2016-11},
  pages = {168--173},
  doi = {10.1109/SBESC.2016.033},
  abstract = {Due to the increasing complexity of embedded systems, simulation is of paramount importance during design phase. Often such systems must obey real-time constraints, calling for worst-case execution time assessment mechanisms. Although there is a wide range of simulation tools in the embedded systems domain, mechanisms for performing high abstraction level estimates for task execution times within a controlled environment are still needed. Non-determinism introduced by cache, multi-core and operating systems, for example, makes timing analysis highly complex or even impossible. We address this problem by presenting a RISC-V Instruction Set Simulation platform equipped with a task profiling mechanism for cache aware execution time measurements. The generated SystemC processor simulation model is integrated within a high abstraction level simulation platform with main memory and cache. Experimental results show that by making use of this kind of platform, designers can easily monitor task execution time as a function of measured code portion, cache sizes or cache policies employed helping in their decisions.},
  eventtitle = {2016 {{VI Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KGCF2H6C\\Dias et al. - 2016 - A Cache Design Assessment Approach for Embedded Re.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HJDA4WHW\\7828301.html},
  keywords = {Estimation,Hardware,Modeling,real-time systems,Registers,Timing}
}

@inproceedings{dingWCETCentricDynamicInstruction2014,
  title = {{{WCET}}-{{Centric}} Dynamic Instruction Cache Locking},
  booktitle = {2014 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Ding, H. and Liang, Y. and Mitra, T.},
  date = {2014-03},
  pages = {1--6},
  doi = {10.7873/DATE.2014.040},
  abstract = {Cache locking is an effective technique to improve timing predictability in real-time systems. In static cache locking, the locked memory blocks remain unchanged throughout the program execution. Thus static locking may not be effective for large programs where multiple memory blocks are competing for few cache lines available for locking. In comparison, dynamic cache locking overcomes cache space limitation through time-multiplexing of locked memory blocks. Prior dynamic locking technique partitions the program into regions and takes independent locking decisions for each region. We propose a flexible loop-based dynamic cache locking approach. We not only select the memory blocks to be locked but also the locking points (e.g., loop level). We judiciously allow memory blocks from the same loop to be locked at different program points for WCET improvement. We design a constraint-based approach that incorporates a global view to decide on the number of locking slots at each loop entry point and then select the memory blocks to be locked for each loop. Experimental evaluation shows that our dynamic cache locking approach achieves substantial improvement of WCET compared to prior techniques.},
  eventtitle = {2014 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CS7DG2N6\\Ding et al. - 2014 - WCET-Centric dynamic instruction cache locking.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QBFPXBZG\\6800241.html},
  keywords = {Abstracts,Benchmark testing,cache lines,cache storage,constraint-based approach,Educational institutions,Electronic mail,flexible loop-based dynamic cache locking approach,independent locking decisions,locked memory blocks,locking points,loop entry point,multiple memory blocks,Nickel,program execution,program points,real-time systems,Resilience,time-multiplexing,Timing,timing predictability,WCET-centric dynamic instruction cache locking,worst-case execution time}
}

@inproceedings{dingWCETcentricPartialInstruction2012,
  title = {{{WCET}}-Centric Partial Instruction Cache Locking},
  booktitle = {{{DAC Design Automation Conference}} 2012},
  author = {Ding, H. and Liang, Y. and Mitra, T.},
  date = {2012-06},
  pages = {412--420},
  abstract = {Caches play an important role in embedded systems by bridging the performance gap between high speed processors and slow memory. At the same time, caches introduce imprecision in Worst-case Execution Time (WCET) estimation due to unpredictable access latencies. Modern embedded processors often include cache locking mechanism for better timing predictability. As the cache contents are statically known, memory access latencies are predictable, leading to precise WCET estimate. Moreover, by carefully selecting the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling without locking. Existing static instruction cache locking techniques strive to lock the entire cache to minimize the WCET. We observe that such aggressive locking mechanisms may have negative impact on the overall WCET as some memory blocks with predictable access behavior get excluded from the cache. We introduce a partial cache locking mechanism that has the flexibility to lock only a fraction of the cache. We judiciously select the memory blocks for locking through accurate cache modeling that determines the impact of the decision on the program WCET. Our synergistic cache modeling and locking mechanism achieves substantial reduction in WCET for a large number of embedded benchmark applications.},
  eventtitle = {{{DAC Design Automation Conference}} 2012},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6UMR5TJR\\Ding et al. - 2012 - WCET-centric partial instruction cache locking.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AV4W9RK9\\6241540.html},
  keywords = {Abstracts,Analytical models,benchmark testing,Benchmark testing,cache contents,cache storage,Concrete,embedded benchmark applications,embedded processor,embedded systems,Estimation,high speed processors,memory access latency,memory block selection,microprocessor chips,Partial Cache Locking,Program processors,Timing,timing circuits,timing predictability,unpredictable access latency,WCET,WCET estimation,WCET minimization,WCET-centric partial instruction cache locking,worst-case execution time}
}

@inproceedings{dongheeleeImplementationPerformanceEvaluation1997,
  title = {Implementation and Performance Evaluation of the {{LRFU}} Replacement Policy},
  booktitle = {Proceedings 23rd {{Euromicro Conference New Frontiers}} of {{Information Technology}} - {{Short Contributions}} -},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Honggi Choe} and {Sam H. Noh} and {Sang Lyul Min} and {Yookun Cho}},
  date = {1997-09},
  pages = {106--111},
  doi = {10.1109/EMSCNT.1997.658446},
  abstract = {Recently, a new block replacement policy called the LRFU (Least Recently/Frequently Used) policy was proposed that subsumes both the LRU and LFU policies, and provides a spectrum of replacement policies between them. We describe an implementation of the LRFU replacement policy in the FreeBSD 2.1.5 and present a performance evaluation of the implementation using the SPEC SDET benchmark. The results show that the new policy gives up to a 30\% performance improvement over the LRU block replacement policy.},
  eventtitle = {Proceedings 23rd {{Euromicro Conference New Frontiers}} of {{Information Technology}} - {{Short Contributions}} -},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X45T4U8Y\\Donghee Lee et al. - 1997 - Implementation and performance evaluation of the L.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GEJN9C9I\\658446.html},
  keywords = {block replacement policy,cache storage,Databases,FreeBSD,Frequency,History,Least Recently/Frequently Used policy,LFU policies,LRFU replacement policy,LRU block replacement policy,Operating systems,performance evaluation,performance improvement,Random access memory,SPEC SDET benchmark,Very large scale integration}
}

@article{dongheeleeLRFUSpectrumPolicies2001,
  title = {{{LRFU}}: A Spectrum of Policies That Subsumes the Least Recently Used and Least Frequently Used Policies},
  shorttitle = {{{LRFU}}},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Jong-Hun Kim} and Noh, S. H. and {Sang Lyul Min} and {Yookun Cho} and {Chong Sang Kim}},
  date = {2001-12},
  journaltitle = {IEEE Transactions on Computers},
  volume = {50},
  pages = {1352--1361},
  doi = {10.1109/TC.2001.970573},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VV9J789A\\Donghee Lee et al. - 2001 - LRFU a spectrum of policies that subsumes the lea.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\K8TTHISS\\970573.html},
  keywords = {Bridges,Databases,File systems,Frequency,Hard disks,History},
  number = {12}
}

@incollection{doughertyOptimizingIntegratedApplication2011,
  title = {Optimizing {{Integrated Application Performance}} with {{Cache}}-{{Aware Metascheduling}}},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}}: {{OTM}} 2011},
  author = {Dougherty, Brian and White, Jules and Kegley, Russell and Preston, Jonathan and Schmidt, Douglas C. and Gokhale, Aniruddha},
  editor = {Meersman, Robert and Dillon, Tharam and Herrero, Pilar and Kumar, Akhil and Reichert, Manfred and Qing, Li and Ooi, Beng-Chin and Damiani, Ernesto and Schmidt, Douglas C. and White, Jules and Hauswirth, Manfred and Hitzler, Pascal and Mohania, Mukesh},
  date = {2011},
  volume = {7045},
  pages = {432--450},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  url = {http://link.springer.com/10.1007/978-3-642-25106-1_2},
  urldate = {2019-09-12},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YZHPCD67\\Dougherty et al. - 2011 - Optimizing Integrated Application Performance with.pdf},
  isbn = {978-3-642-25105-4 978-3-642-25106-1}
}

@article{duongSCOREScoreBasedMemory2010,
  title = {{{SCORE}}: {{A Score}}-{{Based Memory Cache Replacement Policy}}},
  shorttitle = {{{SCORE}}},
  author = {Duong, Nam and Cammarota, Rosario and Zhao, Dali and Kim, Taesu and Veidenbaum, Alex},
  date = {2010-06-20},
  url = {https://hal.inria.fr/inria-00492956},
  urldate = {2019-09-12},
  abstract = {We propose SCORE, a novel adaptive cache replacement policy, which uses a score system to select a cache line to replace. Results show that SCORE o®ers low over-all miss rates on SPEC CPU2006 benchmarks, and provides an average IPC that is 4.9\% higher than LRU and 7.4\% higher than LIP.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VBA6ZPP7\\Duong et al. - 2010 - SCORE A Score-Based Memory Cache Replacement Poli.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZLYYHHS8\\inria-00492956.html},
  langid = {english}
}

@inproceedings{dybdahlLRUbasedReplacementAlgorithm2006,
  title = {An {{LRU}}-Based {{Replacement Algorithm Augmented}} with {{Frequency}} of {{Access}} in {{Shared Chip}}-Multiprocessor {{Caches}}},
  booktitle = {Proceedings of the 2006 {{Workshop}} on {{MEmory Performance}}: {{DEaling}} with {{Applications}}, {{Systems}} and {{Architectures}}},
  author = {Dybdahl, Haakon and Stenström, Per and Natvig, Lasse},
  date = {2006},
  pages = {45--52},
  publisher = {{ACM}},
  location = {{Seattle, Washington, USA}},
  doi = {10.1145/1166133.1166139},
  url = {http://doi.acm.org/10.1145/1166133.1166139},
  urldate = {2019-09-16},
  abstract = {This paper proposes a new replacement algorithm to protect cache lines with potential future reuse from being evicted. In contrast to the recency based approaches used in the past (LRU for example), our algorithm also uses the notion of frequency of access. Instead of evicting the least recently used block, our algorithm identifies among a set of LRU blocks the one that is also least-frequently-used (according to a heuristic) and chooses that as a victim. We have implemented this replacement algorithm in a detailed simulation model of a chip multiprocessor system driven by SPEC2000 benchmarks. We have found that the new scheme improves performance for memory intensive applications. Moreover, as compared to other attempts, our replacement algorithm provides robust improvements across all benchmarks. We have also extended an earlier scheme proposed by Wong and Baer so it is switched off when performance is not improved. Our results show that this makes the scheme much more suitable for CMP configurations.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\69F9ZYYN\\Dybdahl et al. - 2006 - An LRU-based Replacement Algorithm Augmented with .pdf},
  isbn = {978-1-59593-568-7},
  series = {{{MEDEA}} '06}
}

@article{eastonUseBitScanning1979,
  title = {Use {{Bit Scanning}} in {{Replacement Decisions}}},
  author = {Easton and Franaszek},
  date = {1979-02},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-28},
  pages = {133--141},
  issn = {2326-3814},
  doi = {10.1109/TC.1979.1675302},
  abstract = {In paged storage systems, page replacement policies generally depend on a use bit for each page frame. The use bit is automatically turned on when the resident page is referenced. Typically, a page is considered eligible for replacement if its use bit has been scanned and found to be off on µ consecutive occasions, where µ is a parameter of the algorithm. This investigation focuses on the dependence of the number of bit-scanning operations on the value of µ and on properties of the string of page references. The number of such operations is a measure of the system overhead incurred while making replacement decisions. In particular, for several algorithms, the number of scans per reference is shown to be approximately proportional to µ However, empirical results from single-program traces show that the value of µ has little effect on the miss ratio. Although the miss ratios for the bit-scanning algorithms are close to those of least recently used (LRU), it is pointed out that increasing the value of µ need not bring the bit-scanning policies closer to LRU management.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\78TC4848\\Easton and Franaszek - 1979 - Use Bit Scanning in Replacement Decisions.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R6TVFL2Q\\1675302.html},
  keywords = {Miss ratio,operating system overhead,page fault rate,page replacement algorithms,paged memories,paged storage},
  number = {2}
}

@inproceedings{ekanayakeAsynchronousDRAMDesign2003,
  title = {Asynchronous {{DRAM}} Design and Synthesis},
  booktitle = {Ninth {{International Symposium}} on {{Asynchronous Circuits}} and {{Systems}}, 2003. {{Proceedings}}.},
  author = {Ekanayake, V. N. and Manohar, R.},
  date = {2003-05},
  pages = {174--183},
  doi = {10.1109/ASYNC.2003.1199177},
  abstract = {We present the design of a high performance on-chip pipelined asynchronous DRAM suitable for use in a microprocessor cache. Although traditional DRAM structures suffer from long access latency and even longer cycle times, our design achieves a simulated core sub-nanosecond latency and a respectable cycle time of 4.8 ns in a standard 0.25 /spl mu/m logic process. We also show how the cycle time penalty can be overcome by using pipelined interleaved banks with quasi-delay insensitive asynchronous control circuits. We can thus approach the performance of SRAM, which is typically used for caches, while still benefiting from the smaller area footprint of DRAM.},
  eventtitle = {Ninth {{International Symposium}} on {{Asynchronous Circuits}} and {{Systems}}, 2003. {{Proceedings}}.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G6U57F85\\Ekanayake and Manohar - 2003 - Asynchronous DRAM design and synthesis.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FH333QQZ\\1199177.html},
  keywords = {0.25 micron,4.8 ns,asynchronous circuits,asynchronous control circuits,asynchronous DRAM design,Banking,cache storage,Circuit simulation,cycle time penalty,Delay,Design engineering,High performance computing,integrated circuit design,Laboratories,logic design,Logic design,long access latency,microprocessor cache,microprocessor chips,Microprocessors,on-chip asynchronous DRAM,pipeline processing,pipelined asynchronous DRAM,pipelined interleaved banks,quasi-delay insensitive control circuits,Random access memory,random-access storage,System-on-a-chip}
}

@thesis{faizalbinmohdsharifCacheReplacementAlgorithm2015,
  title = {Cache {{Replacement Algorithm Using Hierarchical Allocation Scheduling}}},
  author = {Faizal Bin Mohd Sharif, Mohammad},
  date = {2015-05},
  institution = {{Universiti Putra Malaysia}},
  location = {{Malaysia}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HTBWDYPP\\153824461.pdf},
  type = {Master of Science}
}

@inproceedings{falkCompiletimeDecidedInstruction2007,
  title = {Compile-Time Decided Instruction Cache Locking Using Worst-Case Execution Paths},
  booktitle = {Proceedings of the 5th {{IEEE}}/{{ACM}} International Conference on {{Hardware}}/Software Codesign and System Synthesis  - {{CODES}}+{{ISSS}} '07},
  author = {Falk, Heiko and Plazar, Sascha and Theiling, Henrik},
  date = {2007},
  pages = {143},
  publisher = {{ACM Press}},
  location = {{Salzburg, Austria}},
  doi = {10.1145/1289816.1289853},
  url = {http://portal.acm.org/citation.cfm?doid=1289816.1289853},
  urldate = {2019-08-02},
  abstract = {Caches are notorious for their unpredictability. It is difficult or even impossible to predict if a memory access results in a definite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches.},
  eventtitle = {The 5th {{IEEE}}/{{ACM}} International Conference},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZV9QCPZ5\\Falk et al. - 2007 - Compile-time decided instruction cache locking usi.pdf},
  isbn = {978-1-59593-824-4},
  langid = {english}
}

@inproceedings{faresPerformanceEvaluationTraditional2012,
  title = {Performance {{Evaluation}} of {{Traditional Caching Policies}} on a {{Large System}} with {{Petabytes}} of {{Data}}},
  booktitle = {2012 {{IEEE Seventh International Conference}} on {{Networking}}, {{Architecture}}, and {{Storage}}},
  author = {Fares, R. and Romoser, B. and Zong, Z. and Nijim, M. and Qin, X.},
  date = {2012-06},
  pages = {227--234},
  doi = {10.1109/NAS.2012.32},
  abstract = {Caching is widely known to be an effective method for improving I/O performance by storing frequently used data on higher speed storage components. However, most existing studies that focus on caching performance evaluate fairly small files populating a relatively small cache. Few reports are available that detail the performance of traditional cache replacement policies on extremely large caches. Do such traditional caching policies still work effectively when applied to systems with petabytes of data? In this paper, we comprehensively evaluate the performance of several cache policies, which include First-In-First-Out (FIFO), Least Recently Used (LRU) and Least Frequently Used (LFU), on the global satellite imagery distribution application maintained by the U.S. Geological Survey (USGS) Earth Resources Observation and Science Center (EROS). Evidence is presented suggesting traditional caching policies are capable of providing performance gains when applied to large data sets as with smaller data sets. Our evaluation is based on approximately three million real-world satellite images download requests representing global user download behavior since October 2008.},
  eventtitle = {2012 {{IEEE Seventh International Conference}} on {{Networking}}, {{Architecture}}, and {{Storage}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RDRSFF3U\\Fares et al. - 2012 - Performance Evaluation of Traditional Caching Poli.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RVYUGNMK\\6310897.html},
  keywords = {cache replacement policy,cache storage,caching performance evaluation,Computer science,data petabytes,Earth,Earth Resources Observation and Science Center,EROS,FIFO policy,first-in-first-out policy,frequently used data,global satellite imagery distribution application,global user download behavior,higher speed storage components,I/O performance,least frequently used policy,least recently used policy,LFU policy,LRU policy,NASA,performance evaluation,Performance evaluation,performance gains,real-world satellite images download requests,Remote sensing,Satellites,Servers,traditional caching policy,U.S. Geological Survey,USGS}
}

@inproceedings{ferdmanProactiveInstructionFetch2011,
  title = {Proactive Instruction Fetch},
  booktitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Ferdman, M. and Kaynak, C. and Falsafi, B.},
  date = {2011-12},
  pages = {152--162},
  abstract = {Fast access requirements preclude building L1 instruction caches large enough to capture the working set of server workloads. Efforts exist to mitigate limited L1 instruction cache capacity by relying on the stability and repetitiveness of the instruction stream to predict and prefetch future instruction blocks prior to their use. However, dynamic variation in cache miss sequences prevents correct and timely prediction, leaving many instruction-fetch stalls exposed, resulting in a key performance bottleneck for servers. We observe that, while the vast majority of application instruction references are amenable to prediction, even minor control-flow variations are amplified by microarchitectural components, resulting in a major source of instability and randomness that significantly limit prefetcher utility. Control-flow variation disturbs the L1 instruction cache replacement order and branch predictor state, causing the L1 instruction cache to randomly filter the instruction stream while the branch predictor and spontaneous hardware interrupts inject the stream with unpredictable noise. Based on this observation, we show that an instruction prefetcher, previously plagued by microarchitectural instability, becomes nearly perfect when modified to operate on the correct-path, retire-order instruction stream. We propose Proactive Instruction Fetch, an instruction prefetch mechanism that achieves higher than 99.5\% instruction-cache hit rate, improving server throughput by 27\% and nearly matching the performance of a perfect L1 instruction cache that never misses.},
  eventtitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8GGCKWGI\\Ferdman et al. - 2011 - Proactive instruction fetch.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8SQQA3VT\\7851467.html},
  keywords = {branch prediction,cache miss sequence,cache storage,caching,control-flow variation,Correlation,Filtering,Hardware,History,instruction prefetch mechanism,Instruction sets,instruction streaming,L1 instruction cache,microarchitectural instability,Microarchitecture,Prefetching,proactive instruction fetch,Servers,software architecture}
}

@article{fernandezEffectReplacementAlgorithms1978,
  title = {Effect of {{Replacement Algorithms}} on a {{Paged Buffer Database System}}},
  author = {Fernandez, E. B. and Lang, T. and Wood, C.},
  date = {1978-03},
  journaltitle = {IBM Journal of Research and Development},
  volume = {22},
  pages = {185--196},
  issn = {0018-8646},
  doi = {10.1147/rd.222.0185},
  abstract = {In a database system a buffer may be used to hold recently referenced pages. If this buffer is in virtual memory, the database paging system and the memory paging system affect its performance. The study of the effect of main memory replacement algorithms on the number of main memory page faults is the basic objective of this paper. We assume that the buffer replacement algorithm is least recently used (LRU), and page fault rates for LRU, random (R), and generalized least recently used (GLRU) main memory replacement algorithms are calculated and compared. A set of experiments validates these fault rate expressions and establishes some conditions for the practical application of the results.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\29Y8EBKP\\Fernandez et al. - 1978 - Effect of Replacement Algorithms on a Paged Buffer.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UTVMC8IQ\\5390934.html},
  number = {2}
}

@inproceedings{frittsMultilevelMemoryPrefetching2002,
  title = {Multi-Level Memory Prefetching for Media and Stream Processing},
  booktitle = {Proceedings. {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Fritts, J.},
  date = {2002-08},
  volume = {2},
  pages = {101-104 vol.2},
  doi = {10.1109/ICME.2002.1035522},
  abstract = {This paper presents a multi-level memory prefetch hierarchy for media and stream processing applications. Two major bottlenecks in the performance of multimedia and network applications are long memory latencies and limited off-chip processor bandwidth. Aggressive prefetching can be used to mitigate the memory latency problem, but overly aggressive prefetching may overload the limited external processor bandwidth. To accommodate both problems, we propose multilevel memory prefetching. The multi-level organization enables conservative prefetching on-chip and more aggressive prefetching off-chip. The combination provides aggressive prefetching while minimally impacting off-chip bandwidth, enabling more efficient memory performance for media and stream processing. This paper presents preliminary results for multi-level memory prefetching, which show that combining prefetching at the L1 and DRAM memory levels provides the most effective prefetching with minimal extra bandwidth.},
  eventtitle = {Proceedings. {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BNDPUWHS\\Fritts - 2002 - Multi-level memory prefetching for media and strea.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YKCMRBR7\\1035522.html},
  keywords = {Application software,bandwidth,Bandwidth,buffer storage,Computer networks,Computer science,Delay,DRAM memory,L1 memory,media processing,memory latencies,memory performance,multi-level memory prefetch hierarchy,multi-level memory prefetching,multi-level organization,multimedia applications,multimedia communication,multimedia computing,Multimedia computing,network applications,off-chip processor bandwidth,on-chip,Personal digital assistants,Prefetching,Random access memory,stream processing,Streaming media}
}

@article{furiaModelingTimeComputing2010,
  title = {Modeling Time in Computing: {{A}} Taxonomy and a Comparative Survey},
  shorttitle = {Modeling Time in Computing},
  author = {Furia, Carlo A. and Mandrioli, Dino and Morzenti, Angelo and Rossi, Matteo},
  date = {2010-02-01},
  journaltitle = {ACM Computing Surveys},
  volume = {42},
  pages = {1--59},
  issn = {03600300},
  doi = {10.1145/1667062.1667063},
  url = {http://portal.acm.org/citation.cfm?doid=1667062.1667063},
  urldate = {2019-07-24},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UWY2P8D5\\Furia et al. - 2010 - Modeling time in computing A taxonomy and a compa.pdf},
  langid = {english},
  number = {2}
}

@thesis{garsideRealtimePrefetchingSharedmemory2015,
  title = {Real-Time {{Prefetching On Shared}}-Memory {{Multi}}-Core {{Systems}}},
  author = {Garside, Jamie},
  date = {2015-07},
  institution = {{The University of York}},
  location = {{York, UK}},
  url = {http://etheses.whiterose.ac.uk/10711/1/thesis.pdf},
  urldate = {2018-06-25},
  abstract = {In recent years, there has been a growing trend towards using multi-core processors
in real-time systems to cope with the rising computation requirements of
real-time tasks. Coupled with this, the rising memory requirements of these tasks
pushes demand beyond what can be provided by small, private on-chip caches, requiring
the use of larger, slower off-chip memories such as DRAM. Due to the cost,
power requirements and complexity of these memories, they are typically shared
between all of the tasks within the system.
In order for the execution time of these tasks to be bounded, the response time
of the memory and the interference from other tasks also needs to be bounded.
While there is a great amount of current research on bounding this interference,
one popular method is to effectively partition the available memory bandwidth
between the processors in the system. Of course, as the number of processors
increases, so does the worst-case blocking, and worst-case blocking times quickly
increase with the number of processors.
It is difficult to further optimise the arbitration scheme; instead, this scaling problem
needs to be approached from another angle. Prefetching has previously been
shown to improve the execution time of tasks by speculatively issuing memory
accesses ahead of time for items which may be useful in the near future, although
these prefetchers are typically not used in real-time systems due to their unpredictable
nature. Instead, this work presents a framework by which a prefetcher
can be safely used alongside a composable memory arbiter, a predictable prefetching
scheme, and finally a method by which this predictable prefetcher can be used
to improve the worst-case execution time of a running task.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WZNZW7PQ\\thesis.pdf},
  pagetotal = {185},
  type = {Doctoral}
}

@article{gautschiNearThresholdRISCVCore2017,
  title = {Near-{{Threshold RISC}}-{{V Core With DSP Extensions}} for {{Scalable IoT Endpoint Devices}}},
  author = {Gautschi, Michael and Schiavone, Pasquale Davide and Traber, Andreas and Loi, Igor and Pullini, Antonio and Rossi, Davide and Flamand, Eric and Gurkaynak, Frank K. and Benini, Luca},
  date = {2017-10},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {25},
  pages = {2700--2713},
  issn = {1063-8210, 1557-9999},
  doi = {10.1109/TVLSI.2017.2654506},
  url = {http://ieeexplore.ieee.org/document/7864441/},
  urldate = {2019-07-26},
  abstract = {Endpoint devices for Internet-of-Things not only need to work under extremely tight power envelope of a few milliwatts, but also need to be flexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold (NT) operation can achieve higher energy efficiency, and the performance scalability can be gained through parallelism. In this paper, we describe the design of an open-source RISC-V processor core specifically designed for NT operation in tightly coupled multicore clusters. We introduce instruction extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure toward the shared-memory hierarchy. For typical data-intensive sensor processing workloads, the proposed core is, on average, 3.5× faster and 3.2× more energy efficient, thanks to a smart L0 buffer to reduce cache access contentions and support for compressed instructions. Single Instruction Multiple Data extensions, such as dot products, and a built-in L0 storage further reduce the shared-memory accesses by 8× reducing contentions by 3.2×. With four NT-optimized cores, the cluster is operational from 0.6 to 1.2 V, achieving a peak efficiency of 67 MOPS/mW in a low-cost 65-nm bulk CMOS technology. In a low-power 28-nm FD-SOI process, a peak efficiency of 193 MOPS/mW (40 MHz and 1 mW) can be achieved.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5TKEB9LZ\\Gautschi et al. - 2017 - Near-Threshold RISC-V Core With DSP Extensions for.pdf},
  langid = {english},
  number = {10}
}

@inproceedings{ghasemzadehModifiedPseudoLRU2006,
  title = {Modified Pseudo {{LRU}} Replacement Algorithm},
  booktitle = {13th {{Annual IEEE International Symposium}} and {{Workshop}} on {{Engineering}} of {{Computer}}-{{Based Systems}} ({{ECBS}}'06)},
  author = {Ghasemzadeh, H. and Mazrouee, S. and Kakoee, M. R.},
  date = {2006-03},
  pages = {6 pp.-376},
  doi = {10.1109/ECBS.2006.52},
  abstract = {Although the LRU replacement algorithm has been widely used in cache memory management, it is well-known for its inability to be easily implemented in hardware. Most of primary caches employ a simple block replacement algorithm like pseudo LRU to avoid the disadvantages of a complex hardware design. In this paper, we propose a novel block replacement scheme, MPLRU (modified pseudo LRU), by exploiting second chance concept in pseudo LRU algorithm. A comprehensive comparison is made between our algorithm and both true LRU and other conventional schemes such as FIFO, random and pseudo LRU. Experimental results show that MPLRU significantly reduces the number of cache misses compared to the other algorithms. Simulation results reveal that in average our algorithm can provide a value of 8.52\% improvement on the miss ratio compared to the pseudo LRU algorithm. Moreover, it provides 7.93\% and 11.57\%performance improvement compared to FIFO and random replacement policies respectively},
  eventtitle = {13th {{Annual IEEE International Symposium}} and {{Workshop}} on {{Engineering}} of {{Computer}}-{{Based Systems}} ({{ECBS}}'06)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UKBMBFKC\\Ghasemzadeh et al. - 2006 - Modified pseudo LRU replacement algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ENN8RYAP\\1607387.html},
  keywords = {Algorithm design and analysis,block replacement scheme,Cache memory,cache memory management,cache storage,Computational modeling,Costs,Delay,Engineering management,FIFO,first in first out algorithm,Hardware,High performance computing,least recently used algorithm,Memory management,modified pseudo LRU replacement algorithm,random LRU,Random number generation}
}

@inproceedings{ghasemzadehPseudoFIFOArchitectureLRU2005,
  title = {Pseudo-{{FIFO Architecture}} of {{LRU Replacement Algorithm}}},
  booktitle = {2005 {{Pakistan Section Multitopic Conference}}},
  author = {Ghasemzadeh, H. and Fatemi, S. O.},
  date = {2005-12},
  pages = {1--7},
  doi = {10.1109/INMIC.2005.334496},
  abstract = {Cache replacement algorithms have been widely used in modern computer systems to reduce the number of cache misses. The LRU algorithm has been shown to be an efficient replacement policy in terms of miss rates. However, most of the processors employ a block replacement algorithm which is very simple to implement in hardware or that is an approximation to the true LRU. In this paper, we propose a new implementation of block replacement algorithms in CPU caches by designing the circuitry required to implement an LRU replacement policy in set associative caches. We propose a simple and efficient architecture, Pseudo-FIFO, such that the true LRU replacement algorithm can be implemented without the disadvantages of the traditional implementations. Experimental results show that the Pseudo-FIFO significantly reduces the number of memory cells needed for hardware implementation. Simulation results reveal that our proposed architecture can provide an average value of 26\% improvement in the chip area compared to "reference matrix" and "basic architecture" circuits. Furthermore, it operates about 2.4 times faster than other architectures},
  eventtitle = {2005 {{Pakistan Section Multitopic Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HFKKRY28\\Ghasemzadeh and Fatemi - 2005 - Pseudo-FIFO Architecture of LRU Replacement Algori.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QVD44TI\\4133511.html},
  keywords = {Algorithm design and analysis,Approximation algorithms,basic architecture circuits,block replacement algorithm,Cache memory,cache misses,cache replacement algorithms,cache storage,Central Processing Unit,chip area,Circuit simulation,Computer architecture,first in first out,Hardware,High performance computing,least recently used",LRU replacement algorithm,memory cells,Modems,Optical computing,pseudo FIFO architecture,reference matrix,replacement policy,set associative caches}
}

@inproceedings{glassAdaptivePageReplacement1997,
  title = {Adaptive {{Page Replacement Based}} on {{Memory Reference Behavior}}},
  booktitle = {Proceedings of the 1997 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Glass, Gideon and Cao, Pei},
  date = {1997},
  pages = {115--126},
  publisher = {{ACM}},
  location = {{Seattle, Washington, USA}},
  doi = {10.1145/258612.258681},
  url = {http://doi.acm.org/10.1145/258612.258681},
  urldate = {2019-09-16},
  abstract = {As disk performance continues to lag behind that of memory systems and processors, virtual memory management becomes increasingly important for overall system performance. In this paper we study the page reference behavior of a collection of memory-intensive applications, and propose a new virtual memory page replacement algorithm, SEQ. SEQ detects long sequences of page faults and applies most-recently-used replacement to those sequences. Simulations show that for a large class of applications, SEQ performs close to the optimal replacement algorithm, and significantly better than Least-Recently-Used (LRU). In addition, SEQ performs similarly to LRU for applications that do not exhibit sequential faulting.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HVCBHFN3\\Glass and Cao - 1997 - Adaptive Page Replacement Based on Memory Referenc.pdf},
  isbn = {978-0-89791-909-8},
  series = {{{SIGMETRICS}} '97}
}

@article{gortFormalanalysisbasedTraceComputation2012,
  title = {Formal-Analysis-Based Trace Computation for Post-Silicon Debug},
  author = {Gort, M. and De, Paula and Kuan, J.J.W. and Aamodt, T.M. and Hu, A.J. and Wilton, S.J.E. and Yang, J.},
  date = {2012},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {20},
  pages = {1997--2010},
  doi = {10.1109/TVLSI.2011.2166416},
  abstract = {This paper presents a post-silicon debug methodology that provides a means to rewind, or backspace, a chip from a known crash state using a combination of on-chip real-time data collection and off-chip formal analysis methods. A complete debug flow is presented that considers practical considerations such as area, on-chip non-determinism and signal propagation delay. This flow, along with a low-overhead breakpoint circuit, allows for state-accurate breakpointing capabilities without the need to monitor the entire state of the chip. The flow and associated hardware was tested using a hardware prototype, which consists of an OpenRISC processor instrumented with the debug hardware connected to a PC running the formal verification algorithms. Traces hundreds of cycles long were obtained using the methodology presented in this paper. © 2011 IEEE.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EUFIZGBK\\display.html},
  keywords = {Formal analysis,hardware breakpoint,post-silicon debug,silicon debug,validation},
  number = {11}
}

@incollection{gramacyAdaptiveCachingRefetching2003,
  title = {Adaptive {{Caching}} by {{Refetching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  author = {Gramacy, Robert B and Warmuth, Manfred K. K and Brandt, Scott A. and Ari, Ismail},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  date = {2003},
  pages = {1489--1496},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2296-adaptive-caching-by-refetching.pdf},
  urldate = {2020-03-08},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RQLA4MCZ\\Gramacy et al. - 2003 - Adaptive Caching by Refetching.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KBZ9XU4J\\2296-adaptive-caching-by-refetching.html}
}

@inproceedings{guptaBundledExecutionRecurring2011,
  title = {Bundled Execution of Recurring Traces for Energy-Efficient General Purpose Processing},
  booktitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Gupta, S. and Feng, S. and Ansari, A. and Mahlke, S. and August, D.},
  date = {2011-12},
  pages = {12--23},
  abstract = {Technology scaling has delivered on its promises of increasing device density on a single chip. However, the voltage scaling trend has failed to keep up, introducing tight power constraints on manufactured parts. In such a scenario, there is a need to incorporate energy-efficient processing resources that can enable more computation within the same power budget. Energy efficiency solutions in the past have typically relied on application specific hardware and accelerators. Unfortunately, these approaches do not extend to general purpose applications due to their irregular and diverse code base. Towards this end, we propose BERET, an energy-efficient co-processor that can be configured to benefit a wide range of applications. Our approach identifies recurring instruction sequences as phases of “temporal regularity” in a program's execution, and maps suitable ones to the BERET hardware, a three-stage pipeline with a bundled execution model. This judicious off-loading of program execution to a reduced-complexity hardware demonstrates significant savings on instruction fetch, decode and register file accesses energy. On average, BERET reduces energy consumption by a factor of 3-4X for the program regions selected across a range of general-purpose and media applications. The average energy savings for the entire application run was 35\% over a single-issue in-order processor.},
  eventtitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XKG54E3V\\Gupta et al. - 2011 - Bundled execution of recurring traces for energy-e.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H4J9785B\\7851455.html},
  keywords = {Batteries,BERET hardware,Bridges,bundled execution model,Co-processor,Computer science,Coprocessors,Efficiency,Energy Saving,energy-efficient coprocessor,energy-efficient general purpose processing,Latches,Market research,Microarchitecture,Pipelines,power aware computing,program execution,Program processors,recurring instruction sequence,technology scaling,temporal regularity}
}

@article{guptaManagingBufferCache2012,
  title = {Managing {{Buffer Cache}} by {{Block Access Pattern}}},
  author = {Gupta, Reetu and Shrawankar, Urmila},
  date = {2012},
  volume = {9},
  pages = {7},
  abstract = {As buffer cache is used to overcome the speed gap between processor and storage devices, performance of buffer cache is a deciding factor in verifying the system performance. Need of improved buffer cache hit ratio and inabilities of the Least Recent Used replacement algorithm inspire the development of the proposed algorithm. Data reuse and program locality are the basis for determining the cache performance. The proposed algorithm determines the temporal locality by detecting the access patterns in the program context from which the I/O request are issued, identified by the program counter signature, and the files to which the I/O request are addressed. For accurate pattern detection and enhanced cache performance re-reference behavior exploited in the cache block are associated with unique signature. Use of multiple caching policies is supported by the proposed algorithm so that the cache under that pattern can be best utilized.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F4ESVHUJ\\Gupta and Shrawankar - 2012 - Managing Buffer Cache by Block Access Pattern.pdf},
  langid = {english},
  number = {6}
}

@inproceedings{guptaPreeminentPairReplacement2009,
  title = {Preeminent Pair of Replacement Algorithms for {{L1}} and {{L2}} Cache for Proxy Server},
  booktitle = {2009 {{First Asian Himalayas International Conference}} on {{Internet}}},
  author = {Gupta, R. and Tokekar, S.},
  date = {2009-11},
  pages = {1--5},
  doi = {10.1109/AHICI.2009.5340334},
  abstract = {Access to the internet and WWW is growing extensively, which results in heavy network traffic. To reduce the network traffic proxy server is used. Proxy server reduces the load of server. If the cache replacement algorithm of proxy server's cache is efficient then proxy server will be helpful to reduce the network traffic in more efficient manner. In this paper we are considering proxy server cache to be Level 1 (L1) cache and storage cache of proxy server to be Level 2 (L2) cache. For collecting the real trace CC proxy server is used in an organization. Log of proxy server gives the information of various URLs accessed by various clients with time. For performing experiments various URLs were given a numeric identity. This paper proposes an efficient replacement algorithm on L1 for proxy server. The replacement algorithms taken into consideration are Least Recently Used (LRU), Least Frequently Used (LFU), First In First Out (FIFO). Access Pattern of L1 and L2 are different as if the desired page is not on L1 then it accesses to L2. Thus L1 is having better temporal locality than L2. Thus the replacement algorithm which is giving efficient results for L1 may not be suitable for L2. This paper also proposes a preeminent pair of replacement algorithms for L1 and L2 cache for proxy server.},
  eventtitle = {2009 {{First Asian Himalayas International Conference}} on {{Internet}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XFCP27DN\\Gupta and Tokekar - 2009 - Preeminent pair of replacement algorithms for L1 a.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KF4B3I2S\\5340334.html},
  keywords = {cache replacement algorithm,cache storage,Cache storage,client,Computer science,Costs,Delay,heavy network traffic,Internet,IP networks,least frequently used algorithms,least recently used algorithms,Level 1 cache,Level 1 Cache (L1),Level 2 cache,Level 2 Cache (L2),network servers,Network servers,network traffic proxy server,Proxy server,proxy server cache,Replacement Algorithm,storage cache,Telecommunication traffic,Uniform resource locators,web access pattern,Web server,World Wide Web}
}

@inproceedings{gustafssonMalardalenWCETBenchmarks2010,
  title = {The {{Mälardalen WCET Benchmarks}} -- {{Past}}, {{Present}} and {{Future}}},
  booktitle = {Proceedings 10th {{International Workshop}} on {{Worst}}-{{Case Execution Time Analysis}} ({{WCET}}'2010)},
  author = {Gustafsson, Jan and Betts, Adam and Ermedahl, Andreas and Lisper, Björn},
  editor = {Lisper, Björn},
  date = {2010-07},
  pages = {137--147},
  publisher = {{OCG}},
  location = {{Brussels, Belgium}},
  eventtitle = {10th {{International Workshop}} on {{Worst}}-{{Case Execution Time Analysis}} ({{WCET}}'2010)}
}

@inproceedings{guTheoryPotentialLRUMRU2011,
  title = {On the {{Theory}} and {{Potential}} of {{LRU}}-{{MRU Collaborative Cache Management}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Management}}},
  author = {Gu, Xiaoming and Ding, Chen},
  date = {2011},
  pages = {43--54},
  publisher = {{ACM}},
  location = {{San Jose, California, USA}},
  doi = {10.1145/1993478.1993485},
  url = {http://doi.acm.org/10.1145/1993478.1993485},
  urldate = {2019-09-12},
  abstract = {The goal of cache management is to maximize data reuse. Collaborative caching provides an interface for software to communicate access information to hardware. In theory, it can obtain optimal cache performance. In this paper, we study a collaborative caching system that allows a program to choose different caching methods for its data. As an interface, it may be used in arbitrary ways, sometimes optimal but probably suboptimal most times and even counter productive. We develop a theoretical foundation for collaborative caches to show the inclusion principle and the existence of a distance metric we call LRU-MRU stack distance. The new stack distance is important for program analysis and transformation to target a hierarchical collaborative cache system rather than a single cache configuration. We use 10 benchmark programs to show that optimal caching may reduce the average miss ratio by 24\%, and a simple feedback-driven compilation technique can utilize collaborative cache to realize 50\% of the optimal improvement.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SXHBKDQ4\\Gu and Ding - 2011 - On the Theory and Potential of LRU-MRU Collaborati.pdf},
  isbn = {978-1-4503-0263-0},
  keywords = {bipartite cache,cache replacement algorithm,collaborative caching,lru,mru,opt},
  series = {{{ISMM}} '11}
}

@inproceedings{hadkeDesignEvaluationOptical2008,
  title = {Design and Evaluation of an Optical {{CPU}}-{{DRAM}} Interconnect},
  booktitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  author = {Hadke, A. and Benavides, T. and Amirtharajah, R. and Farrens, M. and Akella, V.},
  date = {2008-10},
  pages = {492--497},
  doi = {10.1109/ICCD.2008.4751906},
  abstract = {We present OCDIMM (Optically Connected DIMM), a CPU-DRAM interface that uses multiwavelength optical interconnects. We show that OCDIMM is more scalable and offers higher bandwidth and lower latency than FBDIMM (Fully-Buffered DIMM), a state-of-the-art electrical alternative. Though OCDIMM is more power efficient than FBDIMM, we show that ultimately the total power consumption in the memory subsystem is a key impediment to scalability and thus to achieving truly balanced computing systems in the terascale era.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DGPSV7V7\\Hadke et al. - 2008 - Design and evaluation of an optical CPU-DRAM inter.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SBIIRJH9\\4751906.html},
  keywords = {balanced computing,Bandwidth,Clocks,CPU-DRAM interface,Delay,DRAM chips,Energy consumption,Frequency,fully-buffered DIMM,memory subsystem,Multicore processing,Optical computing,optical CPU-DRAM interconnect,Optical design,optical interconnections,Optical interconnections,Scalability,total power consumption}
}

@inproceedings{hameedAdaptiveCacheManagement2013,
  title = {Adaptive Cache Management for a Combined {{SRAM}} and {{DRAM}} Cache Hierarchy for Multi-Cores},
  booktitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Hameed, F. and Bauer, L. and Henkel, J.},
  date = {2013-03},
  pages = {77--82},
  doi = {10.7873/DATE.2013.030},
  abstract = {On-chip DRAM caches may alleviate the memory bandwidth problem in future multi-core architectures through reducing off-chip accesses via increased cache capacity. For memory intensive applications, recent research has demonstrated the benefits of introducing high capacity on-chip L4-DRAM as Last-Level-Cache between L3-SRAM and off-chip memory. These multi-core cache hierarchies attempt to exploit the latency benefits of L3-SRAM and capacity benefits of L4-DRAM caches. However, not taking into consideration the cache access patterns of complex applications can cause inter-core DRAM interference and inter-core cache contention. In this paper, we contest to re-architect existing cache hierarchies by proposing a hybrid cache architecture, where the Last-Level-Cache is a combination of SRAM and DRAM caches. We propose an adaptive DRAM placement policy in response to the diverse requirements of complex applications with different cache access behaviors. It reduces inter-core DRAM interference and inter-core cache contention in SRAM/DRAM-based hybrid cache architectures: increasing the harmonic mean instruction-per-cycle throughput by 23.3\% (max. 56\%) and 13.3\% (max. 35.1\%) compared to state-of-the-art.},
  eventtitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I7JXN7A2\\Hameed et al. - 2013 - Adaptive cache management for a combined SRAM and .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FM2S7BRN\\6513476.html},
  keywords = {Arrays,Interference,Magnetic cores,Phase change random access memory,Radiation detectors,System-on-chip}
}

@inproceedings{hameedReducingLatencySRAM2014,
  title = {Reducing Latency in an {{SRAM}}/{{DRAM}} Cache Hierarchy via a Novel {{Tag}}-{{Cache}} Architecture},
  booktitle = {2014 51st {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Hameed, F. and Bauer, L. and Henkel, J.},
  date = {2014-06},
  pages = {1--6},
  doi = {10.1145/2593069.2593197},
  abstract = {Memory speed has become a major performance bottleneck as more and more cores are integrated on a multi-core chip. The widening latency gap between high speed cores and memory has led to the evolution of multi-level SRAM/DRAM cache hierarchies that exploit the latency benefits of smaller caches (e.g. private L1 and L2 SRAM caches) and the capacity benefits of larger caches (e.g. shared L3 SRAM and shared L4 DRAM cache). The main problem of employing large L3/L4 caches is their high tag lookup latency. To solve this problem, we introduce the novel concept of small and low latency SRAM/DRAM Tag-Cache structures that can quickly determine whether an access to the large L3/L4 caches will be a hit or a miss. The performance of the proposed Tag-Cache architecture depends upon the Tag-Cache hit rate and to improve it we propose a novel Tag-Cache insertion policy and a DRAM row buffer mapping policy that reduce the latency of memory requests. For a 16-core system, this improves the average harmonic mean instruction per cycle throughput of latency sensitive applications by 13.3\% compared to state-of-the-art.},
  eventtitle = {2014 51st {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9RFWJJ2X\\Hameed et al. - 2014 - Reducing latency in an SRAMDRAM cache hierarchy v.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H4LE6X34\\6881364.html},
  keywords = {Arrays,average harmonic mean instruction,cache storage,DRAM chips,DRAM row buffer mapping policy,high speed cores,high tag lookup latency,Monitoring,multi-core chip,Multicore processing,Organizations,private L1 SRAM caches,private L2 SRAM caches,Program processors,Radiation detectors,Random access memory,shared L3 SRAM cache,shared L4 DRAM cache,SRAM chips,SRAM/DRAM cache hierarchy,tag-cache architecture,tag-cache insertion policy,widening latency gap}
}

@inproceedings{hashemiAcceleratingDependentCache2016,
  title = {Accelerating {{Dependent Cache Misses}} with an {{Enhanced Memory Controller}}},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Hashemi, M. and {Khubaib} and Ebrahimi, E. and Mutlu, O. and Patt, Y. N.},
  date = {2016-06},
  pages = {444--455},
  doi = {10.1109/ISCA.2016.46},
  abstract = {On-chip contention increases memory access latency for multi-core processors. We identify that this additional latency has a substantial effect on performance for an important class of latency-critical memory operations: those that result in a cache miss and are dependent on data from a prior cache miss. We observe that the number of instructions between the first cache miss and its dependent cache miss is usually small. To minimize dependent cache miss latency, we propose adding just enough functionality to dynamically identify these instructions at the core and migrate them to the memory controller for execution as soon as source data arrives from DRAM. This migration allows memory requests issued by our new Enhanced Memory Controller (EMC) to experience a 20\% lower latency than if issued by the core. On a set of memory intensive quad-core workloads, the EMC results in a 13\% improvement in system performance and a 5\% reduction in energy consumption over a system with a Global History Buffer prefetcher, the highest performing prefetcher in our evaluation.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AKW5FWD2\\Hashemi et al. - 2016 - Accelerating Dependent Cache Misses with an Enhanc.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J5AFZFAB\\7551413.html},
  keywords = {Benchmark testing,cache miss latency,cache storage,Correlation,Delays,dependent cache misses,DRAM,DRAM chips,Electromagnetic compatibility,EMC,energy consumption,enhanced memory controller,global history buffer prefetcher,latency-critical memory operations,memory access latency,memory intensive quad-core workloads,memory requests,multicore processors,multiprocessing systems,on-chip contention,Prefetching,Random access memory,system performance,System-on-chip}
}

@inproceedings{hassanChargeCacheReducingDRAM2016,
  title = {{{ChargeCache}}: {{Reducing DRAM}} Latency by Exploiting Row Access Locality},
  shorttitle = {{{ChargeCache}}},
  booktitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Hassan, H. and Pekhimenko, G. and Vijaykumar, N. and Seshadri, V. and Lee, D. and Ergin, O. and Mutlu, O.},
  date = {2016-03},
  pages = {581--593},
  doi = {10.1109/HPCA.2016.7446096},
  abstract = {DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called ChargeCache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.},
  eventtitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CZS9BHC3\\Hassan et al. - 2016 - ChargeCache Reducing DRAM latency by exploiting r.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UMFE5G4A\\7446096.html},
  keywords = {cache storage,Capacitors,ChargeCache,Computer architecture,DRAM chips,DRAM latency reduction,DRAM request,Hardware,memory controller,multicore system,Parallel processing,row access locality,row address,single-core system,storage allocation,system performance bottleneck,Timing,timing parameters}
}

@inproceedings{hassanOffChipMemoryLatency2018,
  title = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}: {{Is DDR DRAM Really}} the {{Best Option}}?},
  shorttitle = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}},
  booktitle = {2018 {{IEEE Real}}-{{Time Systems Symposium}} ({{RTSS}})},
  author = {Hassan, M.},
  date = {2018-12},
  pages = {495--505},
  doi = {10.1109/RTSS.2018.00062},
  abstract = {Predictable execution time upon accessing shared memories in multi-core real-time systems is a stringent requirement. A plethora of existing works focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR DRAMs), or redesigning its memory to provide predictable memory behavior. In this paper, we show that DDR DRAMs by construction suffer inherent limitations associated with achieving such predictability. These limitations lead to 1) highly variable access latencies that fluctuate based on various factors such as access patterns and memory state from previous accesses, and 2) overly pessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some real-time systems that mandate a strict predictable performance with tight timing constraints. Targeting these systems, we promote an alternative off-chip memory solution that is based on the emerging Reduced Latency DRAM (RLDRAM) protocol, and propose a predictable memory controller (RLDC) managing accesses to this memory. Comparing with the state-of-the-art predictable DDR controllers, the proposed solution provides up to 11× less timing variability and 6.4× reduction in the worst case memory latency.},
  eventtitle = {2018 {{IEEE Real}}-{{Time Systems Symposium}} ({{RTSS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NAAUU4UL\\Hassan - 2018 - On the Off-Chip Memory Latency of Real-Time System.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7SPVFUU6\\8603238.html},
  keywords = {alternative off-chip memory solution,Data transfer,DDR DRAM,Delays,double data rate dynamic random access memories,DRAM,DRAM chips,Latency Analysis,Memory,multicore real-time systems,multiprocessing systems,off-chip memory latency,pessimistic latency bounds,Predictability,predictable execution time,predictable memory controller,Protocols,Random access memory,real-time systems,Real-time systems,Real-Time Systems,reduced latency DRAM protocol,RLDRAM protocol,state-of-the-art predictable DDR controllers,Task analysis,tight timing constraints,variable access latencies}
}

@inproceedings{hasslingerOptimumCachingLRU2018,
  title = {Optimum Caching versus {{LRU}} and {{LFU}}: {{Comparison}} and Combined Limited Look-Ahead Strategies},
  shorttitle = {Optimum Caching versus {{LRU}} and {{LFU}}},
  booktitle = {2018 16th {{International Symposium}} on {{Modeling}} and {{Optimization}} in {{Mobile}}, {{Ad Hoc}}, and {{Wireless Networks}} ({{WiOpt}})},
  author = {Hasslinger, G. and Heikkinen, J. and Ntougias, K. and Hasslinger, F. and Hohlfeld, O.},
  date = {2018-05},
  pages = {1--6},
  doi = {10.23919/WIOPT.2018.8362880},
  abstract = {We compare web caching strategies based on the least recently used (LRU) and the least frequently used (LFU) replacement principles with optimum caching according to Belady's algorithm. The achievable hit rates of the strategies are shown to improve with the exploited knowledge about the request pattern while the computation effort is also increasing. The results give an overview of performance tradeoffs in the whole relevant range for web caching with Zipf request pattern. In a second part, we study a combined approach of the optimum strategy for a limited look-ahead with LRU, LFU or other non-predictive methods. We evaluate the hit rate gain depending on the extent of the look-ahead for request traces and for the independent reference model (IRM) via simulation and derive an analytic confirmation of the observed behaviour. It is shown that caching for video streaming can benefit from the proposed look-ahead technique, when replacement decisions can be partly revised due to new requests being encountered during long lasting content updates.},
  eventtitle = {2018 16th {{International Symposium}} on {{Modeling}} and {{Optimization}} in {{Mobile}}, {{Ad Hoc}}, and {{Wireless Networks}} ({{WiOpt}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\Y9PCC3ME\\Hasslinger et al. - 2018 - Optimum caching versus LRU and LFU Comparison and.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ACDTG5HE\\8362880.html},
  keywords = {achievable hit rates,Analytical models,Belady algorithm,Belady's algorithm,Benchmark testing,cache storage,computation effort,Delays,exploited knowledge,hit rate,hit rate gain,Indexes,Internet,least frequently used (LFU),least frequently used replacement principles,least recently used (LRU),least recently used replacement principles,LFU,look-ahead strategies,look-ahead technique,LRU,Markov processes,optimum caching,optimum strategy,request traces,Shape,simulation,Streaming media,Web cache strategies,web caching strategies,Zipf distributed requests,Zipf request pattern}
}

@book{hennessyComputerArchitectureQuantitative2019,
  title = {Computer {{Architecture}}: A {{Quantitative Approach}}},
  shorttitle = {Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  date = {2019},
  edition = {6th},
  publisher = {{Morgan Kaufmann Publishers}},
  location = {{Cambridge, MA}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JV6VPGZ7\\Hennessy - Computer Architecture A Quantitative Approach.pdf},
  isbn = {978-0-12-811905-1},
  keywords = {Computer architecture},
  pagetotal = {1}
}

@inproceedings{hochbergerAcquiringExhaustiveContinuous2008,
  title = {Acquiring an Exhaustive, Continuous and Real-Time Trace from {{SoCs}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  author = {Hochberger, C. and Weiss, A.},
  date = {2008-10},
  pages = {356--362},
  doi = {10.1109/ICCD.2008.4751885},
  abstract = {The amount of time and resources that have to be spent on debugging of embedded cores continuously increases. Approaches valid 10 years ago can no longer be used due to the variety and complexity of peripheral components of SoC solutions that even might consist of multiple heterogeneous cores. Although there are some initiatives to standardize and leverage the embedded debugging capabilities, current debugging solutions only cover a fraction of the problems present in that area. In this contribution we show a new approach for debugging and tracing SoCs. The new approach, called hidICE (hidden ICE), delivers an exhaustive, continuous and real-time trace with much lower system interference compared to state-of-the-art solutions.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GSGD5DGN\\Hochberger and Weiss - 2008 - Acquiring an exhaustive, continuous and real-time .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WF3RSJRU\\4751885.html},
  keywords = {Central Processing Unit,continuous trace,debugging,embedded cores debugging,embedded debugging capabilities,Embedded software,embedded systems,exhaustive trace,hidden ICE,hidICE,Ice,IEC standards,integrated circuit testing,Interference,ISO standards,multiple heterogeneous cores,peripheral components,Railway safety,Real time systems,real-time trace,Registers,SoC,System-on-Chip}
}

@inproceedings{huangATCacheReducingDRAM2014,
  title = {{{ATCache}}: {{Reducing DRAM}} Cache Latency via a Small {{SRAM}} Tag Cache},
  shorttitle = {{{ATCache}}},
  booktitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Huang, C. and Nagarajan, V.},
  date = {2014-08},
  pages = {51--60},
  doi = {10.1145/2628071.2628089},
  abstract = {3D-stacking technology has enabled the option of embedding a large DRAM onto the processor. Prior works have proposed to use this as a DRAM cache. Because of its large size (a DRAM cache can be in the order of hundreds of megabytes), the total size of the tags associated with it can also be quite large (in the order of tens of megabytes). The large size of the tags has created a problem. Should we maintain the tags in the DRAM and pay the cost of a costly tag access in the critical path? Or should we maintain the tags in the faster SRAM by paying the area cost of a large SRAM for this purpose? Prior works have primarily chosen the former and proposed a variety of techniques for reducing the cost of a DRAM tag access. In this paper, we first establish (with the help of a study) that maintaining the tags in SRAM, because of its smaller access latency, leads to overall better performance. Motivated by this study, we ask if it is possible to maintain tags in SRAM without incurring high area overhead. Our key idea is simple. We propose to cache the tags in a small SRAM tag cache - we show that there is enough spatial and temporal locality amongst tag accesses to merit this idea. We propose the ATCache which is a small SRAM tag cache. Similar to a conventional cache, the ATCache caches recently accessed tags to exploit temporal locality; it exploits spatial locality by prefetching tags from nearby cache sets. In order to avoid the high miss latency and cache pollution caused by excessive prefetching, we use a simple technique to throttle the number of sets prefetched. Our proposed ATCache (which consumes 0.4\% of overall tag size) can satisfy over 60\% of DRAM cache tag accesses on average.},
  eventtitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5JV8LE9V\\Huang and Nagarajan - 2014 - ATCache Reducing DRAM cache latency via a small S.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SIJII4GP\\7855888.html},
  keywords = {3D-stacking technology,ATCache,cache pollution,cache storage,Compounds,Design,DRAM cache,DRAM cache latency,DRAM chips,Media,Performance,Pollution,Prefetching,prefetching tags,Random access memory,Servers,small SRAM tag cache,SRAM chips,storage management,Systems architecture}
}

@inproceedings{huApplicationsOnchipTrace2007,
  title = {Applications of {{On}}-Chip {{Trace}} on {{Debugging Embedded Processor}}},
  booktitle = {Eighth {{ACIS International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}}, and {{Parallel}}/{{Distributed Computing}} ({{SNPD}} 2007)},
  author = {Hu, X. and Chen, S.},
  date = {2007-07},
  volume = {1},
  pages = {140--145},
  doi = {10.1109/SNPD.2007.227},
  abstract = {On-chip trace system records run-time information of the embedded processor with special hardware to overcome the obstacle of non-intrusive debug and optimization of traditional techniques. This paper explores the mechanism, characteristics and applications of on-chip trace technique with TraceDo, an on-chip trace system of a multi-core SoC. The functions and structures of TraceDo are introduced, the working process of path trace is explained and the trace applications with two cases of debug and optimization are also discussed.},
  eventtitle = {Eighth {{ACIS International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}}, and {{Parallel}}/{{Distributed Computing}} ({{SNPD}} 2007)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QQYXBLJF\\Hu and Chen - 2007 - Applications of On-chip Trace on Debugging Embedde.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9PGPS2UP\\4287490.html},
  keywords = {Application software,Concurrent computing,debugging embedded processor,Distributed computing,embedded system,Embedded system,embedded systems,Hardware,microprocessor chips,multicore SoC,Network-on-a-chip,on-chip trace system,performance evaluation,Real time systems,Software debugging,software reliability,Software tools,System-on-a-chip,TraceDo}
}

@inproceedings{huImplementationOnChipDynamic2006,
  title = {Implementation of {{On}}-{{Chip Dynamic Trace}} for {{Microprocessors}}},
  booktitle = {2006 8th {{International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology Proceedings}}},
  author = {Hu, Y. and Xiong, B.},
  date = {2006-10},
  pages = {1429--1431},
  doi = {10.1109/ICSICT.2006.306226},
  abstract = {This paper presents an on-chip dynamic trace design method which is very useful for the real-time dynamic trace of the program executing process of embedded microprocessors. We introduce an on-chip tracer (OCT) module to fulfill this dynamic tracing task by setting watchpoints at some locations of user program in order to trace and output the information on these spots. As a result, debugger can find out program errors and monitor or reconstruct the executing route of user program. This OCT module needs only a few registers rather than a special on-chip trace buffer (OCTB) which is an essential component for traditional trace methods. Finally, the results of simulation prove that the on-chip dynamic trace debugging is accomplishable with the help of this OCT module},
  eventtitle = {2006 8th {{International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology Proceedings}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9FIZJEG8\\Hu and Xiong - 2006 - Implementation of On-Chip Dynamic Trace for Microp.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QI364PK\\4098430.html},
  keywords = {Debugging,Design methodology,dynamic tracing,embedded microprocessors,embedded systems,Information analysis,Laboratories,microprocessor chips,Microprocessors,Monitoring,on-chip trace buffer,on-chip tracer,Power generation,program debugging,Registers,Signal processing,System-on-a-chip,user program}
}

@inproceedings{jainBackFutureLeveraging2016,
  title = {Back to the {{Future}}: {{Leveraging Belady}}'s {{Algorithm}} for {{Improved Cache Replacement}}},
  shorttitle = {Back to the {{Future}}},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Jain, A. and Lin, C.},
  date = {2016-06},
  pages = {78--89},
  doi = {10.1109/ISCA.2016.17},
  abstract = {Belady's algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady's algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efficient, as we introduce a new method of efficiently simulating Belady's behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4\%, as opposed to 6.2\% for the previous state-of-the-art. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0\%, compared to 12.0\% for the previous state-of-the-art.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KG9345TE\\Jain and Lin - 2016 - Back to the Future Leveraging Belady's Algorithm .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XIN4JXM5\\7551384.html},
  keywords = {4-core system,Art,Belady algorithm,Belady behavior,Belady's Algorithm,Benchmark testing,cache accesses,cache replacement,Cache replacement,cache storage,Hardware,History,LLC,Marine vehicles,memory-intensive subset,Optimized production technology,Prediction algorithms,replacement state,sampling techniques,tag array}
}

@inproceedings{jainSoftwareassistedCacheReplacement2001,
  title = {Software-Assisted Cache Replacement Mechanisms for Embedded Systems},
  author = {Jain, P. and Devadas, S. and Engels, D. and Rudolph, L.},
  date = {2001},
  pages = {119--126},
  publisher = {{IEEE}},
  doi = {10.1109/ICCAD.2001.968607},
  url = {http://ieeexplore.ieee.org/document/968607/},
  urldate = {2019-08-05},
  abstract = {We address the problem of improving cache predictability and performance in embedded systems through the use of softwareassisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e., effectively make the element the least recently used element, or keep that cache element, i.e., the element will never be evicted.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VSPKXH5W\\Jain et al. - 2001 - Software-assisted cache replacement mechanisms for.pdf},
  isbn = {978-0-7803-7247-4},
  langid = {english}
}

@inproceedings{jainSoftwareassistedCacheReplacement2001a,
  title = {Software-Assisted Cache Replacement Mechanisms for Embedded Systems},
  booktitle = {{{IEEE}}/{{ACM International Conference}} on {{Computer Aided Design}}. {{ICCAD}} 2001. {{IEEE}}/{{ACM Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{01CH37281}})},
  author = {Jain, P. and Devadas, S. and Engels, D. and Rudolph, L.},
  date = {2001-11},
  pages = {119--126},
  doi = {10.1109/ICCAD.2001.968607},
  abstract = {We address the problem of improving cache predictability and performance in embedded systems through the use of software-assisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e. effectively make the element the least recently used element, or keep that cache element, i.e. the element will never be evicted. We prove basic theorems that provide conditions under which kill and keep instructions can be inserted into program code, such that the resulting performance is guaranteed to be as good as or better than the original program run using the standard LRU policy. We developed a compiler algorithm based on the theoretical results that, given an arbitrary program, determines when to perform software-assisted replacement, i.e., when to insert either a kill or keep instruction. Empirical evidence is provided that shows that performance and predictability (worst-case performance) can be improved for many programs.},
  eventtitle = {{{IEEE}}/{{ACM International Conference}} on {{Computer Aided Design}}. {{ICCAD}} 2001. {{IEEE}}/{{ACM Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{01CH37281}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9WPFYKQR\\Jain et al. - 2001 - Software-assisted cache replacement mechanisms for.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X9AT93S3\\968607.html},
  keywords = {cache element,cache element keep instructions,cache element kill instructions,cache performance,cache predictability,cache replacement decision,cache storage,Code standards,compiler algorithm,Computer science,Embedded software,Embedded system,embedded systems,Hardware,Laboratories,LRU policy,program code,program compilers,Program processors,Random access memory,software controlled state information,software instructions,Software performance,software-assisted cache replacement mechanisms,software-assisted replacement,software-assisted replacement mechanisms,storage management,System-on-a-chip,worst-case performance}
}

@inproceedings{jaleelHighPerformanceCache2010,
  title = {High Performance Cache Replacement Using Re-Reference Interval Prediction ({{RRIP}})},
  booktitle = {Proceedings of the 37th Annual International Symposium on {{Computer}} Architecture - {{ISCA}} '10},
  author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
  date = {2010},
  pages = {60},
  publisher = {{ACM Press}},
  location = {{Saint-Malo, France}},
  doi = {10.1145/1815961.1815971},
  url = {http://portal.acm.org/citation.cfm?doid=1815961.1815971},
  urldate = {2020-01-18},
  eventtitle = {The 37th Annual International Symposium},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R2TMJIAW\\Jaleel et al. - 2010 - High performance cache replacement using re-refere.pdf},
  isbn = {978-1-4503-0053-7},
  langid = {english}
}

@inproceedings{jeongCostsensitiveCacheReplacement2003,
  title = {Cost-Sensitive Cache Replacement Algorithms},
  author = {Jeong, J. and Dubois, M.},
  date = {2003},
  pages = {327--337},
  publisher = {{IEEE Comput. Soc}},
  doi = {10.1109/HPCA.2003.1183550},
  url = {http://ieeexplore.ieee.org/document/1183550/},
  urldate = {2020-01-04},
  abstract = {Cache replacement algorithms originally developed in the context of simple uniprocessor systems aim to reduce the miss count. However, in modern systems, cache misses have different costs. The cost may be latency, penalty, power consumption, bandwidth consumption, or any other ad-hoc numerical property attached to a miss. In many practical situations, it is desirable to inject the cost of a miss into the replacement policy.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NADWS3CC\\Jeong and Dubois - 2003 - Cost-sensitive cache replacement algorithms.pdf},
  isbn = {978-0-7695-1871-8},
  langid = {english}
}

@inproceedings{jeongOptimalReplacementsCaches1999,
  title = {Optimal {{Replacements}} in {{Caches}} with {{Two Miss Costs}}},
  booktitle = {Proceedings of the {{Eleventh Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  author = {Jeong, Jaeheon and Dubois, Michel},
  date = {1999},
  pages = {155--164},
  publisher = {{ACM}},
  location = {{Saint Malo, France}},
  doi = {10.1145/305619.305636},
  url = {http://doi.acm.org/10.1145/305619.305636},
  urldate = {2019-08-02},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XZBS3D8B\\Jeong and Dubois - 1999 - Optimal Replacements in Caches with Two Miss Costs.pdf},
  isbn = {978-1-58113-124-6},
  series = {{{SPAA}} '99}
}

@inproceedings{jevdjicUnisonCacheScalable2014,
  title = {Unison {{Cache}}: {{A Scalable}} and {{Effective Die}}-{{Stacked DRAM Cache}}},
  shorttitle = {Unison {{Cache}}},
  booktitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Jevdjic, D. and Loh, G. H. and Kaynak, C. and Falsafi, B.},
  date = {2014-12},
  pages = {25--37},
  doi = {10.1109/MICRO.2014.51},
  abstract = {Recent research advocates large die-stacked DRAM caches in many core servers to break the memory latency and bandwidth wall. To realize their full potential, die-stacked DRAM caches necessitate low lookup latencies, high hit rates and the efficient use of off-chip bandwidth. Today's stacked DRAM cache designs fall into two categories based on the granularity at which they manage data: block-based and page-based. The state-of-the-art block-based design, called Alloy Cache, collocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches. We introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache's approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14\% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.},
  eventtitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XUD7AKME\\Jevdjic et al. - 2014 - Unison Cache A Scalable and Effective Die-Stacked.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YZW8T95W\\7011375.html},
  keywords = {3D die stacking,Alloy Cache approach,Bandwidth,bandwidth wall,block-based data management,cache storage,caches,die-stacked DRAM cache,DRAM,DRAM access,DRAM chips,footprint cache design,lookup latencies,manycore servers,memory,memory latency,Metals,multigigabyte stacked DRAM caches,off-chip bandwidth,off-chip traffic,on-chip tag storage,Organizations,page granularity,page-based data management,page-based DRAM caches,page-sized cache allocation units,paged storage,Random access memory,Resource management,server workloads,servers,Servers,SRAM chips,SRAM-based tags,stacked-DRAM capacities,System-on-chip,Unison cache}
}

@inproceedings{jiangLIRSEfficientLow2002,
  title = {{{LIRS}}: An Efficient Low Inter-Reference Recency Set Replacement Policy to Improve Buffer Cache Performance},
  shorttitle = {{{LIRS}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGMETRICS}} International Conference on {{Measurement}} and Modeling of Computer Systems  - {{SIGMETRICS}} '02},
  author = {Jiang, Song and Zhang, Xiaodong},
  date = {2002},
  pages = {31},
  publisher = {{ACM Press}},
  location = {{Marina Del Rey, California}},
  doi = {10.1145/511334.511340},
  url = {http://portal.acm.org/citation.cfm?doid=511334.511340},
  urldate = {2020-01-18},
  eventtitle = {The 2002 {{ACM SIGMETRICS}} International Conference},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EIQV69FC\\Jiang and Zhang - LIRS An Efﬁcient Low Inter-reference Recency Set .pdf},
  isbn = {978-1-58113-531-2},
  langid = {english}
}

@inproceedings{jinhyukyoonBufferCacheManagement2002,
  title = {Buffer Cache Management: Predicting the Future from the Past},
  shorttitle = {Buffer Cache Management},
  booktitle = {Proceedings {{International Symposium}} on {{Parallel Architectures}}, {{Algorithms}} and {{Networks}}. {{I}}-{{SPAN}}'02},
  author = {{Jinhyuk Yoon} and {Sang Lyul Min} and {Yookun Cho}},
  date = {2002-05},
  pages = {105--110},
  doi = {10.1109/ISPAN.2002.1004268},
  abstract = {Efficient and effective management of the buffer cache in the operating system becomes increasingly important as the speed gap between microprocessors and hard disks becomes wider This paper presents different techniques for predicting the future disk access patterns from the access history of each block and the access patterns detected for related blocks. The first part of the paper focuses on a block replacement policy called LRFU (least recently/frequently used) that subsumes the well-known LRU (least recently used) and the LFU (least frequently used) policies. Then, the next part discusses techniques for handling regular references such as sequential and looping references. Finally, the results from both trace-driven simulations and our implementation of the techniques within a real operating system are presented.},
  eventtitle = {Proceedings {{International Symposium}} on {{Parallel Architectures}}, {{Algorithms}} and {{Networks}}. {{I}}-{{SPAN}}'02},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6X9P9FTS\\Jinhyuk Yoon et al. - 2002 - Buffer cache management predicting the future fro.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\26CR2K4N\\1004268.html},
  keywords = {access history,block replacement policy,buffer cache management,cache storage,Computer science,Engineering management,File systems,Frequency,future disk access pattern prediction,Hard disks,History,least recently/frequently used policies,looping references,Microprocessors,operating system,Operating systems,operating systems (computers),Pattern analysis,Pattern recognition,regular reference handling,sequential references,trace-driven simulations}
}

@inproceedings{johnson2QLowOverhead1994,
  title = {{{2Q}}: {{A Low Overhead High Performance Buffer Management Replacement Algorithm}}},
  shorttitle = {{{2Q}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Very Large Data Bases}}},
  author = {Johnson, Theodore and Shasha, Dennis},
  date = {1994-09-12},
  pages = {439--450},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  location = {{San Francisco, CA, USA}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AFJKYPES\\Johnson et al. - Q A Low Overhead High Performance Buffer Manageme.pdf},
  isbn = {978-1-55860-153-6},
  series = {{{VLDB}} '94}
}

@article{jongmoochoiDesignImplementationPerformance2002,
  title = {Design, Implementation, and Performance Evaluation of a Detection-Based Adaptive Block Replacement Scheme},
  author = {{Jongmoo Choi} and Noh, S.H. and {Sang Lyul Min} and {Eun-Yong Ha} and {Yookun Cho}},
  date = {2002-07},
  journaltitle = {IEEE Transactions on Computers},
  volume = {51},
  pages = {793--800},
  issn = {0018-9340},
  doi = {10.1109/TC.2002.1017699},
  url = {http://ieeexplore.ieee.org/document/1017699/},
  urldate = {2020-03-08},
  abstract = {ÐA new buffer replacement scheme, called DEAR (DEtection-based Adaptive Replacement), is presented for effective caching of disk blocks in the operating system. The proposed DEAR scheme automatically detects block reference patterns of applications and applies different replacement policies to different applications depending on the detected reference pattern. The detection is made by a periodic process and is based on the relationship between block attribute values, such as backward distance and frequency gathered in a period, and the forward distance observed in the next period. This paper also describes an implementation and performance measurement of the DEAR scheme in FreeBSD. The results from performance measurements of several real applications show that, compared with the LRU scheme, the proposed scheme reduces the number of disk I/Os by up to 51 percent (with an average of 23 percent) and the response time by up to 35 percent (with an average of 12 percent) in the case of single application executions. For multiple application executions, the results show that the proposed scheme reduces the number of disk I/Os by up to 20 percent (with an average of 12 percent) and the overall response time by up to 18 percent (with an average of 8 percent).},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LWSRAZEX\\Jongmoo Choi et al. - 2002 - Design, implementation, and performance evaluation.pdf},
  langid = {english},
  number = {7}
}

@inproceedings{juanImprovedMulticoreShared2012,
  title = {An {{Improved Multi}}-Core {{Shared Cache Replacement Algorithm}}},
  booktitle = {2012 11th {{International Symposium}} on {{Distributed Computing}} and {{Applications}} to {{Business}}, {{Engineering Science}}},
  author = {Juan, F. and Chengyan, L.},
  date = {2012-10},
  pages = {13--17},
  doi = {10.1109/DCABES.2012.39},
  abstract = {Many multi-core processors employ a large last-level cache (LLC) shared among the multiple cores. Past research has demonstrated that traditional LRU and its approximation can lead to poor performance and unfairness when the multiple cores compete for the limited LLC capacity, and is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. As the LLC grows in capacity, associativity, the performance gap between the LRU and the theoretical optimal replacement algorithms has widened. In this paper, we propose FLRU (Frequency based LRU) replacement algorithm, which is applied to multi-core shared L2 cache, and it takes the recent access information, partition and the frequency information into consideration. FLRU manages to filter the less reused blocks through dynamic insertion/promotion policy and victim selection strategy to ensure that some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits and to avoid trashing, meanwhile we augment traditional cache partition with victim selection, insertion and promotion policies to manage shared L2 caches.},
  eventtitle = {2012 11th {{International Symposium}} on {{Distributed Computing}} and {{Applications}} to {{Business}}, {{Engineering Science}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\88TYZ4JD\\Juan and Chengyan - 2012 - An Improved Multi-core Shared Cache Replacement Al.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GPBN84E6\\6385229.html},
  keywords = {access information,cache storage,Computers,dynamic insertion-promotion policy,Educational institutions,FLRU,frequency based LRU replacement algorithm,frequency information,last-level cache,LLC,memory-intensive workloads,multi-core,Multicore processing,multicore processors,multicore shared cache replacement algorithm,multicore shared L2 cache,multiprocessing systems,partition information,Partitioning algorithms,Radiation detectors,replacement,Runtime,shared cache,USA Councils,victim selection strategy}
}

@article{jungCacheReplacementPolicy2013,
  title = {Cache Replacement Policy Based on Dynamic Counter for High Performance Processor},
  author = {Jung, Do Young and Lee, Yong Surk},
  date = {2013},
  journaltitle = {Journal of the Institute of Electronics and Information Engineers},
  volume = {50},
  pages = {52--58},
  issn = {2287-5026},
  doi = {10.5573/ieek.2013.50.4.052},
  url = {http://www.koreascience.or.kr/article/JAKO201315463253488.page},
  urldate = {2019-09-12},
  abstract = {Cache Replacement Policy Based on Dynamic Counter for High Performance Processor Cache memory;Cache replacement policy;LRU;Zero reuse line;Counter based policy; Replacement policy is one of the key factors determining the effectiveness of a cache. The LRU replacement policy has remained the standard for caches for many years. However, the traditional LRU has ineffective performance in zero-reuse line intensive workloads, although it performs well in high temporal locality workloads. To address this problem, We propose a new replacement policy; DCR(Dynamic Counter based Replacement) policy. A temporal locality of workload dynamically changes across time and DCR policy is based on the detection of these changing. DCR policy improves cache miss rate over a traditional LRU policy, by as much as 2.7\% at maximum and 0.47\% at average.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\B8DYCACE\\Jung and Lee - 2013 - Cache Replacement Policy Based on Dynamic Counter .pdf},
  langid = {kor},
  number = {4}
}

@inproceedings{kampeSelfCorrectingLRUReplacement2004,
  title = {Self-{{Correcting LRU Replacement Policies}}},
  booktitle = {In {{Proceedings}} of the 1st {{Conference}} on {{Computing Frontiers}}},
  author = {Kampe, Martin and Stenstrom, Per and Dubois, Michel},
  date = {2004},
  pages = {181--191},
  abstract = {With wider associativity the replacement algorithm becomes critical. Although LRU makes many good replacement decisions, the wide performance gap between OPT, the optimum off-line algorithm, and LRU suggests that LRU still makes too many mistakes. Self-correcting},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7TKWKSHY\\Kampe et al. - 2004 - Self-Correcting LRU Replacement Policies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GKRS54DW\\summary.html}
}

@article{karedlaCachingStrategiesImprove1994,
  title = {Caching Strategies to Improve Disk System Performance},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  date = {1994-03},
  journaltitle = {Computer},
  volume = {27},
  pages = {38--46},
  issn = {0018-9162},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KLR6PJKM\\Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ETYTQD2H\\268884.html},
  keywords = {Algorithm design and analysis,buffer storage,cache replacement algorithms,caching strategies,computer system hierarchy,Control systems,Costs,data throughput,Delay,disk rotation speeds,disk system performance,Drives,frequency-based variation,Hardware,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,latency,least recently used algorithm,magnetic disc storage,Operating systems,performance evaluation,random replacement,segmented LRU,solid-state disks,System performance,system response time,Terminology,Throughput},
  number = {3}
}

@article{karedlaCachingStrategiesImprove1994a,
  title = {Caching Strategies to Improve Disk System Performance},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  date = {1994-03},
  journaltitle = {Computer},
  volume = {27},
  pages = {38--46},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\MWSHVCFE\\Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FBWVQRK9\\268884.html},
  keywords = {Algorithm design and analysis,buffer storage,cache replacement algorithms,caching strategies,computer system hierarchy,Control systems,Costs,data throughput,Delay,disk rotation speeds,disk system performance,Drives,frequency-based variation,Hardware,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,latency,least recently used algorithm,magnetic disc storage,Operating systems,performance evaluation,random replacement,segmented LRU,solid-state disks,System performance,system response time,Terminology,Throughput},
  number = {3}
}

@report{kegleyPredictiveCacheModeling2011,
  title = {Predictive {{Cache Modeling}} and {{Analysis}}},
  author = {Kegley, Russell and Preston, Jonathan and Dougherty, Brian and White, Jules and Gokhale, Anirudda},
  date = {2011-11},
  institution = {{LOCKHEED MARTIN AERONAUTICS CO FORT WORTH TX}},
  url = {https://apps.dtic.mil/docs/citations/ADA552968},
  urldate = {2019-09-16},
  abstract = {This work applied particle swarm heuristic optimization techniques to the problem of finding a near-optimal order in which to schedule tasks in a real-time embedded system in order to minimize cache miss rates experienced by the software. Reducing the number of cache misses is an important component of runtime execution efficiency. We demonstrated runtime reductions of 3-5\% in execution time, significant for embedded systems attempting to add new capability without upgrading hardware. The expectation is that these gains can be improved further by the use of hardware with pseudo-LRU cache behavior.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M5W66TST\\Kegley et al. - 2011 - Predictive Cache Modeling and Analysis.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W6K7EAKC\\ADA552968.html},
  langid = {english}
}

@article{kellyVariableQosShared1999,
  title = {Variable {{Qos}} from {{Shared Web Caches}}: {{User}}-{{Centered Design}} and {{Value}}-{{Sensitive Replacement}}},
  shorttitle = {Variable {{Qos}} from {{Shared Web Caches}}},
  author = {Kelly, Terence and Jamin, Sugih and MacKie-Mason, Jeffrey K.},
  date = {1999},
  journaltitle = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.975737},
  url = {http://www.ssrn.com/abstract=975737},
  urldate = {2019-11-28},
  abstract = {Due to differences in server capacity, external bandwidth, and client demand, some Web servers value cache hits more than others. Assuming that a shared cache knows the extent to which different servers value hits, it may employ a value-sensitive replacement policy in order to generate higher aggregate value for servers. We consider both the prediction and value aspects of this problem and introduce a novel value-sensitive LFU/LRU hybrid that biases the allocation of cache space toward documents whose origin servers value caching most highly. We compare our algorithm with others from the Web caching literature and discuss from an economic standpoint the problems associated with obtaining servers’ private valuation information.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RQ8TT62R\\Kelly et al. - 1999 - Variable Qos from Shared Web Caches User-Centered.pdf},
  langid = {english}
}

@inproceedings{kelwadeReputationBasedCache2017,
  title = {Reputation Based Cache Management Policy for Performance Improvement},
  booktitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  author = {Kelwade, K. and Sahu, S. and Kawade, G. and Korde, N. and Upadhye, S. and Motghare, M.},
  date = {2017-12},
  pages = {582--587},
  doi = {10.1109/ISS1.2017.8389236},
  abstract = {As the performance gap between CPU and main memory speed increases, memory subsystem design more critical. Caches are important part of modern memory hierarchies. Cache performance is important in computer system. As the cache size is limited it should be properly utilized. After the cache block is used by an application it should be moved from cache to main memory so that another block required by other application can use its space [2]. Now a day's multi-level cache are used everywhere to provide better cache performance. As the block which is moved from cache memory to main memory may be required in future, so it is kept in lower level cache in the cache hierarchy. Therefore the cost of accessing the block from main memory is minimized. Many approaches like LRU-K, LFU, LRFU, PROMOTE, DEMOTE are used to increase cache performance by promoting or demoting a memory block into the cache depending upon its latest history [1] [21]. Some are used for single level cache and some for hierarchical cache. The drawback of existing multi-level cache management techniques is that it uses hints of level L1 only. It does not provide sufficient information about the history of the cache block at all previous cache levels. Therefore, a hybrid cache management policy is proposed in this paper which takes its replacement decision based on the multiple level cache level. In this paper, a hybrid cache management policy is proposed which constitute the feature of two existing algorithms PEOMOTE and DEMTE that gives better hit ratio than other existing policies.},
  eventtitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XXVPEPZ4\\Kelwade et al. - 2017 - Reputation based cache management policy for perfo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4CQIK3WJ\\8389236.html},
  keywords = {cache memory,Cache performance,cache storage,Conferences,demote,DEMTE algorithms,hierarchical cache,hierarchical systems,hints,hybrid cache management policy,memory block,memory subsystem design,modern memory hierarchies,Multi-level cache,multilevel cache management,multiple level cache level,PEOMOTE algorithms,promote,recency,reputation based cache management policy,single level cache,storage management}
}

@inproceedings{keramidasCacheReplacementBased2007,
  title = {Cache Replacement Based on Reuse-Distance Prediction},
  author = {Keramidas, Georgios and Petoumenos, Pavlos and Kaxiras, Stefanos},
  date = {2007-10},
  pages = {245--250},
  publisher = {{IEEE}},
  doi = {10.1109/ICCD.2007.4601909},
  url = {http://ieeexplore.ieee.org/document/4601909/},
  urldate = {2020-03-01},
  abstract = {Several cache management techniques have been proposed that indirectly try to base their decisions on cacheline reuse-distance, like Cache Decay which is a postdiction of reuse-distances: if a cacheline has not been accessed for some "decay interval" we know that its reuse-distance is at least as large as this decay interval. In this work, we propose to directly predict reuse-distances via instruction-based (PC) prediction and use this information for cache level optimizations. In this paper, we choose as our target for optimization the replacement policy of the L2 cache, because the gap between the LRU and the theoretical optimal replacement algorithm is comparatively large for L2 caches. This indicates that, in many situations, there is ample room for improvement. We evaluate our reusedistance based replacement policy using a subset of the most memory intensive SPEC2000 and our results show significant benefits across the board.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9XVX79PW\\Keramidas et al. - 2007 - Cache replacement based on reuse-distance predicti.pdf},
  isbn = {978-1-4244-1257-0},
  langid = {english}
}

@inproceedings{khalidPerformanceEvaluationNew1996,
  title = {Performance Evaluation of a New Cache Replacement Scheme Using {{SPEC}}},
  booktitle = {Conference {{Proceedings}} of the 1996 {{IEEE Fifteenth Annual International Phoenix Conference}} on {{Computers}} and {{Communications}}},
  author = {Khalid, H. and Obaidat, M. S.},
  date = {1996-03},
  pages = {144--150},
  doi = {10.1109/PCCC.1996.493626},
  abstract = {Presents a new neural network-based algorithm called KORA (Khalid-Obaidat Replacement Algorithm), that uses a backpropagation neural network (BPNN) for the purpose of guiding the line/block replacement decisions in a cache. The KORA algorithm attempts to approximate the replacement decisions made by the optimal scheme (OPT). The key to our algorithm is to identify and subsequently discard the dead lines in cache memories. This allows our algorithm to provide better cache performance as compared to the conventional LRU (least recently used), MRU (most recently used) and FIFO (first-in, first-out) replacement policies. Extensive trace-driven simulations were performed for 30 different cache configurations using different SPEC (Standard Performance Evaluation Corp.) programs. Simulation results have shown that KORA can provide a substantial improvement in the miss ratio over the conventional algorithms. Our work opens up new dimensions for research in the development of new and improved page replacement schemes for virtual memory systems and disk caches.},
  eventtitle = {Conference {{Proceedings}} of the 1996 {{IEEE Fifteenth Annual International Phoenix Conference}} on {{Computers}} and {{Communications}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PKS4BSLC\\493626.html},
  keywords = {backpropagation,Backpropagation algorithms,backpropagation neural network,block replacement decisions,cache configurations,Cache memory,cache miss ratio,cache replacement scheme,cache storage,Cities and towns,Costs,dead line discarding,discrete event simulation,disk caches,Educational institutions,feedforward neural nets,High performance computing,History,Khalid-Obaidat Replacement Algorithm,KORA algorithm,line replacement decisions,Neural networks,optimal scheme,Optimized production technology,page replacement schemes,paged storage,performance evaluation,Performance evaluation,SPEC benchmark programs,Standard Performance Evaluation Corp.,trace-driven simulations,virtual memory systems}
}

@inproceedings{khanDecoupledDynamicCache2012,
  title = {Decoupled Dynamic Cache Segmentation},
  booktitle = {{{IEEE International Symposium}} on {{High}}-{{Performance Comp Architecture}}},
  author = {Khan, S. M. and Wang, Z. and Jiménez, D. A.},
  date = {2012-02},
  pages = {1--12},
  doi = {10.1109/HPCA.2012.6169030},
  abstract = {The least recently used (LRU) replacement policy performs poorly in the last-level cache (LLC) because temporal locality of memory accesses is filtered by first and second level caches. We propose a cache segmentation technique that dynamically adapts to cache access patterns by predicting the best number of not-yet-referenced and already-referenced blocks in the cache. This technique is independent from the LRU policy so it can work with less expensive replacement policies. It can automatically detect when to bypass blocks to the CPU with no extra overhead. In a 2MB LLC single-core processor with a memory intensive subset of SPEC CPU 2006 benchmarks, it outperforms LRU replacement on average by 5.2\% with not-recently-used (NRU) replacement and on average by 2.2\% with random replacement. The technique also complements existing shared cache partitioning techniques. Our evaluation with 10 multi-programmed workloads shows that this technique improves performance of an 8MB LLC four-core system on average by 12\%, with a random replacement policy requiring only half the space of the LRU policy.},
  eventtitle = {{{IEEE International Symposium}} on {{High}}-{{Performance Comp Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\B7WXJSBQ\\Khan et al. - 2012 - Decoupled dynamic cache segmentation.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\92KZJS84\\6169030.html},
  keywords = {2MB LLC single-core processor,8MB LLC four-core system,already-referenced block prediction,Benchmark testing,cache access patterns,cache storage,Decision trees,decoupled dynamic cache segmentation,Electronics packaging,last-level cache,least recently used replacement policy,LRU replacement,memory access temporal locality,memory intensive subset,microprocessor chips,not-recently-used replacement,not-yet-referenced block prediction,off-chip memory latency mitigation,on-chip caches,Proposals,Resistance,Runtime,shared cache partitioning techniques,SPEC CPU 2006 benchmarks,System-on-a-chip}
}

@inproceedings{kharbutliCounterbasedCacheReplacement2005,
  title = {Counter-Based Cache Replacement Algorithms},
  booktitle = {In {{Proceedings}} of {{ICCD}} 2005},
  author = {Kharbutli, Mazen and Solihin, Yan},
  date = {2005},
  abstract = {Recent studies have shown that in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, suggesting that alternative replacement algorithms can improve the performance of the cache. One main reason for this performance gap is that in the LRU replacement algorithm, a line is only evicted after it becomes the LRU line, long after its last access/touch, while unnecessarily occupying the cache space for a long time. This paper proposes a new approach to deal with the problem: counter-based L2 cache replacement. In this approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest, such as a cache access to the same set, occurs. When the counter exceeds a threshold, the line “expires”, and becomes evictable. When expired lines are evicted early from the cache, they make extra space for lines that may be more useful, reducing the number of capacity and conflict misses. Each line’s threshold is unique and is dynamically learned and stored in a small 40-Kbyte counter prediction table. We propose two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 (out of 21) SPEC2000 benchmarks by up to 40 \% and 11 \% on average. 1.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5PF4PHWP\\Kharbutli and Solihin - 2005 - Counter-based cache replacement algorithms.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QZDW5SLD\\summary.html}
}

@inproceedings{kharbutliCounterBasedCacheReplacement2005,
  title = {Counter-{{Based Cache Replacement Algorithms}}},
  booktitle = {Proceedings of the 2005 {{International Conference}} on {{Computer Design}}},
  author = {Kharbutli, Mazen and Solihin, Yan},
  date = {2005-10-02},
  pages = {61--68},
  publisher = {{IEEE Computer Society}},
  location = {{USA}},
  doi = {10.1109/ICCD.2005.41},
  url = {https://doi.org/10.1109/ICCD.2005.41},
  urldate = {2020-03-01},
  abstract = {Recent studies have shown that in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, suggesting that alternative replacement algorithms can improve the performance of the cache. One main reason for this performance gap is that in the LRU replacement algorithm, a line is only evicted after it becomes the LRU line, long after its last access/touch, while unnecessarily occupying the cache space for a long time. This paper proposes a new approach to deal with the problem: counterbased L2 cache replacement . In this approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest, such as a cache access to the same set, occurs. When the counter exceeds a threshold, the line "expires", and becomes evictable. When expired lines are evicted early from the cache, they make extra space for lines that may be more useful, reducing the number of capacity and conflict misses. Each line s threshold is unique and is dynamically learned and stored in a small 40-Kbyte counter prediction table. We propose two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 (out of 21) SPEC2000 benchmarks by up to 40\% and 11\% on average.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VIA7DTVD\\Kharbutli and Solihin - 2005 - Counter-Based Cache Replacement Algorithms.pdf},
  isbn = {978-0-7695-2451-1},
  series = {{{ICCD}} '05}
}

@article{kharbutliLACSLocalityAwareCostSensitive2014a,
  title = {{{LACS}}: {{A Locality}}-{{Aware Cost}}-{{Sensitive Cache Replacement Algorithm}}},
  shorttitle = {{{LACS}}},
  author = {Kharbutli, Mazen and Sheikh, Rami},
  date = {2014-08},
  journaltitle = {IEEE Transactions on Computers},
  volume = {63},
  pages = {1975--1987},
  issn = {2326-3814},
  doi = {10.1109/TC.2013.61},
  abstract = {The design of an effective last-level cache (LLC) in general-and an effective cache replacement/partitioning algorithm in particular-is critical to the overall system performance. The processor's ability to hide the LLC miss penalty differs widely from one miss to another. The more instructions the processor manages to issue during the miss, the better it is capable of hiding the miss penalty and the lower the cost of that miss. This nonuniformity in the processor's ability to hide LLC miss latencies, and the resultant nonuniformity in the performance impact of LLC misses, opens up an opportunity for a new cost-sensitive cache replacement algorithm. This paper makes two key contributions. First, It proposes a framework for estimating the costs of cache blocks at run-time based on the processor's ability to (partially) hide their miss latencies. Second, It proposes a simple, low-hardware overhead, yet effective, cache replacement algorithm that is locality-aware and cost-sensitive (LACS). LACS is thoroughly evaluated using a detailed simulation environment. LACS speeds up 12 LLC-performance-constrained SPEC CPU2006 benchmarks by up to 51\% and 11\% on average. When evaluated using a dual/quad-core CMP with a shared LLC, LACS significantly outperforms LRU in terms of performance and fairness, achieving improvements up to 54\%.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NP56TFAU\\Kharbutli and Sheikh - 2014 - LACS A Locality-Aware Cost-Sensitive Cache Replac.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C5FAPT5T\\6484053.html},
  keywords = {Aggregates,Algorithm design and analysis,Benchmark testing,cache block cost estimation,Cache memories,cache replacement algorithms,cache storage,caches,cost-sensitive cache replacement,costing,effective cache replacement-partitioning algorithm,Estimation,LACS,last-level cache,LLC miss penalty,locality-aware cost-sensitive cache replacement algorithm,low-hardware overhead,Optimization,Partitioning algorithms,Prediction algorithms,processor ability,shared caches},
  number = {8}
}

@inproceedings{kimLowoverheadHighperformanceUnified2000,
  title = {A {{Low}}-Overhead {{High}}-Performance {{Unified Buffer Management Scheme That Exploits Sequential}} and {{Looping References}}},
  booktitle = {Proceedings of the 4th {{Conference}} on {{Symposium}} on {{Operating System Design}} \& {{Implementation}} - {{Volume}} 4},
  author = {Kim, Jong Min and Choi, Jongmoo and Kim, Jesung and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun and Kim, Chong Sang},
  date = {2000},
  publisher = {{USENIX Association}},
  location = {{San Diego, California}},
  url = {http://dl.acm.org/citation.cfm?id=1251229.1251238},
  urldate = {2019-09-12},
  abstract = {In traditional file system implementations, the Least Recently Used (LRU) block replacement scheme is widely used to manage the buffer cache due to its simplicity and adaptability. However, the LRU scheme exhibits performance degradations because it does not make use of reference regularities such as sequential and looping references. In this paper, we present a Unified Buffer Management (UBM) scheme that exploits these regularities and yet, is simple to deploy. The UBM scheme automatically detects sequential and looping references and stores the detected blocks in separate partitions of the buffer cache. These partitions are managed by appropriate replacement schemes based on their detected patterns. The allocation problem among the divided partitions is also tackled with the use of the notion of marginal gains. In both trace-driven simulation experiments and experimental studies using an actual implementation in the FreeBSD operating system, the performance gains obtained through the use of this scheme are substantial. The results show that the hit ratios improve by as much as 57.7\% (with an average of 29.2\%) and the elapsed times are reduced by as much as 67.2\% (with an average of 28.7\%) compared to the LRU scheme for the workloads we used.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4HJFLF7N\\Kim et al. - 2000 - A Low-overhead High-performance Unified Buffer Man.pdf},
  series = {{{OSDI}}'00}
}

@inproceedings{kimSolarDRAMReducingDRAM2018,
  title = {Solar-{{DRAM}}: {{Reducing DRAM Access Latency}} by {{Exploiting}} the {{Variation}} in {{Local Bitlines}}},
  shorttitle = {Solar-{{DRAM}}},
  booktitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Kim, J. and Patel, M. and Hassan, H. and Mutlu, O.},
  date = {2018-10},
  pages = {282--291},
  doi = {10.1109/ICCD.2018.00051},
  abstract = {DRAM latency is a major bottleneck for many applications in modern computing systems. In this work, we rigorously characterize the effects of reducing DRAM access latency on 282 state-of-the-art LPDDR4 DRAM modules. As found in prior work on older DRAM generations (DDR3), we show that regions of LPDDR4 DRAM modules can be accessed with latencies that are significantly lower than manufacturer-specified values without causing failures. We present novel data that 1) further supports the viability of such latency reduction mechanisms and 2) exposes a variety of new cases in which access latencies can be effectively reduced. Using our observations, we propose a new low-cost mechanism, Solar-DRAM, that 1) identifies failure-prone regions of DRAM at reduced latency and 2) robustly reduces average DRAM access latency while maintaining data correctness, by issuing DRAM requests with reduced access latencies to non-failure-prone DRAM regions. We evaluate Solar-DRAM on a wide variety of multi-core workloads and show that for 4-core homogeneous workloads, Solar-DRAM provides an average (maximum) system performance improvement of 4.31\% (10.87\%) compared to using the default fixed DRAM access latency.},
  eventtitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FY5PMU3Y\\Kim et al. - 2018 - Solar-DRAM Reducing DRAM Access Latency by Exploi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3PI38KHX\\8615701.html},
  keywords = {average DRAM access,Capacitors,Decoding,DRAM access latency,DRAM chips,DRAM generations,DRAM requests,DRAM-Characterization,DRAM-Latency,failure analysis,latency reduction mechanisms,local bitlines,LPDDR4,LPDDR4 DRAM modules,Memory,Memory-Controllers,nonfailure-prone DRAM regions,Organizations,Process-Variation,Random access memory,reduced access latencies,Reliability,Solar-DRAM,Timing,Transistors}
}

@article{kiniwaLookaheadSchedulingRequests,
  title = {Lookahead {{Scheduling Requests}} for {{Eﬃcient Paging}}},
  author = {Kiniwa, Jun and Kameda, Tiko},
  pages = {9},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PCLEMN47\\Kiniwa and Kameda - Lookahead Scheduling Requests for Eﬃcient Paging.pdf},
  langid = {english}
}

@inproceedings{krishnapillaiRankSwitchingOpenRowDRAM2014,
  title = {A {{Rank}}-{{Switching}}, {{Open}}-{{Row DRAM Controller}} for {{Time}}-{{Predictable Systems}}},
  booktitle = {2014 26th {{Euromicro Conference}} on {{Real}}-{{Time Systems}}},
  author = {Krishnapillai, Y. and Wu, Z. P. and Pellizzoni, R.},
  date = {2014-07},
  pages = {27--38},
  doi = {10.1109/ECRTS.2014.37},
  abstract = {We introduce ROC, a Rank-switching, Open-row Controller for Double Data Rate Dynamic RAM (DDR DRAM). ROC is optimized for mixed-criticality multicore systems using modern DDR devices: compared to existing real-time memory controllers, it provides significantly lower worst case latency bounds for hard real-time tasks and supports throughput-oriented optimizations for soft real-time applications. The key to improved performance is an innovative rank-switching mechanism which hides the latency of write-read transitions in DRAM devices without requiring unpredictable request reordering. We further employ open row policy to take advantage of the data caching mechanism (row buffering) in each device. ROC provides complete timing isolation between hard and soft tasks and allows for compositional timing analysis over the number of cores and memory ranks in the system. We implemented and synthesized the ROC back end in Verilog RTL, and evaluated its performance on both synthetic tasks and a set of representative benchmarks.},
  eventtitle = {2014 26th {{Euromicro Conference}} on {{Real}}-{{Time Systems}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\K5UD57JB\\Krishnapillai et al. - 2014 - A Rank-Switching, Open-Row DRAM Controller for Tim.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GRGK4BQ6\\6932587.html},
  keywords = {cache storage,Clocks,compositional timing analysis,data caching mechanism,DDR DRAM,Delays,double data rate dynamic RAM,DRAM chips,Memory management,microcontrollers,mixed-criticality multicore systems,multiprocessing systems,optimisation,Random access memory,rank-switching open-row DRAM controller,Real-time systems,ROC,row buffering,Switches,throughput-oriented optimizations,time-predictable systems,Verilog RTL,worst case latency bounds,write-read transition latency}
}

@inproceedings{kumarOverviewModernCache2016,
  title = {An Overview of Modern Cache Memory and Performance Analysis of Replacement Policies},
  booktitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  author = {Kumar, S. and Singh, P. K.},
  date = {2016-03},
  pages = {210--214},
  doi = {10.1109/ICETECH.2016.7569243},
  abstract = {Memory hierarchy in current generation computers is formed by keeping registers inside, cache on or outside the processor and virtual memory on Hard disk. The principle of locality of reference is used to make memory hierarchy work efficiently. In recent years various advances have been made to improve the cache memory performance on the basis of hit rate, latency, speed, replacement policies and energy consumption. Cache replacement policy is one of the important design parameter which affects the overall processor performance and also become more important with recent technological moves towards highly associative cache. This paper yields a survey of current generation processors on the basis of various factors effecting cache memory performance and throughput. The main focus of this paper is the study and performance analysis of the cache replacement policies on the basis of simulation on several benchmarks.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4LDS84RN\\Kumar and Singh - 2016 - An overview of modern cache memory and performance.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PEAXWT9X\\7569243.html},
  keywords = {associative cache,benchmark testing,cache memory,Cache memory,Cache Performance,cache replacement policy,cache storage,Clocks,Conferences,current generation processors,design parameter,energy consumption,FIFO,hard disk,Hardware,hit rate,Hit rate,LFU,LRU,memory architecture,memory hierarchy,microprocessor chips,Miss rate,Multicore processing,Organizations,performance analysis,performance evaluation,performance improvement,processor performance,RANDOM,registers,Registers,Replacement Policy,virtual memory,virtual storage}
}

@inproceedings{lamichhaneNonintrusiveProgramTracing2018,
  title = {Non-Intrusive Program Tracing of Non-Preemptive Multitasking Systems Using Power Consumption},
  booktitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Lamichhane, K. and Moreno, C. and Fischmeister, S.},
  date = {2018-03},
  pages = {1147--1150},
  doi = {10.23919/DATE.2018.8342184},
  abstract = {System tracing, runtime monitoring, execution reconstruction are useful techniques for protecting the safety and integrity of systems. Furthermore, with time-aware or overhead-aware techniques being available, these techniques can also be used to monitor and secure production systems. As operating systems gain in popularity, even in deeply embedded systems, these techniques face the challenge to support multitasking. In this paper, we propose a novel non-intrusive technique, which efficiently reconstructs the execution trace of non-preemptive multitasking system by observing power consumption characteristics. Our technique uses the control-flow graph (CFG) of the application program to identify the most likely block of code that the system is executing at any given point in time. For the purpose of the experimental evaluation, we first instrument the source code to obtain power consumption information for each basic block, which is used as the training data for our Dynamic Time Warping and k-Nearest Neighbours (k-NN) classifier. Once the system is trained, this technique is used to identify live code-block execution (LCBE). We show that the technique can reconstruct the execution flow of programs in a multi-tasking environment with high accuracy.},
  eventtitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LL32ETJ8\\Lamichhane et al. - 2018 - Non-intrusive program tracing of non-preemptive mu.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YKAJ3DWV\\8342184.html},
  keywords = {CFG,control-flow graph,Debugging,deeply embedded systems,Dynamic Time Warping,Embedded software,embedded systems,execution reconstruction,execution trace,Instruments,k-Nearest Neighbours classifier,k-NN,LCBE,live code-block execution,Monitoring,multi-tasking environment,multiprogramming,Multitasking,nearest neighbour methods,nonintrusive program tracing,nonpreemptive multitasking system,novel nonintrusive technique,Operating system,operating systems,operating systems (computers),overhead-aware techniques,pattern classification,power consumption,power consumption characteristics,power consumption information,Power demand,Power tracing,production systems,program debugging,program diagnostics,Runtime,runtime monitoring,Runtime monitoring,safety,Scheduling,security of data,source code (software),System tracing,Time series analysis,useful techniques}
}

@article{leeImprovingPrefetchingEffects2008,
  title = {Improving Prefetching Effects by Exploiting Reference Patterns},
  author = {Lee, Hyo-Jeong and Doh, In-Hwan and Noh, Sam-H.},
  date = {2008},
  journaltitle = {Journal of KIISE:Computing Practices and Letters},
  volume = {14},
  pages = {226--230},
  issn = {1229-7712},
  url = {http://www.koreascience.or.kr/article/JAKO200814357783707.page},
  urldate = {2019-09-16},
  abstract = {Improving Prefetching Effects by Exploiting Reference Patterns Prefetching;Reference Pattern; Prefetching is one of widely used techniques to improve performance of I/O. But it has been reported that prefetching can bring adverse result on some reference pattern. This paper proposes a prefet-ching frame that can be adopted on existing prefetching techniques simply. The frame called IPRP (Improving Prefetching Effects by Exploiting Reference Patterns) and detects reference patterns online and control pre-fetching upon the characteristics of the detected pattern. In our experiment, we adopted IPRP on Linux read-ahead prefetching. IPRP could prevent adverse result clearly when Linux read-ahead prefetching increases total execution time about \$40\%\{\textbackslash{}sim\}70\%\$. When Linux read-ahead prefetching could bring some benefit, IPRP with read- ahead performed similar or slightly better benefit on execution time. With this result we could see our IPRP can complement and improve legacy prefetching techniques efficiently.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZD9W9QG3\\Lee et al. - 2008 - Improving Prefetching Effects by Exploiting Refere.pdf},
  langid = {kor},
  number = {2}
}

@inproceedings{leeTieredlatencyDRAMLow2013,
  title = {Tiered-Latency {{DRAM}}: {{A}} Low Latency and Low Cost {{DRAM}} Architecture},
  shorttitle = {Tiered-Latency {{DRAM}}},
  booktitle = {2013 {{IEEE}} 19th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Lee, D. and Kim, Y. and Seshadri, V. and Liu, J. and Subramanian, L. and Mutlu, O.},
  date = {2013-02},
  pages = {615--626},
  doi = {10.1109/HPCA.2013.6522354},
  abstract = {The capacity and cost-per-bit of DRAM have historically scaled to satisfy the needs of increasingly large and complex computer systems. However, DRAM latency has remained almost constant, making memory latency the performance bottleneck in today's systems. We observe that the high access latency is not intrinsic to DRAM, but a trade-off made to decrease cost-per-bit. To mitigate the high area overhead of DRAM sensing structures, commodity DRAMs connect many DRAM cells to each sense-amplifier through a wire called a bitline. These bitlines have a high parasitic capacitance due to their long length, and this bitline capacitance is the dominant source of DRAM latency. Specialized low-latency DRAMs use shorter bitlines with fewer cells, but have a higher cost-per-bit due to greater sense-amplifier area overhead. In this work, we introduce Tiered-Latency DRAM (TL-DRAM), which achieves both low latency and low cost-per-bit. In TL-DRAM, each long bitline is split into two shorter segments by an isolation transistor, allowing one segment to be accessed with the latency of a short-bitline DRAM without incurring high cost-per-bit. We propose mechanisms that use the low-latency segment as a hardware-managed or software-managed cache. Evaluations show that our proposed mechanisms improve both performance and energy-efficiency for both single-core and multi-programmed workloads.},
  eventtitle = {2013 {{IEEE}} 19th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\85DI4VUK\\Lee et al. - 2013 - Tiered-latency DRAM A low latency and low cost DR.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RWMTV3ES\\6522354.html},
  keywords = {bitline,cache storage,Capacitance,Capacitors,complex computer systems,Computer architecture,DRAM capacity,DRAM chips,DRAM cost-per-bit,DRAM sensing structures,energy conservation,energy-efficiency,hardware-managed cache,high parasitic capacitance,isolation transistor,low-latency low-cost DRAM architecture,memory architecture,memory latency,multiprogrammed workloads,multiprogramming,performance evaluation,power aware computing,single-core workloads,software-managed cache,tiered-latency DRAM,Timing,TL-DRAM,transistors,Transistors}
}

@article{leeWhenPrefetchingWorks2012,
  title = {When {{Prefetching Works}}, {{When It Doesn}}’t, and {{Why}}},
  author = {Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  date = {2012-03-01},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  volume = {9},
  pages = {1--29},
  issn = {15443566},
  doi = {10.1145/2133382.2133384},
  url = {http://dl.acm.org/citation.cfm?doid=2133382.2133384},
  urldate = {2019-07-25},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HCFV4W26\\Lee et al. - 2012 - When Prefetching Works, When It Doesn’t, and Why.pdf},
  langid = {english},
  number = {1}
}

@patent{lewchukPrefetchingDataUsing2000,
  title = {Prefetching Data Using Profile of Cache Misses from Earlier Code Executions},
  author = {Lewchuk, W. Kurt},
  date = {2000-04-04},
  url = {https://patents.google.com/patent/US6047363A/en},
  urldate = {2019-07-24},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YFSUN2LR\\Lewchuk - 2000 - Prefetching data using profile of cache misses fro.pdf},
  holder = {{Advanced Micro Devices Inc}},
  keywords = {addresses,cache,code sequence,miss,profile},
  number = {6047363A},
  type = {patentus}
}

@inproceedings{liCRFPNovelAdaptive2008,
  title = {{{CRFP}}: {{A Novel Adaptive Replacement Policy Combined}} the {{LRU}} and {{LFU Policies}}},
  shorttitle = {{{CRFP}}},
  booktitle = {2008 {{IEEE}} 8th {{International Conference}} on {{Computer}} and {{Information Technology Workshops}}},
  author = {Li, Z. and Liu, D. and Bi, H.},
  date = {2008-07},
  pages = {72--79},
  doi = {10.1109/CIT.2008.Workshops.22},
  abstract = {A variety of cache replacement algorithms have been proposed and applied in different situations, in which the LRU (least recently used) and LFU (least frequently used) replacement policies are two of the most popular policies. However, most real systems donpsilat consider obtaining a maximized throughput by switching between the two policies in response to the access pattern. In this paper, we propose a novel adaptive replacement policy that combined the LRU and LFU Policies (CRFP); CRFP is self-tuning and can switch between different cache replacement policies adaptively and dynamically in response to the access pattern changes.Conducting simulations with a variety of file access patterns and a wide range of buffer size, we show that the CRFP outperforms other algorithms in many cases and performs the best in most of these cases.},
  eventtitle = {2008 {{IEEE}} 8th {{International Conference}} on {{Computer}} and {{Information Technology Workshops}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\26J4WQQQ\\Li et al. - 2008 - CRFP A Novel Adaptive Replacement Policy Combined.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TIWG2ELW\\4568482.html},
  keywords = {adaptive replacement policy,Bismuth,cache replacement algorithms,cache storage,Caching,Computers,Conferences,CRFP,Data engineering,Database systems,file access patterns,Frequency,Information technology,Laboratories,least frequently used replacement policy,least recently used replacement policy,LFU,LFU policy,LRU,LRU policy,Replacement Policy,Switches,Throughput}
}

@inproceedings{liHighperformanceDRAMController2011,
  title = {A High-Performance {{DRAM}} Controller Based on Multi-Core System through Instruction Prefetching},
  booktitle = {2011 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Control}} ({{ICECC}})},
  author = {Li, K. and Guang, Q. and Lei, L. and Peng, Y. and Shi, J.},
  date = {2011-09},
  pages = {1220--1223},
  doi = {10.1109/ICECC.2011.6066295},
  abstract = {In this paper, we propose a cost-effective way to improve the performance of DRAM based on multi-core system. A novel DRAM controller with instruction prefecthing mechanism is introduced. The controller dynamically selects Open Page(OP) or Close Page(CP) policy by getting some information of future accesses in advance. This DRAM controller with dynamic policy based on instruction prefetching(DP\_BIF), can provide DRAM the lowest possible latency without increasing too many areas of chip when compared with the controller only with OP policy or CP policy. The analysis of the simulation results show that the access latency of the DRAM memory can be improved nearly 10.4\%, and the throughput of the DRAM is also increased nearly 10.2\% by adopting the DP\_BIF policy.},
  eventtitle = {2011 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Control}} ({{ICECC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XPAYD58J\\Li et al. - 2011 - A high-performance DRAM controller based on multi-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\Y9H52EIK\\6066295.html},
  keywords = {Bandwidth,close page policy,Delay,DRAM controller,dynamic policy,high-performance DRAM controller,instruction prefecthing,instruction prefetching,multicore system,multiprocessing systems,open page policy,Prefetching,Process control,Random access memory,random-access storage,Registers,storage management,Throughput}
}

@inproceedings{liLocalityDrivenDynamicGPU2015,
  title = {Locality-{{Driven Dynamic GPU Cache Bypassing}}},
  booktitle = {Proceedings of the 29th {{ACM}} on {{International Conference}} on {{Supercomputing}}},
  author = {Li, Chao and Song, Shuaiwen Leon and Dai, Hongwen and Sidelnik, Albert and Hari, Siva Kumar Sastry and Zhou, Huiyang},
  date = {2015},
  pages = {67--77},
  publisher = {{ACM}},
  location = {{Newport Beach, California, USA}},
  doi = {10.1145/2751205.2751237},
  url = {http://doi.acm.org/10.1145/2751205.2751237},
  urldate = {2019-09-12},
  abstract = {This paper presents novel cache optimizations for massively parallel, throughput-oriented architectures like GPUs. L1 data caches (L1 D-caches) are critical resources for providing high-bandwidth and low-latency data accesses. However, the high number of simultaneous requests from single-instruction multiple-thread (SIMT) cores makes the limited capacity of L1 D-caches a performance and energy bottleneck, especially for memory-intensive applications. We observe that the memory access streams to L1 D-caches for many applications contain a significant amount of requests with low reuse, which greatly reduce the cache efficacy. Existing GPU cache management schemes are either based on conditional/reactive solutions or hit-rate based designs specifically developed for CPU last level caches, which can limit overall performance. To overcome these challenges, we propose an efficient locality monitoring mechanism to dynamically filter the access stream on cache insertion such that only the data with high reuse and short reuse distances are stored in the L1 D-cache. Specifically, we present a design that integrates locality filtering based on reuse characteristics of GPU workloads into the decoupled tag store of the existing L1 D-cache through simple and cost-effective hardware extensions. Results show that our proposed design can dramatically reduce cache contention and achieve up to 56.8\% and an average of 30.3\% performance improvement over the baseline architecture, for a range of highly-optimized cache-unfriendly applications with minor area overhead and better energy efficiency. Our design also significantly outperforms the state-of-the-art CPU and GPU bypassing schemes (especially for irregular applications), without generating extra L2 and DRAM level contention.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CJNEDWI2\\Li et al. - 2015 - Locality-Driven Dynamic GPU Cache Bypassing.pdf},
  isbn = {978-1-4503-3559-1},
  keywords = {cache bypassing,gpu architecture optimization,locality},
  series = {{{ICS}} '15}
}

@inproceedings{limayeWorkloadCharacterizationSPEC2018,
  title = {A {{Workload Characterization}} of the {{SPEC CPU2017 Benchmark Suite}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Limaye, Ankur and Adegbija, Tosiron},
  date = {2018-04},
  pages = {149--158},
  publisher = {{IEEE}},
  location = {{Belfast}},
  doi = {10.1109/ISPASS.2018.00028},
  url = {https://ieeexplore.ieee.org/document/8366949/},
  urldate = {2019-07-25},
  eventtitle = {2018 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TJR2M8LS\\Limaye and Adegbija - 2018 - A Workload Characterization of the SPEC CPU2017 Be.pdf},
  isbn = {978-1-5386-5010-3}
}

@inproceedings{linDRAMLevelPrefetchingFullyBuffered2007,
  title = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}: {{Design}}, {{Performance}} and {{Power Saving}}},
  shorttitle = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}},
  booktitle = {2007 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  author = {Lin, J. and Zheng, H. and Zhu, Z. and Zhang, Z. and David, H.},
  date = {2007-04},
  pages = {94--104},
  doi = {10.1109/ISPASS.2007.363740},
  abstract = {We have studied DRAM-level prefetching for the fully buffered DIMM (FB-DIMM) designed for multi-core processors. FB-DIMM has a unique two-level interconnect structure, with FB-DIMM channels at the first-level connecting the memory controller and advanced memory buffers (AMBs); and DDR2 buses at the second-level connecting the AMBs with DRAM chips. We propose an AMB prefetching method that prefetches memory blocks from DRAM chips to AMBs. It utilizes the redundant bandwidth between the DRAM chips and AMBs but does not consume the crucial channel bandwidth. The proposed method fetches K memory blocks of L2 cache block sizes around the demanded block, where K is a small value ranging from two to eight. The method may also reduce the DRAM power consumption by merging some DRAM precharges and activations. Our cycle-accurate simulation shows that the average performance improvement is 16\% for single-core and multi-core workloads constructed from memory-intensive SPEC2000 programs with software cache prefetching enabled; and no workload has negative speedup. We have found that the performance gain comes from the reduction of idle memory latency and the improvement of channel bandwidth utilization. We have also found that there is only a small overlap between the performance gains from the AMB prefetching and the software cache prefetching. The average of estimated power saving is 15\%},
  eventtitle = {2007 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2GHPXF54\\Lin et al. - 2007 - DRAM-Level Prefetching for Fully-Buffered DIMM De.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BQCS9QVC\\4211026.html},
  keywords = {Bandwidth,channel bandwidth utilization,DRAM chip,DRAM chips,DRAM power consumption,DRAM-level prefetching,dual in-line memory module,dynamic random access memory,Energy consumption,fully-buffered DIMM,idle memory latency,interconnect structure,Joining processes,L2 cache block,memory block,memory controller,Merging,Multicore processing,multicore processor,Performance gain,power saving,Prefetching,Process design,Random access memory,redundant bandwidth,software cache prefetching,Software performance,SPEC2000 program,storage management,storage management chips}
}

@inproceedings{linExtractingMemorylevelParallelism2013,
  title = {Extracting Memory-Level Parallelism through Reconfigurable Hardware Traces},
  booktitle = {2013 {{International Conference}} on {{Reconfigurable Computing}} and {{FPGAs}} ({{ReConFig}})},
  author = {Lin, M. and Cheng, S. and Wawrzynek, J.},
  date = {2013-12},
  pages = {1--8},
  doi = {10.1109/ReConFig.2013.6732290},
  abstract = {This paper proposes a new FPGA-based embedded computer architecture, which focuses on how to construct an application-specific memory access network capable of extracting the maximum amount of memory-level parallelism on a per-application basis. Specifically, through performing dynamic memory analysis and utilizing the capabilities of modern FPGA devices: abundant distributed block RAMs and programmability, the proposed reconfigurable architecture synthesizes highly efficient accelerators that enable parallelized memory accesses, and therefore accomplish effective data orchestration by maximally extracting the target application's instruction, loop and memory-level parallelism. To validate our proposed architecture, we implemented a baseline embedded processor platform, a conventional CPU +accelerator with a centralized single memory, and a prototype based on Xilinx MicroBlaze technology. Our experimental results have shown that on average for 5 benchmark applications from SPEC2006 and MiBench [1], our proposed architecture achieves 8.6 times speedup compared to the baseline embedded processor platform and 1.7 times speedup compared to a conventional CPU+accelcrator platform. More interestingly, the proposed platform achieves more than 40\% reduction in energy-delay product compared to a conventional CPU+accelerator with a centralized memory.},
  eventtitle = {2013 {{International Conference}} on {{Reconfigurable Computing}} and {{FPGAs}} ({{ReConFig}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EN68BP3W\\Lin et al. - 2013 - Extracting memory-level parallelism through reconf.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\86XMHZG2\\6732290.html},
  keywords = {Acceleration,application-specific memory access network,baseline embedded processor platform,centralized memory,centralized single memory,Coherence,Computer architecture,conventional CPU +accelerator,data orchestration,distributed block RAM,dynamic memory analysis,embedded systems,energy-delay product,field programmable gate arrays,Field programmable gate arrays,FPGA devices,FPGA-based embedded computer architecture,Hardware,memory architecture,memory-level parallelism,MiBench,microprocessor chips,Parallel processing,parallelized memory accesses,Performance evaluation,programmability,random-access storage,reconfigurable architecture,reconfigurable architectures,reconfigurable hardware traces,SPEC2006,Xilinx MicroBlaze technology}
}

@report{linPredictingLastTouchReferences2002,
  title = {Predicting {{Last}}-{{Touch References}} under {{Optimal Replacement}}},
  author = {Lin, Wei-Fen and Reinhardt, Steven K},
  date = {2002},
  pages = {17},
  institution = {{University of Michigan}},
  abstract = {Effective cache replacement is becoming an increasingly important issue in cache hierarchy design as large set-associative caches are widely used in high-performance systems. This paper proposes a novel approach to approximate the decisions made by an optimal replacement algorithm (OPT) using last-touch prediction. The central idea is to identify, via prediction, the final reference to a cache block before the block would be evicted under OPT—the “OPT last touch”. Given perfect prediction, replacing the referenced block immediately after each OPT last touch would give optimal replacement behavior.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6LFUIS36\\Lin and Reinhardt - Predicting Last-Touch References under Optimal Rep.pdf},
  langid = {english},
  number = {CSE-TR-447-02},
  type = {Technical Report}
}

@inproceedings{liTracebasedAnalysisMethodology2016,
  title = {Trace-Based {{Analysis Methodology}} of {{Program Flash Contention}} in {{Embedded Multicore Systems}}},
  booktitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Li, Lin and Mayer, Albrecht},
  date = {2016},
  pages = {199--204},
  publisher = {{Research Publishing Services}},
  doi = {10.3850/9783981537079_0442},
  url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7459304},
  urldate = {2019-01-28},
  abstract = {Contention for shared resources is a major performance issue in multicore systems. In embedded multicore microcontrollers, contentions of program flash accesses have a significant performance impact, because the flash access has a large latency compared to a core clock cycle. Therefore, the detection and analysis of program flash contentions are necessary to remedy this situation. With a lack of existing tools being able to fulfill this task, a novel post-processing analysis methodology is proposed in this paper to acquire the information of program flash contentions in detail based on the non-intrusive trace. This information can be utilized to improve the overall performance and particularly to enhance the real-time performance of specific threads or functions for hard real-time multicore systems.},
  eventtitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QLMYXHQ\\Li and Mayer - 2016 - Trace-based Analysis Methodology of Program Flash .pdf},
  isbn = {978-3-9815370-7-9},
  langid = {english}
}

@article{liuBridgingProcessormemoryPerformance2005,
  title = {Bridging the Processor-Memory Performance Gap with {{3D IC}} Technology},
  author = {Liu, C. C. and Ganusov, I. and Burtscher, M. and {Sandip Tiwari}},
  date = {2005-11},
  journaltitle = {IEEE Design Test of Computers},
  volume = {22},
  pages = {556--564},
  issn = {0740-7475},
  doi = {10.1109/MDT.2005.134},
  abstract = {Microprocessor performance has been improving at roughly 60\% per year. Memory access times, however, have improved by less than 10\% per year. The resulting gap between logic and memory performance has forced microprocessor designs toward complex and power-hungry architectures that support out-of-order and speculative execution. Moreover, processors have been designed with increasingly large cache hierarchies to hide main memory latency. This article examines how 3D IC technology can improve interactions between the processor and memory. Our work examines the performance of a single-core, single-threaded processor under representative work loads. We have shown that reducing memory latency by bringing main memory on chip gives us near-perfect performance. Three-dimensional IC technology can provide the much needed bandwidth without the cost, design complexity, and power issues associated with a large number of off-chip pins. The principal challenge remains the demonstration of a highly manufacturable 3D IC technology with high yield and low cost.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PLDS6YDF\\Liu et al. - 2005 - Bridging the processor-memory performance gap with.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7AKAYTWA\\1541918.html},
  keywords = {3D IC technology,Bandwidth,cache storage,Costs,Delay,DRAM chips,integrated circuit technology,Logic design,main memory on chip,Manufacturing,memory access,memory architecture,microprocessor chips,microprocessor design,microprocessor performance,Microprocessors,Out of order,Pins,Process design,processor-memory performance,single-threaded processor,Three-dimensional integrated circuits,three-dimensional integration 3-D ICs microprocessor cache design stream prefetching embedded DRAM},
  number = {6}
}

@inproceedings{liuCacheBurstsNew2008a,
  title = {Cache Bursts: {{A}} New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency},
  shorttitle = {Cache Bursts},
  booktitle = {2008 41st {{IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Liu, Haiming and Ferdman, Michael and Huh, Jaehyuk and Burger, Doug},
  date = {2008-11},
  pages = {222--233},
  issn = {2379-3155},
  doi = {10.1109/MICRO.2008.4771793},
  abstract = {Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96\% of the dead blocks with a 96\% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8\% to 17\% and the L2 efficiency from 17\% to 27\%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12\% at the L1 and by 13\% at the L2.},
  eventtitle = {2008 41st {{IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SBJUL4NZ\\Liu et al. - 2008 - Cache bursts A new approach for eliminating dead .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\74XCECAG\\4771793.html},
  keywords = {Accuracy,cache efficiency,cache storage,data cache burst,dead block elimination,dead-block prediction,general-purpose microprocessor,Hardware,History,LI cache,microprocessor chips,Microprocessors,Prefetching,System performance}
}

@article{liuControllerArchitectureLowPower2017,
  title = {Controller {{Architecture}} for {{Low}}-{{Power}}, {{Low}}-{{Latency DRAM With Built}}-in {{Cache}}},
  author = {Liu, Z. and Shih, H. and Lin, B. and Wu, C.},
  date = {2017-04},
  journaltitle = {IEEE Design Test},
  volume = {34},
  pages = {69--78},
  issn = {2168-2356},
  doi = {10.1109/MDAT.2016.2524445},
  abstract = {Memory wall is a critical issue for many today's electronic systems. Tiered latency DRAM with asymmetric bit lines was proposed to optimize the power and latency. This paper proposes a controller architecture for the tiered latency DRAM in which the small array is operated like a cache.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DG9BL29C\\Liu et al. - 2017 - Controller Architecture for Low-Power, Low-Latency.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\L272EZ4H\\7397924.html},
  keywords = {3D DRAM,asymmetric bit lines,built-in cache,built-in cache DRAM (BC-DRAM),cache storage,controller architecture,controllers,Delays,DRAM,DRAM chips,DRAM controller,electronic systems,low-power electronics,low-power low-latency DRAM,memory hierarchy,memory wall,Microprocessors,Random access memory,tiered latency DRAM,tiered latency DRAM (TL-DRAM),Transistors},
  number = {2}
}

@inproceedings{liuLAMSLatencyawareMemory2016,
  title = {{{LAMS}}: {{A}} Latency-Aware Memory Scheduling Policy for Modern {{DRAM}} Systems},
  shorttitle = {{{LAMS}}},
  booktitle = {2016 {{IEEE}} 35th {{International Performance Computing}} and {{Communications Conference}} ({{IPCCC}})},
  author = {Liu, W. and Huang, P. and Kun, T. and Lu, T. and Zhou, K. and Li, C. and He, X.},
  date = {2016-12},
  pages = {1--8},
  doi = {10.1109/PCCC.2016.7820660},
  abstract = {This paper introduces a new memory scheduling policy called LAMS, which is inspired by a recently proposed memory architecture and targets for future high capacity memory systems. As memory capacity increases, the bit-lines connected to memory row buffers become much longer, dramatically lengthening memory access latency, due to increased parasitic capacitance. Recent study has proposed to partition long bit-lines into near and far (relative to the row buffer) segments via inserting isolation transistors such that access to near segment can be accomplished much faster, while access to far segment remains nearly the same. However, how to effectively leverage the new memory architecture still remains unexplored. We suggest to take advantage of this new memory architecture via performing latency-aware memory scheduling for pending requests to explore their performance potentials. In this scheduling policy, each memory request is classified to one of the following three categories, row-buffer hit, near-buffer, and far-buffer. Based on the classification, it issues requests in the order of row-buffer hit → near-buffer → far-buffer. In doing so, it avoids long-latency requests blocking short-latency memory requests, reducing total memory queuing time in the memory controller and improving overall memory performance. Our evaluation results on a simulated memory system show that comparing with the commonly used FR-FCFS scheduler, our LAMS improves performance and energy efficiency by up to 20.6\% and 34\%, respectively. Even comparing with the four competitive schedulers chosen from memory scheduling champion (MSC), LAMS still improves performance and energy efficiency by up to 6.1\% and 23.4\%, respectively.},
  eventtitle = {2016 {{IEEE}} 35th {{International Performance Computing}} and {{Communications Conference}} ({{IPCCC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H55ZJHPU\\Liu et al. - 2016 - LAMS A latency-aware memory scheduling policy for.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M8BIZSVW\\7820660.html},
  keywords = {buffer storage,DRAM chips,DRAM systems,far-buffer classification,FR-FCFS scheduler,LAMS,latency-aware memory scheduling policy,memory access latency,memory architecture,Memory architecture,memory capacity,memory queuing time,near-buffer classification,Parasitic capacitance,Processor scheduling,Random access memory,row-buffer hit classification,Scheduling,System performance,Transistors}
}

@inproceedings{liuMultibankMemoryAccess2010,
  title = {Multi-Bank Memory Access Scheduler and Scalability},
  booktitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  author = {Liu, D. and Pan, G. and Xie, L.},
  date = {2010-04},
  volume = {2},
  pages = {V2-723-V2-727},
  doi = {10.1109/ICCET.2010.5485729},
  abstract = {With the progress of semiconductor manufacture techniques and the development of processor architecture, the gap between processor and DRAM speed is becoming larger and larger, memory bandwidth is now the primary bottleneck of improving computer system performance. Modern DRAM provide several independent memory banks, according to this character, we present a virtual channel based memory access scheduler, and least wait time and read-fist schedule approach. This approach significantly reduce observed main memory access latency and improve the effective memory bandwidth.},
  eventtitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FFS6FDZQ\\Liu et al. - 2010 - Multi-bank memory access scheduler and scalability.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5FFBJM2U\\5485729.html},
  keywords = {Bandwidth,Computer aided manufacturing,Computer architecture,computer system performance,DRAM chips,DRAM memory system,DRAM speed,Job shop scheduling,Manufacturing processes,memory access latency,memory access scheduling,memory bandwidth,multibank memory access scheduler,processor scheduling,Processor scheduling,Random access memory,Scalability,Semiconductor device manufacture,semiconductor manufacture technique,storage management,System performance,virtual channel,virtual channel based memory access scheduler}
}

@inproceedings{luImprovingDRAMLatency2015,
  title = {Improving {{DRAM}} Latency with Dynamic Asymmetric Subarray},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Lu, S. and Lin, Y. and Yang, C.},
  date = {2015-12},
  pages = {255--266},
  doi = {10.1145/2830772.2830827},
  abstract = {The evolution of DRAM technology has been driven by capacity and bandwidth during the last decade. In contrast, DRAM access latency stays relatively constant and is trending to increase. Much efforts have been devoted to tolerate memory access latency but these techniques have reached the point of diminishing returns. Having shorter bitline and wordline length in a DRAM device will reduce the access latency. However by doing so it will impact the array efficiency. In the mainstream market, manufacturers are not willing to trade capacity for latency. Prior works had proposed hybrid-bitline DRAM design to overcome this problem. However, those methods are either intrusive to the circuit and layout of the DRAM design, or there is no direct way to migrate data between the fast and slow levels. In this paper, we proposed a novel asymmetric DRAM with capability to perform low cost data migration between subarrays. Having this design we determined a simple management mechanism and explored many management related policies. We showed that with this new design and our simple management technique we could achieve 7.25\% and 11.77\% performance improvement in single- and multi-programming workloads, respectively, over a system with traditional homogeneous DRAM. This gain is above 80\% of the potential performance gain of a system based on a hypothetical DRAM which is made out of short bitlines entirely.},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\S5PNHIBT\\Lu et al. - 2015 - Improving DRAM latency with dynamic asymmetric sub.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GIW27Z6W\\7856603.html},
  keywords = {asymmetric DRAM,Capacitors,Computer architecture,Delays,DRAM access latency,DRAM chips,DRAM device,DRAM technology,dynamic asymmetric subarray,hybrid-bitline DRAM design,low cost data migration,memory access latency,Microprocessors,multiprogramming workloads,performance evaluation,performance improvement,single-programming workloads,Transistors,wordline length}
}

@inproceedings{luoDesignRealizationOptimized2010,
  title = {Design and {{Realization}} of an {{Optimized Memory Access Scheduler}}},
  booktitle = {2010 {{Third International Joint Conference}} on {{Computational Science}} and {{Optimization}}},
  author = {Luo, L. and He, H. and Liao, C. and Dou, Q. and Xu, W.},
  date = {2010-05},
  volume = {2},
  pages = {288--292},
  doi = {10.1109/CSO.2010.81},
  abstract = {Memory Wall is a bottleneck of enhancing the performance of computer system, and appearance of multiprocessors (CMPs) makes it more. How to reduce Memory Access Latency is a critical issue we have to deal with. Memory controller is difficult to optimize, the controller needs to obey all DRAM timing constraints to provide correct functionality. State-of -the-art DDR2 SDRAM chips often have a large number of timing constraints that must be obeyed when scheduling commands, for instance, over 50 timing constrains. We have made deep research on optimized memory access scheduling. In order to efficiently utilize the bandwidth and reduce the latency, Memory Access Scheduling optimization adapts the characters of DRAM to reschedule the memory access. By studying effective data bar which is generated by Genetic Algorithm, we mine four rules. So we just use these four rules to schedule in Memory Access Controller. The results of experiment show that compared with FR-FCFS (first-ready first-come first-serve) scheduling strategy, the rule based algorithm improves the performance of scheduling and the ideal speedup is near 1.5 times. The best speedup of the test of spec2000 is 1.467, and the worst speedup is 1.078.},
  eventtitle = {2010 {{Third International Joint Conference}} on {{Computational Science}} and {{Optimization}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\72D9UFMK\\Luo et al. - 2010 - Design and Realization of an Optimized Memory Acce.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\44V4AJRA\\5533016.html},
  keywords = {Bandwidth,Constraint optimization,data mining,DDR2,DDR2 SDRAM chips,Delay,Design optimization,DRAM chips,first-ready first-come first-serve scheduling,FR-FCFS scheduling strategy,genetic algorithm,genetic algorithm (GA),genetic algorithms,Genetic algorithms,memory access controller,memory access latency,memory access scheduling,memory controller,multiprocessing systems,multiprocessors,optimized memory access scheduler,processor scheduling,Processor scheduling,Random access memory,rule based algorithm,Scheduling algorithm,scheduling commands,SDRAM,timing,Timing}
}

@article{malamy1994methods,
  title = {Methods and Apparatus for Implementing a Pseudo-Lru Cache Memory Replacement Scheme with a Locking Feature},
  author = {Malamy, Adam and Patel, Rajiv N and Hayes, Norman M},
  date = {1994-04-10},
  publisher = {{Google Patents}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XP468KC5\\Malamy et al. - 1994 - Methods and apparatus for implementing a pseudo-lr.pdf}
}

@inproceedings{maLowlatencySDRAMController2010,
  title = {Low-Latency {{SDRAM}} Controller for Shared Memory in {{MPSoC}}},
  booktitle = {2010 10th {{IEEE International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology}}},
  author = {Ma, P. and Zhao, J. and Li, K. and Zhu, L. and Shi, J.},
  date = {2010-11},
  pages = {321--323},
  doi = {10.1109/ICSICT.2010.5667736},
  abstract = {In a memory structure shared by multiple processors based on Multiprocessor Systems on Chip (MPSoC), the efficiency of memory bus access becomes the bottleneck of the overall system efficiency. This paper presents a low-latency SDARM controller structure integrated in MPSoC, which controls the off-chip SDRAM memory. Consecutive same row optimization and odd-even bank optimization are used to eliminate precharge time and active to read/write execution in memory access. Burst mode supported by data transmit block improves the efficiency of the memory bus. Simulation results show that memory performance improves maximally by 56\% compared to pre-optimized, making it meet the high throughput requirements of shared-memory controller in MPSoC.},
  eventtitle = {2010 10th {{IEEE International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TRLP7B57\\Ma et al. - 2010 - Low-latency SDRAM controller for shared memory in .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FBGVYY2P\\5667736.html},
  keywords = {burst mode,Classification algorithms,Conferences,data transmit block,DRAM chips,low-latency SDRAM controller structure,Mathematical model,memory bus access,Memory management,memory structure,multiprocessor systems on chip,odd-even bank optimization,off-chip SDRAM memory,Optimization,same row optimization,SDRAM,shared memory systems,shared-memory controller,system-on-chip,Timing}
}

@inproceedings{manikantanNUcacheEfficientMulticore2011,
  title = {{{NUcache}}: {{An}} Efficient Multicore Cache Organization Based on {{Next}}-{{Use}} Distance},
  shorttitle = {{{NUcache}}},
  booktitle = {2011 {{IEEE}} 17th {{International Symposium}} on {{High Performance Computer Architecture}}},
  author = {Manikantan, R and Rajan, Kaushik and Govindarajan, R},
  date = {2011-02},
  pages = {243--253},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2011.5749733},
  abstract = {The effectiveness of the last-level shared cache is crucial to the performance of a multi-core system. In this paper, we observe and make use of the DelinquentPC - Next-Use characteristic to improve shared cache performance. We propose a new PC-centric cache organization, NUcache, for the shared last level cache of multi-cores. NUcache logically partitions the associative ways of a cache set into MainWays and DeliWays. While all lines have access to the MainWays, only lines brought in by a subset of delinquent PCs, selected by a PC selection mechanism, are allowed to enter the DeliWays. The PC selection mechanism is an intelligent cost-benefit analysis based algorithm that utilizes Next-Use information to select the set of PCs that can maximize the hits experienced in DeliWays. Performance evaluation reveals that NUcache improves the performance over a baseline design by 9.6\%, 30\% and 33\% respectively for dual, quad and eight core workloads comprised of SPEC benchmarks. We also show that NUcache is more effective than other well-known cache-partitioning algorithms.},
  eventtitle = {2011 {{IEEE}} 17th {{International Symposium}} on {{High Performance Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JHQ9M72A\\Manikantan et al. - 2011 - NUcache An efficient multicore cache organization.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EDN89G7I\\5749733.html},
  keywords = {Algorithm design and analysis,Benchmark testing,cache storage,cache-partitioning algorithms,Correlation,DelinquentPC,DeliWays,Histograms,intelligent cost-benefit analysis,last-level shared cache,MainWays,Measurement,multicore cache organization,Multicore processing,multicore system,multiprocessing systems,next-use distance,NUcache,Organizations,PC selection mechanism,PC-centric cache organization,performance evaluation,shared cache performance improvement}
}

@inproceedings{mcfarlingProgramOptimizationInstruction1989,
  title = {Program Optimization for Instruction Caches},
  author = {McFarling, S.},
  date = {1989},
  pages = {183--191},
  publisher = {{ACM Press}},
  doi = {10.1145/70082.68200},
  url = {http://portal.acm.org/citation.cfm?doid=70082.68200},
  urldate = {2020-01-06},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2M8IVMBF\\McFarling - Program Optimization for Instruction Caches.pdf},
  isbn = {978-0-89791-300-3},
  langid = {english}
}

@inproceedings{megiddoARCSelfTuningLow2003,
  title = {{{ARC}}: {{A Self}}-{{Tuning}}, {{Low Overhead Replacement Cache}}},
  booktitle = {Proceedings of {{FAST}} ’03: 2nd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  author = {Megiddo, Nimrod and Dharmendra S., Modha},
  date = {2003-04},
  volume = {3},
  pages = {115--130},
  publisher = {{USENIX Association}},
  location = {{San Francisco}},
  abstract = {We consider the problem of cache management in a demand paging scenario with uniform page sizes. We propose a new cache management policy, namely, Adaptive Replacement Cache (ARC), that has several advantages.},
  eventtitle = {{{FAST}} '03: 2nd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6WSSJ92Q\\Francisco - Proceedings of FAST ’03 2nd USENIX Conference on .pdf},
  langid = {english}
}

@inproceedings{mekhielMultiLevelCacheMost29,
  title = {Multi-{{Level Cache With Most Frequently Used Policy}}: {{A New Concept In Cache Design}}},
  booktitle = {Proceedings of the {{ISCA}} 8th {{International Conference}}},
  author = {Mekhiel, Nagi N},
  date = {0029-11/1995-12-01},
  pages = {5},
  location = {{Honolulu, Hawaii}},
  abstract = {The number of unique references in any program represents a small part of the total number of requested references. The unique references (small part of the total requested references) consist of two types: unique references that are used several times (most frequently used references) and unique references that are used once (least frequently used references).},
  eventtitle = {Computer {{Applications}} in {{Industry}} and {{Engineering}} ({{CAINE}}-95), 8th {{Int}}'l. {{Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I4ZD4QTM\\Mekhiel - Multi-Level Cache With Most Frequently Used Policy.pdf},
  isbn = {1-880843-14-5},
  langid = {english}
}

@incollection{menaudImprovingEffectivenessWeb2000,
  title = {Improving the {{Effectiveness}} of {{Web Caching}}},
  booktitle = {Advances in {{Distributed Systems}}},
  author = {Menaud, Jean-Marc and Issarny, Valérie and Banâtre, Michel},
  editor = {Krakowiak, Sacha and Shrivastava, Santosh},
  date = {2000},
  volume = {1752},
  pages = {375--401},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-46475-1_16},
  url = {http://link.springer.com/10.1007/3-540-46475-1_16},
  urldate = {2020-01-18},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\S9XUKFYZ\\Menaud et al. - 2000 - Improving the Effectiveness of Web Caching.pdf},
  isbn = {978-3-540-67196-1 978-3-540-46475-4},
  options = {useprefix=true}
}

@article{midorikawaAdaptiveReplacementBased2008,
  title = {On {{Adaptive Replacement Based}} on {{LRU}} with {{Working Area Restriction Algorithm}}},
  author = {Midorikawa, Edson T. and Piantola, Ricardo L. and Cassettari, Hugo H.},
  date = {2008-10},
  journaltitle = {SIGOPS Oper. Syst. Rev.},
  volume = {42},
  pages = {81--92},
  issn = {0163-5980},
  doi = {10.1145/1453775.1453790},
  url = {http://doi.acm.org/10.1145/1453775.1453790},
  urldate = {2019-09-16},
  abstract = {Adaptive algorithms are capable of modifying their own behavior through time, depending on the execution characteristics. Recently, we have proposed LRU-WAR, an adaptive replacement algorithm whose objective is to minimize failures detected in LRU policy, preserving its simplicity and low overhead. In this paper, we present our contribution to the study of adaptive replacement algorithms describing their behavior under a number of workloads. Simulations include an analysis of the performance sensibility with the variation of the control parameters and its application in a multiprogrammed environment. In order to address LRU-WAR weakness as a global policy, we also introduce LRU-WARlock. The simulation results show that substantial performance improvements can be obtained.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7PLKVN5X\\Midorikawa et al. - 2008 - On Adaptive Replacement Based on LRU with Working .pdf},
  keywords = {adaptive replacement,demand paging,LRU,virtual memory},
  number = {6}
}

@article{milenkovicCachesPredictorsRealTime2011,
  title = {Caches and {{Predictors}} for {{Real}}-{{Time}}, {{Unobtrusive}}, and {{Cost}}-{{Effective Program Tracing}} in {{Embedded Systems}}},
  author = {Milenkovic, A. and Uzelac, V. and Milenkovic, M. and Burtscher, M.},
  date = {2011-07},
  journaltitle = {IEEE Transactions on Computers},
  volume = {60},
  pages = {992--1005},
  issn = {0018-9340},
  doi = {10.1109/TC.2010.146},
  abstract = {The increasing complexity of modern embedded computer systems makes software development and system verification the most critical steps in system development. To expedite verification and program debugging, chip manufacturers increasingly consider hardware infrastructure for program debugging and tracing, including logic to capture and filter traces, buffers to store traces, and a trace port through which the trace is read by the debug tools. In this paper, we introduce a new approach to capture and compress program execution traces in hardware. The proposed trace compressor encompasses two cost-effective structures, a stream descriptor cache, and a last stream predictor. Information about the program flow is translated into a sequence of hit and miss events in these structures, thus dramatically reducing the number of bits that need to be sent out of the chip. We evaluate the efficiency of the proposed mechanism by measuring the trace port bandwidth on a set of benchmark programs. Our mechanism requires only 0.15 bits/instruction/CPU on average on the trace port, which is a sixfold improvement over state-of-the-art commercial solutions. The trace compressor requires an on-chip area that is equivalent to one third of a 1 kilobyte cache and it allows for continual and unobtrusive program tracing in real time.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SAFXJA9L\\Milenkovic et al. - 2011 - Caches and Predictors for Real-Time, Unobtrusive, .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FGLE5U6P\\5487509.html},
  keywords = {Bandwidth,buffers,cache storage,chip manufacturer,Complexity theory,Compression technologies,cost-effective program tracing,data compression,debug tools,Debugging,Detectors,embedded systems,Hardware,hardware infrastructure,last stream predictor,on-chip area,program debugging,program diagnostics,program execution trace capture,program execution trace compression,program flow information,Program processors,program verification,real time and embedded systems,real-time program tracing,Silicon,software development,stream descriptor cache,system development,system verification,system-on-chip,testing and debugging,trace port bandwidth,tracing.,unobtrusive program tracing},
  number = {7}
}

@inproceedings{milenkovicPerformanceEvaluationMemory2003,
  title = {A Performance Evaluation of Memory Hierarchy in Embedded Systems},
  booktitle = {Proceedings of the 35th {{Southeastern Symposium}} on {{System Theory}}, 2003.},
  author = {Milenkovic, A. and Milenkovic, M. and Barnes, N.},
  date = {2003-03},
  pages = {427--431},
  doi = {10.1109/SSST.2003.1194606},
  abstract = {The increasing speed gap between processors and memory makes the design of memory hierarchy one of the critical issues in general purpose embedded systems. As memory requirements for embedded applications grow, especially in emerging area of handheld multimedia devices, cache memories become crucial for providing high performance and reducing power. This paper describes a performance evaluation of typical cache design issues such as cache size and organization, block size, and replacement policy. The evaluation is done using simulation tools for architectural exploration based on ARM instruction set and MiBench benchmark suite. Our performance evaluation includes monitoring of dynamic cache behavior, since embedded systems designers are interested not only in the total number of cache misses, but also in the number of cache misses throughout application execution.},
  eventtitle = {Proceedings of the 35th {{Southeastern Symposium}} on {{System Theory}}, 2003.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WI5X4Q2N\\Milenkovic et al. - 2003 - A performance evaluation of memory hierarchy in em.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KF8Q7DSM\\1194606.html},
  keywords = {ARM instruction set,block size,cache memories,Cache memory,cache misses,cache organization,cache size,cache storage,Clocks,Computer architecture,Computer industry,Costs,Design engineering,dynamic behavior,Embedded computing,Embedded system,embedded systems,handheld multimedia devices,memory architecture,memory hierarchy,MiBench benchmark suite,Monitoring,performance evaluation,Performance gain,replacement policy,simulation tools}
}

@thesis{minUSINGRUNTIMEINFORMATION2005,
  title = {{{USING RUNTIME INFORMATION TO IMPROVE MEMORY SYSTEM PERFORMANCE}}},
  author = {Min, Rui},
  date = {2005},
  institution = {{University of Cincinnati}},
  url = {https://etd.ohiolink.edu/pg_10?0::NO:10:P10_ACCESSION_NUM:ucin1134043707},
  urldate = {2019-09-16},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M47J8S5T\\Min - 2005 - USING RUNTIME INFORMATION TO IMPROVE MEMORY SYSTEM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VYC68M47\\pg_10.html},
  langid = {english}
}

@article{mittalSurveyTechniquesCache2016,
  title = {A {{Survey}} of {{Techniques}} for {{Cache Locking}}},
  author = {Mittal, Sparsh},
  date = {2016-05},
  journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {21},
  pages = {49:1--49:24},
  issn = {1084-4309},
  doi = {10.1145/2858792},
  url = {http://doi.acm.org/10.1145/2858792},
  urldate = {2019-08-02},
  abstract = {Cache memory, although important for boosting application performance, is also a source of execution time variability, and this makes its use difficult in systems requiring worst-case execution time (WCET) guarantees. Cache locking is a promising approach for simplifying WCET estimation and providing predictability, and hence, several commercial processors provide ability for locking cache. However, cache locking also has several disadvantages (e.g., extra misses for unlocked blocks, complex algorithms required for selection of locking contents) and hence, a careful management is required to realize the full potential of cache locking. In this article, we present a survey of techniques proposed for cache locking. We categorize the techniques into several groups to underscore their similarities and differences. We also discuss the opportunities and obstacles in using cache locking. We hope that this article will help researchers gain insight into cache locking schemes and will also stimulate further work in this area.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5VYJUFSS\\Mittal - 2016 - A Survey of Techniques for Cache Locking.pdf},
  keywords = {cache locking,cache partitioning,classification,CPU,GPU,hard real-time system,multitasking,Review,worst-case execution time (WCET)},
  number = {3}
}

@inproceedings{miuraMemoryControllerThat2005,
  title = {A Memory Controller That Reduces Latency of Cached {{SDRAM}}},
  booktitle = {2005 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Miura, S. and Akiyama, S.},
  date = {2005-05},
  pages = {5250-5253 Vol. 5},
  doi = {10.1109/ISCAS.2005.1465819},
  abstract = {The proposed controller has two main control schemes, address-alignment control and dummy-cache control. These two schemes cooperatively control cached SDRAM to reduce its latency. Testing of the controller using benchmark programs demonstrated that latency was reduced 25\% and execution time was reduced 13\% compared to those of a sense-amplifier cache controller for standard SDRAM. The proposed controller requires 9.2 Kgates at a supply voltage of 1.8 V and an operating frequency of 133 MHz.},
  eventtitle = {2005 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QFVCVC2L\\Miura and Akiyama - 2005 - A memory controller that reduces latency of cached.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NEDWVNM9\\1465819.html},
  keywords = {1.8 V,133 MHz,address-alignment control,benchmark programs,Benchmark testing,Cache memory,cache storage,cached SDRAM latency reduction,Centralized control,Delay,DRAM chips,dummy-cache control,Frequency,integrated circuit design,Laboratories,memory controller,Operational amplifiers,Random access memory,SDRAM,sense-amplifier cache controller,standard SDRAM,synchronous DRAM,Voltage control}
}

@inproceedings{modgilImprovingPerformanceChip2018,
  title = {Improving the {{Performance}} of {{Chip Multiprocessor}} by {{Delayed Write Drain}} and {{Prefetcher}} Based {{Memory Scheduler}}},
  booktitle = {2018 {{Second International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Modgil, A. and Sehgal, V. K.},
  date = {2018-03},
  pages = {1864--1869},
  doi = {10.1109/ICECA.2018.8474846},
  abstract = {To improve the performance and energy consumption of chip multiprocessor (CMP) system, memory request serving latencies should be minimized. These latencies can be minimized by scheduling appropriate memory command at appropriate time. This paper proposes a scheduler that reduces latency related to serving memory read requests by delaying switching into write drain mode when memory traffic is not heavy and write queue is not full. Memory reads are more important to handle than memory writes for system's performance. Further precharge and activate operations are performed using constant stride prefetcher. In idle memory cycles the scheduler issues row precharge commands using cache prefetching technique based on Global History Buffer. Authors in [1] have used stride detector and Global History Buffer based speculative precharges and activates, but they treat memory reads and memory writes equally. Whereas, proposed scheduler in this paper prioritizes reads over writes for better system performance. Our evaluations show that proposed scheduling policy significantly outperforms previous schedulers [1], [2] in varied multicore environments in terms of performance as well as energy consumption. Across a wide range of workloads based on PARSEC benchmark suite, proposed policy improves systems performance by 2.51\%, on 2-core, 0.012\% on 4-core environment in comparison to scheduler proposed in [1].},
  eventtitle = {2018 {{Second International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YNRWI8GF\\Modgil and Sehgal - 2018 - Improving the Performance of Chip Multiprocessor b.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2AHCGKDF\\8474846.html},
  keywords = {Aerospace electronics,cache prefetching technique,cache storage,chip multiprocessor system,CMP,Conferences,constant stride prefetcher,delayed write drain,delaying switching,DRAM,DRAM chips,energy consumption,Energy consumption,Global History Buffer,History,idle memory cycles,Memory Access Scheduler,memory command,Memory management,memory request,memory traffic,multiprocessing systems,Performance,precharge commands,Prefetcher,prefetcher based memory scheduler,Prefetching,Random access memory,scheduling,scheduling policy,systems performance,write drain mode}
}

@inproceedings{morenoNonintrusiveProgramTracing2013,
  title = {Non-Intrusive {{Program Tracing}} and {{Debugging}} of {{Deployed Embedded Systems Through Side}}-Channel {{Analysis}}},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN}}/{{SIGBED Conference}} on {{Languages}}, {{Compilers}} and {{Tools}} for {{Embedded Systems}}},
  author = {Moreno, Carlos and Fischmeister, Sebastian and Hasan, M. Anwar},
  date = {2013},
  pages = {77--88},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2491899.2465570},
  url = {http://doi.acm.org/10.1145/2491899.2465570},
  urldate = {2019-01-30},
  abstract = {One of the hardest aspects of embedded software development is that of debugging, especially when faulty behavior is observed at the production or deployment stage. Non-intrusive observation of the system's behavior is often insufficient to infer the cause of the problem and identify and fix the bug. In this work, we present a novel approach for non-intrusive program tracing aimed at assisting developers in the task of debugging embedded systems at deployment or production stage, where standard debugging tools are usually no longer available. The technique is rooted in cryptography, in particular the area of side-channel attacks. Our proposed technique expands the scope of these cryptographic techniques so that we recover the sequence of operations from power consumption observations (power traces). To this end, we use digital signal processing techniques (in particular, spectral analysis) combined with pattern recognition techniques to determine blocks of source code being executed given the observed power trace. One of the important highlights of our contribution is the fact that the system works on a standard PC, capturing the power traces through the recording input of the sound card. Experimental results are presented and confirm that the approach is viable.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XVYA3LR2\\Moreno et al. - 2013 - Non-intrusive Program Tracing and Debugging of Dep.pdf},
  isbn = {978-1-4503-2085-6},
  keywords = {debugging,embedded systems,side-channel analysis,simple power analysis,tracing},
  series = {{{LCTES}} '13}
}

@article{mutluRunaheadExecutionEffective2003,
  title = {Runahead Execution: {{An}} Effective Alternative to Large Instruction Windows},
  shorttitle = {Runahead Execution},
  author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y. N.},
  date = {2003-11},
  journaltitle = {IEEE Micro},
  volume = {23},
  pages = {20--25},
  issn = {0272-1732},
  doi = {10.1109/MM.2003.1261383},
  abstract = {An instruction window that can tolerate latencies to DRAM memory is prohibitively complex and power hungry. To avoid having to build such large windows, runahead execution uses otherwise-idle clock cycles to achieve an average 22 percent performance improvement for processors with instruction windows of contemporary sizes. This technique incurs only a small hardware cost and does not significantly increase the processor's complexity.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4UCPZSDD\\Mutlu et al. - 2003 - Runahead execution An effective alternative to la.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NJL46HQQ\\1261383.html},
  keywords = {Clocks,computer architecture,Costs,Delay,DRAM memory,Energy consumption,Hardware,instruction sets,instruction windows,Microarchitecture,Out of order,Random access memory,Registers,Retirement,runahead execution,storage management},
  number = {6}
}

@article{nairEvolutionMemoryArchitecture2015,
  title = {Evolution of {{Memory Architecture}}},
  author = {Nair, R.},
  date = {2015},
  journaltitle = {Proceedings of the IEEE},
  volume = {103},
  pages = {1331--1345},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2435018},
  abstract = {Computer memories continue to serve the role that they first served in the electronic discrete variable automatic computer (EDVAC) machine documented by John von Neumann, namely that of supplying instructions and operands for calculations in a timely manner. As technology has made possible significantly larger and faster machines with multiple processors, the relative distance in processor cycles of this memory has increased considerably. Microarchitectural techniques have evolved to share this memory across ever-larger systems of processors with deep cache hierarchies and have managed to hide this latency for many applications, but are proving to be expensive and energy-inefficient for newer types of problems working on massive amounts of data. New paradigms include scale-out systems distributed across hundreds and even thousands of nodes, in-memory databases that keep data in memory much longer than the duration of a single task, and near-data computation, where some of the computation is off-loaded to the location of the data to avoid wasting energy in the movement of data. This paper provides a historical perspective on the evolution of memory architecture, and suggests that the requirements of new problems and new applications are likely to fundamentally change processor and system architecture away from the currently established von Neumann model.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9ER7W34I\\Nair - 2015 - Evolution of Memory Architecture.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RYZCC6CF\\7151782.html},
  keywords = {Approximate memories,cache storage,Computer architecture,computer memory architecture,data location,data movement,deep-cache hierarchies,disk,dynamic random access memory (DRAM),EDVAC machine,electronic discrete variable automatic computer machine,Flash memories,flash memory,in-memory databases,Information processing,main memory,memory architecture,memory hierarchy,Memory management,microarchitectural techniques,multiple processors,near-data processing,non-von Neumann architectures,processing-in-memory,processor cycles,Random access memory,read,Registers,relative distance,scale-out systems,storage-class memory,von Neumann architecture},
  number = {8}
}

@misc{NiosIIProcessor2019,
  title = {Nios {{II Processor Reference Guide}}},
  date = {2019-07-01},
  publisher = {{Intel}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G9PF6HA3\\Nios II Processor Reference Guide.pdf}
}

@inproceedings{nowatzykMissingMemoryWall1996,
  title = {Missing the {{Memory Wall}}: {{The Case}} for {{Processor}}/{{Memory Integration}}},
  shorttitle = {Missing the {{Memory Wall}}},
  booktitle = {23rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}}'96)},
  author = {Nowatzyk, A. and {Fong Pong} and Saulsbury, A.},
  date = {1996-05},
  pages = {90--90},
  doi = {10.1109/ISCA.1996.10008},
  abstract = {Current high performance computer systems use complex, large superscalar CPUs that interface to the main memory through a hierarchy of caches and interconnect systems. These CPU-centric designs invest a lot of power and chip area to bridge the widening gap between CPU and main memory speeds. Yet, many large applications do not operate well on these systems and are limited by the memory subsystem performance.This paper argues for an integrated system approach that uses less-powerful CPUs that are tightly integrated with advanced memory technologies to build competitive systems with greatly reduced cost and complexity. Based on a design study using the next generation 0.25µm, 256Mbit dynamic random-access memory (DRAM) process and on the analysis of existing machines, we show that processor memory integration can be used to build competitive, scalable and cost-effective MP systems.We present results from execution driven uni- and multi-processor simulations showing that the benefits of lower latency and higher bandwidth can compensate for the restrictions on the size and complexity of the integrated processor. In this system, small direct mapped instruction caches with long lines are very effective, as are column buffer data caches augmented with a victim cache.},
  eventtitle = {23rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}}'96)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\39SLDQLP\\Nowatzyk et al. - 1996 - Missing the Memory Wall The Case for ProcessorMe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TB95MCA6\\1563038.html},
  keywords = {Analytical models,Application software,backward error recovery,Bridges,coherence protocol,Computer interfaces,Cost benefit analysis,Delay,fault-tolerance,High performance computing,Paper technology,Power system interconnection,Random access memory,Scalable Shared Memory Multiprocessors}
}

@inproceedings{olanrewajuStudyPerformanceEvaluation2016,
  title = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms: {{A}} Review},
  shorttitle = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms},
  booktitle = {2016 {{Fourth International Conference}} on {{Parallel}}, {{Distributed}} and {{Grid Computing}} ({{PDGC}})},
  author = {Olanrewaju, R. F. and Baba, A. and Khan, B. U. I. and Yaacob, M. and Azman, A. W. and Mir, M. S.},
  date = {2016-12},
  pages = {550--556},
  doi = {10.1109/PDGC.2016.7913185},
  abstract = {Cache Replacement Policies play a significant and contributory role in the context of determining the effectiveness of cache memory cells. It has also become one of the major key features for efficient memory management from the technological aspect. Hence, owing to the existing critical computing systems, it has become too essential to attain faster processing of executable instructions under any adverse situations. In the current scenario, the state of art processors such as Intel multi-core processors for application specific integrated circuits, usually employ various cache replacement policies such as Least Recently Used (LRU) and Pseudo LRU (pLRU), Round Robin, etc. However, fewer amounts of existing research works are found till date to utter about explicit performance issues associated with the conventional cache replacement algorithms. Therefore, the proposed study intended to carry out a performance evaluation to explore the design space of conventional cache replacement policies under SPEC CPU2000 benchmark suite. It initiates and configures the experimental Simple Scalar toolbox prototype on a wide range of cache sizes. The experimental outcomes obtained from the benchmark suite show that PLRU outperforms the conventional LRU concerning computational complexity and a variety of cache blocks organization.},
  eventtitle = {2016 {{Fourth International Conference}} on {{Parallel}}, {{Distributed}} and {{Grid Computing}} ({{PDGC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XDIQGC4I\\Olanrewaju et al. - 2016 - A study on performance evaluation of conventional .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QTGL6JS\\7913185.html},
  keywords = {Benchmark testing,cache blocks organization,cache memory,Cache memory,cache memory cells,cache replacement algorithms,cache replacement policies,cache storage,Context,Decision support systems,Heuristic algorithms,memory management,performance evaluation,Performance evaluation,replacement algorithms,Simple Scalar toolbox prototype,Space exploration,SPEC CPU2000 benchmark suite,storage management}
}

@article{olukotunMultilevelOptimizationPipelined1997,
  title = {Multilevel Optimization of Pipelined Caches},
  author = {Olukotun, K. and Mudge, T. N. and Brown, R. B.},
  date = {1997-10},
  journaltitle = {IEEE Transactions on Computers},
  volume = {46},
  pages = {1093--1102},
  doi = {10.1109/12.628394},
  abstract = {This paper formulates and shows how to solve the problem of selecting the cache size and depth of cache pipelining that maximizes the performance of a given instruction-set architecture. The solution combines trace-driven architectural simulations and the timing analysis of the physical implementation of the cache. Increasing cache size tends to improve performance but this improvement is limited because cache access time increases with its size. This trade-off results in an optimization problem we referred to as multilevel optimization, because it requires the simultaneous consideration of two levels of machine abstraction: the architectural level and the physical implementation level. The introduction of pipelining permits the use of larger caches without increasing their apparent access time, however, the bubbles caused by load and branch delays limit this technique. In this paper we also show how multilevel optimization can be applied to pipelined systems if software- and hardware-based strategies are considered for hiding the branch and load delays. The multilevel optimization technique is illustrated with the design of a pipelined cache for a high clock rate MIPS-based architecture. The results of this design exercise show that, because processors with pipelined caches can have shorter CPU cycle times and larger caches, a significant performance advantage is gained by using two or three pipeline stages to fetch data from the cache. Of course, the results are only optimal for the implementation technologies chosen for the design exercise; other choices could result in quite different optimal designs. The exercise is primarily to illustrate the steps in the design of pipelined caches using multilevel optimization; however, it does exemplify the importance of pipelined caches if high clock rate processors are to achieve high performance.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5FS3XDXT\\Olukotun et al. - 1997 - Multilevel optimization of pipelined caches.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YVAARZIE\\628394.html},
  keywords = {cache pipelining,cache storage,clock rate processors,Clocks,Computer architecture,CPU cycle times,Delay effects,Design optimization,discrete event simulation,Gallium arsenide,instruction sets,instruction-set architecture,Logic design,memory architecture,Multichip modules,multilevel optimization,optimization problem,Packaging,performance,Pipeline processing,pipelined caches,pipelining,timing,Timing,timing analysis,trace-driven architectural simulations},
  number = {10}
}

@inproceedings{oneilLRUKPageReplacement1993,
  title = {The {{LRU}}-{{K}} Page Replacement Algorithm for Database Disk Buffering},
  author = {O'Neil, Elizabeth J. and O'Neil, Patrick E. and Weikum, Gerhard},
  date = {1993},
  pages = {297--306},
  publisher = {{ACM Press}},
  doi = {10.1145/170035.170081},
  url = {http://portal.acm.org/citation.cfm?doid=170035.170081},
  urldate = {2020-01-03},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\E7TV8E25\\O’Neil and O’Neill - The LRU-K Page Replacement Algorithm For Database .pdf},
  isbn = {978-0-89791-592-2},
  langid = {english}
}

@online{OpenSourceHardware,
  title = {Open {{Source Hardware Association}} - {{Homepage}}},
  journaltitle = {Open Source Hardware Association},
  url = {https://www.oshwa.org/},
  urldate = {2019-07-25}
}

@inproceedings{oRowbufferDecouplingCase2014,
  title = {Row-Buffer Decoupling: {{A}} Case for Low-Latency {{DRAM}} Microarchitecture},
  shorttitle = {Row-Buffer Decoupling},
  booktitle = {2014 {{ACM}}/{{IEEE}} 41st {{International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {O, S. and Son, Y. H. and Kim, N. S. and Ahn, J. H.},
  date = {2014-06},
  pages = {337--348},
  doi = {10.1109/ISCA.2014.6853230},
  abstract = {Modern DRAM devices for the main memory are structured to have multiple banks to satisfy ever-increasing throughput, energy-efficiency, and capacity demands. Due to tight cost constraints, only one row can be buffered (opened) per bank and actively service requests at a time, while the row must be deactivated (closed) before a new row is stored into the row buffers. Hasty deactivation unnecessarily re-opens rows for otherwise row-buffer hits while hindsight accompanies the deactivation process on the critical path of accessing data for row-buffer misses. The time to (de)activate a row is comparable to the time to read an open row while applications are often sensitive to DRAM latency. Hence, it is critical to make the right decision on when to close a row. However, the increasing number of banks per DRAM device over generations reduces the number of requests per bank. This forces a memory controller to frequently predict when to close a row due to a lack of information on future requests, while the dynamic nature of memory access patterns limits the prediction accuracy. In this paper, we propose a novel DRAM microarchitecture that can eliminate the need for any prediction. First, we identify that precharging the bitlines dominates the deactivate time, while sense amplifiers that work as a row buffer are physically coupled with the bitlines such that a single command precharges both bitlines and sense amplifiers simultaneously. By decoupling the bitlines from the row buffers using isolation transistors, the bitlines can be precharged right after a row becomes activated. Therefore, only the sense amplifiers need to be precharged for a miss in most cases, taking an order of magnitude shorter time than the conventional deactivation process. Second, we show that this row-buffer decoupling enables internal DRAM μ-operations to be separated and recombined, which can be exploited by memory controllers to make the main memory system more energy efficient. Our experiments demonstrate that row-buffer decoupling improves the geometric mean of the instructions per cycle and MIPS2/W by 14\% and 29\%, respectively, for memory-intensive SPEC CPU2006 applications.},
  eventtitle = {2014 {{ACM}}/{{IEEE}} 41st {{International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F2TEIUI5\\O et al. - 2014 - Row-buffer decoupling A case for low-latency DRAM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GPVT642Y\\6853230.html},
  keywords = {buffer storage,Buffer storage,Capacitance,deactivation process,DRAM chips,DRAM latency,energy efficiency,isolation transistors,low-latency DRAM microarchitecture,memory access patterns,memory controller,Memory management,memory system,memory-intensive SPEC CPU2006 applications,Microarchitecture,modern DRAM devices,Random access memory,row-buffer decoupling,row-buffer misses,Transistors}
}

@incollection{osawaGenerationalReplacementSchemes1997,
  title = {Generational Replacement Schemes for a {{WWW}} Caching Proxy Server},
  booktitle = {High-{{Performance Computing}} and {{Networking}}},
  author = {Osawa, Noritaka and Yuba, Toshitsugu and Hakozaki, Katsuya},
  editor = {Hertzberger, Bob and Sloot, Peter},
  date = {1997},
  volume = {1225},
  pages = {940--949},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0031665},
  url = {http://link.springer.com/10.1007/BFb0031665},
  urldate = {2020-01-18},
  abstract = {This paper proposes and evaluates generational replacement schemes that suit access patterns to the World Wide Web (WWW) proxy server cache. The schemes partition a cache into generations and put frequently accessed data into older generations where entries are less likely to be replaced. Using our schemes, the hit rate per page is improved by about 5.2 percentage points over the Least Recently Used (LRU) algorithm on the basis of logs of more than 8 million accesses. This improvement reduces the number of cache misses by about 10.8 percent with respect to LRU. Our improvement is roughly twice as good as the improvement of LRU over the First-In First-Out (FIFO) algorithm.},
  editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
  editorbtype = {redactor},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\D6QXG4EE\\Osawa et al. - 1997 - Generational replacement schemes for a WWW caching.pdf},
  isbn = {978-3-540-62898-9 978-3-540-69041-2},
  langid = {english},
  options = {useprefix=true}
}

@inproceedings{ozawaCacheMissHeuristics1995,
  title = {Cache Miss Heuristics and Preloading Techniques for General-Purpose Programs},
  booktitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  author = {Ozawa, T. and Kimura, Y. and Nishizaki, S.},
  date = {1995-11},
  pages = {243--248},
  doi = {10.1109/MICRO.1995.476832},
  abstract = {Previous research on hiding memory latencies has tended to focus on regular numerical programs. This paper presents a latency-hiding compiler technique that is applicable to general-purpose C programs. By assuming a lock-up free cache and instruction score-boarding, our technique 'preloads' the data that are likely to cause a cache-miss before they are used, and thereby hiding the cache miss latency. We have developed simple compiler heuristics to identify load instructions that are likely to cause a cache-miss. Experimentation with a set of SPEC92 benchmarks shows that our heuristics are successful in identifying 85\% of cache misses. We have also developed an algorithm that flexibly schedules the selected load instruction and instructions that use the loaded data to hide memory latency. Our simulation suggests that our technique is successful in hiding memory latency and improves the overall performance.},
  eventtitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LMZYV6S3\\Ozawa et al. - 1995 - Cache miss heuristics and preloading techniques fo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LKPBV58X\\476832.html},
  keywords = {C language,C programs,cache miss,cache storage,compiler,compiler heuristics,Computational modeling,Computer simulation,Delay,general-purpose programs,instruction score-boarding,Laboratories,latency-hiding,Load flow analysis,load instructions,lock-up free cache,performance,preloading,Processor scheduling,program compilers,Program processors,Scheduling algorithm,SPEC92 benchmarks,storage management,Testing}
}

@article{pandaExpertPrefetchPrediction2016,
  title = {Expert {{Prefetch Prediction}}: {{An Expert Predicting}} the {{Usefulness}} of {{Hardware Prefetchers}}},
  shorttitle = {Expert {{Prefetch Prediction}}},
  author = {Panda, B. and Balachandran, S.},
  date = {2016-01},
  journaltitle = {IEEE Computer Architecture Letters},
  volume = {15},
  pages = {13--16},
  issn = {1556-6056},
  doi = {10.1109/LCA.2015.2428703},
  abstract = {Hardware prefetching improves system performance by hiding and tolerating the latencies of lower levels of cache and off-chip DRAM. An accurate prefetcher improves system performance whereas an inaccurate prefetcher can cause cache pollution and consume additional bandwidth. Prefetch address filtering techniques improve prefetch accuracy by predicting the usefulness of a prefetch address and based on the outcome of the prediction, the prefetcher decides whether or not to issue a prefetch request. Existing techniques use only one signature to predict the usefulness of a prefetcher but no single predictor works well across all the applications. In this work, we propose weighted-majority filter, an expert way of predicting the usefulness of prefetch addresses. The proposed filter is adaptive in nature and uses the prediction of the best predictor(s) from a pool of predictors. Our filter is orthogonal to the underlying prefetching algorithm. We evaluate the effectiveness of our technique on 22 SPEC-2000/2006 applications. On an average, when employed with three state-of-the-art prefetchers such as AMPM, SMS, and GHB-PC/DC, our filter provides performance improvement of 8.1, 9.3, and 11 percent respectively.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G7L6PJ8W\\Panda and Balachandran - 2016 - Expert Prefetch Prediction An Expert Predicting t.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5E528GPU\\7110318.html},
  keywords = {Accuracy,AMPM,cache,Cache,cache storage,filtering theory,GHB-PC/DC,Hardware,hardware prefetchers,Hardware prefetching,Hardware Prefetching,memory systems,Memory systems,Pollution,Prediction algorithms,prefetch addresses,Prefetching,prefetching algorithm,Radiation detectors,Random access memory,SMS,weighted-majority filter},
  number = {1}
}

@inproceedings{pandaSurveyReplacementStrategies2016,
  title = {A Survey on Replacement Strategies in Cache Memory for Embedded Systems},
  booktitle = {2016 {{IEEE Distributed Computing}}, {{VLSI}}, {{Electrical Circuits}} and {{Robotics}} ({{DISCOVER}})},
  author = {Panda, Parag and Patil, Geeta and Raveendran, Biju},
  date = {2016-08},
  pages = {12--17},
  publisher = {{IEEE}},
  location = {{Mangalore, India}},
  doi = {10.1109/DISCOVER.2016.7806218},
  url = {http://ieeexplore.ieee.org/document/7806218/},
  urldate = {2019-08-02},
  abstract = {Cache is one of the most power-consuming components in computer architecture. Power reduction in cache can be achieved by reducing miss rate, miss penalty, latency per access, and power consumption per access. The power reduction can also be achieved by shutting down unused part of the cache, by allowing not so recently used cache banks to sleep, reconfiguring the cache for specific application and various combinations of one or more of these. The cache hit depends on the cache size, associativity and the cache line size. Replacement strategies in associative mapping schemes play an important role in cache hit rate performance. This survey paper proposes a classification of these strategies with detailed discussion on their advantages and disadvantages.},
  eventtitle = {2016 {{IEEE Distributed Computing}}, {{VLSI}}, {{Electrical Circuits}} and {{Robotics}} ({{DISCOVER}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LLZFK7ZJ\\Panda et al. - 2016 - A survey on replacement strategies in cache memory.pdf},
  isbn = {978-1-5090-1623-5},
  langid = {english}
}

@inproceedings{panImprovingVLIWProcessor2009,
  title = {Improving {{VLIW Processor Performance Using Three}}-{{Dimensional}} ({{3D}}) {{DRAM Stacking}}},
  booktitle = {2009 20th {{IEEE International Conference}} on {{Application}}-Specific {{Systems}}, {{Architectures}} and {{Processors}}},
  author = {Pan, Y. and Zhang, T.},
  date = {2009-07},
  pages = {38--45},
  doi = {10.1109/ASAP.2009.11},
  abstract = {This work studies the potential of using emerging 3D integration to improve embedded VLIW computing system. We focus on the 3D integration of one VLIW processor die with multiple high-capacity DRAM dies. Our proposed memory architecture employs 3D stacking technology to bond one die containing several processing clusters to multiple DRAM dies for a primary memory. The 3D technology also enables wide low-latency buses between clusters and memory and enable the latency of 3D DRAM L2 cache comparable to 2D SRAM L2 cache. These enable it to replace the 2D SRAM L2 cache with 3D DRAM L2 cache. The die area for 2D SRAM L2 cache can be re-allocated to additional clusters that can improve the performance of the system. From the simulation results, we find 3D stacking DRAM main memory can improve the system performance by 10\% 80\% than 2D off-chip DRAM main memory depending on different benchmarks. Also, for a similar logic die area, a four clusters system with 3D DRAM L2 cache and 3D DRAM main memory outperforms a two clusters system with 2D SRAM L2 cache and 3D DRAM main memory by about 10\%.},
  eventtitle = {2009 20th {{IEEE International Conference}} on {{Application}}-Specific {{Systems}}, {{Architectures}} and {{Processors}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BUZIPI6T\\Pan and Zhang - 2009 - Improving VLIW Processor Performance Using Three-D.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\48AKA72V\\5200008.html},
  keywords = {2D SRAM L2 cache,3D DRAM,3D DRAM L2 cache,3D DRAM stacking,cache storage,Computer architecture,Delay,Digital signal processing,DRAM chips,DSP,Embedded computing,embedded VLIW computing system,instruction sets,low-latency buses,memory architecture,multiprocessing systems,parallel architectures,parallel machines,Parallel processing,Random access memory,SRAM chips,Stacking,System performance,USA Councils,VLIW}
}

@article{parkWWCLOCKPageReplacement2009,
  title = {WWCLOCK: Page Replacement Algorithm Considering Asymmetric I/O Cost of Flash Memory},
  shorttitle = {WWCLOCK},
  author = {Park, Jun-Seok and Lee, Eun-Ji and Seo, Hyun-Min and Koh, Kern},
  date = {2009},
  journaltitle = {Journal of KIISE:Computing Practices and Letters},
  volume = {15},
  pages = {913--917},
  issn = {1229-7712},
  url = {http://www.koreascience.or.kr/article/JAKO200909659865050.page},
  urldate = {2019-09-16},
  abstract = {WWCLOCK: Page Replacement Algorithm Considering Asymmetric I/O Cost of Flash Memory Buffer Cache Replacement Algorithm;Cost-aware;Heterogeneous storage; Flash memories have asymmetric I/O costs for read and write in terms of latency and energy consumption. However, the ratio of these costs is dependent on the type of storage. Moreover, it is becoming more common to use two flash memories on a system as an internal memory and an external memory card. For this reason, buffer cache replacement algorithms should consider I/O costs of device as well as possibility of reference. This paper presents WWCLOCK(Write-Weighted CLOCK) algorithm which directly uses I/O costs of devices along with recency and frequency of cache blocks to selecting a victim to evict from the buffer cache. WWCLOCK can be used for wide range of storage devices with different I/O cost and for systems that are using two or more memory devices at the same time. In addition to this, it has low time and space complexity comparable to CLOCK algorithm. Trace-driven simulations show that the proposed algorithm reduces the total I/O time compared with LRU by 36.2\% on average.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HQRBRVH6\\Park et al. - 2009 - WWCLOCK Page Replacement Algorithm Considering As.pdf},
  langid = {kor},
  number = {12}
}

@article{pattersonCaseIntelligentRAM1997,
  title = {A Case for Intelligent {{RAM}}},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  date = {1997},
  journaltitle = {IEEE Micro},
  volume = {17},
  pages = {34--44},
  issn = {02721732},
  doi = {10.1109/40.592312},
  url = {http://ieeexplore.ieee.org/document/592312/},
  urldate = {2019-07-22},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WMAV5QJG\\Patterson et al. - 1997 - A case for intelligent RAM.pdf},
  number = {2}
}

@book{pattersonComputerOrganizationDesign2018,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  shorttitle = {Computer Organization and Design},
  author = {Patterson, David A. and Hennessy, John L.},
  date = {2018},
  edition = {RISC-V edition},
  publisher = {{Morgan Kaufmann Publishers, an imprint of Elsevier}},
  location = {{Cambridge, Massachusetts}},
  abstract = {The new RISC-V Edition of Computer Organization and Design features the RISC-V open source instruction set architecture, the first open source architecture designed to be used in modern computing environments such as cloud computing, mobile devices, and other embedded systems. With the post-PC era now upon us, Computer Organization and Design moves forward to explore this generational change with examples, exercises, and material highlighting the emergence of mobile computing and the Cloud. Updated content featuring tablet computers, Cloud infrastructure, and the x86 (cloud computing) and ARM (mobile computing devices) architectures is included},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4Q6FYX39\\Computer-Organization-and-Design-The-Hardware-Software-Interface-RISC-V-Edition-.pdf},
  isbn = {978-0-12-812275-4},
  keywords = {Computer engineering,Computer interfaces,Computer organization},
  note = {OCLC: ocn993666159},
  pagetotal = {565}
}

@inproceedings{pattersonIntelligentRAMIRAM1997,
  title = {Intelligent {{RAM}} ({{IRAM}}): Chips That Remember and Compute},
  shorttitle = {Intelligent {{RAM}} ({{IRAM}})},
  booktitle = {1997 {{IEEE International Solids}}-{{State Circuits Conference}}. {{Digest}} of {{Technical Papers}}},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  date = {1997-02},
  pages = {224--225},
  doi = {10.1109/ISSCC.1997.585348},
  abstract = {It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.},
  eventtitle = {1997 {{IEEE International Solids}}-{{State Circuits Conference}}. {{Digest}} of {{Technical Papers}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7W5BM63I\\Patterson et al. - 1997 - Intelligent RAM (IRAM) chips that remember and co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KI8AD45T\\585348.html},
  keywords = {adjustable memory width,Bandwidth,board space,chip yield,Computer science,Delay,Electronics industry,high-speed interface,integrated circuit yield,integrated memory circuits,intelligent RAM,IRAM,Logic,memory architecture,memory bandwidth,memory latency,memory retention rates,merged chip,metal layers,microprocessor chips,Microprocessors,Random access memory,random-access storage,Read-write memory,Switches,system power,Vector processors}
}

@inproceedings{penttinenRunTimeDebuggingMonitoring2006,
  title = {Run-{{Time Debugging}} and {{Monitoring}} of {{FPGA Circuits Using Embedded Microprocessor}}},
  booktitle = {2006 {{IEEE Design}} and {{Diagnostics}} of {{Electronic Circuits}} and Systems},
  author = {Penttinen, A. and Jastrzebski, R. and Pollanen, R. and Pyrhonen, O.},
  date = {2006-04},
  pages = {147--148},
  doi = {10.1109/DDECS.2006.1649598},
  abstract = {Field programmable gate arrays (FPGAs) provide a fast and flexible hardware for embedded control systems and signal processing. Despite this, tracing and monitoring of internal signals is awkward. FPGA vendors provide their own tools to solve the debugging problems but they are not sufficient for real time monitoring. Instead, these signal tracing tools are good especially for tracing timing issues. This paper presents a method to monitor the internal signals of FPGA circuits by using an embedded microprocessor. The efficiency of this method is demonstrated with an FPGA-based active magnetic bearing control hardware},
  eventtitle = {2006 {{IEEE Design}} and {{Diagnostics}} of {{Electronic Circuits}} and Systems},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QLPTTALE\\Penttinen et al. - 2006 - Run-Time Debugging and Monitoring of FPGA Circuits.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5TAWRU5B\\1649598.html},
  keywords = {Array signal processing,automatic test equipment,Circuits,Control systems,Debugging,embedded control systems,embedded microprocessor,field programmable gate arrays,Field programmable gate arrays,FPGA circuits,Hardware,logic testing,microprocessor chips,Microprocessors,Monitoring,run-time debugging,run-time monitoring,Runtime,signal processing,Timing}
}

@inproceedings{pitkowSimpleRobustCaching1994,
  title = {A {{Simple Yet Robust Caching Algorithm Based}} on {{Dynamic Access Patterns}}},
  booktitle = {Proceedings of the {{Second International WWW Conference}}},
  author = {Pitkow, James E and Recker, Margaret M},
  date = {1994-10},
  pages = {8},
  abstract = {The World-Wide Web continues its remarkable and seemingly unregulated growth. This growth has seen a corresponding increase in network loads and user response times. One common approach for improving the retrieval rate of large, distributed documents is via caching. In this paper, we present a caching algorithm that flexibly adapt its parameters to the hit rates and access patterns of users requesting documents. The algorithm is derived from an analysis of user accesses in a WWW database. In particular, the analysis is based upon a model from psychological research on human memory, which has long studied retrieval of memory items based on frequency and recency rates of past item occurrences. Results show that the model predicts document access with a high degree of accuracy. Furthermore, the model indicates that a caching algorithm based upon the recency rates of prior document access will reliably handle future document requests. The algorithm presented is simple, robust, and easily implementable.},
  eventtitle = {Second {{International WWW Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\55HXWSK9\\Pitkow and Recker - A Simple Yet Robust Caching Algorithm Based on Dyn.pdf},
  langid = {english}
}

@article{podlipnigSurveyWebCache2003,
  title = {A {{Survey}} of {{Web Cache Replacement Strategies}}},
  author = {Podlipnig, Stefan and Böszörmenyi, Laszlo},
  date = {2003-12},
  journaltitle = {ACM Comput. Surv.},
  volume = {35},
  pages = {374--398},
  issn = {0360-0300},
  doi = {10.1145/954339.954341},
  url = {http://doi.acm.org/10.1145/954339.954341},
  urldate = {2019-08-30},
  abstract = {Web caching is an important technique to scale the Internet. One important performance factor of Web caches is the replacement strategy. Due to specific characteristics of the World Wide Web, there exist a huge number of proposals for cache replacement. This article proposes a classification for these proposals that subsumes prior classifications. Using this classification, different proposals and their advantages and disadvantages are described. Furthermore, the article discusses the importance of cache replacement strategies in modern proxy caches and outlines potential future research topics.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U8RJRSPT\\Podlipnig and Böszörmenyi - 2003 - A Survey of Web Cache Replacement Strategies.pdf},
  keywords = {replacement strategies,Web caching},
  number = {4}
}

@article{ponugotiEnablingOntheFlyHardware2019,
  title = {Enabling {{On}}-the-{{Fly Hardware Tracing}} of {{Data Reads}} in {{Multicores}}},
  author = {Ponugoti, Mounika and Milenkovic, Aleksandar},
  date = {2019-06-10},
  journaltitle = {ACM Transactions on Embedded Computing Systems},
  volume = {18},
  pages = {1--27},
  issn = {15399087},
  doi = {10.1145/3322642},
  url = {http://dl.acm.org/citation.cfm?doid=3340300.3322642},
  urldate = {2019-07-25},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4E6A5JFS\\Ponugoti and Milenkovic - 2019 - Enabling On-the-Fly Hardware Tracing of Data Reads.pdf},
  langid = {english},
  number = {4}
}

@inproceedings{ponugotiExploitingCacheCoherence2016,
  title = {Exploiting Cache Coherence for Effective On-the-Fly Data Tracing in Multicores},
  booktitle = {2016 {{IEEE}} 34th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Ponugoti, M. and Milenković, A.},
  date = {2016-10},
  pages = {312--319},
  doi = {10.1109/ICCD.2016.7753295},
  abstract = {Software testing and debugging of modern embedded computer systems become increasingly a challenging task due to growing hardware and software complexity, increased integration and miniaturization, and ever tightening time-to-market. To find software bugs faster, developers often rely on on-chip trace and debug resources. However, these resources offer limited visibility of the system, increase the system cost, and do not scale well with a growing number of processor cores. This paper introduces a new hardware/software mechanism for capturing and filtering load data value traces in multicores that enables a complete reconstruction of a parallel program execution. The proposed mechanism exploits data caches and cache coherence protocol states to minimize the number of trace events that are necessary to stream out of the target platform to the software debugger. The mechanism relies on a single trace bit per data cache block, thus minimizing the cost of hardware implementation. Our experimental evaluation explores the effectiveness of the proposed technique by measuring the trace port bandwidth as a function of the cache size and the number of processor cores. The results show that the proposed mechanism significantly reduces the required trace port bandwidth when compared to the Nexus-like load data value tracing. Depending on data cache size, the improvements range from 9.9 to 23.5 times for single cores and from 18.6 to 37.3 times for octa cores.},
  eventtitle = {2016 {{IEEE}} 34th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SJ2T9ZVS\\Ponugoti and Milenković - 2016 - Exploiting cache coherence for effective on-the-fl.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZWPU8FQ3\\7753295.html},
  keywords = {Bandwidth,Benchmark testing,cache coherence protocol,cache size,Clocks,Complexity theory,Compression,data cache size,data handling,debug resources,Debugging,Debugging aids,hardware implementation cost minimization,hardware-software mechanism,minimisation,Multicore processing,Nexus-like load data value tracing,on-chip trace,on-the-fly data tracing,processor cores,program debugging,program testing,Real-time embedded systems,Software,software bugs,software complexity,software debugging,software developers,software testing,trace event number minimization,trace port bandwidth,Tracing}
}

@inproceedings{ponugotiOntheflyLoadData2016,
  title = {On-the-Fly Load Data Value Tracing in Multicores},
  booktitle = {2016 {{International Conference}} on {{Compliers}}, {{Architectures}}, and {{Sythesis}} of {{Embedded Systems}} ({{CASES}})},
  author = {Ponugoti, M. and Tewar, A. K. and Milenković, A.},
  date = {2016-10},
  pages = {1--10},
  doi = {10.1145/2968455.2968507},
  abstract = {Software testing and debugging of modern multicore-based embedded systems is a challenging proposition because of growing hardware and software complexity, increased integration, and tightening time-to-market. To find more bugs faster, software developers of real-time embedded systems increasingly rely on on-chip trace and debug resources, including hefty on-chip buffers and wide trace ports. However, these resources often offer limited visibility of the system, increase the system cost, and do not scale well with a growing number of cores. This paper introduces mlvCFiat, a hardware/software mechanism for capturing and filtering load data value traces in multicores. It relies on first-access tracking in data caches and equivalent modules in the software debugger to significantly reduce the number of trace events streamed out of the target platform. Our experimental evaluation explores the effectiveness of the proposed technique as a function of cache sizes, encoding mechanism, and the number of cores. The results show that mlvCFiat significantly reduces the total trace port bandwidth. The improvements relative to the existing Nexus-like load data value tracing range from 15 to 33 times for a single core and from 14 to 20 times for an octa core.},
  eventtitle = {2016 {{International Conference}} on {{Compliers}}, {{Architectures}}, and {{Sythesis}} of {{Embedded Systems}} ({{CASES}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F68ISMJX\\Ponugoti et al. - 2016 - On-the-fly load data value tracing in multicores.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QJ892MH7\\7745281.html},
  keywords = {Bandwidth,Benchmark testing,debugging,embedded systems,hardware complexity,hardware-software codesign,load data value tracing,mlvCFiat mechanism,Multicore processing,multicore-based embedded systems,Multicores,Multiprocessing systems,Nexus-like load data value tracing,on-chip trace,Program processors,program testing,Program tracing,Real-time embedded systems,software complexity,Software debugging,software testing,Software testing and debugging,trace ports}
}

@book{przybylskiCacheMemoryHierarchy1990,
  title = {Cache and Memory Hierarchy Design: A Performance-Directed Approach},
  shorttitle = {Cache and Memory Hierarchy Design},
  author = {Przybylski, Steven A.},
  date = {1990},
  publisher = {{Morgan Kaufmann Publishers}},
  location = {{San Mateo, Calif}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GJ4LFRQ6\\Przybylski - 1990 - Cache and memory hierarchy design a performance-d.pdf},
  isbn = {978-1-55860-136-9},
  keywords = {Cache memory,Memory hierarchy (Computer science)},
  langid = {english},
  pagetotal = {223}
}

@inproceedings{puautWCETCentricSoftwarecontrolledInstruction2006,
  title = {{{WCET}}-{{Centric Software}}-Controlled {{Instruction Caches}} for {{Hard Real}}-{{Time Systems}}},
  booktitle = {18th {{Euromicro Conference}} on {{Real}}-{{Time Systems}} ({{ECRTS}}'06)},
  author = {Puaut, I.},
  date = {2006},
  pages = {217--226},
  publisher = {{IEEE}},
  location = {{Dresden, Germany}},
  doi = {10.1109/ECRTS.2006.32},
  url = {http://ieeexplore.ieee.org/document/1647740/},
  urldate = {2019-08-02},
  abstract = {Cache memories have been extensively used to bridge the gap between high speed processors and relatively slower main memories. However, they are sources of predictability problems because of their dynamic and adaptive behavior, and thus need special attention to be used in hard real-time systems. A lot of progress has been achieved in the last ten years to statically predict worst-case execution times (WCETs) of tasks on architectures with caches. However, cache-aware WCET analysis techniques are not always applicable due to the lack of documentation of hardware manuals concerning the cache replacement policies. Moreover, they tend to be pessimistic with some cache replacement policies (e.g. random replacement policies) [6]. Lastly, caches are sources of timing anomalies in dynamically scheduled processors [13] (a cache miss may in some cases result in a shorter execution time than a hit).},
  eventtitle = {18th {{Euromicro Conference}} on {{Real}}-{{Time Systems}} ({{ECRTS}}'06)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U6HITHT4\\Puaut - 2006 - WCET-Centric Software-controlled Instruction Cache.pdf},
  isbn = {978-0-7695-2619-5},
  langid = {english}
}

@inproceedings{qaziOptimizationAccessLatency2016,
  title = {Optimization of Access Latency in {{DRAM}}},
  booktitle = {2016 {{International Conference}} on {{Computing}}, {{Electronic}} and {{Electrical Engineering}} ({{ICE Cube}})},
  author = {Qazi, A. and Ullah, Z. and Rehman, K. and Khan, M. H. and Bilal, M.},
  date = {2016-04},
  pages = {163--168},
  doi = {10.1109/ICECUBE.2016.7495216},
  abstract = {Modern digital systems, which involve high data computations, suffer from high memory access latency; thus, latency becomes a core issue in the performance enhancement of these advance digital machines. Different factors are behind the high latency of advance digital systems. Approaches like array binding and allocation, code rewriting, and others are adopted to reduce the overall latency of these systems. In this paper, we explore new dimensions to achieve maximum latency optimization in applications that involve extensive memory access. The proposed algorithm of idle/slack time management utilizes empty slots in memory access of different memory modules by appropriately activating upcoming commands in advance. The optimization of latency is further increased by incorporating the multi-way conflict resolution algorithm in second stage and followed by the use of advance dynamic buffers in third stage. Our successive three stages approach of adopting slack time management, multi-way partitioning using min-cut algorithm, and the use of advance dynamic buffers yields better results. Comparison of the experimental results with different benchmarks shows that our proposed technique optimizes the existing page-mode technique by 9\%. Hence, adopting this proposed optimization strategy significantly reduces overall latency of modern digital systems.},
  eventtitle = {2016 {{International Conference}} on {{Computing}}, {{Electronic}} and {{Electrical Engineering}} ({{ICE Cube}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9TU9TPY5\\Qazi et al. - 2016 - Optimization of access latency in DRAM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TTIFJIH4\\7495216.html},
  keywords = {access latency optimization,allocation,array binding,Arrays,Bandwidth,buffer storage,Clocks,code rewriting,data computation,Delays,digital machines,digital system,Digital systems,DRAM,DRAM chips,dynamic buffer,extensive memory access,idle time management,memory access latency,memory modules,min-cut algorithm,multiway conflict resolution algorithm,multiway partitioning,Optimization,page-mode technique,performance enhancement,Resource management,slack time management}
}

@inproceedings{qiwangReducingDRAMCache2016,
  title = {Reducing {{DRAM Cache Access}} in Cache Miss via an Effective Predictor},
  booktitle = {2016 7th {{IEEE International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  author = {{Qi Wang} and {Yanzhen Xing} and {Donghui Wang}},
  date = {2016-08},
  pages = {501--504},
  doi = {10.1109/ICSESS.2016.7883118},
  abstract = {As more and more cores are integrated on a single chip, memory speed has become a major performance bottleneck. The widening latency gap between high speed cores and main memory has led to the evolution of multi-level caches and using DRAM as the Last-Level-Cache (LLC). The main problem of employing DRAM cache is their high tag lookup latency. If DRAM cache misses, the latency of memory access will be increased comparing with the system without DRAM cache. To solve this problem, we propose an effective predictor to Reduce DRAM Cache Access (RCA) in cache miss. The predictor composes of a saturating counter and a Partial MissMap (P\_Map). If the saturating counter indicates a hit, then the request will be send to the P\_Map to further lookup whether it is a hit or not. The evaluation results show that RCA can improve system performance by 8.2\% and 3.4\% on average, compared to MissMap and MAP\_G, respectively.},
  eventtitle = {2016 7th {{IEEE International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BGTM47V4\\Qi Wang et al. - 2016 - Reducing DRAM Cache Access in cache miss via an ef.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U9QHFR6I\\7883118.html},
  keywords = {cache miss,Cache miss,cache storage,DRAM cache,DRAM cache access,DRAM chips,effective predictor,last-level-cache,LLC,multilevel cache,P_Map,partial missmap,predictor,Random access memory,RCA,saturating counter,Two dimensional displays}
}

@inproceedings{quinonesUsingRandomizedCaches2009,
  title = {Using {{Randomized Caches}} in {{Probabilistic Real}}-{{Time Systems}}},
  author = {Quiñones, Eduardo and Berger, Emery D. and Bernat, Guillem and Cazorla, Francisco J.},
  date = {2009-07},
  pages = {129--138},
  publisher = {{IEEE}},
  doi = {10.1109/ECRTS.2009.30},
  url = {http://ieeexplore.ieee.org/document/5161509/},
  urldate = {2019-08-06},
  abstract = {While hardware caches are generally effective at improving application performance, they greatly complicate performance prediction. Slight changes in memory layout or data access patterns can lead to large and systematic increases in cache misses, degrading performance. In the worst case, these misses can effectively render the cache useless. These pathological cases, or “cache risk patterns”, are difficult to predict, test or debug, and their presence limits the usefulness of caches in safety critical real-time systems, especially in hard real-time environments.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JQHCGV28\\Quiñones et al. - 2009 - Using Randomized Caches in Probabilistic Real-Time.pdf},
  isbn = {978-0-7695-3724-5},
  langid = {english}
}

@article{qureshiAdaptiveInsertionPolicies2007,
  title = {Adaptive Insertion Policies for High Performance Caching},
  author = {Qureshi, Moinuddin K. and Jaleel, Aamer and Patt, Yale N. and Steely, Simon C. and Emer, Joel},
  date = {2007-06-09},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  volume = {35},
  pages = {381},
  issn = {01635964},
  doi = {10.1145/1273440.1250709},
  url = {http://portal.acm.org/citation.cfm?doid=1273440.1250709},
  urldate = {2019-11-29},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BHHPMZSD\\Qureshi et al. - Adaptive Insertion Policies for High Performance C.pdf},
  langid = {english},
  number = {2}
}

@article{qureshiCaseMLPAwareCache2006,
  title = {A {{Case}} for {{MLP}}-{{Aware Cache Replacement}}},
  author = {Qureshi, Moinuddin K. and Lynch, Daniel N. and Mutlu, Onur and Patt, Yale N.},
  date = {2006-05-01},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  shortjournal = {SIGARCH Comput. Archit. News},
  volume = {34},
  pages = {167--178},
  issn = {0163-5964},
  doi = {10.1145/1150019.1136501},
  url = {https://doi.org/10.1145/1150019.1136501},
  urldate = {2020-01-13},
  abstract = {Performance loss due to long-latency memory accesses can be reduced by servicing multiple memory accesses concurrently. The notion of generating and servicing long-latency cache misses in parallel is called Memory Level Parallelism (MLP). MLP is not uniform across cache misses - some misses occur in isolation while some occur in parallel with other misses. Isolated misses are more costly on performance than parallel misses. However, traditional cache replacement is not aware of the MLP-dependent cost differential between different misses. Cache replacement, if made MLP-aware, can improve performance by reducing the number of performance-critical isolated misses. This paper makes two key contributions. First, it proposes a framework for MLP-aware cache replacement by using a runtime technique to compute the MLP-based cost for each cache miss. It then describes a simple cache replacement mechanism that takes both MLP-based cost and recency into account. Second, it proposes a novel, low-hardware overhead mechanism called Sampling Based Adaptive Replacement (SBAR), to dynamically choose between an MLP-aware and a traditional replacement policy, depending on which one is more effective at reducing the number of memory related stalls. Evaluations with the SPEC CPU2000 benchmarks show that MLP-aware cache replacement can improve performance by as much as 23\%.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TTY6FRKA\\Qureshi et al. - 2006 - A Case for MLP-Aware Cache Replacement.pdf},
  number = {2}
}

@article{qureshiSetDuelingControlledAdaptiveInsertion2008,
  title = {Set-{{Dueling}}-{{Controlled Adaptive Insertion}} for {{High}}-{{Performance Caching}}},
  author = {Qureshi, M. K. and Jaleel, A. and Patt, Y. N. and Steely, S. C. and Emer, J.},
  date = {2008-01},
  journaltitle = {IEEE Micro},
  volume = {28},
  pages = {91--98},
  doi = {10.1109/MM.2008.14},
  abstract = {The commonly used LRU replacement policy causes thrashing for memory- intensive workloads. A simple mechanism that dynamically changes the insertion policy used by LRU replacement reduces cache misses by 21 percent and requires a total storage overhead of less than 2 bytes.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NTP2QE89\\Qureshi et al. - 2008 - Set-Dueling-Controlled Adaptive Insertion for High.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DYNNZ8PJ\\4460516.html},
  keywords = {Art,Bridges,cache,cache storage,Cache storage,Data structures,Filters,Hardware,high-performance caching,insertion,LRU replacement policy,memory-intensive workloads,Proposals,replacement,set dueling,set sampling,set-dueling-controlled adaptive insertion,storage management,System performance,thrashing},
  number = {1}
}

@inproceedings{rajanEmulatingOptimalReplacement2007,
  title = {Emulating {{Optimal Replacement}} with a {{Shepherd Cache}}},
  booktitle = {40th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}} 2007)},
  author = {Rajan, K. and Ramaswamy, G.},
  date = {2007-12},
  pages = {445--454},
  doi = {10.1109/MICRO.2007.25},
  abstract = {The inherent temporal locality in memory accesses is filtered out by the L1 cache. As a consequence, an L2 cache with LRU replacement incurs significantly higher misses than the optimal replacement policy (OPT). We propose to narrow this gap through a novel replacement strategy that mimics the replacement decisions of OPT. The L2 cache is logically divided into two components, a Shepherd Cache (SC) with a simple FIFO replacement and a Main Cache (MC) with an emulation of optimal replacement. The SC plays the dual role of caching lines and guiding the replacement decisions in MC. Our proposed organization can cover 40\% of the gap between OPT and LRU for a 2MB cache resulting in 7\% overall speedup. Comparison with the dynamic insertion policy, a victim buffer, a V-Way cache and an LRU based fully associative cache demonstrates that our scheme performs better than all these strategies.},
  eventtitle = {40th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}} 2007)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WKTJIP29\\4408275.html},
  keywords = {associative cache,Automation,cache storage,Computer science,Computer science education,content-addressable storage,dynamic insertion policy,Emulation,FIFO replacement,Frequency,History,L1 cache,L2 cache,main cache,memory access,Microarchitecture,optimal replacement policy,Optimized production technology,Proposals,Shepherd cache,Supercomputers,temporal locality}
}

@inproceedings{raveendranLLRULateLRU2007,
  title = {{{LLRU}}: {{Late LRU Replacement Strategy}} for {{Power Efficient Embedded Cache}}},
  shorttitle = {{{LLRU}}},
  booktitle = {15th {{International Conference}} on {{Advanced Computing}} and {{Communications}} ({{ADCOM}} 2007)},
  author = {Raveendran, Biju K. and Sudarshan, T.S.B. and Kumar, P. Dilip and Tangudu, Priyanka and Gurunarayanan, S.},
  date = {2007-12},
  pages = {339--344},
  issn = {null},
  doi = {10.1109/ADCOM.2007.86},
  abstract = {This paper proposes a new cache replacement scheme, late least recently used (LLRU). LLRU takes care of shared pages improves its accessibility and offers improved cache performance. LLRU modifies the existing least recently used (LRU) algorithm. This scheme, improves cache performance for applications, which has shared pages. We also propose square matrix and counter based hardware design for LLRU. We show that the proposed scheme will achieve considerable improvement in hit rate. The experimental results are obtained using Simplescalar2.0 cache simulator benchmark. The hardware performance of LLRU counter and square matrix implementation is measured by using Modelsim and Leonardo spectrum.},
  eventtitle = {15th {{International Conference}} on {{Advanced Computing}} and {{Communications}} ({{ADCOM}} 2007)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6CM2GNBV\\Raveendran et al. - 2007 - LLRU Late LRU Replacement Strategy for Power Effi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I427MUNQ\\4425994.html},
  keywords = {Cache memory,cache storage,Circuit simulation,Complexity theory,counter based hardware design,Counting circuits,Dynamic scheduling,Embedded computing,Energy consumption,Hardware,integrated circuit design,late least recently used cache replacement scheme,Leonardo spectrum,low-power electronics,Modelsim,power efficient embedded cache,Power measurement,Round robin,shared pages,Simplescalar2.0 cache simulator benchmark,square matrix}
}

@inproceedings{reddyIntelligentWebCaching1998,
  title = {Intelligent Web Caching Using Document Life Histories: {{A}} Comparison with Existing Cache Management Techniques},
  shorttitle = {Intelligent Web Caching Using Document Life Histories},
  author = {Reddy, Mike and Fletcher, Graham P.},
  date = {1998},
  abstract = {Hierarchical storage of web pages in proxy server and client browser caches introduce coherence problems, which require cache management techniques which are both accurate and computationally efficient. We suggest that current approaches, such as the most common Least Recently Used (LRU) technique, are inadequate for future network loads as they do not incorporate the dynamics of document selection and modification. We propose the use of an intelligent, adaptive cache management technique to overcome the coherence problem by using document life h stories to optimise cache performance. This work addresses the use of damped exponential smoothing to model accurately the frequency of file requests and modifications, in order to predict the future value of cached files. Finally, we make a mathematical analysis of LRU in comparison with our technique, showing how and why the use of document lif histories is a more effective cache management technique without imposing major computational overheads.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6KZMUCTW\\Reddy and Fletcher - 1998 - Intelligent web caching using document life histor.pdf},
  keywords = {Algorithmic efficiency,leukemia inhibitory factor,Proxy server,Server (computing),Smoothing (statistical technique),Web cache,Web page}
}

@article{reinekeRandomizedCachesConsidered2014,
  title = {Randomized {{Caches Considered Harmful}} in {{Hard Real}}-{{Time Systems}}},
  author = {Reineke, Jan},
  date = {2014-06-10},
  journaltitle = {Leibniz Transactions on Embedded Systems},
  volume = {Vol 1},
  pages = {No 1 (2014)-},
  doi = {10.4230/lites-v001-i001-a003},
  url = {http://ojs.dagstuhl.de/index.php/lites/article/view/LITES-v001-i001-a003},
  urldate = {2019-08-05},
  abstract = {We investigate the suitability of caches with randomized placement and replacement in the context of hard real-time systems. Such caches have been claimed to drastically reduce the amount of information required by static worst-case execution time (WCET) analysis, and to be an enabler for measurement-based probabilistic timing analysis. We refute these claims and conclude that with prevailing static and measurement-based analysis techniques caches with deterministic placement and least-recently-used replacement are preferable over randomized ones.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SNUCVCYD\\Reineke - 2014 - Randomized Caches Considered Harmful in Hard Real-.pdf},
  langid = {english}
}

@incollection{riaz-ud-dinAcmeDBAdaptiveCaching2006,
  title = {Acme-{{DB}}: {{An Adaptive Caching Mechanism Using Multiple Experts}} for {{Database Buffers}}},
  shorttitle = {Acme-{{DB}}},
  booktitle = {Enterprise {{Information Systems VI}}},
  author = {Riaz-ud-Din, Faizal and Kirchberg, Markus},
  editor = {Seruca, Isabel and Cordeiro, José and Hammoudi, Slimane and Filipe, Joaquim},
  date = {2006},
  pages = {72--81},
  publisher = {{Springer-Verlag}},
  location = {{Berlin/Heidelberg}},
  doi = {10.1007/1-4020-3675-2_9},
  url = {http://link.springer.com/10.1007/1-4020-3675-2_9},
  urldate = {2020-01-18},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7UR5DVQZ\\Riaz-ud-Din and Kirchberg - 2006 - Acme-DB An Adaptive Caching Mechanism Using Multi.pdf},
  isbn = {978-1-4020-3674-3},
  langid = {english}
}

@inproceedings{rixnerMemoryAccessScheduling2000,
  title = {Memory Access Scheduling},
  booktitle = {Proceedings of 27th {{International Symposium}} on {{Computer Architecture}} ({{IEEE Cat}}. {{No}}.{{RS00201}})},
  author = {Rixner, S. and Dally, W. J. and Kapasi, U. J. and Mattson, P. and Owens, J. D.},
  date = {2000-06},
  pages = {128--138},
  doi = {10.1145/339647.339668},
  abstract = {The bandwidth and latency of a memory system are strongly dependent on the manner in which accesses interact with the "3-D" structure of banks, rows, and columns characteristic of contemporary DRAM chips. There is nearly an order of magnitude difference in bandwidth between successive references to different columns within a row and different rows within a bank. This paper introduces memory access scheduling, a technique that improves the performance of a memory system by reordering memory references to exploit locality within the 3-D memory structure. Conservative reordering, in which the first ready reference in a sequence is performed, improves bandwidth by 40\% for traces from five media benchmarks. Aggressive reordering, in which operations are scheduled to optimize memory bandwidth, improves bandwidth by 93\% for the same set of applications. Memory access scheduling is particularly important for media processors where it enables the processor to make the most efficient use of scarce memory bandwidth.},
  eventtitle = {Proceedings of 27th {{International Symposium}} on {{Computer Architecture}} ({{IEEE Cat}}. {{No}}.{{RS00201}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ATEENGHW\\Rixner et al. - 2000 - Memory access scheduling.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2EZMNPJD\\854384.html},
  keywords = {Arithmetic,bandwidth,Bandwidth,Delay,DRAM chips,Laboratories,latency,memory access scheduling,memory system,Out of order,Permission,Pipeline processing,processor scheduling,Processor scheduling,Random access memory,storage management,Streaming media}
}

@article{rizzoReplacementPoliciesProxy2000,
  title = {Replacement Policies for a Proxy Cache},
  author = {Rizzo, Luigi and Vicisano, Lorenzo},
  date = {2000-04-01},
  journaltitle = {IEEE/ACM Transactions on Networking},
  shortjournal = {IEEE/ACM Trans. Netw.},
  volume = {8},
  pages = {158--170},
  issn = {1063-6692},
  doi = {10.1109/90.842139},
  url = {https://doi.org/10.1109/90.842139},
  urldate = {2020-03-08},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UAL3HQYV\\Rizzo and Vicisano - 2000 - Replacement policies for a proxy cache.pdf},
  keywords = {caching,communication networks,policies,replacement,Web},
  number = {2}
}

@inproceedings{robinsonDataCacheManagement1990,
  title = {Data {{Cache Management Using Frequency}}-Based {{Replacement}}},
  booktitle = {Proceedings of the 1990 {{ACM SIGMETRICS Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Robinson, John T. and Devarakonda, Murthy V.},
  date = {1990},
  pages = {134--142},
  publisher = {{ACM}},
  location = {{Univ. of Colorado, Boulder, Colorado, USA}},
  doi = {10.1145/98457.98523},
  url = {http://doi.acm.org/10.1145/98457.98523},
  urldate = {2019-09-12},
  abstract = {We propose a new frequency-based replacement algorithm for managing caches used for disk blocks by a file system, database management system, or disk control unit, which we refer to here as data caches. Previously, LRU replacement has usually been used for such caches. We describe a replacement algorithm based on the concept of maintaining reference counts in which locality has been “factored out”. In this algorithm replacement choices are made using a combination of reference frequency and block age. Simulation results based on traces of file system and I/O activity from actual systems show that this algorithm can offer up to 34\% performance improvement over LRU replacement, where the improvement is expressed as the fraction of the performance gain achieved between LRU replacement and the theoretically optimal policy in which the reference string must be known in advance. Furthermore, the implementation complexity and efficiency of this algorithm is comparable to one using LRU replacement.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2LPDE7P8\\Robinson and Devarakonda - 1990 - Data Cache Management Using Frequency-based Replac.pdf},
  isbn = {978-0-89791-359-1},
  series = {{{SIGMETRICS}} '90}
}

@article{robinsonDataCacheManagement1990a,
  title = {Data Cache Management Using Frequency-Based Replacement},
  author = {Robinson, John T. and Devarakonda, Murthy V.},
  date = {1990-04-01},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  volume = {18},
  pages = {134--142},
  issn = {01635999},
  doi = {10.1145/98460.98523},
  url = {http://portal.acm.org/citation.cfm?doid=98460.98523},
  urldate = {2019-11-28},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KISL4NL5\\Robinson and Devarakonda - Data Cache Management Using Frequency-Based Replac.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{samieeWRPWeightingReplacement2008,
  title = {{{WRP}}: {{Weighting Replacement Policy}} to {{Improve Cache Performance}}},
  shorttitle = {{{WRP}}},
  booktitle = {International {{Symposium}} on {{Computer Science}} and Its {{Applications}}},
  author = {Samiee, K. and Rad, G. R.},
  date = {2008-10},
  pages = {38--41},
  doi = {10.1109/CSA.2008.61},
  abstract = {As the performance gap between memory systems and processors has increased, virtual memory management plays an important role in system performance. Different caching policies have different effects on the system performance. This paper studies an adaptive replacement policy which has low overhead on system and is easy to implement. Simulations show that our algorithm performs better than least-recently-used (LRU) and least-frequently-used (LFU). In addition, it performs similarly to LRU in worst cases.},
  eventtitle = {International {{Symposium}} on {{Computer Science}} and Its {{Applications}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WXHTTMWG\\Samiee and Rad - 2008 - WRP Weighting Replacement Policy to Improve Cache.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GUB5F8K4\\4654057.html},
  keywords = {Adaptive systems,Algorithm design and analysis,cache performance improvement,cache storage,Computational modeling,Computers,least-frequently-used algorithm,least-recently-used algorithm,Memory management,paged storage,processor system,Radiation detectors,System performance,virtual memory management,weighting adaptive page replacement policy}
}

@inproceedings{santhanakrishnanGoalorientedSelftuningCaching2004,
  title = {A Goal-Oriented Self-Tuning Caching Algorithm},
  booktitle = {{{IEEE International Conference}} on {{Performance}}, {{Computing}}, and {{Communications}}, 2004},
  author = {Santhanakrishnan, G. and Amer, A. and Chrysanthis, P. K.},
  date = {2004-04},
  pages = {311--312},
  doi = {10.1109/PCCC.2004.1395012},
  abstract = {The contribution of this paper is a novel approach to adaptivity that combines alternatives rather than selecting one among alternatives. Using only three, homogenous, cache replacement algorithms, GD-GhOST were able to provide a cache replacement policy that requires no tuning or user-intervention beyond the initial selection of the performance criteria to be optimized. Overall, at its worst observed performance GD-GhOST was within approximately 1\% of the best policy's miss ratio, and at its best, GD-GhOST reduced byte miss rates by well over 50\%.},
  eventtitle = {{{IEEE International Conference}} on {{Performance}}, {{Computing}}, and {{Communications}}, 2004},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GHAILCAJ\\Santhanakrishnan et al. - 2004 - A goal-oriented self-tuning caching algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AJ5ZVYBU\\1395012.html},
  keywords = {Adaptive algorithm,Automation,cache replacement policy,cache storage,Computer science,Frequency,goal-oriented self-tuning caching algorithm,greedy dual size caching algorithm,Internet,Measurement,Performance evaluation,Testing,Tuning}
}

@inproceedings{scheipelSystemAwarePerformanceMonitoring2017,
  title = {System-{{Aware Performance Monitoring Unit}} for {{RISC}}-{{V Architectures}}},
  booktitle = {2017 {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  author = {Scheipel, T. and Mauroner, F. and Baunach, M.},
  date = {2017-08},
  pages = {86--93},
  doi = {10.1109/DSD.2017.28},
  abstract = {Due to increasing complexity of software in embedded systems, performance aspects become much more important this days. This should happen early in the development process. Often execution times and events are not easily countable or measurable due to a lack of functionality in these systems. Execution time monitoring is also relevant in terms of reacting to internal and external events dynamically.Especially for systems using multiple tasks with internal or external resource dependencies, this is a major discipline. Another problem is that measurements during the development process are often done by interfering the system as a whole. This method leads to biases in the measurement results, because the finalized system gets deployed without these interfering functionalities and can therefore work more efficiently than the development system.The scope of the present work is to develop a module in a hardware description language (HDL) which is able to measure execution times and events task-aware and unaware without interfering the system. The measurements of this module must be handed to the programmer through an easy accessible interface. The main focuses of the project are the scalability, platform independency concerning processor and operating system (OS), as well as easy extendibility. Also, reusability of counters during runtime is included in this work.},
  eventtitle = {2017 {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HP438XX3\\Scheipel et al. - 2017 - System-Aware Performance Monitoring Unit for RISC-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KD3RHSUU\\8049771.html},
  keywords = {embedded systems,field programmable gate array,Hardware,hardware/software codesign,Monitoring,performance monitoring unit,Phasor measurement units,Pipelines,Radiation detectors,Registers}
}

@inproceedings{seong-ilparkHistorybasedMemoryMode2003,
  title = {History-Based Memory Mode Prediction for Improving Memory Performance},
  booktitle = {Proceedings of the 2003 {{International Symposium}} on {{Circuits}} and {{Systems}}, 2003. {{ISCAS}} '03.},
  author = {{Seong-Il Park} and {In-Cheol Park}},
  date = {2003-05},
  volume = {5},
  pages = {V-V},
  doi = {10.1109/ISCAS.2003.1206226},
  abstract = {To increase the bandwidth of synchronous memories that are widely adopted for high performance memory systems, a predictive mode control scheme is proposed in this paper. Memory latency can be reduced by effectively managing the states of banks. The local access history of each bank is considered to predict the memory mode. Experimental results show that the proposed scheme, at the cost of negligible area overhead, reduces the memory latency by 19.0\% over the conventional scheme that always keeps the memory in idle state.},
  eventtitle = {Proceedings of the 2003 {{International Symposium}} on {{Circuits}} and {{Systems}}, 2003. {{ISCAS}} '03.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2VUA6ZWA\\Seong-Il Park and In-Cheol Park - 2003 - History-based memory mode prediction for improving.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HYY2MUEJ\\1206226.html},
  keywords = {area overhead,bandwidth,Bandwidth,Control systems,Costs,Decoding,Delay,DRAM chips,History,history-based memory mode prediction,idle state,integrated circuit design,latency,local access history,Memory management,memory mode,memory performance,predictive mode control scheme,Random access memory,SDRAM,synchronous memories,System-on-a-chip}
}

@inproceedings{seshadriGatherScatterDRAMInDRAM2015,
  title = {Gather-{{Scatter DRAM}}: {{In}}-{{DRAM}} Address Translation to Improve the Spatial Locality of Non-Unit Strided Accesses},
  shorttitle = {Gather-{{Scatter DRAM}}},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Seshadri, V. and Mullins, T. and Boroumand, A. and Mutlu, O. and Gibbons, P. B. and Kozuch, M. A. and Mowry, T. C.},
  date = {2015-12},
  pages = {267--280},
  doi = {10.1145/2830772.2830820},
  abstract = {Many data structures (e.g., matrices) are typically accessed with multiple access patterns. Depending on the layout of the data structure in physical address space, some access patterns result in non-unit strides. In existing systems, which are optimized to store and access cache lines, non-unit strided accesses exhibit low spatial locality. Therefore, they incur high latency, and waste memory bandwidth and cache space. We propose the Gather-Scatter DRAM (GS-DRAM) to address this problem. We observe that a commodity DRAM module contains many chips. Each chip stores a part of every cache line mapped to the module. Our idea is to enable the memory controller to access multiple values that belong to a strided pattern from different chips using a single read/write command. To realize this idea, GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips. Second, instead of sending a separate address to each chip, GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module. Based on the pattern ID, each chip independently computes the address of the value to be accessed. The cache line returned by the module contains different values of the strided pattern gathered from different chips. We show that this approach enables GS-DRAM to achieve near-ideal memory bandwidth and cache utilization for many common access patterns. We design an end-to-end system to exploit GS-DRAM. Our evaluations show that 1) for in-memory databases, GS-DRAM obtains the best of the row store and the column store layouts, in terms of both performance and energy, and 2) for matrix-matrix multiplication, GS-DRAM seamlessly enables SIMD optimizations and outperforms the best tiled layout. Our framework is general, and can benefit many modern data-intensive applications.},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KICKZTBD\\Seshadri et al. - 2015 - Gather-Scatter DRAM In-DRAM address translation t.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YGS6DMYZ\\7856604.html},
  keywords = {cache line,cache utilization,Caches,commodity module,data structures,data-intensive applications,DRAM,DRAM chips,Energy,explixit near-ideal memory bandwidth,gather-scatter DRAM,GS-DRAM,in-memory databases,In-memory databases,matrix multiplication,matrix-matrix multiplication,Memory bandwidth,memory controller,multiple access patterns,nonunit strided accesses,parallel processing,pattern ID,Performance,physical address space,Servers,SIMD,SIMD optimizations,single read command,single write command,spatial locality improvement,Strided accesses}
}

@inproceedings{shaoBurstSchedulingAccess2007,
  title = {A {{Burst Scheduling Access Reordering Mechanism}}},
  booktitle = {2007 {{IEEE}} 13th {{International Symposium}} on {{High Performance Computer Architecture}}},
  author = {Shao, J. and Davis, B. T.},
  date = {2007-02},
  pages = {285--294},
  doi = {10.1109/HPCA.2007.346206},
  abstract = {Utilizing the nonuniform latencies of SDRAM devices, access reordering mechanisms alter the sequence of main memory access streams to reduce the observed access latency. Using a revised M5 simulator with an accurate SDRAM module, the burst scheduling access reordering mechanism is proposed and compared to conventional in order memory scheduling as well as existing academic and industrial access reordering mechanisms. With burst scheduling, memory accesses to the same rows of the same banks are clustered into bursts to maximize bus utilization of the SDRAM device. Subject to a static threshold, memory reads are allowed to preempt ongoing writes for reduced read latency, while qualified writes are piggybacked at the end of bursts to exploit row locality in writes and prevent write queue saturation. Performance improvements contributed by read preemption and write piggybacking are identified. Simulation results show that burst scheduling reduces the average execution time of selected SPEC CPU2000 benchmarks by 21\% over conventional bank in order memory scheduling. Burst scheduling also outperforms Intel's patented out of order memory scheduling and the row hit access reordering mechanism by 11\% and 6\% respectively},
  eventtitle = {2007 {{IEEE}} 13th {{International Symposium}} on {{High Performance Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZV8GWYP9\\Shao and Davis - 2007 - A Burst Scheduling Access Reordering Mechanism.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZWPBSKJ6\\4147669.html},
  keywords = {access latency,Aerospace industry,Bandwidth,burst scheduling access reordering mechanism,bus utilization,Computational modeling,Delay,DRAM chips,Job shop scheduling,M5 simulator,main memory access streams,order memory scheduling,Out of order,Processor scheduling,Read-write memory,scheduling,SDRAM,SDRAM devices,Space exploration,SPEC CPU2000 benchmarks,storage management,system buses}
}

@article{shinDRAMLatencyOptimizationInspired2016,
  title = {{{DRAM}}-{{Latency Optimization Inspired}} by {{Relationship}} between {{Row}}-{{Access Time}} and {{Refresh Timing}}},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Moon, Y. and Kwon, Y. and Kim, L.},
  date = {2016-10},
  journaltitle = {IEEE Transactions on Computers},
  volume = {65},
  pages = {3027--3040},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2512863},
  abstract = {It is widely known that relatively long DRAM latency forms a bottleneck in computing systems. However, DRAM vendors are strongly reluctant to decrease DRAM latency due to the additional manufacturing cost. Therefore, we set our goal to reduce DRAM latency without any modification in the existing DRAM structure. To accomplish our goal, we focus on an intrinsic phenomenon in DRAM: electric charge variation in DRAM cell capacitors. Then, we draw two key insights: i) DRAM row-access latency of a row is a function of the elapsed time from when the row was last refreshed, and ii) DRAM row-access latency of a row is also a function of the remaining time until the row is next refreshed. Based on these two insights, we propose two mechanisms to reduce DRAM latency: NUAT-1 and NUAT-2. NUAT-1 exploits the first key insight and NUAT-2 exploits the second key insight. For evaluation, circuit- and system-level simulations are performed, which show the performance improvement for various environments.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9AWRMECC\\Shin et al. - 2016 - DRAM-Latency Optimization Inspired by Relationship.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G7DNNSVB\\7366754.html},
  keywords = {Capacitors,circuit simulation,circuit-level simulations,computing systems,Decoding,DRAM,DRAM cell capacitors,DRAM chips,DRAM latency optimization,DRAM row-access latency,DRAM structure,DRAM vendors,DRAM-latency,electric charge,electric charge variation,memory controller,Memory management,non-uniform access time (NUAT),nonuniform access time,NUAT-1,NUAT-2,Random access memory,refresh,refresh timing,row-access time,system-level simulations,timing,Transistors},
  number = {10}
}

@article{shinMcDRAMLowLatency2018,
  title = {{{McDRAM}}: {{Low Latency}} and {{Energy}}-{{Efficient Matrix Computations}} in {{DRAM}}},
  shorttitle = {{{McDRAM}}},
  author = {Shin, H. and Kim, D. and Park, E. and Park, S. and Park, Y. and Yoo, S.},
  date = {2018-11},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {37},
  pages = {2613--2622},
  issn = {0278-0070},
  doi = {10.1109/TCAD.2018.2857044},
  abstract = {We propose a novel memory architecture for in-memory computation called McDRAM, where DRAM dies are equipped with a large number of multiply accumulate (MAC) units to perform matrix computation for neural networks. By exploiting high internal memory bandwidth and reducing offchip memory accesses, McDRAM realizes both low latency and energy efficient computation. In our experiments, we obtained the chip layout based on the state-of-the-art memory, LPDDR4 where McDRAM is equipped with 2048 MACs in a single chip package with a small area overhead (4.7\%). Compared with the state-ofthe-art accelerator, TPU and the power-efficient GPU, Nvidia P4, McDRAM offers 9.5× and 14.4× speedup, respectively, in the case that the large-scale MLPs and RNNs adopt the batch size of 1. McDRAM also gives 2.1× and 3.7× better computational efficiency in TOPS/W than TPU and P4, respectively, for the large batches.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QBFSJIT\\Shin et al. - 2018 - McDRAM Low Latency and Energy-Efficient Matrix Co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JFGECBAF\\8493536.html},
  keywords = {Bandwidth,computational efficiency,Decoding,DRAM chips,Dynamic memory,energy efficient computation,energy-efficient matrix computations,high internal memory bandwidth,in-memory computation,matrix computation,McDRAM,memory architecture,Memory management,multiply accumulate units,neural nets,Neural networks,neural networks (NNs),Performance evaluation,power-efficient GPU,processing in memory,Random access memory},
  number = {11}
}

@inproceedings{shinNUATNonuniformAccess2014,
  title = {{{NUAT}}: {{A}} Non-Uniform Access Time Memory Controller},
  shorttitle = {{{NUAT}}},
  booktitle = {2014 {{IEEE}} 20th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Shin, W. and Yang, J. and Choi, J. and Kim, L.},
  date = {2014-02},
  pages = {464--475},
  doi = {10.1109/HPCA.2014.6835956},
  abstract = {With rapid development of micro-processors, off-chip memory access becomes a system bottleneck. DRAM, a main memory in most computers, has concentrated only on capacity and bandwidth for decades to achieve high performance computing. However, DRAM access latency should also be considered to keep the development trend in multi-core era. Therefore, we propose NUAT which is a new memory controller focusing on reducing memory access latency without any modification of the existing DRAM structure. We only exploit DRAM's intrinsic phenomenon: electric charge variation in DRAM cell capacitors. Given the cost-sensitive DRAM market, it is a big advantage in terms of actual implementation. NUAT gives a score to every memory access request and the request with the highest score obtains a priority. For scoring, we introduce two new concepts: Partitioned Bank Rotation (PBR) and PBR Page Mode (PPM). First, PBR is a mechanism that draws information of access speed from refresh timing and position; the request which has faster access speed gains higher score. Second, PPM selects a better page mode between open- and close-page modes based on the information from PBR. Evaluations show that NUAT decreases memory access latency significantly for various environments.},
  eventtitle = {2014 {{IEEE}} 20th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C25T4M26\\Shin et al. - 2014 - NUAT A non-uniform access time memory controller.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8RTQX2Q5\\6835956.html},
  keywords = {Abstracts,capacitors,Capacitors,cost-sensitive DRAM market,DRAM access latency,DRAM cell capacitors,DRAM chips,electric charge variation,Lead,memory access latency reduction,memory access request,microcontrollers,microprocessor development,nonuniform access time memory controller,NUAT,off-chip memory access,partitioned bank rotation,PBR page mode,PPM,Random access memory,Sensors,Standards,system bottleneck,Timing}
}

@article{shinQDRAMQuickAccessDRAM2016,
  title = {Q-{{DRAM}}: {{Quick}}-{{Access DRAM}} with {{Decoupled Restoring}} from {{Row}}-{{Activation}}},
  shorttitle = {Q-{{DRAM}}},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, H. and Kim, L.},
  date = {2016-07},
  journaltitle = {IEEE Transactions on Computers},
  volume = {65},
  pages = {2213--2227},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2479587},
  abstract = {The relatively high latency of DRAM is mostly caused by the long row-activation time which in fact consists of sensing and restoring time. Memory controllers cannot distinguish between them since they are performed consecutively by a single row-activation command. If these two steps are separated, the restoring can be delayed until DRAM access is uncongested. Hence, we propose Quick-Access DRAM (Q-DRAM) which discriminates between sensing and restoring. Our approach is to allow destructive access (i.e., only sensing is performed without restoring by a row-activation command) using per-bank multiple row-buffers. We call the destructive access and per-bank multiple row-buffers quick-access and quick-buffers (q-buffers) respectively. In addition, we propose Quick-access Trigger (Q-TRIGGER) and RESTORER to utilize Q-DRAM. Q-TRIGGER makes a decision whether quick-access is required or not, and RESTORER decides when to restore the data at the destructed cell. Specifically, RESTORER detects the proper timing to hide restoring time by predicting data bus occupation and by exploiting bank-level locality. Evaluations show that Q-DRAM significantly improved performance for both single- and multi-core systems.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9RRH87RD\\Shin et al. - 2016 - Q-DRAM Quick-Access DRAM with Decoupled Restoring.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDB8AWTC\\7271043.html},
  keywords = {bank-level locality,Computer architecture,data bus occupation prediction,decoupled restoring,decoupling,Decoupling,destructive access,Destructive access,DRAM,DRAM access,DRAM chips,dynamic random access memory,memory controllers,Microprocessors,multi-core systems,Parallel processing,per-bank multiple row-buffers,Per-bank multiple row-buffers,Q-DRAM,Q-TRIGGER,quick-access,Quick-access,quick-access DRAM,quick-access trigger,Radiation detectors,Random access memory,RESTORER,restoring time,row-activation,Row-activation,row-activation time,sensing time,single-core systems,Timing},
  number = {7}
}

@inproceedings{shoukryProactiveSchedulingContent2014,
  title = {Proactive Scheduling for Content Pre-Fetching in Mobile Networks},
  booktitle = {2014 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  author = {Shoukry, O. and ElMohsen, M. A. and Tadrous, J. and Gamal, H. E. and ElBatt, T. and Wanas, N. and Elnakieb, Y. and Khairy, M.},
  date = {2014-06},
  pages = {2848--2854},
  doi = {10.1109/ICC.2014.6883756},
  abstract = {The global adoption of smart phones has raised major concerns about a potential surge in the wireless traffic due to the excessive demand on multimedia services. This ever increasing demand is projected to cause significant congestions and degrade the quality of service for network users. In this paper, we develop a proactive caching framework that utilizes the predictability of the mobile user behavior to offload predictable traffic through the WiFi networks ahead of time. First, we formulate the proactive scheduling problem with the objective of maximizing the user-content hit ratio subject to constrains stemming from the user behavioral models. Second, we propose a quadratic-complexity (in the number of slots per day) greedy, yet, high performance heuristic algorithm that pinpoints the best download slot for each content item to attain maximal hit ratio. We confirm the merits of the proposed scheme based on the traces of a real dataset leveraging a large number of smart phone users who consistently utilized our framework for two months.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9KVCQZEL\\Shoukry et al. - 2014 - Proactive scheduling for content pre-fetching in m.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4WIAE3B5\\6883756.html},
  keywords = {Batteries,behavioral models,content pre-fetching,Content pre-fetching,Data models,greedy algorithms,high performance heuristic algorithm,IEEE 802.11 Standards,maximal hit ratio,Mobile communication,Mobile computing,mobile networks,mobile radio,mobile user behavior predictability,multimedia services,offload predictable traffic,proactive caching framework,proactive scheduling problem,quadratic-complexity,quality of service,scheduling,smart phone user traces,smart phones,Smart phones,storage management,telecommunication traffic,traffic offloading,WiFi networks,Wireless communication,wireless LAN,wireless traffic}
}

@article{singhResourceThroughputAware2016,
  title = {Resource and {{Throughput Aware Execution Trace Analysis}} for {{Efficient Run}}-{{Time Mapping}} on {{MPSoCs}}},
  author = {Singh, Amit Kumar and Shafique, Muhammad and Kumar, Akash and Henkel, Jorg},
  date = {2016-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {35},
  pages = {72--85},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2015.2446938},
  url = {http://ieeexplore.ieee.org/document/7128364/},
  urldate = {2019-01-28},
  abstract = {There have been several efforts on run-time mapping of applications on multiprocessor-systems-on-chip. These traditional efforts perform either on-the-fly processing or use design-time analyzed results. However, on-the-fly processing often leads to low-quality mappings, and design-time analysis becomes computationally costly for large-size problems and require huge storage for large number of applications. In this paper, we present a novel run-time mapping approach, where identification of an efficient mapping for a use-case is done by the online execution trace analysis of the active applications. The trace analysis facilitates for fast identification of the mapping while optimizing for the system resource usage and throughput of the active applications, leading to reduced energy consumption as well. By rapidly identifying the efficient mapping at run-time, the proposed approach overcomes the mappings’ exploration time bottleneck for large-size problems and their storage overhead problem when compared to the traditional approaches. Our experiments show that on average the exploration time to identify the mapping is reduced 14× when compared to stateof-the-art approaches and storage overhead is reduced by 92\%. Additionally, energy and resource savings are achieved along with identification of high-quality mapping.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\D2ED9PFG\\Singh et al. - 2016 - Resource and Throughput Aware Execution Trace Anal.pdf},
  langid = {english},
  number = {1}
}

@inproceedings{smaragdakisEELRUSimpleEffective1999,
  title = {{{EELRU}}: {{Simple}} and {{Effective Adaptive Page Replacement}}},
  shorttitle = {{{EELRU}}},
  booktitle = {Proceedings of the 1999 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Smaragdakis, Yannis and Kaplan, Scott and Wilson, Paul},
  date = {1999},
  pages = {122--133},
  publisher = {{ACM}},
  location = {{Atlanta, Georgia, USA}},
  doi = {10.1145/301453.301486},
  url = {http://doi.acm.org/10.1145/301453.301486},
  urldate = {2019-09-16},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9Y4K9D4P\\Smaragdakis et al. - 1999 - EELRU Simple and Effective Adaptive Page Replacem.pdf},
  isbn = {978-1-58113-083-6},
  series = {{{SIGMETRICS}} '99}
}

@inproceedings{smaragdakisGeneralAdaptiveReplacement2004,
  title = {General Adaptive Replacement Policies},
  author = {Smaragdakis, Yannis},
  date = {2004},
  pages = {108},
  publisher = {{ACM Press}},
  doi = {10.1145/1029873.1029887},
  url = {http://portal.acm.org/citation.cfm?doid=1029873.1029887},
  urldate = {2019-08-05},
  abstract = {We propose a general scheme for creating adaptive replacement policies with good performance and strong theoretical guarantees. Specifically, we show how to combine any two existing replacement policies so that the resulting policy provably can never perform worse than either of the original policies by more than a small factor. To show that our scheme performs very well with real application data, we derive a virtual memory replacement policy that adapts between LRU, loop detection, LFU, and MRU-like replacement. The resulting policy often performs better than all of the policies it adapts over, as well as two other hand-tuned adaptive policies from the recent literature.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8HFTAD4R\\Smaragdakis - 2004 - General adaptive replacement policies.pdf},
  isbn = {978-1-58113-945-7},
  langid = {english}
}

@article{smithCacheMemories1982,
  title = {Cache {{Memories}}},
  author = {Smith, Alan Jay},
  date = {1982-09},
  journaltitle = {ACM Comput. Surv.},
  volume = {14},
  pages = {473--530},
  issn = {0360-0300},
  doi = {10.1145/356887.356892},
  url = {http://doi.acm.org/10.1145/356887.356892},
  urldate = {2019-08-06},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JALV7R5I\\Smith - 1982 - Cache Memories.pdf},
  number = {3}
}

@article{soCacheOperationsMRU1988,
  title = {Cache Operations by {{MRU}} Change},
  author = {So, K. and Rechtschaffen, R. N.},
  date = {1988-06},
  journaltitle = {IEEE Transactions on Computers},
  volume = {37},
  pages = {700--709},
  issn = {0018-9340},
  doi = {10.1109/12.2208},
  abstract = {The performance of set associative caches is analyzed. The method used is to group the cache lines into regions according to their positions in the replacement stacks of a cache, and then to observe how the memory access of a CPU is distributed over these regions. Results from the preserved CPU traces show that the memory accesses are heavily concentrated on the most recently used (MRU) region in the cache. The concept of MRU change is introduced; the idea is to use the event that the CPU accesses a non-MRU line to approximate the time the CPU is changing its working set. The concept is shown to be useful in many aspects of cache design and performance evaluation, such as comparison of various replacement algorithms, improvement of prefetch algorithms, and speedup of cache simulation.{$<>$}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\STZIW7R2\\So and Rechtschaffen - 1988 - Cache operations by MRU change.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\N4EDTW82\\2208.html},
  keywords = {Algorithm design and analysis,cache simulation,Cache storage,Central Processing Unit,Computational modeling,content-addressable storage,CPU,Memory,memory access,Microcomputers,most recently used,MRU change,performance,performance evaluation,prefetch algorithms,Prefetching,replacement algorithms,set associative caches,storage management,Very large scale integration,virtual storage},
  number = {6}
}

@inproceedings{sokolinskyLFUKEffectiveBuffer2004,
  title = {{{LFU}}-{{K}}: {{An Effective Buffer Management Replacement Algorithm}}},
  shorttitle = {{{LFU}}-{{K}}},
  booktitle = {Database {{Systems}} for {{Advanced Applications}}},
  author = {Sokolinsky, Leonid B.},
  editor = {Lee, YoonJoon and Li, Jianzhong and Whang, Kyu-Young and Lee, Doheon},
  date = {2004},
  pages = {670--681},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24571-1_60},
  abstract = {This paper introduces a new approach to database disk buffering, called the LFU- K method. The LFU- K page replacement algorithm is an improvement to the Least Frequently Used (LFU) algorithm. The paper proposes a theoretical-probability model for formal description of LFU- K algorithm. Using this model we evaluate estimations for the LFU- K parameters. This paper also describes an implementation of LFU-2 policy. As we demonstrate by trace-driven simulation experiments, the LFU-2 algorithm provides significant improvement over conventional buffering algorithms for the shared-nothing database systems.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\54AUTECS\\Sokolinsky - 2004 - LFU-K An Effective Buffer Management Replacement .pdf},
  isbn = {978-3-540-24571-1},
  keywords = {Buffer Management,Buffer Size,Periodic Distribution,Placement Algorithm,Reference Probability},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{solihinUsingUserlevelMemory2002,
  title = {Using a User-Level Memory Thread for Correlation Prefetching},
  booktitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Solihin, Y. and {Jaejin Lee} and Torrellas, J.},
  date = {2002-05},
  pages = {171--182},
  doi = {10.1109/ISCA.2002.1003576},
  abstract = {This paper introduces the idea of using a user-level memory thread (ULMT) for correlation prefetching. In this approach, a user thread runs on a general-purpose processor in main memory, either in the memory controller chip or in a DRAM chip. The thread performs correlation prefetching in software, sending the prefetched data into the L2 cache of the main processor. This approach requires minimal hardware beyond the memory processor: the correlation table is a software data structure that resides in the main memory, while the main processor only needs a few modifications to its L2 cache so that it can accept incoming prefetches. In addition, the approach has wide usability, as it can effectively prefetch even for irregular applications. Finally, it is very flexible, as the prefetching algorithm can be customized by the user on an application basis. Our simulation results show that, through a new design of the correlation table and prefetching algorithm, our scheme delivers good results. Specifically, nine mostly-irregular applications show an average speedup of 1.32. Furthermore, our scheme works well in combination with a conventional processor-side sequential prefetcher, in which case the average speedup increases to 1.46. Finally, by exploiting the customization of the prefetching algorithm, we increase the average speed up to 1.53.},
  eventtitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\455NZ3WH\\Solihin et al. - 2002 - Using a user-level memory thread for correlation p.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZFDFIZ4\\1003576.html},
  keywords = {Application software,correlation methods,correlation prefetching,correlation table,customization,data structure prefetching,Data structures,Engines,Graphics,Hardware,memory access latencies,memory architecture,Prefetching,Proposals,Software performance,storage management,Usability,user-level memory thread,Yarn}
}

@inproceedings{soryaniPerformanceEvaluationCache2007,
  title = {Performance {{Evaluation}} of {{Cache Memory Organizations}} in {{Embedded Systems}}},
  booktitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  author = {Soryani, M. and Sharifi, M. and Rezvani, M. H.},
  date = {2007-04},
  pages = {1045--1050},
  doi = {10.1109/ITNG.2007.150},
  abstract = {The tremendous rise in microprocessor technology has offered high speed processors and has increased the processor-memory speed gap dramatically. On the other hand, real-time embedded systems often have a hard deadline to complete their instructions. Consequently, the design of cache memory hierarchy is a critical issue in embedded systems. This paper describes a simulation-based performance evaluation of typical cache design issues in embedded systems such as using split caches for data and instruction versus unified cache for data and instruction, cache size and associativity and replacement policy. The evaluation is done using SimpleScalar simulation tools based on its Alpha version. We select some benchmarks for this study based on some previous researches about the clustering of SPEC CPU2000 benchmark suite. The contribution of this work is identifying important parameters for cache design in general-purpose embedded systems. Our results show that the Pseudo LRU techniques for cache replacement, such as MRU can approximate LRU with much lower complexity for a wide variety of cache sizes and degree of associativities},
  eventtitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HMGSFECC\\Soryani et al. - 2007 - Performance Evaluation of Cache Memory Organizatio.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8RWY5RDL\\4151842.html},
  keywords = {Cache memory,cache memory hierarchy design,cache memory organization,cache storage,Computer architecture,Costs,Embedded computing,Embedded system,embedded systems,Hardware,High performance computing,integrated circuit design,Microprocessors,Optimized production technology,performance evaluation,Real time systems,real-time embedded systems,split caches,unified cache}
}

@inproceedings{sreedharanCacheReplacementPolicy2017,
  title = {A Cache Replacement Policy Based on Re-Reference Count},
  booktitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  author = {Sreedharan, S. and Asokan, S.},
  date = {2017-03},
  pages = {129--134},
  doi = {10.1109/ICICCT.2017.7975173},
  abstract = {The cache replacement policy is a major factor which determines the effectiveness of memory hierarchy. The replacement policy affects both the hit rate and the access latency of the cache. It decides the cache block to be replaced to give room for the incoming block. The replacement policy has to be chosen in such a way that the cache misses are reduced. Last level cache misses causes hundreds of stall cycles due to the need for main memory access. So last level cache misses are given more priority over L1 cache misses. The traditional cache replacement policy used is Least Recently Used (LRU) policy. LRU policy favors workloads having cyclic access pattern which fit in cache, but it exhibit thrashing behavior for memory-intensive workloads that does not fit in the available cache. Hence, many replacement policies were proposed to improve the miss rate for last level caches while maintaining low hardware overhead and minimum design changes. Here a novel replacement policy which is a variation of LRU Insertion policy (LIP) based on re-reference count is proposed. The promotion policy in LIP is modified to implement the new policy which is based on the re-reference count. The proposed replacement policy was implemented and performance comparisons of the replacement policies were done on Gem5 simulator using cpu2006 benchmarks. Under this policy, the memory intensive workload mcf attains 64\% improvement in L2 cache miss rate over LRU policy.},
  eventtitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDMF4RQQ\\Sreedharan and Asokan - 2017 - A cache replacement policy based on re-reference c.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FJV9V2XT\\7975173.html},
  keywords = {Algorithm design and analysis,Arrays,cache access latency,cache block,Cache Hit Rate,cache replacement policy,cache storage,Computer science,Conferences,cpu2006 benchmarks,cyclic access pattern,Electronics packaging,Gem5 simulator,hit rate,L1 cache,last-level cache miss rate,least recently used policy,Lips,low-hardware overhead,LRU,LRU insertion policy,LRU policy,memory hierarchy,memory intensive workload,Memory management,Promotion Policy,re-reference count,Re-reference count,thrashing behavior}
}

@inproceedings{stankovicDRAMControllerComplete2005,
  title = {{{DRAM Controller}} with a {{Complete Predictor}}: {{Preliminary Results}}},
  shorttitle = {{{DRAM Controller}} with a {{Complete Predictor}}},
  booktitle = {℡{{SIKS}} 2005 - 2005 Uth {{International Conference}} on {{Telecommunication}} in {{ModernSatellite}}, {{Cable}} and {{Broadcasting Services}}},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  date = {2005-09},
  volume = {2},
  pages = {593--596},
  doi = {10.1109/℡SKS.2005.1572183},
  abstract = {In the arsenal of solutions for computer memory system performance improvement, predictors have gained an increasing role in the past years. They enable hiding the latencies when accessing cache or main memory. Recently the technique of using temporal parameters of cache memory accesses and tag patterns observing has been applied by some authors for prediction of data prefetching. In this paper a possibility of applying analog techniques on controlling DRAM rows opening/closing, is being researched. Obtained results confirm such a possibility, in a form of a complete predictor, which predicts not only when to close the currently open row but also which is the next row to be opened. Using such a predictor can decrease the average DRAM latency, which is very important in many areas, including telecommunications},
  eventtitle = {℡{{SIKS}} 2005 - 2005 Uth {{International Conference}} on {{Telecommunication}} in {{ModernSatellite}}, {{Cable}} and {{Broadcasting Services}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QT2ZCLBE\\Stankovic and Milenkovic - 2005 - DRAM Controller with a Complete Predictor Prelimi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9CM8S23P\\1572183.html},
  keywords = {analog techniques,Cache memory,computer memory system,data prefetching,Delay,Digital images,DRAM,DRAM chips,DRAM controller,latency,policy,predictor,Prefetching,Random access memory,System performance,Table lookup,Telecommunication buffers,Telecommunication control}
}

@inproceedings{stankovicTechniquesPerformanceImprovements2001,
  title = {Some Techniques for Performance Improvements of {{DRAMs}} in Multimedia Applications},
  booktitle = {5th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}. ℡{{SIKS}} 2001. {{Proceedings}} of {{Papers}} ({{Cat}}. {{No}}.{{01EX517}})},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  date = {2001-09},
  volume = {2},
  pages = {794-797 vol.2},
  doi = {10.1109/℡SKS.2001.955891},
  abstract = {DRAM memory performance is critical, factor in many multimedia applications. Some techniques, which improve DRAM memory performance, are proposed in this paper. These are, first, combined strategies of opening and closing DRAM pages, and second, address remapping in DRAM memory referencing. Simulations we have done showed some improvements in latency of memory references. Implied requirements on DRAM controller are also discussed.},
  eventtitle = {5th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}. ℡{{SIKS}} 2001. {{Proceedings}} of {{Papers}} ({{Cat}}. {{No}}.{{01EX517}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BVRE73SK\\Stankovic and Milenkovic - 2001 - Some techniques for performance improvements of DR.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JXMQTIFM\\955891.html},
  keywords = {address remapping,Bandwidth,Clocks,closing pages,combined strategies,Data structures,DC generators,Delay,Digital signal processing,DRAM chips,DRAM controller,DRAM memory performance,Image processing,latency,memory architecture,memory referencing,Microprocessors,multimedia applications,multimedia systems,multiple concurrent transactions,opening pages,performance improvement techniques,Random access memory,row buffers,SDRAM,SimpleScalar Tool Set,storage allocation,synchronous DRAM}
}

@article{stuecheliCoordinatingDRAMLastLevelCache2011,
  title = {Coordinating {{DRAM}} and {{Last}}-{{Level}}-{{Cache Policies}} with the {{Virtual Write Queue}}},
  author = {Stuecheli, J. and Kaseridis, D. and Daly, D. and Hunter, H. and John, L.},
  date = {2011-01},
  journaltitle = {IEEE Micro},
  volume = {31},
  pages = {90--98},
  issn = {0272-1732},
  doi = {10.1109/MM.2010.102},
  abstract = {To alleviate bottlenecks in this era of many-core architectures, the authors propose a virtual write queue to expand the memory controller's scheduling window through visibility of cache behavior. Awareness of the physical main memory layout and a focus on writes can shorten both read and write average latency, reduce memory power consumption, and improve overall system performance.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R33ZGLEH\\Stuecheli et al. - 2011 - Coordinating DRAM and Last-Level-Cache Policies wi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2T3837LK\\5661752.html},
  keywords = {Bandwidth,cache,Cache memory,cache replacement,cache storage,cache write-back,DRAM,DRAM chips,DRAM page-mode,DRAM parameters,last level cache policy,last-level cache,many core architecture,Memory,memory bandwidth,memory controller scheduling,memory scheduling,multiprocessing systems,performance evaluation,processor scheduling,Processor scheduling,Program processors,Queueing analysis,Random access memory,system performance,virtual write queue},
  number = {1}
}

@inproceedings{subhaAlgorithmBufferCache2009,
  title = {An {{Algorithm}} for {{Buffer Cache Management}}},
  booktitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}},
  author = {Subha, S.},
  date = {2009-04},
  pages = {889--893},
  doi = {10.1109/ITNG.2009.100},
  abstract = {This paper proposes an algorithm for buffer cache management with prefetching. The buffer cache contains two units, the main cache unit and prefetch unit. The sizes of both the units are fixed. The total sizes of both the units are a constant. Blocks are fetched in one block look ahead prefetch principle. The block placement and replacement policies are defined. The replacement strategy depends on the most recently accessed block and the defined miss counts of the blocks. FIFO algorithm is used for the prefetch unit. The proposed algorithm is compared with W2 R algorithm for sequential and random input. For sequential input, the performance is comparable with that of W2 R algorithm. For random input, the proposed algorithm performs better than W2 R by 9\%.},
  eventtitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AS4PWXHS\\Subha - 2009 - An Algorithm for Buffer Cache Management.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WIYT5DQ5\\5070736.html},
  keywords = {block placement,buffer cache management,cache storage,Conference management,data access,DBMS,FIFO algorithm,File systems,Information technology,prefetch unit,prefetching,Prefetching,replacement policy,Technology management}
}

@inproceedings{subramanianAdaptiveCachesEffective2006a,
  title = {Adaptive {{Caches}}: {{Effective Shaping}} of {{Cache Behavior}} to {{Workloads}}},
  shorttitle = {Adaptive {{Caches}}},
  booktitle = {Proceedings of the 39th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Subramanian, Ranjith and Smaragdakis, Yannis and Loh, Gabriel H.},
  date = {2006},
  pages = {385--396},
  publisher = {{IEEE Computer Society}},
  location = {{Washington, DC, USA}},
  doi = {10.1109/MICRO.2006.7},
  url = {https://doi.org/10.1109/MICRO.2006.7},
  urldate = {2019-09-12},
  abstract = {We present and evaluate the idea of adaptive processor cache management. Specifically, we describe a novel and general scheme by which we can combine any two cache management algorithms (e.g., LRU, LFU, FIFO, Random) and adaptively switch between them, closely tracking the locality characteristics of a given program. The scheme is inspired by recent work in virtual memory management at the operating system level, which has shown that it is possible to adapt over two replacement policies to provide an aggregate policy that always performs within a constant factor of the better component policy. A hardware implementation of adaptivity requires very simple logic but duplicate tag structures. To reduce the overhead, we use partial tags, which achieve good performance with a small hardware cost. In particular, adapting between LRU and LFU replacement policies on an 8-way 512KB L2 cache yields a 12.7\% improvement in average CPI on applications that exhibit a non-negligible L2 miss ratio. Our approach increases total cache storage by 4.0\%, but it still provides slightly better performance than a conventional 10-way setassociative 640KB cache which requires 25\% more storage.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7W7HCXBR\\Subramanian et al. - 2006 - Adaptive Caches Effective Shaping of Cache Behavi.pdf},
  isbn = {978-0-7695-2732-1},
  series = {{{MICRO}} 39}
}

@inproceedings{subramanianClosedOpenDRAM2018,
  title = {Closed yet {{Open DRAM}}: {{Achieving Low Latency}} and {{High Performance}} in {{DRAM Memory Systems}}},
  shorttitle = {Closed yet {{Open DRAM}}},
  booktitle = {2018 55th {{ACM}}/{{ESDA}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Subramanian, L. and Vaidyanathan, K. and Nori, A. and Subramoney, S. and Karnik, T. and Wang, H.},
  date = {2018-06},
  pages = {1--6},
  doi = {10.1109/DAC.2018.8465817},
  abstract = {DRAM memory access is a critical performance bottleneck. To access one cache block, an entire row needs to be sensed and amplified, data restored into the bitcells and the bitlines precharged, incurring high latency. Isolating the bitlines and sense amplifiers after activation enables reads and precharges to happen in parallel. However, there are challenges in achieving this isolation. We tackle these challenges and propose an effective scheme, simultaneous read and precharge (SRP), to isolate the sense amplifiers and bitlines and serve reads and precharges in parallel. Our detailed architecture and circuit simulations demonstrate that our simultaneous read and precharge (SRP) mechanism is able to achieve an 8.6\% performance benefit over baseline, while reducing sense amplifier idle power by 30\%, as compared to prior work, over a wide range of workloads.},
  eventtitle = {2018 55th {{ACM}}/{{ESDA}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9XHCYX32\\Subramanian et al. - 2018 - Closed yet Open DRAM Achieving Low Latency and Hi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H6EUQWEM\\8465817.html},
  keywords = {access one cache block,amplifier idle power,amplifiers,cache storage,circuit simulation,circuit simulations,critical performance bottleneck,DRAM chips,DRAM memory access,DRAM memory systems,high performance,History,isolation,memory architecture,Memory management,open DRAM,Organizations,Parallel processing,Random access memory,sense amplifiers,simultaneous read and precharge mechanism,SRAM chips,SRP,Timing,Transistors}
}

@article{tadaCacheReplacementPolicy2019,
  title = {A {{Cache Replacement Policy}} with {{Considering Global Fluctuations}} of {{Priority Values}}},
  author = {Tada, Jubee},
  date = {2019-07},
  journaltitle = {International Journal of Networking and Computing},
  volume = {9},
  pages = {161--170},
  issn = {2185-2847},
  abstract = {In the high-associativity caches, the hardware overheads of the cache replacement policy become a problem. To avoid this problem, the Adaptive Demotion Policy (ADP) is proposed. The ADP focuses on the priority value demotion at a cache miss, and it can achieve a higher performance compared with conventional cache replacement policies. The ADP can be implemented with small hardware resources, and the priority value update logic can be implemented with a small hardware cost. The ADP can suit for various applications by the appropriate selection of its insertion, promotion and selection policies. If the dynamic selection of the suitable policies for the running application is possible, the performance of the cache replacement policy will be increased. In order to achieve the dynamic selection of the suitable policies, this paper focuses on the global fluctuations of the priority values. At first, the cache is partitioned into several partitions. At every cache access, the total of priority values in each partition is calculated. At every set interval, the fluctuations of total priority values in all the partitions are checked, and the information is used to detect the behavior of the application. This paper adapts this mechanism to the ADP, and the adapted cache replacement policy is called the ADP-G. The performance evaluation shows that the ADP-G achieves the MPKI reductions and the IPC improvements, compared to the LRU policy, the RRIP policy and the ADP.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CNYU8Q5L\\Tada - A Cache Replacement Policy with Considering Global.pdf},
  langid = {english},
  number = {2}
}

@inproceedings{taozhangCustomizedDesignDRAM2010,
  title = {A Customized Design of {{DRAM}} Controller for On-Chip {{3D DRAM}} Stacking},
  booktitle = {{{IEEE Custom Integrated Circuits Conference}} 2010},
  author = {{Tao Zhang} and {Kui Wang} and {Yi Feng} and {Xiaodi Song} and {Lian Duan} and Xie, Y. and {Xu Cheng} and {Youn-Long Lin}},
  date = {2010-09},
  pages = {1--4},
  doi = {10.1109/CICC.2010.5617465},
  abstract = {To address the “memory wall” challenge, on-chip memory stacking has been proposed as a promising solution. The stacking memory adopts three-dimensional (3D) IC technology, which leverages through-silicon-vias (TSVs) to connect layers, to dramatically reduce the access latency and improve the bandwidth without the constraint of I/O pins. To demonstrate the feasibility of 3D memory stacking, this paper introduces a customized 3D Double-Data-Rate (DDR) SDRAM controller design, which communicates with DRAM layers by TSVs. In addition, we propose a parallel access policy to further improve the performance. The 3D DDR controller is integrated in a 3D stacking System-on-Chip (SoC) architecture, where a high-bandwidth 3D DRAM chip is stacked on the top. The 3D SoC is divided into two logic layers with each having an area of 2.5 × 5.0mm2, with a 3-layer 2Gb DRAM stacking. The whole chip has been fabricated in Chartered 130nm low-power process and Tezzaron's 3D bonding technology. The simulation result shows that the on-chip DRAM controller can run as fast as 133MHz and provide 4.25GB/s data bandwidth in a single channel and 8.5GB/s with parallel access policy.},
  eventtitle = {{{IEEE Custom Integrated Circuits Conference}} 2010},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ILDYCDTB\\Tao Zhang et al. - 2010 - A customized design of DRAM controller for on-chip.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TXIARCL7\\5617465.html},
  keywords = {3D bonding technology,3D DDR controller,3D DRAM chip,3D IC technology,3D memory stacking,3D SoC,3D stacking system-on-chip architecture,access latency,Bandwidth,Computer architecture,customized 3D double-data-rate,customized design,data bandwidth,DRAM chips,DRAM layers,logic design,logic layers,memory wall challenge,on-chip 3D DRAM stacking,on-chip memory stacking,parallel access policy,Random access memory,Registers,SDRAM controller design,SoC architecture,Stacking,stacking memory,System-on-a-chip,system-on-chip,Three dimensional displays,through-silicon-vias,TSV}
}

@article{tewarMcfTRaptorUnobtrusiveOnthefly2015,
  title = {{{mcfTRaptor}}: {{Toward}} Unobtrusive on-the-Fly Control-Flow Tracing in Multicores},
  shorttitle = {{{mcfTRaptor}}},
  author = {Tewar, Amrish K. and Myers, Albert R. and Milenković, Aleksandar},
  date = {2015-11},
  journaltitle = {Journal of Systems Architecture},
  shortjournal = {Journal of Systems Architecture},
  volume = {61},
  pages = {601--614},
  issn = {1383-7621},
  doi = {10.1016/j.sysarc.2015.07.005},
  url = {http://www.sciencedirect.com/science/article/pii/S1383762115000752},
  abstract = {Software testing and debugging has become the most critical aspect of the development of modern embedded systems, mainly driven by growing software and hardware complexity, increasing integration, and tightening time-to-market deadlines. Software developers increasingly rely on on-chip trace and debug infrastructure to locate software bugs faster. However, the existing infrastructure offers limited visibility or relies on hefty on-chip buffers and wide trace ports that significantly increase system cost. This paper introduces a new technique called mcfTRaptor for capturing and compressing functional and time-stamped control-flow traces on-the-fly in modern multicore systems. It relies on private on-chip predictor structures and corresponding software modules in the debugger to significantly reduce the number of events that needs to be streamed out of the target platform. Our experimental evaluation explores the effectiveness of mcfTRaptor as a function of the number of cores, encoding mechanisms, and predictor configurations. When compared to the Nexus-like control-flow tracing, mcfTRaptor reduces the trace port bandwidth in the range from 14 to 23.8 times for functional traces and 10.8–18.6 times for time-stamped traces.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QVAVNQZ\\Tewar et al. - 2015 - mcfTRaptor Toward unobtrusive on-the-fly control-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\63TKITWW\\S1383762115000752.html},
  keywords = {Multicores,Program tracing,Real-time embedded systems,Software testing and debugging},
  number = {10},
  series = {Special Section on {{Architecture}} of {{Computing Systems}} Edited by {{Editors}}: {{Wolfgang Karl}}, {{Erik Maehle}}, {{Kay Römer}}, {{Eduardo Tovar}}, {{Martin DanekSpecial}} Section on {{Testing}}, {{Prototyping}}, and {{Debugging}} of {{Multi}}-{{Core Architectures}} Edited by {{Editors}}: {{Frank Hannig}} \& {{Andreas HerkersdorfSpecial}} Section on {{Embedded Vision Architectures}} and {{Applications}} Edited by {{Editors}}: {{Christophe Bobda}}, {{Walter Stechele}}, {{Ali Ahmadinia}} and {{Miaoqing Huang}}}
}

@inproceedings{thakkar3DWizNovelHigh2014,
  title = {{{3D}}-{{Wiz}}: {{A}} Novel High Bandwidth, Optically Interfaced {{3D DRAM}} Architecture with Reduced Random Access Time},
  shorttitle = {{{3D}}-{{Wiz}}},
  booktitle = {2014 {{IEEE}} 32nd {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Thakkar, I. G. and Pasricha, S.},
  date = {2014-10},
  pages = {1--7},
  doi = {10.1109/ICCD.2014.6974654},
  abstract = {This paper introduces 3D-Wiz, which is a high bandwidth, low latency, optically interfaced 3D DRAM architecture with fine grained data organization and activation. 3D-Wiz integrates sub-bank level 3D partitioning of the data array to enable fine-grained activation and greater memory parallelism. A novel method of routing the internal memory bus using TSVs and fan-out buffers enables 3D-Wiz to use smaller dimension subarrays without significant area overhead. This in turn reduces the random access latency and activation-precharge energy. 3D-Wiz demonstrates access latency of 19.5ns and row cycle time of 25ns. It yields per access activation energy and precharge energy of 0.78nJ and 0.62nJ respectively with 42.5\% area efficiency. 3D-Wiz yields the best latency and energy consumption values per access among other well-known 3D DRAM architectures. Experimental results with PARSEC benchmarks indicate that 3D-Wiz achieves 38.8\% improvement in performance, 81.1\% reduction in power consumption, and 77.1\% reduction in energy-delay product (EDP) on average over 3D DRAM architectures from prior work.},
  eventtitle = {2014 {{IEEE}} 32nd {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J6D4QVDS\\Thakkar and Pasricha - 2014 - 3D-Wiz A novel high bandwidth, optically interfac.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\IIEGCK6U\\6974654.html},
  keywords = {3D-Wiz,access activation energy,activation-precharge energy,Arrays,Bandwidth,dimension subarrays,DRAM chips,EDP,efficiency 42.5 percent,energy consumption value,energy-delay product,fan-out buffers,fine-grained data organization,high-bandwidth 3D DRAM architecture,internal memory bus routing,low-latency 3D DRAM architecture,memory parallelism,optically-interfaced 3D DRAM architecture,PARSEC benchmarks,Photonics,precharge energy,random access latency,Random access memory,reduced random access time,row cycle time,subbank level 3D partitioning,Three-dimensional displays,three-dimensional integrated circuits,Through-silicon vias,time 19.5 ns,time 25 ns,TSV}
}

@article{tianEffectivenessbasedAdaptiveCache2014,
  title = {An Effectiveness-Based Adaptive Cache Replacement Policy},
  author = {Tian, Geng and Liebelt, Michael},
  date = {2014-02-01},
  journaltitle = {Microprocessors and Microsystems},
  shortjournal = {Microprocessors and Microsystems},
  volume = {38},
  pages = {98--111},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2013.11.011},
  url = {http://www.sciencedirect.com/science/article/pii/S014193311300197X},
  urldate = {2019-09-12},
  abstract = {Belady’s optimal cache replacement policy is an algorithm to work out the theoretical minimum number of cache misses, but the rationale behind it was too simple. In this work, we revisit the essential function of caches to develop an underlying analytical model. We argue that frequency and recency are the only two affordable attributes of cache history that can be leveraged to predict a good replacement. Based on those two properties, we propose a novel replacement policy, the Effectiveness-Based Replacement policy (EBR) and a refinement, Dynamic EBR (D-EBR), which combines measures of recency and frequency to form a rank sequence inside each set and evict blocks with lowest rank. To evaluate our design, we simulated all 30 applications from SPEC CPU2006 for uni-core system and a set of combinations for 4-core systems, for different cache sizes. The results show that EBR achieves an average miss rate reduction of 12.4\%. With the help of D-EBR, we can tune the weight ratio between ‘frequency’ and ‘recency’ dynamically. D-EBR can nearly double the miss reduction achieved by EBR alone. In terms of hardware overhead, EBR requires half the hardware overhead of real LRU and even compared with Pseudo LRU the overhead is modest.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EWQW8W3A\\Tian and Liebelt - 2014 - An effectiveness-based adaptive cache replacement .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FEIKKXWD\\S014193311300197X.html},
  keywords = {Cache memory,Configurable cache,Micro architecture,Performance,Scan resistance,Thrashing},
  number = {1}
}

@article{tomiyamaCodePlacementTechniques1997,
  title = {Code Placement Techniques for Cache Miss Rate Reduction},
  author = {Tomiyama, Hiroyuki and Yasuura, Hiroto},
  date = {1997-10-01},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  volume = {2},
  pages = {410--429},
  issn = {10844309},
  doi = {10.1145/268424.268469},
  url = {http://portal.acm.org/citation.cfm?doid=268424.268469},
  urldate = {2020-01-06},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\MJAWER2U\\Tomiyama and Yasuura - 1997 - Code placement techniques for cache miss rate redu.pdf},
  langid = {english},
  number = {4}
}

@inproceedings{tsaiJengaSoftwaredefinedCache2017,
  title = {Jenga: {{Software}}-Defined Cache Hierarchies},
  shorttitle = {Jenga},
  booktitle = {2017 {{ACM}}/{{IEEE}} 44th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Tsai, P. and Beckmann, N. and Sanchez, D.},
  date = {2017-06},
  pages = {652--665},
  doi = {10.1145/3079856.3080214},
  abstract = {Caches are traditionally organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, since working sets settle at the smallest (i.e., fastest and most energy-efficient) level they fit in. However, rigid hierarchies also add overheads, because each level adds latency and energy even when it does not fit the working set. These overheads are expensive on emerging systems with heterogeneous memories, where the differences in latency and energy across levels are small. Significant gains are possible by specializing the hierarchy to applications. We propose Jenga, a reconfigurable cache hierarchy that dynamically and transparently specializes itself to applications. Jenga builds virtual cache hierarchies out of heterogeneous, distributed cache banks using simple hardware mechanisms and an OS runtime. In contrast to prior techniques that trade energy and bandwidth for performance (e.g., dynamic bypassing or prefetching), Jenga eliminates accesses to unwanted cache levels. Jenga thus improves both performance and energy efficiency. On a 36-core chip with a 1 GB DRAM cache, Jenga improves energy-delay product over a combination of state-of-the-art techniques by 23\% on average and by up to 85\%.},
  eventtitle = {2017 {{ACM}}/{{IEEE}} 44th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QZDZ8SK8\\Tsai et al. - 2017 - Jenga Software-defined cache hierarchies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZTP33ABZ\\8192509.html},
  keywords = {Bandwidth,Cache,cache storage,DRAM cache,DRAM chips,energy-delay product,Hardware,heterogeneous distributed cache banks,heterogeneous memories,Heterogeneous memories,Hierarchy,Jenga,latency energy,memory architecture,memory size 1.0 GByte,Multicore processing,multiprocessing systems,NUCA,Organizations,Partitioning,Random access memory,reconfigurable cache hierarchy,rigid hierarchy,simple hardware mechanisms,slower memories,Software,storage management,System-on-chip,unwanted cache levels,virtual cache hierarchies}
}

@incollection{tsaoMultiFactorPagingExperiment1972,
  title = {A {{Multi}}-{{Factor Paging Experiment}}: {{I}}. {{The Experiment}} and {{Conclusions}}},
  shorttitle = {A {{MULTI}}-{{FACTOR PAGING EXPERIMENT}}},
  booktitle = {Statistical {{Computer Performance Evaluation}}},
  author = {Tsao, R. F. and Comeau, L. W. and Margolin, B. H.},
  editor = {Freiberger, Walter},
  date = {1972-01-01},
  pages = {103--134},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-266950-7.50012-8},
  url = {http://www.sciencedirect.com/science/article/pii/B9780122669507500128},
  urldate = {2020-01-03},
  abstract = {This paper reports on a statistically designed and analyzed experiment performed to study the effects of four factors, memory size, problem program, load sequence of system program subroutines and replacement algorithm, upon the paging process. The usefulness of the number of page swaps as a measure for comparing replacement algorithms is evaluated and alternative measures are proposed. The experiment and the conclusions drawn are presented in this paper, while the statistical methodology used to derive these conclusions is presented in a companion paper.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9YCTKAWW\\B9780122669507500128.html},
  isbn = {978-0-12-266950-7},
  langid = {english}
}

@inproceedings{turnerSegmentedFIFOPage1981,
  title = {Segmented {{FIFO}} Page Replacement},
  author = {Turner, Rollins and Levy, Henry},
  date = {1981},
  pages = {48--51},
  publisher = {{ACM Press}},
  doi = {10.1145/800189.805473},
  url = {http://portal.acm.org/citation.cfm?doid=800189.805473},
  urldate = {2020-01-02},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9QZS46BA\\Turner and Levy - 1981 - Segmented FIFO page replacement.pdf},
  isbn = {978-0-89791-051-4},
  langid = {english}
}

@inproceedings{tzu-chiehlinQualityawareMemoryController2003,
  title = {Quality-Aware Memory Controller for Multimedia Platform {{SoC}}},
  booktitle = {2003 {{IEEE Workshop}} on {{Signal Processing Systems}} ({{IEEE Cat}}. {{No}}.{{03TH8682}})},
  author = {{Tzu-Chieh Lin} and {Kun-Bin Lee} and {Chein-Wei Jen}},
  date = {2003-08},
  pages = {328--333},
  doi = {10.1109/SIPS.2003.1235691},
  abstract = {The ongoing advancements in VLSI technology allow SoC design to integrate heterogeneous control and computing functions into a single chip. On the other hand, the pressures of area and cost lead to the requirement for a single, shared off-chip DRAM memory subsystem. To satisfy different memory access requirements (for latency and bandwidth) of these heterogeneous functions to this kind of DRAM memory subsystem, a quality-aware memory controller is important. The paper presents an efficient memory controller that contains a quality-aware scheduler and a configurable DRAM memory interface socket to achieve high DRAM utilization while still meeting different requirements for bandwidth and latency. Simulation results show that the latency of the latency-sensitive data flow can be reduced to 50\%, and the memory bandwidths can be precisely allocated to bandwidth-sensitive data flows with a high degree of control.},
  eventtitle = {2003 {{IEEE Workshop}} on {{Signal Processing Systems}} ({{IEEE Cat}}. {{No}}.{{03TH8682}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VN6NF4GS\\Tzu-Chieh Lin et al. - 2003 - Quality-aware memory controller for multimedia pla.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\93XMK69I\\1235691.html},
  keywords = {Bandwidth,bandwidth allocation,bandwidth-sensitive data flows,Communication system control,Control systems,Costs,Delay,Design engineering,interface socket,latency-sensitive data flow,memory access,memory bandwidth allocation,multimedia platform SoC,off-chip DRAM memory subsystem,Processor scheduling,Quality of service,quality-aware memory controller,quality-aware scheduler,Random access memory,random-access storage,scheduling,storage management,System-on-a-chip,system-on-chip,VLSI technology}
}

@report{UltraSPARCT2Supplement2007,
  title = {{{UltraSPARC T2}}™ {{Supplement}} to the {{UltraSPARC Architecture}} 2007},
  date = {2007-09-19},
  pages = {940},
  institution = {{Sun Microsystems}},
  location = {{Santa Clara, CA}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W8TMVKT9\\Microsystems and Clara - Hyperprivileged, Privileged, and Nonprivileged.pdf},
  langid = {english},
  number = {Draft D1.4.3},
  type = {Technical Report}
}

@article{uzelacHardwareBasedLoadValue2013,
  title = {Hardware-{{Based Load Value Trace Filtering}} for {{On}}-the-{{Fly Debugging}}},
  author = {Uzelac, Vladimir and Milenković, Aleksandar},
  date = {2013-05},
  journaltitle = {ACM Trans. Embed. Comput. Syst.},
  volume = {12},
  pages = {97:1--97:18},
  issn = {1539-9087},
  doi = {10.1145/2465787.2465799},
  url = {http://doi.acm.org/10.1145/2465787.2465799},
  urldate = {2019-01-30},
  abstract = {Capturing program and data traces during program execution unobtrusively on-the-fly is crucial in debugging and testing of cyber-physical systems. However, tracing a complete program unobtrusively is often cost-prohibitive, requiring large on-chip trace buffers and wide trace ports. This article describes a new hardware-based load data value filtering technique called Cache First-access Tracking. Coupled with an effective variable encoding scheme, this technique achieves a significant reduction of load data value traces, from 5.86 to 56.39 times depending on the data cache size, thus enabling cost-effective, unobtrusive on-the-fly tracing and debugging.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3VWT5UG7\\Uzelac and Milenković - 2013 - Hardware-Based Load Value Trace Filtering for On-t.pdf},
  issue = {2s},
  keywords = {Debugging,load value filtering,program tracing,software debugger,trace compression,trace module,variable encoding}
}

@article{uzelacUsingBranchPredictors2014,
  title = {Using {{Branch Predictors}} and {{Variable Encoding}} for {{On}}-the-{{Fly Program Tracing}}},
  author = {Uzelac, V. and Milenković, A. and Milenković, M. and Burtscher, M.},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Computers},
  volume = {63},
  pages = {1008--1020},
  issn = {0018-9340},
  doi = {10.1109/TC.2012.267},
  abstract = {Unobtrusive capturing of program execution traces in real-time is crucial for debugging many embedded systems. However, tracing even limited program segments is often cost-prohibitive, requiring wide trace ports and large on-chip trace buffers. This paper introduces a new cost-effective technique for capturing and compressing program execution traces on-the-fly. It relies on branch predictor-like structures in the trace module and corresponding software modules in the debugger to significantly reduce the number of events that need to be streamed out of the target system. Coupled with an effective variable encoding scheme that adapts to changing program patterns, our technique requires merely 0.029 bits per instruction of trace port bandwidth, providing a 34-fold improvement over the commercial state-of-the-art and a five-fold improvement over academic proposals, at the low cost of under 5,000 logic gates.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZW8AG3D4\\Uzelac et al. - 2014 - Using Branch Predictors and Variable Encoding for .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R2PAJVVR\\6342940.html},
  keywords = {Bandwidth,branch predictor-like structures,Compression technologies,cost-effective technique,Debugging,embedded systems,Hardware,logic gates,on-the-fly program tracing,program debugging,program diagnostics,program execution traces,program patterns,Program processors,Radiation detectors,real time and embedded systems,Real-time systems,software modules,testing and debugging,tracing,variable encoding},
  number = {4}
}

@inproceedings{vakaliLRUbasedAlgorithmsWeb2000,
  title = {{{LRU}}-Based Algorithms for {{Web Cache Replacement}}},
  booktitle = {Electronic {{Commerce}} and {{Web Technologies}}},
  author = {Vakali, A. I.},
  editor = {Bauknecht, Kurt and Madria, Sanjay Kumar and Pernul, Günther},
  date = {2000},
  pages = {409--418},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44463-7_36},
  abstract = {Caching has been introduced and applied in prototype and commercial Web-based information systems in order to reduce the overall bandwidth and increase system’s fault tolerance. This paper presents a track of Web cache replacement algorithms based on the Least Recently Used (LRU) idea. We propose an extension to the conventional LRU algorithm by considering the number of references to Web objects as a critical parameter for the cache content replacement. The proposed algorithms are validated and experimented under Web cache traces provided by a major Squid proxy cache server installation environment. Cache and bytes hit rates are reported showing that the proposed cache replacement algorithms improve cache content.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\926BNS5V\\Vakali - 2000 - LRU-based algorithms for Web Cache Replacement.pdf},
  isbn = {978-3-540-44463-3},
  keywords = {Cache consistency,Cache replacement algorithms,Web caching and proxies,Web-based information systems},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{veidenbaumDecoupledAccessDRAM1997,
  title = {Decoupled Access {{DRAM}} Architecture},
  booktitle = {Proceedings {{Innovative Architecture}} for {{Future Generation High}}-{{Performance Processors}} and {{Systems}}},
  author = {Veidenbaum, A. V. and Gallivan, K. A.},
  date = {1997-10},
  pages = {94--103},
  doi = {10.1109/IWIA.1997.670415},
  abstract = {This paper discusses an approach to reducing memory latency in future systems. It focuses on systems where a single chip DRAM/processor will not be feasible even in 10 years, e.g. systems requiring a large memory and/or many CPU's. In such systems a solution needs to be found to DRAM latency and bandwidth as well as to inter-chip communication. Utilizing the projected advances in chip I/O bandwidth we propose to implement a decoupled access-execute processor where the access processor is placed in memory. A program is compiled to run as a computational process and several access processes with the latter executing in the DRAM processors. Instruction set extensions are discussed to support this paradigm. Using multi-level branch prediction the access processor stays ahead of the execute processor and keeps the latter supplied with data. The system reduces latency by moving address computation to memory and thus avoiding sending address to memory by the computational processor. This and the fetch-ahead capabilities of the access processor are combined with multiple DRAM "streaming" to improve performance. DRAM caching is assumed to be used to assist in this as well.},
  eventtitle = {Proceedings {{Innovative Architecture}} for {{Future Generation High}}-{{Performance Processors}} and {{Systems}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\A9ZXBAFX\\Veidenbaum and Gallivan - 1997 - Decoupled access DRAM architecture.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XGK3SY5D\\670415.html},
  keywords = {access processor,Access protocols,Bandwidth,chip I/O bandwidth,Circuit noise,Circuit synthesis,Clocks,Computer architecture,Computer science,decoupled access-execute processor,Delay,DRAM caching,execute processor,memory architecture,multi-level branch prediction,multiple DRAM,parallel architectures,Random access memory,reducing memory latency,single chip DRAM,Very large scale integration}
}

@article{vergeHardwareassistedSoftwareEvent2017,
  title = {Hardware-Assisted Software Event Tracing},
  author = {Vergé, Adrien and Ezzati-Jivan, Naser and Dagenais, Michel R.},
  date = {2017-05-25},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  shortjournal = {Concurrency Computat.: Pract. Exper.},
  volume = {29},
  pages = {n/a-n/a},
  issn = {1532-0634},
  doi = {10.1002/cpe.4069},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.4069/abstract},
  urldate = {2017-06-01},
  abstract = {Event tracing is a reliable and a low-intrusiveness method to debug and optimize systems and processes. Low overhead is particularly important in embedded systems where resources and energy consumption is critical. The most advanced tracing infrastructures achieve a very low footprint on the traced software, bringing each tracepoint overhead to less than a microsecond. To reduce this still non-negligible impact, the use of dedicated hardware resources is promising. In this paper, we propose complementary methods for tracing that rely on hardware modules to assist software tracing. We designed solutions to take advantage of CoreSight STM, CoreSight ETM, and Intel BTS, which are present on most newer ARM-based systems-on-chip and Intel x86 processors. Our results show that the time overhead for tracing can be reduced by up to 10 times when assisted by hardware, as compared to software tracing with LTTng, a high-performance tracer for Linux. We also propose a modification to the Perf tool to speed BTS execution tracing up to 65\%.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JN9T57JI\\Vergé et al. - 2017 - Hardware-assisted software event tracing.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QSZ8MBB3\\abstract.html},
  keywords = {ARM CoreSight,debugging,dedicated hardware,event tracing,Intel BTS,LTTng},
  langid = {english},
  number = {10}
}

@inproceedings{wahabARMHExFrameworkEfficient2017,
  title = {{{ARMHEx}}: {{A}} Framework for Efficient {{DIFT}} in Real-World {{SoCs}}},
  shorttitle = {{{ARMHEx}}},
  booktitle = {2017 27th {{International Conference}} on {{Field Programmable Logic}} and {{Applications}} ({{FPL}})},
  author = {Wahab, M. A. and Cotret, P. and Allah, M. N. and Hiet, G. and Lapôtre, V. and Gogniat, G.},
  date = {2017-09},
  pages = {1--1},
  doi = {10.23919/FPL.2017.8056799},
  abstract = {Security in embedded systems remains a major concern. Untrustworthy authorities use a wide range of software attacks. This demo introduces ARMHEx, a practical solution targeting DIFT (Dynamic Information Flow Tracking) implementations on ARM-based SoCs. DIFT is a solution that consists in tracking the dissemination of data inside the system and allows to enforce some security properties. In this demo, we show an implementation of ARMHEx on Xilinx Zynq SoC. Especially, we show how the required information for DIFT is recovered with the help of traces produced by CoreSight components, static analysis and instrumentation.},
  eventtitle = {2017 27th {{International Conference}} on {{Field Programmable Logic}} and {{Applications}} ({{FPL}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8SD64AUF\\Wahab et al. - 2017 - ARMHEx A framework for efficient DIFT in real-wor.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TZ8FU35B\\8056799.html},
  keywords = {Coprocessors,Instruments,Monitoring,Runtime,security,Software,Target tracking}
}

@inproceedings{wangBuildingLowLatency2016,
  title = {Building a {{Low Latency}}, {{Highly Associative DRAM Cache}} with the {{Buffered Way Predictor}}},
  booktitle = {2016 28th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  author = {Wang, Z. and Jiménez, D. A. and Zhang, T. and Loh, G. H. and Xie, Y.},
  date = {2016-10},
  pages = {109--117},
  doi = {10.1109/SBAC-PAD.2016.22},
  abstract = {The emerging die-stacked DRAM technology allows computer architects to design a last-level cache (LLC) with high memory bandwidth and large capacity. There are four key requirements for DRAM cache design: minimizing on-chip tag storage overhead, optimizing access latency, improving hit rate, and reducing off-chip traffic. These requirements seem mutually incompatible. For example, to reduce the tag storage overhead, the recent proposed LH-cache co-locates tags and data in the same DRAM cache row, and the Alloy Cache proposed to alloy data and tags in the same cache line in a direct-mapped design. However, these ideas either require significant tag lookup latency or sacrifice hit rate for hit latency. To optimize all four key requirements, we propose the Buffered Way Predictor (BWP). The BWP predicts the way ID of a DRAM cache request with high accuracy and coverage, allowing data and tag to be fetched back to back. Thus, the read latency for the data can be completely hidden so that DRAM cache hitting requests have low access latency. The BWP technique is designed for highly associative block-based DRAM caches and achieves a low miss rate and low off-chip traffic. Our evaluation with multi-programmed workloads and a 128MB DRAM cache shows that a 128KB BWP achieves a 76.2\% hit rate. The BWP improves performance by 8.8\% and 12.3\% compared to LH-cache and Alloy Cache, respectively.},
  eventtitle = {2016 28th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LE4LJECF\\Wang et al. - 2016 - Building a Low Latency, Highly Associative DRAM Ca.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4VD5KSS2\\7789330.html},
  keywords = {access latency optimization,alloy cache,alloy data,Arrays,Bandwidth,buffered way predictor,BWP technique,cache storage,Compounds,content-addressable storage,die-stacked DRAM technology,direct-mapped design,DRAM cache hitting requests,DRAM chips,integrated circuit design,last-level cache,LH-cache,LLC,low latency highly associative DRAM cache design,memory bandwidth,Memory management,Metals,off-chip traffic reduction,on-chip tag storage overhead minimization,Random access memory,read latency,sacrifice hit rate,storage capacity 128 Mbit,System-on-chip,tag lookup latency,tag storage overhead reduction}
}

@inproceedings{wangRealTimeCache2013,
  title = {Real {{Time Cache Performance Analyzing}} for {{Multi}}-Core {{Parallel Programs}}},
  booktitle = {2013 {{International Conference}} on {{Cloud}} and {{Service Computing}}},
  author = {Wang, R. and Gao, Y. and Zhang, G.},
  date = {2013-11},
  pages = {16--23},
  doi = {10.1109/CSC.2013.11},
  abstract = {Modern processors mostly use cache to hide the memory access latency, so cache performance is very important to application program. A detailed cache performance analysis will provide programmers a clear view of their program behaviors, which can help them to identify the performance bottleneck and to optimize the source code. As the chip industry turn to integrate multiple cores into one chip, multi-core/many-core processor becomes the new approach to maintain the Moor's Law. Therefore, Parallel programs turn to be more important even in the personal computers. In parallel programs, the interaction between tasks is the source of bugs and errors and is hard to handling for most of programmers. The detailed cache behaviors will greatly helpful to the programmer to find the errors and optimize the programs. However, the existing cache performance analysis tools, due to the limitations of the hardware performance counters they depend on to get data, cannot get as much data as we expected. Those tools cannot reveal the program routines characteristics on shared cache and the source of cache misses with limited metrics on cache misses. In this paper, we propose a method to obtain and analysis real time cache performance with binary instrumentation and cache emulation. We instrument the parallel program while it is running, and get the trace data about memory access. Then we transport the trace data to an carefully configured cache emulation module to get the detailed cache behavior information. The emulation module can not only get more information than hardware performance counter but also can be configured to simulate different target hardware environment. Additionally, we use the performance data to form a group of cache performance metrics which can intuitively help programmers to optimize their codes. The accuracy of this method is demonstrated by comparing the summary result with the hardware performance counter. Finally, we design an cache performance analysis tool named CC-Analyzer for parallel programs. Comparing with the existing technologies, CC-Analyzer is able to analyze the cause of cache misses and gather much more performance statistics when the parallel program is running on different cache architectures.},
  eventtitle = {2013 {{International Conference}} on {{Cloud}} and {{Service Computing}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DWRQN76V\\Wang et al. - 2013 - Real Time Cache Performance Analyzing for Multi-co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6AZKHWY5\\6693173.html},
  keywords = {application program,binary instrumentation,cache architectures,cache behavior information,cache misses,cache performance analysis tools,cache performance metrics,cache storage,CC-Analyzer,chip industry,configured cache emulation module,data tracing,detailed cache performance analysis,Emulation,Hardware,hardware performance counter,Instruments,limited metrics,many-core processor,Measurement,memory access latency,modern processors,Moor's law,multicore parallel programs,multicore processor,multiple cores,multiprocessing systems,parallel programming,Performance analysis,performance statistics,personal computers,program behaviors,Program processors,program routines characteristics,Radiation detectors,real time cache performance,shared cache,source code}
}

@inproceedings{wangReducingDRAMLatency2018,
  title = {Reducing {{DRAM Latency}} via {{Charge}}-{{Level}}-{{Aware Look}}-{{Ahead Partial Restoration}}},
  booktitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Wang, Y. and Tavakkol, A. and Orosa, L. and Ghose, S. and Ghiasi, N. Mansouri and Patel, M. and Kim, J. S. and Hassan, H. and Sadrosadati, M. and Mutlu, O.},
  date = {2018-10},
  pages = {298--311},
  doi = {10.1109/MICRO.2018.00032},
  abstract = {Long DRAM access latency is a major bottleneck for system performance. In order to access data in DRAM, a memory controller (1) activates (i.e., opens) a row of DRAM cells in a cell array, (2) restores the charge in the activated cells back to their full level, (3) performs read and write operations to the activated row, and (4) precharges the cell array to prepare for the next activation. The restoration operation is responsible for a large portion (up to 43.6\%) of the total DRAM access latency. We find two frequent cases where the restoration operations performed by DRAM do not need to fully restore the charge level of the activated DRAM cells, which we can exploit to reduce the restoration latency. First, DRAM rows are periodically refreshed (i.e., brought back to full charge) to avoid data loss due to charge leakage from the cell. The charge level of a DRAM row that will be refreshed soon needs to be only partially restored, providing just enough charge so that the refresh can correctly detect the cells' data values. Second, the charge level of a DRAM row that will be activated again soon can be only partially restored, providing just enough charge for the activation to correctly detect the data value. However, partial restoration needs to be done carefully: for a row that will be activated again soon, restoring to only the minimum possible charge level can undermine the benefits of complementary mechanisms that reduce the activation time of highly-charged rows. To enable effective latency reduction for both activation and restoration, we propose charge-level-aware look-ahead partial restoration (CAL). CAL consists of two key components. First, CAL accurately predicts the next access time, which is the time between the current restoration operation and the next activation of the same row. Second, CAL uses the predicted next access time and the next refresh time to reduce the restoration time, ensuring that the amount of partial charge restoration is enough to maintain the benefits of reducing the activation time of a highly-charged row. We implement CAL fully in the memory controller, without any changes to the DRAM module. Across a wide variety of applications, we find that CAL improves the average performance of an 8-core system by 14.7\%, and reduces average DRAM energy consumption by 11.3\%.},
  eventtitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AGBUL9LY\\8574549.html},
  keywords = {activation-latency,Arrays,CAL,Capacitors,charge-level-aware look-ahead partial restoration,data access time,DRAM cells array activation,DRAM chips,DRAM energy consumption,DRAM latency reduction,DRAM-latency,memory controller,partial-restoration,restoration-latency,System performance,Timing}
}

@inproceedings{waslyHidingMemoryLatency2014,
  title = {Hiding Memory Latency Using Fixed Priority Scheduling},
  booktitle = {2014 {{IEEE}} 19th {{Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  author = {Wasly, S. and Pellizzoni, R.},
  date = {2014-04},
  pages = {75--86},
  doi = {10.1109/RTAS.2014.6925992},
  abstract = {Modern embedded platforms contain a variety of physical resources, such as caches, interconnects, main memory, etc., which the processor must access during the execution of a task. We argue that processor task execution and accesses to physical resources should be co-scheduled in real-time systems to predictably hide resource access latency. In particular, in this work we focus on co-scheduling task execution and accesses to main memory to hide DRAM access latency. Since modern systems implement DMA controllers that can be operated independently of processor execution, this allows us to hide memory transfer latency by scheduling DMA transfer in parallel with processor execution. The main contribution of this paper is a dynamic scheduling algorithm for a set of sporadic real-time tasks that efficiently co-schedules processor and DMA execution to hide memory transfer latency. The proposed algorithm can be applied to either uniprocessor or partitioned multiprocessor systems. We demonstrate that we improve processor utilization significantly compared to existing scratchpad and cache management systems.},
  eventtitle = {2014 {{IEEE}} 19th {{Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QEZCWNCP\\Wasly and Pellizzoni - 2014 - Hiding memory latency using fixed priority schedul.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HQ8M63W6\\6925992.html},
  keywords = {Algorithm design and analysis,cache management systems,cache storage,coscheduling task execution,DMA controllers,DMA execution,DMA transfer scheduling,DRAM access latency,DRAM chips,embedded platforms,fixed priority scheduling,interconnects,Interference,main memory,memory latency,Memory management,memory transfer latency,multiprocessing systems,partitioned multiprocessor systems,physical resources,processor scheduling,processor task execution,real-time systems,Real-time systems,Schedules,Scheduling algorithms,scratchpad management systems,Time factors,uniprocessor}
}

@misc{watermanRISCVInstructionSet2019,
  title = {The {{RISC}}-{{V Instruction Set Manual}}},
  editor = {Waterman, Andrew and Asanovic, Krste},
  date = {2019-06-08},
  publisher = {{RISC-V Foundation}},
  url = {https://riscv.org/specifications/},
  urldate = {2019-07-26},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H9R784BP\\Waterman et al. - Volume I Unprivileged ISA.pdf},
  langid = {english}
}

@inproceedings{wei-chetsengOptimalSchedulingMinimize2010,
  title = {Optimal Scheduling to Minimize Non-Volatile Memory Access Time with Hardware Cache},
  booktitle = {2010 18th {{IEEE}}/{{IFIP International Conference}} on {{VLSI}} and {{System}}-on-{{Chip}}},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  date = {2010-09},
  pages = {131--136},
  doi = {10.1109/VLSISOC.2010.5642609},
  abstract = {In power and size sensitive embedded systems, flash memory and phase change memory are replacing DRAM as the main memory. Unfortunately, these technologies are limited by their endurance and long write latencies. To minimize the main memory access time, we optimally schedule tasks by an ILP formulation that can be generally applied to other main memory technologies, including DRAM. We also present a heuristic, Wander Scheduling, to solve larger instances in a reasonable amount of time. Our experimental results show that when compared with list scheduling, Wander Scheduling can reduce memory access times by an average of 40.73\% and increase the lifetime of flash and phase change memory by 82.56\%.},
  eventtitle = {2010 18th {{IEEE}}/{{IFIP International Conference}} on {{VLSI}} and {{System}}-on-{{Chip}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EVVH9Z7G\\Wei-Che Tseng et al. - 2010 - Optimal scheduling to minimize non-volatile memory.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HJP9WQ5Y\\5642609.html},
  keywords = {Ash,cache storage,DRAM,DRAM chips,embedded system,flash memories,flash memory,hardware cache,Law,Nonvolatile memory,nonvolatile memory access time minimization,optimal scheduling,Optimal scheduling,phase change memories,phase change memory,processor scheduling,Schedules,Scheduling,wander scheduling}
}

@inproceedings{wei-chetsengPRRLowoverheadCache2012,
  title = {{{PRR}}: {{A}} Low-Overhead Cache Replacement Algorithm for Embedded Processors},
  shorttitle = {{{PRR}}},
  booktitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  date = {2012-01},
  pages = {35--40},
  doi = {10.1109/ASPDAC.2012.6164972},
  abstract = {In embedded systems power consumption and area tightly constrain the cache capacity and management logic. Many good cache replacement policies have been proposed in the past, but none approach the performance of the least recently used (LRU) algorithm without incurring high overheads. In fact, many embedded designers consider even pseudo-LRU too complex for their embedded systems processors. In this paper, we propose a new level 1 (L1) data cache replacement algorithm, Protected Round-Robin (PRR) that is simple enough to be incorporated into embedded processors while providing miss rates that are very similar to the miss rates of LRU. Our experiments showed that on average the miss rates of PRR are only 0.22\% higher than the miss rates of LRU on a 32KB, 4-way L1 data cache with 32 byte long cache lines. PRR has miss rates that are on average 4.72\% and 4.66\% lower than random and round-robin replacement algorithms, respectively.},
  eventtitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EMTYUQ72\\Wei-Che Tseng et al. - 2012 - PRR A low-overhead cache replacement algorithm fo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SB2XYXES\\6164972.html},
  keywords = {Algorithm design and analysis,Benchmark testing,cache capacity,cache replacement policies,cache storage,Complexity theory,data cache replacement algorithm,embedded systems,Embedded systems,embedded systems power consumption,embedded systems processor,least recently used algorithm,low-overhead cache replacement algorithm,LRU algorithm,management logic,miss rates,Multicore processing,Program processors,protected round-robin,pseudoLRU,random algorithm,Round robin,round-robin replacement algorithm}
}

@inproceedings{wei-fenlinReducingDRAMLatencies2001,
  title = {Reducing {{DRAM}} Latencies with an Integrated Memory Hierarchy Design},
  booktitle = {Proceedings {{HPCA Seventh International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {{Wei-Fen Lin} and Reinhardt, S. K. and Burger, D.},
  date = {2001-01},
  pages = {301--312},
  doi = {10.1109/HPCA.2001.903272},
  abstract = {In this paper we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half of its time stalling for L2 misses. Large cache blocks can improve performance, but only when coupled with wide memory channels. DRAM address mappings also affect performance significantly. We evaluate an aggressive prefetch unit integrated with the L2 cache and memory, controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 43\% speedup across 10 of the 26 SPEC2000 benchmarks, without degrading performance an the others. With eight Rambus channels, these ten benchmarks improve to within 10\% of the performance of a perfect L2 cache.},
  eventtitle = {Proceedings {{HPCA Seventh International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QZ59WSBZ\\Wei-Fen Lin et al. - 2001 - Reducing DRAM latencies with an integrated memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EUUYIX7N\\903272.html},
  keywords = {Banking,benchmarks,cache blocks,cache storage,Clocks,Computer science,Degradation,Delay,DRAM accesses,Dynamic scheduling,Frequency,High performance computing,integrated memory hierarchy,memory architecture,next-generation memory system,performance,performance evaluation,performance gap,Prefetching,Rambus channels,Random access memory}
}

@inproceedings{weiExploitingProgramSemantics2015,
  title = {Exploiting {{Program Semantics}} to {{Place Data}} in {{Hybrid Memory}}},
  booktitle = {2015 {{International Conference}} on {{Parallel Architecture}} and {{Compilation}} ({{PACT}})},
  author = {Wei, W. and Jiang, D. and McKee, S. A. and Xiong, J. and Chen, M.},
  date = {2015-10},
  pages = {163--173},
  doi = {10.1109/PACT.2015.10},
  abstract = {Large-memory applications like data analytics and graph processing benefit from extended memory hierarchies, and hybrid DRAM/NVM (non-volatile memory) systems represent an attractive means by which to increase capacity at reasonable performance/energy tradeoffs. Compared to DRAM, NVMs generally have longer latencies and higher energies for writes, which makes careful data placement essential for efficient system operation. Data placement strategies that resort to monitoring all data accesses and migrating objects to dynamically adjust data locations incur high monitoring overhead and unnecessary memory copies due to mispredicted migrations. We find that program semantics (specifically, global access characteristics) can effectively guide initial data placement with respect to memory types, which, in turn, makes run-time migration more efficient. We study a combined offline/online placement scheme that uses access profiling information to place objects statically and then selectively monitors run-time behaviors to optimize placements dynamically. We present a software/hardware cooperative framework, 2PP, and evaluate it with respect to state-of-the-art migratory placement, finding that it improves performance by an average of 12.1\%. Furthermore, 2PP improves energy efficiency by up to 51.8\%, and by an average of 18.4\%. It does so by reducing run-time monitoring and migration overheads.},
  eventtitle = {2015 {{International Conference}} on {{Parallel Architecture}} and {{Compilation}} ({{PACT}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZJ69QVFR\\Wei et al. - 2015 - Exploiting Program Semantics to Place Data in Hybr.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9NUV2RPH\\7429303.html},
  keywords = {2PP,access profiling information,computer centres,data center,Data mining,data placement strategy,DRAM,DRAM chips,energy efficiency,hybird memory systems,hybrid memory,Monitoring,Nonvolatile memory,nonvolatile memory system,NVM system,offline/online placement scheme,Organizations,Phase change materials,program semantics,Random access memory,Semantics,software/hardware cooperative framework,storage management}
}

@patent{wen-tzerImplementationPseudoLRUAlgorithm2006,
  title = {Implementation of a Pseudo-{{LRU}} Algorithm in a Partitioned Cache},
  author = {Wen-Tzer, Thomas Chen and Peichun, Peter Liu and Kevin, C. Stelzer},
  date = {2006-06-27},
  url = {https://patentimages.storage.googleapis.com/3c/d9/d9/b90257ee8c4d87/US7069390.pdf},
  urldate = {2020-03-01},
  abstract = {The present invention provides for a plurality of partitioned ways of an associative cache. A pseudo-least recently used binary tree is provided, as is a way partition binary tree, and signals are derived from the way partition binary tree as a function of a mapped partition. Signals from the way partition binary tree and the pseudo-least recently used binary tree are combined. A cache line replacement signal is employable to select one way of a partition as a function of the pseudo-least recently used binary tree and the signals derived from the way partition binary tree.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3STJMZZQ\\US7069390.pdf},
  holder = {{International Business Machines Corp}},
  number = {7069390B2},
  type = {patent}
}

@article{whithamTimePredictableOutofOrderExecution2010,
  title = {Time-{{Predictable Out}}-of-{{Order Execution}} for {{Hard Real}}-{{Time Systems}}},
  author = {Whitham, Jack and Audsley, Neil},
  date = {2010-09},
  journaltitle = {IEEE Transactions on Computers},
  volume = {59},
  pages = {1210--1223},
  issn = {0018-9340},
  doi = {10.1109/TC.2010.109},
  url = {http://ieeexplore.ieee.org/document/5467051/},
  urldate = {2019-07-23},
  abstract = {Superscalar out-of-order CPU designs can achieve higher performance than simpler in-order designs through exploitation of instruction-level parallelism in software. However, these CPU designs are often considered to be unsuitable for hard real-time systems because of the difficulty of guaranteeing the worst-case execution time (WCET) of software. This paper proposes and evaluates modifications for a superscalar out-of-order CPU core to allow instruction-level parallelism to be exploited without sacrificing time predictability and support for WCET analysis. Experiments using the M5 O3 CPU simulator show that WCETs can be two-four times smaller than those obtained using an idealized in-order CPU design, as instruction-level parallelism is exploited without compromising timing safety.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\56ZI6LUR\\Whitham and Audsley - 2010 - Time-Predictable Out-of-Order Execution for Hard R.pdf},
  langid = {english},
  number = {9}
}

@article{wielWhenCachesAren1997,
  title = {When Caches Aren't Enough: Data Prefetching Techniques},
  shorttitle = {When Caches Aren't Enough},
  author = {Wiel, S. P. Vander and Lilja, D. J.},
  date = {1997-07},
  journaltitle = {Computer},
  volume = {30},
  pages = {23--30},
  issn = {0018-9162},
  doi = {10.1109/2.596622},
  abstract = {With data prefetching, memory systems call data into the cache before the processor needs it, thereby reducing memory-access latency. Using the most suitable techniques is critical to maximizing data prefetching's effectiveness. The authors review three popular prefetching techniques: software-initiated prefetching, sequential hardware-initiated prefetching, and prefetching via reference prediction tables.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XS2UJFYW\\Wiel and Lilja - 1997 - When caches aren't enough data prefetching techni.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HGEQ8KMX\\596622.html},
  keywords = {Application software,cache,Cache memory,cache storage,data prefetching techniques,Delay,DRAM chips,Fabrication,Hardware,memory architecture,memory systems,memory-access latency,Prefetching,Processor scheduling,Random access memory,Read-write memory,reference prediction tables,sequential hardware-initiated prefetching,software-initiated prefetching},
  number = {7}
}

@article{wilkesMemoryGapFuture2001,
  title = {The Memory Gap and the Future of High Performance Memories},
  author = {Wilkes, Maurice V.},
  date = {2001-03-01},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  volume = {29},
  pages = {2--7},
  issn = {01635964},
  doi = {10.1145/373574.373576},
  url = {http://portal.acm.org/citation.cfm?doid=373574.373576},
  urldate = {2019-07-22},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\85ND6GEF\\Wilkes - 2001 - The memory gap and the future of high performance .pdf},
  langid = {english},
  number = {1}
}

@article{wilkesSlaveMemoriesDynamic1965,
  title = {Slave {{Memories}} and {{Dynamic Storage Allocation}}},
  author = {Wilkes, M. V.},
  date = {1965-04},
  journaltitle = {IEEE Transactions on Electronic Computers},
  volume = {EC-14},
  pages = {270--271},
  issn = {0367-7508},
  doi = {10.1109/PGEC.1965.264263},
  url = {http://ieeexplore.ieee.org/document/4038419/},
  urldate = {2019-07-22},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SERKATC8\\Wilkes - 1965 - Slave Memories and Dynamic Storage Allocation.pdf},
  langid = {english},
  number = {2}
}

@inproceedings{wolfLargeEventTraces2006,
  title = {Large Event Traces in Parallel Performance Analysis.},
  author = {Wolf, Felix and Freitag, Fèlix and Mohr, Bernd and Moore, Shirley and Wylie, Brian J. N.},
  date = {2006},
  publisher = {{Springer Verlag}},
  url = {http://upcommons.upc.edu/handle/2117/2281},
  urldate = {2017-02-03},
  abstract = {A powerful and widely-used method for analyzing the performance behavior of
parallel programs is event tracing. When an application is traced, performancerelevant
events, such as entering functions or sending messages, are recorded at runtime
and analyzed post-mortem to identify and potentially remove performance problems.
While event tracing enables the detection of performance problems at a high
level of detail, growing trace-file size often constrains its scalability on large-scale
systems and complicates management, analysis, and visualization of trace data. In this
article, we survey current approaches to handle large traces and classify them according
to the primary issues they address and the primary benefits they offer.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CMRQSNVM\\Wolf et al. - 2006 - Large event traces in parallel performance analysi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HNZ7JA8S\\2281.html},
  langid = {english}
}

@inproceedings{wongModifiedLRUPolicies2000,
  title = {Modified {{LRU}} Policies for Improving Second-Level Cache Behavior},
  booktitle = {Proceedings {{Sixth International Symposium}} on {{High}}-{{Performance Computer Architecture}}. {{HPCA}}-6 ({{Cat}}. {{No}}.{{PR00550}})},
  author = {Wong, W.A. and Baer, J.-L.},
  date = {2000-01},
  pages = {49--60},
  issn = {null},
  doi = {10.1109/HPCA.2000.824338},
  abstract = {Main memory accesses continue to be a significant bottleneck for applications whose working sets do not fit in second-level caches. With the trend of greater associativity in second-level caches, implementing effective replacement algorithms might become more important than reducing conflict misses. After showing that an opportunity exists to close part of the gap between the OPT and the LRU algorithms, we present a replacement algorithm based on the detection of temporal locality in lines residing in the L2 cache. Rather than always replacing the LRU line, the victim is chosen by considering both its priority in the LRU stack and whether it exhibits temporal locality or not. We consider two strategies which use this replacement algorithm: a profile-based scheme where temporal locality is detected by processing a trace from a training set of the application and an on-line scheme, where temporal locality is detected with the assistance of a small locality table. Both schemes improve on the second-level cache miss rate over a pure LRU algorithm, by as much as 12\% in the profiling case and 20\% in the dynamic case.},
  eventtitle = {Proceedings {{Sixth International Symposium}} on {{High}}-{{Performance Computer Architecture}}. {{HPCA}}-6 ({{Cat}}. {{No}}.{{PR00550}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\528YVZBJ\\Wong and Baer - 2000 - Modified LRU policies for improving second-level c.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EXTBC33N\\824338.html},
  keywords = {associativity,cache storage,content-addressable storage,Delay,effective replacement algorithms,Hardware,modified LRU policies,Optimized production technology,performance evaluation,profile-based scheme,Radio access networks,replacement algorithm,second-level cache behavior,second-level caches,temporal locality}
}

@article{woodMinimizationDemandPaging1983,
  title = {Minimization of Demand Paging for the {{LRU}} Stack Model of Program Behavior},
  author = {Wood, Christopher and Fernandez, Eduardo B. and Lang, Tomas},
  date = {1983-02-26},
  journaltitle = {Information Processing Letters},
  shortjournal = {Information Processing Letters},
  volume = {16},
  pages = {99--104},
  issn = {0020-0190},
  doi = {10.1016/0020-0190(83)90034-0},
  url = {http://www.sciencedirect.com/science/article/pii/0020019083900340},
  urldate = {2020-01-11},
  abstract = {The minimization of page faults in a demand paging environment where program behavior is described by the LRU stack model is studied. Previous work on this subject considered a specific type of stack probability distribution. As there exist practical situations which do not satisfy this assumption, we extend the analysis to arbitrary distributions. The minimization is stated as an optimization problem under constraints, a method to obtain a class of optimal solution is presented, and a fixed-space replacement algorithm to implement these solutions is proposed. The parameters of this replacement algorithm can be varied to adapt to specific stack probability distributions and to the number of allocated pages in memory. This paper also determines a necessary and sufficient condition for the optimality of the LRU algorithm.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DI2CPZ73\\Wood et al. - 1983 - Minimization of demand paging for the LRU stack mo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TYYCZD5R\\0020019083900340.html},
  keywords = {demand paging,LRU stack model,memory management,Page replacement algorithms,performance optimization,virtual memory},
  langid = {english},
  number = {2}
}

@inproceedings{xiangLessReusedFilter2009,
  title = {Less {{Reused Filter}}: {{Improving L2 Cache Performance}} via {{Filtering Less Reused Lines}}},
  shorttitle = {Less {{Reused Filter}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  author = {Xiang, Lingxiang and Chen, Tianzhou and Shi, Qingsong and Hu, Wei},
  date = {2009},
  pages = {68--79},
  publisher = {{ACM}},
  location = {{Yorktown Heights, NY, USA}},
  doi = {10.1145/1542275.1542290},
  url = {http://doi.acm.org/10.1145/1542275.1542290},
  urldate = {2019-09-12},
  abstract = {The L2 cache is commonly managed using LRU policy. For workloads that have a working set larger than L2 cache, LRU behaves poorly, resulting in a great number of less reused lines that are never reused or reused for few times. In this case, the cache performance can be improved through retaining a portion of working set in cache for a period long enough. Previous schemes approach this by bypassing never reused lines. Nevertheless, severely constrained by the number of never reused lines, sometimes they deliver no benefit due to the lack of never reused lines. This paper proposes a new filtering mechanism that filters out the less reused lines rather than just never reused lines. The extended scope of bypassing provides more opportunities to fit the working set into cache. This paper also proposes a Less Reused Filter (LRF), a separate structure that precedes L2 cache, to implement the above mechanism. LRF employs a reuse frequency predictor to accurately identify the less reused lines from incoming lines. Meanwhile, based on our observation that most less reused lines have a short life span, LRF places the filtered lines into a small filter buffer to fully utilize them, avoiding extra misses. Our evaluation, for 24 SPEC 2000 benchmarks, shows that augmenting a 512KB LRU-managed L2 cache with a LRF having 32KB filter buffer reduces the average MPKI by 27.5\%, narrowing the gap between LRU and OPT by 74.4\%.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DYWMAUSB\\Xiang et al. - 2009 - Less Reused Filter Improving L2 Cache Performance.pdf},
  isbn = {978-1-60558-498-0},
  keywords = {cache filtering,less reused line},
  series = {{{ICS}} '09}
}

@inproceedings{xieHybridLRUCachingScheme2016,
  title = {Hybrid-{{LRU Caching Scheme}} for {{PDRAM Hybrid Memory Architecture}} in {{Cloud Computing}}},
  booktitle = {2016 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}} ({{IIKI}})},
  author = {Xie, H. and Jia, G. and Han, G. and Wan, J. and Ren, Y. and Huang, J. and You, X.},
  date = {2016-10},
  pages = {14--21},
  doi = {10.1109/IIKI.2016.22},
  abstract = {This in recent years, the mixed memory (PDRAM) which is consisted of the phase-change memory (PRAM) and the conventional memory (DRAM) has drawn a lot of attention from both industry and academia. It is a promising alternative to replace the traditional memory architecture. PDRAM has good characteristics as large capacity, good stability and non-volatile, while PRAM has disadvantages of a limited life and large access latency. Traditional policies are not sufficient to be directly applied to the PDRAM memory architecture. They cannot adapt to the new features of the hybrid architecture because of their undifferentiated operation, extensive use of PRAM can cause performance degradation and shorten the life of PDRAM memory. This paper proposed Hybrid-LRU caching scheme to address these problems. Our scheme mainly uses the cache consistency address resolution mode to distinguish between different physical mediums in PDRAM, and then take different actions depending on different physical mediums. We have taken a variety of tradition policies like LRU, FIFO, RANDOM, CFLRU to make experimental comparisons. The experimental results have shown that our scheme can reduce the PRAM utilization rate of 11.8\% and improve the performance by 4.6\%, energy consumption of write and read operation can be reduced up to 88.2\%.},
  eventtitle = {2016 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}} ({{IIKI}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GITHWZQI\\Xie et al. - 2016 - Hybrid-LRU Caching Scheme for PDRAM Hybrid Memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C9I9C53Q\\8281166.html},
  keywords = {cache consistency,cache policy,cache storage,cloud computing,DRAM chips,energy,Energy consumption,Flash memories,hybrid architecture,hybrid memory architecture,hybrid-LRU caching scheme,Material storage,memory architecture,Memory architecture,Nonvolatile memory,PDRAM hybrid memory architecture,PDRAM memory architecture,performance,phase change memory,Phase change random access memory}
}

@inproceedings{yabeNextGenerationChanneledDRAM2000,
  title = {A next Generation Channeled-{{DRAM}} Architecture with Direct Background-Operation and Delayed Channel-Replacement Techniques},
  booktitle = {2000 {{Symposium}} on {{VLSI Circuits}}. {{Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{00CH37103}})},
  author = {Yabe, Y. and Nakamura, N. and Aimoto, Y. and Motomura, M. and Matsui, Y. and Adakura, Y.},
  date = {2000-06},
  pages = {108--111},
  doi = {10.1109/VLSIC.2000.852864},
  abstract = {As processor performance is reaching the level of executing a single instruction in 1 ns, long memory latencies have become a critical problem, because a single memory access could stall the execution of hundreds of instructions. A recently announced channeled-DRAM approaches this problem by integrating a small low-latency buffer, called "channels", in front of a DRAM core in order to reduce the effective memory latency. Since the channels can provide intrinsically faster access than that of a bare DRAM core when they hit, key considerations in this architecture become (1) how to achieve high channel hit rates and (2) how to reduce the channel-miss latencies. Since channeled-DRAMs rely on an external memory controller to handle all the channel management, design of the memory controller heavily dominates the first issue. In this paper, we propose two novel techniques for reducing the channel-miss latencies: direct background operation and delayed channel replacement. We examined these techniques in a future 256-Mb DRAM with a 200-MHz double-data-rate (DDR) synchronous interface. Both SPICE simulation results (that show channel-miss latency reduction) and system-level simulation results (that reveal system-level performance improvement) are presented.},
  eventtitle = {2000 {{Symposium}} on {{VLSI Circuits}}. {{Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{00CH37103}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TZKGEQH3\\Yabe et al. - 2000 - A next generation channeled-DRAM architecture with.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VG4NHJ46\\852864.html},
  keywords = {1 ns,200 MHz,256 Mbit,Bandwidth,channel hit rates,channel management,channel-miss latencies,channeled-DRAM architecture,Delay,delayed channel replacement,Delays,direct background operation,double-data-rate synchronous interface,DRAM chips,DRAM core,external memory controller,Graphics,Laboratories,Large scale integration,low-latency buffer,memory access,Memory architecture,memory latency,Memory management,National electric code,Prefetching,processor performance,Random access memory,Silicon,SPICE simulation,system-level performance improvement,system-level simulation}
}

@inproceedings{yamauchiHierarchicalMultibankDRAM1997,
  title = {The Hierarchical Multi-Bank {{DRAM}}: A High-Performance Architecture for Memory Integrated with Processors},
  shorttitle = {The Hierarchical Multi-Bank {{DRAM}}},
  booktitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  author = {Yamauchi, T. and Hammond, L. and Olukotun, K.},
  date = {1997-09},
  pages = {303--319},
  doi = {10.1109/ARVLSI.1997.634862},
  abstract = {A microprocessor integrated with DRAM on the same die has the potential to improve system performance by reducing the memory latency and improving the memory bandwidth. However a high performance microprocessor will typically send more accesses than the DRAM can handle due to the long cycle time of the embedded DRAM, especially in applications with significant memory requirements. A multi-bank DRAM can hide the long cycle time by allowing the DRAM to process multiple accesses in parallel, but it will incur a significant area penalty and will therefore restrict the density of the embedded DRAM main memory. In this paper we propose a hierarchical multi-bank DRAM architecture to achieve high system performance with a minimal area penalty. In this architecture, the independent memory banks are each divided into many semi-independent subbanks that share I/O and decoder resources. A hierarchical multi-bank DRAM with 4 main banks each composed of 32 subbanks occupies approximately the same area as a conventional 4 bank DRAM while performing like a 32 bank one-up to 65\% better than a conventional 4 bank DRAM when integrated with a single-chip multiprocessor.},
  eventtitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9JSHHHNP\\Yamauchi et al. - 1997 - The hierarchical multi-bank DRAM a high-performan.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\P6T3597Y\\634862.html},
  keywords = {Bandwidth,Computer architecture,decoder resources,Delay,DRAM chips,embedded DRAM,hierarchical multi-bank DRAM,High performance computing,high-performance architecture,Laboratories,memory architecture,Memory architecture,microprocessor,microprocessor chips,Microprocessors,multiple accesses,processor integrated memory,Random access memory,System performance,Ultra large scale integration}
}

@inproceedings{yuIMPIndirectMemory2015,
  title = {{{IMP}}: {{Indirect}} Memory Prefetcher},
  shorttitle = {{{IMP}}},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Yu, X. and Hughes, C. J. and Satish, N. and Devadas, S.},
  date = {2015-12},
  pages = {178--190},
  doi = {10.1145/2830772.2830807},
  abstract = {Machine learning, graph analytics and sparse linear algebra-based applications are dominated by irregular memory accesses resulting from following edges in a graph or non-zero elements in a sparse matrix. These accesses have little temporal or spatial locality, and thus incur long memory stalls and large bandwidth requirements. A traditional streaming or striding prefetcher cannot capture these irregular access patterns. A majority of these irregular accesses come from indirect patterns of the form A[B[j]]. We propose an efficient hardware indirect memory prefetcher (IMP) to capture this access pattern and hide latency. We also propose a partial cacheline accessing mechanism for these prefetches to reduce the network and DRAM bandwidth pressure from the lack of spatial locality. Evaluated on 7 applications, IMP shows 56\% speedup on average (up to 2.3×) compared to a baseline 64 core system with streaming prefetchers. This is within 23\% of an idealized system. With partial cacheline accessing, we see another 9.4\% speedup on average (up to 46.6\%).},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W7P2BQS8\\Yu et al. - 2015 - IMP Indirect memory prefetcher.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C24P7438\\7856597.html},
  keywords = {Arrays,Bandwidth,DRAM bandwidth pressure,DRAM chips,graph analytics,graph theory,Hardware,IMP,Indexes,indirect memory prefetcher,irregular memory accesses,learning (artificial intelligence),linear algebra,machine learning,Multicore processing,non-zero elements,Prefetching,sparse linear algebra,Sparse matrices,sparse matrix,spatial locality,temporal locality}
}

@inproceedings{zebchukReexaminingCacheReplacement2008,
  title = {Re-Examining Cache Replacement Policies},
  author = {Zebchuk, Jason and Makineni, Srihari and Newell, Don},
  date = {2008-10},
  pages = {671--678},
  publisher = {{IEEE}},
  doi = {10.1109/ICCD.2008.4751933},
  url = {http://ieeexplore.ieee.org/document/4751933/},
  urldate = {2020-01-04},
  abstract = {The replacement policies commonly used in modern processors perform an average of 57\% worse than an optimal replacement policy for commercial applications using large, shared caches in a chip-multiprocessor (CMP). Recent proposals that improve the performance of smaller, uniprocessor caches with SPEC CPU workloads do not achieve similar benefits with commercial workloads and larger caches , even though these caches still perform worse than optimal. The recently proposed Shepherd Cache replacement policy reduces miss-ratios by 7.3\% on average, but it relies on an impractical LRU policy and requires 5.3\% overhead relative to the total cache capacity. We propose two new, practical, low-overhead replacement policies that mimic Shepherd Cache with significantly less meta-data overhead. First, we propose a Lightweight Shepherd Cache design that reduces miss-ratios by 8\% on average and up to 19\%, while requiring only 1.9\% meta-data overhead . We also propose an Extra-Lightweight Shepherd Cache design that reduces overhead to only 0.5\% when combined with a practical Clock replacement policy while reducing miss-ratios by an average of 5.4\% and up to 14\%.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PZ5VNPMM\\Zebchuk et al. - 2008 - Re-examining cache replacement policies.pdf},
  isbn = {978-1-4244-2657-7},
  langid = {english}
}

@article{zhangCachedDRAMILP2001,
  title = {Cached {{DRAM}} for {{ILP}} Processor Memory Access Latency Reduction},
  author = {Zhang, Z. and Zhu, Z. and Zhang, X.},
  date = {2001-07},
  journaltitle = {IEEE Micro},
  volume = {21},
  pages = {22--32},
  issn = {0272-1732},
  doi = {10.1109/40.946676},
  abstract = {Cached DRAM adds a small cache onto a DRAM chip to reduce average DRAM access latency. The authors compare cached DRAM with other advanced DRAM techniques for reducing memory access latency in instruction-level-parallelism processors.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VH8GNL7D\\Zhang et al. - 2001 - Cached DRAM for ILP processor memory access latenc.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3MRLLL4U\\946676.html},
  keywords = {Bandwidth,cache storage,cached DRAM,Capacitors,Computer aided instruction,Concurrent computing,Delay,DRAM access latency,DRAM chips,ILP,Impedance,instruction-level-parallelism processors,Interleaved codes,latency reduction,memory architecture,parallel architectures,Parallel processing,processor memory access latency,Random access memory,SDRAM},
  number = {4}
}

@inproceedings{zhangCacheReplacementPolicy2010,
  title = {A {{Cache Replacement Policy Using Adaptive Insertion}} and {{Re}}-Reference {{Prediction}}},
  booktitle = {2010 22nd {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}},
  author = {Zhang, X. and Li, C. and Wang, H. and Wang, D.},
  date = {2010-10},
  pages = {95--102},
  doi = {10.1109/SBAC-PAD.2010.21},
  abstract = {Previous research shows that LRU replacement policy is not efficient when applications exhibit a distant re-reference interval. Recently proposed RRIP policy improves performance for such workloads. However, RRIP lacks of access recency information, which may confuse the replacement policy to make accurate prediction. Consequently, RRIP is not robust for recency-friendly workloads. This paper proposes an Adaptive Insertion and Re-reference Prediction (AI-RRP) policy which evicts data based on both re-reference prediction value and the access recency information. To make the replacement policy more adaptive across different workloads and different phases during execution, Dynamic AI-RRP (DAI-RRP) is proposed which adjusts the insertion position and prediction value for different access patterns. Simulation results show DAI-RRP reduces CPI over LRU and Dynamic RRIP by an average of 8.3\% and 4.1\% respectively on a single-core processor with a 1MB 16-way set last-level cache (LLC). Evaluations on quad-core CMP with a 4MB shared LLC show that DAI-RRP outperforms LRU and Dynamic RRIP (DRRIP) on the weighted speedup metric by an average of 13.2\% and 26.7\% respectively. Furthermore, compred to LRU, DAI-RRP requires similar hardware, or even less hardware for high-associativity cache.},
  eventtitle = {2010 22nd {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WIVN9I7B\\Zhang et al. - 2010 - A Cache Replacement Policy Using Adaptive Insertio.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X9QNV5M9\\5644963.html},
  keywords = {access pattern,Adaptive Insertion,adaptive insertion and re-reference prediction,Cache Replacement,cache replacement policy,cache storage,Electronics packaging,Hardware,last level cache,Proposals,Radiation detectors,Registers,Robustness,Set Dueling,Shared Cache,Simulation,single core processor}
}

@inproceedings{zhangDivideandconquerBubbleReplacement2009,
  title = {Divide-and-Conquer: {{A Bubble Replacement}} for {{Low Level Caches}}},
  shorttitle = {Divide-and-Conquer},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  author = {Zhang, Chuanjun and Xue, Bing},
  date = {2009},
  pages = {80--89},
  publisher = {{ACM}},
  location = {{Yorktown Heights, NY, USA}},
  doi = {10.1145/1542275.1542291},
  url = {http://doi.acm.org/10.1145/1542275.1542291},
  urldate = {2019-09-12},
  abstract = {The widely used LRU replacement policy suffers from the following problems. First, LRU does not exploit fre-quency information of cache accesses. Second, LRU may experience cache thrashing when access to cache exhibits cyclic patterns and the cache capacity is less than the working set. Finally, LRU is expensive to implement in hardware. We propose a bubble replacement for low-level caches, where cache blocks in one set are arranged in a queue for replacement determination. An incoming block enters the queue from the bottom and exchanges its posi-tion with the block above when the block hits, therefore, both recency and frequency information of a program are exploited. A victim block can be chosen from either the bottom or the top block of the queue, which is controlled by a single-bit set-hit flag per set. Choosing the bottom block as the victim makes the bubble replacement resistant to less frequently used blocks from polluting the cache while choosing the top block as the victim makes the bub-ble replacement adaptable to changes in the working set. We also propose to divide the blocks in a cache set into groups where each group implements the bubble replace-ment (we name it the DC-Bubble) to resolve the problems of the bubble replacement. The victim block is chosen ran-domly from the bottom block of each group. The proposed DC-Bubble reduces the average MPKI of the baseline 1MB 16-way L2 cache by 14\%, bridges 47\% of the gap between LRU and the OPT, reduces the storage require-ment by 61\% and simplifies the circuit design compared to LRU.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NXVXF6HB\\Zhang and Xue - 2009 - Divide-and-conquer A Bubble Replacement for Low L.pdf},
  isbn = {978-1-60558-498-0},
  keywords = {cache replacement policy,divide-and-conquer,high-performance computing},
  series = {{{ICS}} '09}
}

@inproceedings{zhangLaxityAwareMemoryAccess2011,
  title = {A {{Laxity}}-{{Aware Memory Access Scheduler}} for {{High Performance Multimedia SoC}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Computer}} and {{Information Technology}}},
  author = {Zhang, G. and Jiang, Y. and Wang, W. and Su, M.},
  date = {2011-08},
  pages = {603--608},
  doi = {10.1109/CIT.2011.13},
  abstract = {Nowadays high-performance multimedia SoC design always integrates a variety of function units (FU) into a single chip and these FUs impose great stress on the shared memory system. To improve the memory system utilization and meet a wide range of bandwidth and latency requirements of these FUs, a well-designed memory scheduler that takes the quality-of-service (QoS) into account must be adopted. In this paper, a laxity-aware memory scheduler that can adaptively measure the laxity of each memory access task is proposed. Known the laxity of each memory access task, the proposed memory scheduler can guarantee the necessary bandwidth within a certain time interval, which is crucial to the performance and user experience of multimedia SoCs. Compared to previous proposed memory scheduling algorithms based on bandwidth allocating, the laxity-aware memory scheduler can obtain 14.5\% decrease in memory access latency while preserving high DRAM data bus utilization.},
  eventtitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Computer}} and {{Information Technology}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J7YQ3AHG\\Zhang et al. - 2011 - A Laxity-Aware Memory Access Scheduler for High Pe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6N9GT9WY\\6036832.html},
  keywords = {Bandwidth,bandwidth allocation,bandwidth requirement,DRAM data bus utilization,Graphics processing unit,high performance multimedia SoC,latency requirement,laxity-aware memory access scheduler,Memory management,memory scheduling algorithm,Multimedia communication,multimedia systems,quality of service,Quality of service,quality-of-service,Random access memory,scheduling,shared memory system,shared memory systems,System-on-a-chip,system-on-chip}
}

@inproceedings{zhangPACPLRUCacheReplacement2011,
  title = {{{PAC}}-{{PLRU}}: {{A Cache Replacement Policy}} to {{Salvage Discarded Predictions}} from {{Hardware Prefetchers}}},
  shorttitle = {{{PAC}}-{{PLRU}}},
  booktitle = {2011 11th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  author = {Zhang, K. and Wang, Z. and Chen, Y. and Zhu, H. and Sun, X.},
  date = {2011-05},
  pages = {265--274},
  doi = {10.1109/CCGrid.2011.27},
  abstract = {Cache replacement policy plays an important role in guaranteeing the availability of cache blocks, reducing miss rates, and improving applications' overall performance. However, recent research efforts on improving replacement policies require either significant additional hardware or major modifications to the organization of the existing cache. In this study, we propose the PAC-PLRU cache replacement policy. PAC-PLRU not only utilizes but also judiciously salvages the prediction information discarded from a widely-adopted stride prefetcher. The main idea behind PAC-PLRU is utilizing the prediction results generated by the existing stride prefetcher and preventing these predicted cache blocks from being replaced in the near future. Experimental results show that leveraging the PAC-PLRU with a stride prefetcher reduces the average L2 cache miss rate by 91\% over a baseline system with only PLRU policy, and by 22\% over a system using PLRU with an unconnected stride prefetcher. Most importantly, PAC-PLRU only requires minor modifications to existing cache architecture to get these benefits. The proposed PAC-PLRU policy is promising in fostering the connection between prefetching and replacement policies, and have a lasting impact on improving the overall cache performance.},
  eventtitle = {2011 11th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2FSSUDHL\\Zhang et al. - 2011 - PAC-PLRU A Cache Replacement Policy to Salvage Di.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LI5DFS7J\\5948617.html},
  keywords = {Binary trees,cache architecture,cache blocks,cache replacement policy,cache storage,computer architecture,Filtering,Hardware,hardware prefetchers,high-performance processors,L2 cache miss rate,Mathematical model,memory architecture,memory wall,Microarchitecture,PAC-PLRU,Prefetching,salvage discarded predictions,stride prefetcher}
}

@inproceedings{zhangTagbasedCacheReplacement2010,
  title = {A Tag-Based Cache Replacement},
  booktitle = {2010 {{IEEE International Conference}} on {{Computer Design}}},
  author = {Zhang, C. and Xue, B.},
  date = {2010-10},
  pages = {92--97},
  doi = {10.1109/ICCD.2010.5647602},
  abstract = {Conventional cache replacement policies use access information of each cache block for replacement decisions. We observe that there are many identical tags across different cache sets because programs exhibit spatial locality. The number of different tags in cache memory is significantly less than the total number of cache blocks in a cache. We propose a tag-based replacement that uses access frequency and recency of tags instead of cache blocks for the replacement decision. The tag-based replacement reduces the average miss rate of the baseline 1MB L2 cache by 15\% over conventional LRU with 95\% status bits reduction over conventional LRU. The performance improvement of a processor using the tag-based replacement is up to 40\% with an average of 4.5\% over LRU.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Computer Design}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W9SHQL3V\\Zhang and Xue - 2010 - A tag-based cache replacement.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\28FVPR5F\\5647602.html},
  keywords = {access frequency,Art,Benchmark testing,cache memory,Cache memory,cache storage,Frequency conversion,Hardware,Indexes,LRU,processor performance improvement,Radiation detectors,tag-based cache replacement}
}

@inproceedings{zhangWebCachingFramework1999,
  title = {Web Caching Framework: Analytical Models and Beyond},
  shorttitle = {Web Caching Framework},
  booktitle = {Proceedings 1999 {{IEEE Workshop}} on {{Internet Applications}} ({{Cat}}. {{No}}.{{PR00197}})},
  author = {Zhang, J. and Izmailov, R. and Reininger, D. and Ott, M.},
  date = {1999-07},
  pages = {132--141},
  issn = {null},
  doi = {10.1109/WIAPP.1999.788030},
  abstract = {Many Web caching algorithms have been proposed in recent years. However the lack of analytical support and systematic evaluation environment significantly affect the applicability of these algorithms. We introduce a framework within which Web caching algorithms can be consistently analyzed and empirically examined. The framework consists of two complementary parts. The statistical model and the simulation environment. The analytical model covers both the Web trace characteristics and the caching algorithm behaviors. The simulation system, referred to as WebCASE (Web Caching Algorithm Simulation Environment), consists of an extensible simulation core and a front end graphical interface showing the running algorithm behaviors. By using this framework, we are able to better understand the performance discrepancies exhibited by different algorithms and develop more efficient new algorithms. These new algorithms take into consideration practical issues and make noticeable performance improvements over existing algorithms.},
  eventtitle = {Proceedings 1999 {{IEEE Workshop}} on {{Internet Applications}} ({{Cat}}. {{No}}.{{PR00197}})},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KEIGEMGK\\Zhang et al. - 1999 - Web caching framework analytical models and beyon.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2NTFETQZ\\788030.html},
  keywords = {Algorithm design and analysis,analytical models,Analytical models,cache storage,caching algorithm behaviors,digital simulation,Electrical capacitance tomography,Explosives,extensible simulation core,file servers,front end graphical interface,graphical user interfaces,information resources,Internet,Laboratories,Measurement,National electric code,Performance analysis,performance discrepancies,performance improvements,running algorithm behaviors,search engines,simulation environment,simulation system,statistical model,Web Caching Algorithm Simulation Environment,Web caching algorithms,Web caching framework,Web server,Web sites,Web trace characteristics,WebCASE}
}

@inproceedings{zhengWCETAwareDynamic2014,
  title = {{{WCET}}: Aware Dynamic Instruction Cache Locking},
  shorttitle = {{{WCET}}},
  booktitle = {Proceedings of the 2014 {{SIGPLAN}}/{{SIGBED}} Conference on {{Languages}}, Compilers and Tools for Embedded Systems - {{LCTES}} '14},
  author = {Zheng, Wenguang and Wu, Hui},
  date = {2014},
  pages = {53--62},
  publisher = {{ACM Press}},
  location = {{Edinburgh, United Kingdom}},
  doi = {10.1145/2597809.2597820},
  url = {http://dl.acm.org/citation.cfm?doid=2597809.2597820},
  urldate = {2019-08-02},
  abstract = {Caches are widely used in embedded systems to bridge the increasing speed gap between processors and off-chip memory. However, caches make it significantly harder to compute the WCET( Worst Case Execution Time) of a task. To alleviate this problem, cache locking has been proposed. We investigate the I-cache locking problem, and propose a WCET-aware, min-cut based dynamic instruction cache locking approach for reducing the WCET of a single task. We have implemented our approach and compared it with the two state-of-the-art cache locking approaches by using a set of benchmarks from the MRTC benchmark suite. The experimental results show that our approach achieves the average improvements of 41\%, 15\% and 7\% over the partial locking approach for the 256B, 512B and 1KB caches, respectively, and 7\%, 18\% and 17\% over the longest path based dynamic locking approach for the 256B, 512B and 1KB caches, respectively.},
  eventtitle = {The 2014 {{SIGPLAN}}/{{SIGBED}} Conference},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BYJ974RM\\Zheng and Wu - 2014 - WCET aware dynamic instruction cache locking.pdf},
  isbn = {978-1-4503-2877-7},
  langid = {english}
}

@inproceedings{zhenlinwangUsingCompilerImprove2002,
  title = {Using the Compiler to Improve Cache Replacement Decisions},
  booktitle = {Proceedings.{{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  author = {{Zhenlin Wang} and McKinley, K. S. and Rosenberg, A. L. and Weems, C. C.},
  date = {2002-09},
  pages = {199--208},
  doi = {10.1109/PACT.2002.1106018},
  abstract = {Memory performance is increasingly determining microprocessor performance and technology trends are exacerbating this problem. Most architectures use set-associative caches with LRU replacement policies to combine fast access with relatively low miss rates. To improve replacement decisions in set-associative caches, we develop a new set of compiler algorithms that predict which data will and will not be reused and provide these hints to the architecture. We prove that the hints either match or improve hit rates over LRU. We describe a practical one-bit cache-line tag implementation of our algorithm, called evict-me. On a cache replacement, the architecture will replace a line for which the evict-me bit is set, or if none is set, it will use the LRU bits. We implement our compiler analysis and its output in the Scale compiler. On a variety of scientific programs, using the evict-me algorithm in both the level 1 and 2 caches improves simulated cycle times by up to 34\% over the LRU policy by increasing hit rates. In addition, a combination of simple hardware prefetching and evict-me works together to further improve performance.},
  eventtitle = {Proceedings.{{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZR6QY6PF\\Zhenlin Wang et al. - 2002 - Using the compiler to improve cache replacement de.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DFHI92ZY\\1106018.html},
  keywords = {Bridges,cache replacement decisions,cache storage,Clocks,Computer architecture,Computer science,content-addressable storage,Delay,evict-me,Hardware,hardware prefetching,hit rates,LRU replacement policies,memory performance,Microarchitecture,microprocessor performance,Microprocessors,one-bit cache-line tag implementation,performance evaluation,Prediction algorithms,Prefetching,program compiler,program compilers,Scale compiler,scientific programs,set-associative cache,storage management}
}

@inproceedings{zhiganghuTimekeepingMemorySystem2002,
  title = {Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior},
  shorttitle = {Timekeeping in the Memory System},
  booktitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Zhigang Hu and Kaxiras, S. and Martonosi, M.},
  date = {2002-05},
  pages = {209--220},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2002.1003579},
  abstract = {This paper offers afresh perspective on the problem of predicting and optimizing memory behavior. We show quantitatively the extent to which detailed timing characteristics of past memory reference events are strongly predictive of future program reference behavior. We propose a family of timekeeping techniques that optimize behavior based on observations about particular cache time durations, such as the cache access interval or the cache dead time. Timekeeping techniques can be used to build small, simple, and high-accuracy (often 90\% or more) predictors for identifying conflict misses, for predicting dead blocks, and even for estimating the time at which the next reference to a cache frame will occur and the address that will be accessed. Based on these predictors, we demonstrate two new and complementary time-based hardware structures: (1) a time-based victim cache that improves performance by only storing conflict miss lines with likely reuse, and (2) a time-based prefetching technique that hones in on the right address to prefetch, and the right time to schedule the prefetch.},
  eventtitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CBSRLBIE\\Zhigang Hu et al. - 2002 - Timekeeping in the memory system predicting and o.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NUANHIAP\\1003579.html},
  keywords = {cache access interval,cache dead time,Cache memory,cache storage,cache time durations,Communication system software,future program reference behavior,Hardware,memory architecture,memory referencing behavior,memory system,past memory reference,Performance analysis,Power dissipation,Prefetching,Proposals,Software performance,Software systems,storage management,time-based prefetching,time-based victim cache,timekeeping techniques,Timing}
}

@inproceedings{zhouMultiQueueReplacementAlgorithm2001,
  title = {The {{Multi}}-{{Queue Replacement Algorithm}} for {{Second Level Buffer Caches}}},
  booktitle = {Proceedings of the {{General Track}}: 2001 {{USENIX Annual Technical Conference}}},
  author = {Zhou, Yuanyuan and Philbin, James and Li, Kai},
  date = {2001-06-25},
  pages = {91--104},
  publisher = {{USENIX Association}},
  location = {{USA}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PVY2Q3VI\\zhou.pdf},
  isbn = {978-1-880446-09-6}
}

@article{zhuRACERobustAdaptive2008,
  title = {{{RACE}}: {{A Robust Adaptive Caching Strategy}} for {{Buffer Cache}}},
  shorttitle = {{{RACE}}},
  author = {Zhu, Y. and Jiang, H.},
  date = {2008-01},
  journaltitle = {IEEE Transactions on Computers},
  volume = {57},
  pages = {25--40},
  doi = {10.1109/TC.2007.70788},
  abstract = {Although many block replacement algorithms for buffer caches have been proposed to address the well-known drawbacks of the LRU algorithm, they are not robust and cannot maintain a consistent performance improvement over all workloads. This paper proposes a novel and simple replacement scheme, called the Robust Adaptive buffer Cache management schemE (RACE), which differentiates the locality of I/O streams by actively detecting access patterns that are inherently exhibited in two correlated spaces, that is, the discrete block space of program contexts from which I/O requests are issued and the continuous block space within files to which I/O requests are addressed. This scheme combines the global I/O regularities of an application and the local I/O regularities of individual files that are accessed in that application to accurately estimate the locality strength, which is crucial in deciding which blocks are to be replaced upon a cache miss. Through comprehensive simulations on 10 real-application traces, RACE is shown to have higher hit ratios than LRU and all other state-of-the-art cache management schemes studied in this paper.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\S3PBXCYC\\Zhu and Jiang - 2008 - RACE A Robust Adaptive Caching Strategy for Buffe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZR2W7Y4\\4358240.html},
  keywords = {access pattern detection,Accuracy,Algorithm design and analysis,block replacement algorithm,Buffering,cache storage,Classification algorithms,correlated space,discrete block space,file system,File systems,Input/output,Main memory,Memory management,RACE,robust adaptive buffer cache management scheme,Robustness,Stability analysis,Training},
  number = {1}
}

@inproceedings{zhuSurveyComputerSystem,
  title = {A {{Survey}} on {{Computer System Memory Management}}},
  booktitle = {{{WorldComp}} 2012 {{Proceedings}}},
  author = {Zhu, Qi},
  pages = {7},
  location = {{Las Vegas, Nevada}},
  abstract = {Computer memory is central to the operation of a modern computer system; it stores data or program instructions on a temporary or permanent basis for use in a computer. In this paper, various memory management and optimization techniques to improve computer performance are reviewed, such as the hardware design of the memory organization, the memory management algorithms and optimization techniques, and some hardware and software memory optimization techniques.},
  eventtitle = {2012 {{World Congress}} in {{Computer Science}}, {{Computer Engineering}} \& {{Applied Computing}}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZI42W2KX\\Zhu - A Survey on Computer System Memory Management.pdf},
  langid = {english}
}


