
@inproceedings{scheipelSystemAwarePerformanceMonitoring2017,
  title = {System-{{Aware Performance Monitoring Unit}} for {{RISC}}-{{V Architectures}}},
  doi = {10.1109/DSD.2017.28},
  abstract = {Due to increasing complexity of software in embedded systems, performance aspects become much more important this days. This should happen early in the development process. Often execution times and events are not easily countable or measurable due to a lack of functionality in these systems. Execution time monitoring is also relevant in terms of reacting to internal and external events dynamically.Especially for systems using multiple tasks with internal or external resource dependencies, this is a major discipline. Another problem is that measurements during the development process are often done by interfering the system as a whole. This method leads to biases in the measurement results, because the finalized system gets deployed without these interfering functionalities and can therefore work more efficiently than the development system.The scope of the present work is to develop a module in a hardware description language (HDL) which is able to measure execution times and events task-aware and unaware without interfering the system. The measurements of this module must be handed to the programmer through an easy accessible interface. The main focuses of the project are the scalability, platform independency concerning processor and operating system (OS), as well as easy extendibility. Also, reusability of counters during runtime is included in this work.},
  eventtitle = {2017 {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  booktitle = {2017 {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  date = {2017-08},
  pages = {86-93},
  keywords = {embedded systems,field programmable gate array,Hardware,hardware/software codesign,Monitoring,performance monitoring unit,Phasor measurement units,Pipelines,Radiation detectors,Registers},
  author = {Scheipel, T. and Mauroner, F. and Baunach, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HP438XX3\\Scheipel et al. - 2017 - System-Aware Performance Monitoring Unit for RISC-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KD3RHSUU\\8049771.html}
}

@inproceedings{wahabARMHExFrameworkEfficient2017,
  title = {{{ARMHEx}}: {{A}} Framework for Efficient {{DIFT}} in Real-World {{SoCs}}},
  doi = {10.23919/FPL.2017.8056799},
  shorttitle = {{{ARMHEx}}},
  abstract = {Security in embedded systems remains a major concern. Untrustworthy authorities use a wide range of software attacks. This demo introduces ARMHEx, a practical solution targeting DIFT (Dynamic Information Flow Tracking) implementations on ARM-based SoCs. DIFT is a solution that consists in tracking the dissemination of data inside the system and allows to enforce some security properties. In this demo, we show an implementation of ARMHEx on Xilinx Zynq SoC. Especially, we show how the required information for DIFT is recovered with the help of traces produced by CoreSight components, static analysis and instrumentation.},
  eventtitle = {2017 27th {{International Conference}} on {{Field Programmable Logic}} and {{Applications}} ({{FPL}})},
  booktitle = {2017 27th {{International Conference}} on {{Field Programmable Logic}} and {{Applications}} ({{FPL}})},
  date = {2017-09},
  pages = {1-1},
  keywords = {Instruments,Monitoring,Coprocessors,Runtime,security,Software,Target tracking},
  author = {Wahab, M. A. and Cotret, P. and Allah, M. N. and Hiet, G. and Lapôtre, V. and Gogniat, G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8SD64AUF\\Wahab et al. - 2017 - ARMHEx A framework for efficient DIFT in real-wor.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TZ8FU35B\\8056799.html}
}

@inproceedings{ponugotiOntheflyLoadData2016,
  title = {On-the-Fly Load Data Value Tracing in Multicores},
  doi = {10.1145/2968455.2968507},
  abstract = {Software testing and debugging of modern multicore-based embedded systems is a challenging proposition because of growing hardware and software complexity, increased integration, and tightening time-to-market. To find more bugs faster, software developers of real-time embedded systems increasingly rely on on-chip trace and debug resources, including hefty on-chip buffers and wide trace ports. However, these resources often offer limited visibility of the system, increase the system cost, and do not scale well with a growing number of cores. This paper introduces mlvCFiat, a hardware/software mechanism for capturing and filtering load data value traces in multicores. It relies on first-access tracking in data caches and equivalent modules in the software debugger to significantly reduce the number of trace events streamed out of the target platform. Our experimental evaluation explores the effectiveness of the proposed technique as a function of cache sizes, encoding mechanism, and the number of cores. The results show that mlvCFiat significantly reduces the total trace port bandwidth. The improvements relative to the existing Nexus-like load data value tracing range from 15 to 33 times for a single core and from 14 to 20 times for an octa core.},
  eventtitle = {2016 {{International Conference}} on {{Compliers}}, {{Architectures}}, and {{Sythesis}} of {{Embedded Systems}} ({{CASES}})},
  booktitle = {2016 {{International Conference}} on {{Compliers}}, {{Architectures}}, and {{Sythesis}} of {{Embedded Systems}} ({{CASES}})},
  date = {2016-10},
  pages = {1-10},
  keywords = {embedded systems,Bandwidth,Benchmark testing,debugging,hardware complexity,hardware-software codesign,load data value tracing,mlvCFiat mechanism,Multicore processing,multicore-based embedded systems,Multicores,Multiprocessing systems,Nexus-like load data value tracing,on-chip trace,Program processors,program testing,Program tracing,Real-time embedded systems,software complexity,Software debugging,software testing,Software testing and debugging,trace ports},
  author = {Ponugoti, M. and Tewar, A. K. and Milenković, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F68ISMJX\\Ponugoti et al. - 2016 - On-the-fly load data value tracing in multicores.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QJ892MH7\\7745281.html}
}

@article{tewarMcfTRaptorUnobtrusiveOnthefly2015,
  title = {{{mcfTRaptor}}: {{Toward}} Unobtrusive on-the-Fly Control-Flow Tracing in Multicores},
  volume = {61},
  issn = {1383-7621},
  url = {http://www.sciencedirect.com/science/article/pii/S1383762115000752},
  doi = {10.1016/j.sysarc.2015.07.005},
  shorttitle = {{{mcfTRaptor}}},
  abstract = {Software testing and debugging has become the most critical aspect of the development of modern embedded systems, mainly driven by growing software and hardware complexity, increasing integration, and tightening time-to-market deadlines. Software developers increasingly rely on on-chip trace and debug infrastructure to locate software bugs faster. However, the existing infrastructure offers limited visibility or relies on hefty on-chip buffers and wide trace ports that significantly increase system cost. This paper introduces a new technique called mcfTRaptor for capturing and compressing functional and time-stamped control-flow traces on-the-fly in modern multicore systems. It relies on private on-chip predictor structures and corresponding software modules in the debugger to significantly reduce the number of events that needs to be streamed out of the target platform. Our experimental evaluation explores the effectiveness of mcfTRaptor as a function of the number of cores, encoding mechanisms, and predictor configurations. When compared to the Nexus-like control-flow tracing, mcfTRaptor reduces the trace port bandwidth in the range from 14 to 23.8 times for functional traces and 10.8–18.6 times for time-stamped traces.},
  number = {10},
  journaltitle = {Journal of Systems Architecture},
  shortjournal = {Journal of Systems Architecture},
  series = {Special Section on {{Architecture}} of {{Computing Systems}} Edited by {{Editors}}: {{Wolfgang Karl}}, {{Erik Maehle}}, {{Kay Römer}}, {{Eduardo Tovar}}, {{Martin DanekSpecial}} Section on {{Testing}}, {{Prototyping}}, and {{Debugging}} of {{Multi}}-{{Core Architectures}} Edited by {{Editors}}: {{Frank Hannig}} \& {{Andreas HerkersdorfSpecial}} Section on {{Embedded Vision Architectures}} and {{Applications}} Edited by {{Editors}}: {{Christophe Bobda}}, {{Walter Stechele}}, {{Ali Ahmadinia}} and {{Miaoqing Huang}}},
  date = {2015-11},
  pages = {601-614},
  keywords = {Multicores,Program tracing,Real-time embedded systems,Software testing and debugging},
  author = {Tewar, Amrish K. and Myers, Albert R. and Milenković, Aleksandar},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QVAVNQZ\\Tewar et al. - 2015 - mcfTRaptor Toward unobtrusive on-the-fly control-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\63TKITWW\\S1383762115000752.html}
}

@inproceedings{hochbergerAcquiringExhaustiveContinuous2008,
  title = {Acquiring an Exhaustive, Continuous and Real-Time Trace from {{SoCs}}},
  doi = {10.1109/ICCD.2008.4751885},
  abstract = {The amount of time and resources that have to be spent on debugging of embedded cores continuously increases. Approaches valid 10 years ago can no longer be used due to the variety and complexity of peripheral components of SoC solutions that even might consist of multiple heterogeneous cores. Although there are some initiatives to standardize and leverage the embedded debugging capabilities, current debugging solutions only cover a fraction of the problems present in that area. In this contribution we show a new approach for debugging and tracing SoCs. The new approach, called hidICE (hidden ICE), delivers an exhaustive, continuous and real-time trace with much lower system interference compared to state-of-the-art solutions.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  date = {2008-10},
  pages = {356-362},
  keywords = {embedded systems,Registers,debugging,Embedded software,Ice,multiple heterogeneous cores,SoC,System-on-Chip,Central Processing Unit,continuous trace,embedded cores debugging,embedded debugging capabilities,exhaustive trace,hidden ICE,hidICE,IEC standards,integrated circuit testing,Interference,ISO standards,peripheral components,Railway safety,Real time systems,real-time trace},
  author = {Hochberger, C. and Weiss, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GSGD5DGN\\Hochberger and Weiss - 2008 - Acquiring an exhaustive, continuous and real-time .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WF3RSJRU\\4751885.html}
}

@article{vergeHardwareassistedSoftwareEvent2017,
  langid = {english},
  title = {Hardware-Assisted Software Event Tracing},
  volume = {29},
  issn = {1532-0634},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/cpe.4069/abstract},
  doi = {10.1002/cpe.4069},
  abstract = {Event tracing is a reliable and a low-intrusiveness method to debug and optimize systems and processes. Low overhead is particularly important in embedded systems where resources and energy consumption is critical. The most advanced tracing infrastructures achieve a very low footprint on the traced software, bringing each tracepoint overhead to less than a microsecond. To reduce this still non-negligible impact, the use of dedicated hardware resources is promising. In this paper, we propose complementary methods for tracing that rely on hardware modules to assist software tracing. We designed solutions to take advantage of CoreSight STM, CoreSight ETM, and Intel BTS, which are present on most newer ARM-based systems-on-chip and Intel x86 processors. Our results show that the time overhead for tracing can be reduced by up to 10 times when assisted by hardware, as compared to software tracing with LTTng, a high-performance tracer for Linux. We also propose a modification to the Perf tool to speed BTS execution tracing up to 65\%.},
  number = {10},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  shortjournal = {Concurrency Computat.: Pract. Exper.},
  urldate = {2017-06-01},
  date = {2017-05-25},
  pages = {n/a-n/a},
  keywords = {debugging,ARM CoreSight,dedicated hardware,event tracing,Intel BTS,LTTng},
  author = {Vergé, Adrien and Ezzati-Jivan, Naser and Dagenais, Michel R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JN9T57JI\\Vergé et al. - 2017 - Hardware-assisted software event tracing.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QSZ8MBB3\\abstract.html}
}

@article{backaschRuntimeVerificationMulticore2013,
  title = {Runtime {{Verification}} for {{Multicore SoC}} with {{High}}-Quality {{Trace Data}}},
  volume = {18},
  issn = {1084-4309},
  url = {http://doi.acm.org/10.1145/2442087.2442089},
  doi = {10.1145/2442087.2442089},
  abstract = {Multicore System-on-Chip (SoC) implementations of embedded systems are becoming very popular. In these systems it is possible to spread out computations over many cores. On one hand this leads to better energy efficiency if clock frequencies and core voltages are reduced. On the other hand this delivers very high performance to the software developer and thus enables complex software systems to be implemented. Unfortunately, debugging and validation of these systems becomes extremely difficult. Various technological approaches try to solve this dilemma. In this contribution we will show a new approach to observe multi-core SoCs and make their internal operations visible to external analysis tools. Also, we show that runtime verification can be employed to analyze and validate these internal operations while the system operates in its normal environment. The combination of these two approaches delivers unprecedented options to the developer to understand and verify system behavior even in complex multicore SoCs.},
  number = {2},
  journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
  urldate = {2017-03-07},
  date = {2013-04},
  pages = {18:1--18:26},
  keywords = {Embedded system,Multicore SoC,runtime verification,synchronisation,test driven development,trace data},
  author = {Backasch, Rico and Hochberger, Christian and Weiss, Alexander and Leucker, Martin and Lasslop, Richard},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DFIGAC8K\\Backasch et al. - 2013 - Runtime Verification for Multicore SoC with High-q.pdf}
}

@inproceedings{ferdmanProactiveInstructionFetch2011,
  title = {Proactive Instruction Fetch},
  abstract = {Fast access requirements preclude building L1 instruction caches large enough to capture the working set of server workloads. Efforts exist to mitigate limited L1 instruction cache capacity by relying on the stability and repetitiveness of the instruction stream to predict and prefetch future instruction blocks prior to their use. However, dynamic variation in cache miss sequences prevents correct and timely prediction, leaving many instruction-fetch stalls exposed, resulting in a key performance bottleneck for servers. We observe that, while the vast majority of application instruction references are amenable to prediction, even minor control-flow variations are amplified by microarchitectural components, resulting in a major source of instability and randomness that significantly limit prefetcher utility. Control-flow variation disturbs the L1 instruction cache replacement order and branch predictor state, causing the L1 instruction cache to randomly filter the instruction stream while the branch predictor and spontaneous hardware interrupts inject the stream with unpredictable noise. Based on this observation, we show that an instruction prefetcher, previously plagued by microarchitectural instability, becomes nearly perfect when modified to operate on the correct-path, retire-order instruction stream. We propose Proactive Instruction Fetch, an instruction prefetch mechanism that achieves higher than 99.5\% instruction-cache hit rate, improving server throughput by 27\% and nearly matching the performance of a perfect L1 instruction cache that never misses.},
  eventtitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2011-12},
  pages = {152-162},
  keywords = {Correlation,Hardware,branch prediction,cache miss sequence,cache storage,caching,control-flow variation,Filtering,History,instruction prefetch mechanism,Instruction sets,instruction streaming,L1 instruction cache,microarchitectural instability,Microarchitecture,Prefetching,proactive instruction fetch,Servers,software architecture},
  author = {Ferdman, M. and Kaynak, C. and Falsafi, B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8GGCKWGI\\Ferdman et al. - 2011 - Proactive instruction fetch.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8SQQA3VT\\7851467.html}
}

@inproceedings{guptaBundledExecutionRecurring2011,
  title = {Bundled Execution of Recurring Traces for Energy-Efficient General Purpose Processing},
  abstract = {Technology scaling has delivered on its promises of increasing device density on a single chip. However, the voltage scaling trend has failed to keep up, introducing tight power constraints on manufactured parts. In such a scenario, there is a need to incorporate energy-efficient processing resources that can enable more computation within the same power budget. Energy efficiency solutions in the past have typically relied on application specific hardware and accelerators. Unfortunately, these approaches do not extend to general purpose applications due to their irregular and diverse code base. Towards this end, we propose BERET, an energy-efficient co-processor that can be configured to benefit a wide range of applications. Our approach identifies recurring instruction sequences as phases of “temporal regularity” in a program's execution, and maps suitable ones to the BERET hardware, a three-stage pipeline with a bundled execution model. This judicious off-loading of program execution to a reduced-complexity hardware demonstrates significant savings on instruction fetch, decode and register file accesses energy. On average, BERET reduces energy consumption by a factor of 3-4X for the program regions selected across a range of general-purpose and media applications. The average energy savings for the entire application run was 35\% over a single-issue in-order processor.},
  eventtitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2011-12},
  pages = {12-23},
  keywords = {Computer science,Pipelines,Coprocessors,Program processors,Microarchitecture,Batteries,BERET hardware,Bridges,bundled execution model,Co-processor,Efficiency,Energy Saving,energy-efficient coprocessor,energy-efficient general purpose processing,Latches,Market research,power aware computing,program execution,recurring instruction sequence,technology scaling,temporal regularity},
  author = {Gupta, S. and Feng, S. and Ansari, A. and Mahlke, S. and August, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XKG54E3V\\Gupta et al. - 2011 - Bundled execution of recurring traces for energy-e.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H4J9785B\\7851455.html}
}

@inproceedings{wolfLargeEventTraces2006,
  langid = {english},
  title = {Large Event Traces in Parallel Performance Analysis.},
  url = {http://upcommons.upc.edu/handle/2117/2281},
  abstract = {A powerful and widely-used method for analyzing the performance behavior of
parallel programs is event tracing. When an application is traced, performancerelevant
events, such as entering functions or sending messages, are recorded at runtime
and analyzed post-mortem to identify and potentially remove performance problems.
While event tracing enables the detection of performance problems at a high
level of detail, growing trace-file size often constrains its scalability on large-scale
systems and complicates management, analysis, and visualization of trace data. In this
article, we survey current approaches to handle large traces and classify them according
to the primary issues they address and the primary benefits they offer.},
  publisher = {{Springer Verlag}},
  urldate = {2017-02-03},
  date = {2006},
  author = {Wolf, Felix and Freitag, Fèlix and Mohr, Bernd and Moore, Shirley and Wylie, Brian J. N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CMRQSNVM\\Wolf et al. - 2006 - Large event traces in parallel performance analysi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HNZ7JA8S\\2281.html}
}

@inproceedings{diasCacheDesignAssessment2016,
  title = {A {{Cache Design Assessment Approach}} for {{Embedded Real}}-{{Time Systems Based}} on {{Execution Time Measurement}}},
  doi = {10.1109/SBESC.2016.033},
  abstract = {Due to the increasing complexity of embedded systems, simulation is of paramount importance during design phase. Often such systems must obey real-time constraints, calling for worst-case execution time assessment mechanisms. Although there is a wide range of simulation tools in the embedded systems domain, mechanisms for performing high abstraction level estimates for task execution times within a controlled environment are still needed. Non-determinism introduced by cache, multi-core and operating systems, for example, makes timing analysis highly complex or even impossible. We address this problem by presenting a RISC-V Instruction Set Simulation platform equipped with a task profiling mechanism for cache aware execution time measurements. The generated SystemC processor simulation model is integrated within a high abstraction level simulation platform with main memory and cache. Experimental results show that by making use of this kind of platform, designers can easily monitor task execution time as a function of measured code portion, cache sizes or cache policies employed helping in their decisions.},
  eventtitle = {2016 {{VI Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  booktitle = {2016 {{VI Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  date = {2016-11},
  pages = {168-173},
  keywords = {Hardware,Registers,Estimation,Modeling,real-time systems,Timing},
  author = {Dias, D. and Lima, G. and Barros, E.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KGCF2H6C\\Dias et al. - 2016 - A Cache Design Assessment Approach for Embedded Re.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HJDA4WHW\\7828301.html}
}

@inproceedings{yuIMPIndirectMemory2015,
  title = {{{IMP}}: {{Indirect}} Memory Prefetcher},
  doi = {10.1145/2830772.2830807},
  shorttitle = {{{IMP}}},
  abstract = {Machine learning, graph analytics and sparse linear algebra-based applications are dominated by irregular memory accesses resulting from following edges in a graph or non-zero elements in a sparse matrix. These accesses have little temporal or spatial locality, and thus incur long memory stalls and large bandwidth requirements. A traditional streaming or striding prefetcher cannot capture these irregular access patterns. A majority of these irregular accesses come from indirect patterns of the form A[B[j]]. We propose an efficient hardware indirect memory prefetcher (IMP) to capture this access pattern and hide latency. We also propose a partial cacheline accessing mechanism for these prefetches to reduce the network and DRAM bandwidth pressure from the lack of spatial locality. Evaluated on 7 applications, IMP shows 56\% speedup on average (up to 2.3×) compared to a baseline 64 core system with streaming prefetchers. This is within 23\% of an idealized system. With partial cacheline accessing, we see another 9.4\% speedup on average (up to 46.6\%).},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2015-12},
  pages = {178-190},
  keywords = {Hardware,Bandwidth,Multicore processing,Prefetching,Arrays,DRAM bandwidth pressure,DRAM chips,graph analytics,graph theory,IMP,Indexes,indirect memory prefetcher,irregular memory accesses,learning (artificial intelligence),linear algebra,machine learning,non-zero elements,sparse linear algebra,Sparse matrices,sparse matrix,spatial locality,temporal locality},
  author = {Yu, X. and Hughes, C. J. and Satish, N. and Devadas, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W7P2BQS8\\Yu et al. - 2015 - IMP Indirect memory prefetcher.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C24P7438\\7856597.html}
}

@article{pandaExpertPrefetchPrediction2016,
  title = {Expert {{Prefetch Prediction}}: {{An Expert Predicting}} the {{Usefulness}} of {{Hardware Prefetchers}}},
  volume = {15},
  issn = {1556-6056},
  doi = {10.1109/LCA.2015.2428703},
  shorttitle = {Expert {{Prefetch Prediction}}},
  abstract = {Hardware prefetching improves system performance by hiding and tolerating the latencies of lower levels of cache and off-chip DRAM. An accurate prefetcher improves system performance whereas an inaccurate prefetcher can cause cache pollution and consume additional bandwidth. Prefetch address filtering techniques improve prefetch accuracy by predicting the usefulness of a prefetch address and based on the outcome of the prediction, the prefetcher decides whether or not to issue a prefetch request. Existing techniques use only one signature to predict the usefulness of a prefetcher but no single predictor works well across all the applications. In this work, we propose weighted-majority filter, an expert way of predicting the usefulness of prefetch addresses. The proposed filter is adaptive in nature and uses the prediction of the best predictor(s) from a pool of predictors. Our filter is orthogonal to the underlying prefetching algorithm. We evaluate the effectiveness of our technique on 22 SPEC-2000/2006 applications. On an average, when employed with three state-of-the-art prefetchers such as AMPM, SMS, and GHB-PC/DC, our filter provides performance improvement of 8.1, 9.3, and 11 percent respectively.},
  number = {1},
  journaltitle = {IEEE Computer Architecture Letters},
  date = {2016-01},
  pages = {13-16},
  keywords = {Hardware,Radiation detectors,cache storage,Prefetching,Accuracy,AMPM,cache,Cache,filtering theory,GHB-PC/DC,hardware prefetchers,Hardware prefetching,Hardware Prefetching,memory systems,Memory systems,Pollution,Prediction algorithms,prefetch addresses,prefetching algorithm,Random access memory,SMS,weighted-majority filter},
  author = {Panda, B. and Balachandran, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G7L6PJ8W\\Panda and Balachandran - 2016 - Expert Prefetch Prediction An Expert Predicting t.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5E528GPU\\7110318.html}
}

@article{shinQDRAMQuickAccessDRAM2016,
  title = {Q-{{DRAM}}: {{Quick}}-{{Access DRAM}} with {{Decoupled Restoring}} from {{Row}}-{{Activation}}},
  volume = {65},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2479587},
  shorttitle = {Q-{{DRAM}}},
  abstract = {The relatively high latency of DRAM is mostly caused by the long row-activation time which in fact consists of sensing and restoring time. Memory controllers cannot distinguish between them since they are performed consecutively by a single row-activation command. If these two steps are separated, the restoring can be delayed until DRAM access is uncongested. Hence, we propose Quick-Access DRAM (Q-DRAM) which discriminates between sensing and restoring. Our approach is to allow destructive access (i.e., only sensing is performed without restoring by a row-activation command) using per-bank multiple row-buffers. We call the destructive access and per-bank multiple row-buffers quick-access and quick-buffers (q-buffers) respectively. In addition, we propose Quick-access Trigger (Q-TRIGGER) and RESTORER to utilize Q-DRAM. Q-TRIGGER makes a decision whether quick-access is required or not, and RESTORER decides when to restore the data at the destructed cell. Specifically, RESTORER detects the proper timing to hide restoring time by predicting data bus occupation and by exploiting bank-level locality. Evaluations show that Q-DRAM significantly improved performance for both single- and multi-core systems.},
  number = {7},
  journaltitle = {IEEE Transactions on Computers},
  date = {2016-07},
  pages = {2213-2227},
  keywords = {Radiation detectors,Timing,DRAM chips,Random access memory,bank-level locality,Computer architecture,data bus occupation prediction,decoupled restoring,decoupling,Decoupling,destructive access,Destructive access,DRAM,DRAM access,dynamic random access memory,memory controllers,Microprocessors,multi-core systems,Parallel processing,per-bank multiple row-buffers,Per-bank multiple row-buffers,Q-DRAM,Q-TRIGGER,quick-access,Quick-access,quick-access DRAM,quick-access trigger,RESTORER,restoring time,row-activation,Row-activation,row-activation time,sensing time,single-core systems},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, H. and Kim, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9RRH87RD\\Shin et al. - 2016 - Q-DRAM Quick-Access DRAM with Decoupled Restoring.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDB8AWTC\\7271043.html}
}

@article{shinDRAMLatencyOptimizationInspired2016,
  title = {{{DRAM}}-{{Latency Optimization Inspired}} by {{Relationship}} between {{Row}}-{{Access Time}} and {{Refresh Timing}}},
  volume = {65},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2512863},
  abstract = {It is widely known that relatively long DRAM latency forms a bottleneck in computing systems. However, DRAM vendors are strongly reluctant to decrease DRAM latency due to the additional manufacturing cost. Therefore, we set our goal to reduce DRAM latency without any modification in the existing DRAM structure. To accomplish our goal, we focus on an intrinsic phenomenon in DRAM: electric charge variation in DRAM cell capacitors. Then, we draw two key insights: i) DRAM row-access latency of a row is a function of the elapsed time from when the row was last refreshed, and ii) DRAM row-access latency of a row is also a function of the remaining time until the row is next refreshed. Based on these two insights, we propose two mechanisms to reduce DRAM latency: NUAT-1 and NUAT-2. NUAT-1 exploits the first key insight and NUAT-2 exploits the second key insight. For evaluation, circuit- and system-level simulations are performed, which show the performance improvement for various environments.},
  number = {10},
  journaltitle = {IEEE Transactions on Computers},
  date = {2016-10},
  pages = {3027-3040},
  keywords = {DRAM chips,Random access memory,DRAM,Capacitors,circuit simulation,circuit-level simulations,computing systems,Decoding,DRAM cell capacitors,DRAM latency optimization,DRAM row-access latency,DRAM structure,DRAM vendors,DRAM-latency,electric charge,electric charge variation,memory controller,Memory management,non-uniform access time (NUAT),nonuniform access time,NUAT-1,NUAT-2,refresh,refresh timing,row-access time,system-level simulations,timing,Transistors},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Moon, Y. and Kwon, Y. and Kim, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9AWRMECC\\Shin et al. - 2016 - DRAM-Latency Optimization Inspired by Relationship.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G7DNNSVB\\7366754.html}
}

@inproceedings{hassanChargeCacheReducingDRAM2016,
  title = {{{ChargeCache}}: {{Reducing DRAM}} Latency by Exploiting Row Access Locality},
  doi = {10.1109/HPCA.2016.7446096},
  shorttitle = {{{ChargeCache}}},
  abstract = {DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called ChargeCache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.},
  eventtitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  booktitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  date = {2016-03},
  pages = {581-593},
  keywords = {Hardware,cache storage,Timing,DRAM chips,Computer architecture,Parallel processing,Capacitors,memory controller,ChargeCache,DRAM latency reduction,DRAM request,multicore system,row access locality,row address,single-core system,storage allocation,system performance bottleneck,timing parameters},
  author = {Hassan, H. and Pekhimenko, G. and Vijaykumar, N. and Seshadri, V. and Lee, D. and Ergin, O. and Mutlu, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CZS9BHC3\\Hassan et al. - 2016 - ChargeCache Reducing DRAM latency by exploiting r.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UMFE5G4A\\7446096.html}
}

@inproceedings{qaziOptimizationAccessLatency2016,
  title = {Optimization of Access Latency in {{DRAM}}},
  doi = {10.1109/ICECUBE.2016.7495216},
  abstract = {Modern digital systems, which involve high data computations, suffer from high memory access latency; thus, latency becomes a core issue in the performance enhancement of these advance digital machines. Different factors are behind the high latency of advance digital systems. Approaches like array binding and allocation, code rewriting, and others are adopted to reduce the overall latency of these systems. In this paper, we explore new dimensions to achieve maximum latency optimization in applications that involve extensive memory access. The proposed algorithm of idle/slack time management utilizes empty slots in memory access of different memory modules by appropriately activating upcoming commands in advance. The optimization of latency is further increased by incorporating the multi-way conflict resolution algorithm in second stage and followed by the use of advance dynamic buffers in third stage. Our successive three stages approach of adopting slack time management, multi-way partitioning using min-cut algorithm, and the use of advance dynamic buffers yields better results. Comparison of the experimental results with different benchmarks shows that our proposed technique optimizes the existing page-mode technique by 9\%. Hence, adopting this proposed optimization strategy significantly reduces overall latency of modern digital systems.},
  eventtitle = {2016 {{International Conference}} on {{Computing}}, {{Electronic}} and {{Electrical Engineering}} ({{ICE Cube}})},
  booktitle = {2016 {{International Conference}} on {{Computing}}, {{Electronic}} and {{Electrical Engineering}} ({{ICE Cube}})},
  date = {2016-04},
  pages = {163-168},
  keywords = {Bandwidth,Delays,Arrays,DRAM chips,DRAM,access latency optimization,allocation,array binding,buffer storage,Clocks,code rewriting,data computation,digital machines,digital system,Digital systems,dynamic buffer,extensive memory access,idle time management,memory access latency,memory modules,min-cut algorithm,multiway conflict resolution algorithm,multiway partitioning,Optimization,page-mode technique,performance enhancement,Resource management,slack time management},
  author = {Qazi, A. and Ullah, Z. and Rehman, K. and Khan, M. H. and Bilal, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9TU9TPY5\\Qazi et al. - 2016 - Optimization of access latency in DRAM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TTIFJIH4\\7495216.html}
}

@inproceedings{hashemiAcceleratingDependentCache2016,
  title = {Accelerating {{Dependent Cache Misses}} with an {{Enhanced Memory Controller}}},
  doi = {10.1109/ISCA.2016.46},
  abstract = {On-chip contention increases memory access latency for multi-core processors. We identify that this additional latency has a substantial effect on performance for an important class of latency-critical memory operations: those that result in a cache miss and are dependent on data from a prior cache miss. We observe that the number of instructions between the first cache miss and its dependent cache miss is usually small. To minimize dependent cache miss latency, we propose adding just enough functionality to dynamically identify these instructions at the core and migrate them to the memory controller for execution as soon as source data arrives from DRAM. This migration allows memory requests issued by our new Enhanced Memory Controller (EMC) to experience a 20\% lower latency than if issued by the core. On a set of memory intensive quad-core workloads, the EMC results in a 13\% improvement in system performance and a 5\% reduction in energy consumption over a system with a Global History Buffer prefetcher, the highest performing prefetcher in our evaluation.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  date = {2016-06},
  pages = {444-455},
  keywords = {Correlation,Benchmark testing,cache storage,Prefetching,Delays,DRAM chips,Random access memory,DRAM,memory access latency,cache miss latency,dependent cache misses,Electromagnetic compatibility,EMC,energy consumption,enhanced memory controller,global history buffer prefetcher,latency-critical memory operations,memory intensive quad-core workloads,memory requests,multicore processors,multiprocessing systems,on-chip contention,system performance,System-on-chip},
  author = {Hashemi, M. and {Khubaib} and Ebrahimi, E. and Mutlu, O. and Patt, Y. N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AKW5FWD2\\Hashemi et al. - 2016 - Accelerating Dependent Cache Misses with an Enhanc.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J5AFZFAB\\7551413.html}
}

@inproceedings{wangBuildingLowLatency2016,
  title = {Building a {{Low Latency}}, {{Highly Associative DRAM Cache}} with the {{Buffered Way Predictor}}},
  doi = {10.1109/SBAC-PAD.2016.22},
  abstract = {The emerging die-stacked DRAM technology allows computer architects to design a last-level cache (LLC) with high memory bandwidth and large capacity. There are four key requirements for DRAM cache design: minimizing on-chip tag storage overhead, optimizing access latency, improving hit rate, and reducing off-chip traffic. These requirements seem mutually incompatible. For example, to reduce the tag storage overhead, the recent proposed LH-cache co-locates tags and data in the same DRAM cache row, and the Alloy Cache proposed to alloy data and tags in the same cache line in a direct-mapped design. However, these ideas either require significant tag lookup latency or sacrifice hit rate for hit latency. To optimize all four key requirements, we propose the Buffered Way Predictor (BWP). The BWP predicts the way ID of a DRAM cache request with high accuracy and coverage, allowing data and tag to be fetched back to back. Thus, the read latency for the data can be completely hidden so that DRAM cache hitting requests have low access latency. The BWP technique is designed for highly associative block-based DRAM caches and achieves a low miss rate and low off-chip traffic. Our evaluation with multi-programmed workloads and a 128MB DRAM cache shows that a 128KB BWP achieves a 76.2\% hit rate. The BWP improves performance by 8.8\% and 12.3\% compared to LH-cache and Alloy Cache, respectively.},
  eventtitle = {2016 28th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  booktitle = {2016 28th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  date = {2016-10},
  pages = {109-117},
  keywords = {Bandwidth,cache storage,Arrays,DRAM chips,Random access memory,Memory management,access latency optimization,System-on-chip,alloy cache,alloy data,buffered way predictor,BWP technique,Compounds,content-addressable storage,die-stacked DRAM technology,direct-mapped design,DRAM cache hitting requests,integrated circuit design,last-level cache,LH-cache,LLC,low latency highly associative DRAM cache design,memory bandwidth,Metals,off-chip traffic reduction,on-chip tag storage overhead minimization,read latency,sacrifice hit rate,storage capacity 128 Mbit,tag lookup latency,tag storage overhead reduction},
  author = {Wang, Z. and Jiménez, D. A. and Zhang, T. and Loh, G. H. and Xie, Y.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LE4LJECF\\Wang et al. - 2016 - Building a Low Latency, Highly Associative DRAM Ca.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4VD5KSS2\\7789330.html}
}

@inproceedings{liuLAMSLatencyawareMemory2016,
  title = {{{LAMS}}: {{A}} Latency-Aware Memory Scheduling Policy for Modern {{DRAM}} Systems},
  doi = {10.1109/PCCC.2016.7820660},
  shorttitle = {{{LAMS}}},
  abstract = {This paper introduces a new memory scheduling policy called LAMS, which is inspired by a recently proposed memory architecture and targets for future high capacity memory systems. As memory capacity increases, the bit-lines connected to memory row buffers become much longer, dramatically lengthening memory access latency, due to increased parasitic capacitance. Recent study has proposed to partition long bit-lines into near and far (relative to the row buffer) segments via inserting isolation transistors such that access to near segment can be accomplished much faster, while access to far segment remains nearly the same. However, how to effectively leverage the new memory architecture still remains unexplored. We suggest to take advantage of this new memory architecture via performing latency-aware memory scheduling for pending requests to explore their performance potentials. In this scheduling policy, each memory request is classified to one of the following three categories, row-buffer hit, near-buffer, and far-buffer. Based on the classification, it issues requests in the order of row-buffer hit → near-buffer → far-buffer. In doing so, it avoids long-latency requests blocking short-latency memory requests, reducing total memory queuing time in the memory controller and improving overall memory performance. Our evaluation results on a simulated memory system show that comparing with the commonly used FR-FCFS scheduler, our LAMS improves performance and energy efficiency by up to 20.6\% and 34\%, respectively. Even comparing with the four competitive schedulers chosen from memory scheduling champion (MSC), LAMS still improves performance and energy efficiency by up to 6.1\% and 23.4\%, respectively.},
  eventtitle = {2016 {{IEEE}} 35th {{International Performance Computing}} and {{Communications Conference}} ({{IPCCC}})},
  booktitle = {2016 {{IEEE}} 35th {{International Performance Computing}} and {{Communications Conference}} ({{IPCCC}})},
  date = {2016-12},
  pages = {1-8},
  keywords = {Processor scheduling,DRAM chips,Random access memory,Transistors,buffer storage,memory access latency,DRAM systems,far-buffer classification,FR-FCFS scheduler,LAMS,latency-aware memory scheduling policy,memory architecture,Memory architecture,memory capacity,memory queuing time,near-buffer classification,Parasitic capacitance,row-buffer hit classification,Scheduling,System performance},
  author = {Liu, W. and Huang, P. and Kun, T. and Lu, T. and Zhou, K. and Li, C. and He, X.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H55ZJHPU\\Liu et al. - 2016 - LAMS A latency-aware memory scheduling policy for.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M8BIZSVW\\7820660.html}
}

@inproceedings{qiwangReducingDRAMCache2016,
  title = {Reducing {{DRAM Cache Access}} in Cache Miss via an Effective Predictor},
  doi = {10.1109/ICSESS.2016.7883118},
  abstract = {As more and more cores are integrated on a single chip, memory speed has become a major performance bottleneck. The widening latency gap between high speed cores and main memory has led to the evolution of multi-level caches and using DRAM as the Last-Level-Cache (LLC). The main problem of employing DRAM cache is their high tag lookup latency. If DRAM cache misses, the latency of memory access will be increased comparing with the system without DRAM cache. To solve this problem, we propose an effective predictor to Reduce DRAM Cache Access (RCA) in cache miss. The predictor composes of a saturating counter and a Partial MissMap (P\_Map). If the saturating counter indicates a hit, then the request will be send to the P\_Map to further lookup whether it is a hit or not. The evaluation results show that RCA can improve system performance by 8.2\% and 3.4\% on average, compared to MissMap and MAP\_G, respectively.},
  eventtitle = {2016 7th {{IEEE International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  booktitle = {2016 7th {{IEEE International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  date = {2016-08},
  pages = {501-504},
  keywords = {cache storage,DRAM chips,Random access memory,LLC,cache miss,Cache miss,DRAM cache,DRAM cache access,effective predictor,last-level-cache,multilevel cache,P_Map,partial missmap,predictor,RCA,saturating counter,Two dimensional displays},
  author = {{Qi Wang} and {Yanzhen Xing} and {Donghui Wang}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BGTM47V4\\Qi Wang et al. - 2016 - Reducing DRAM Cache Access in cache miss via an ef.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U9QHFR6I\\7883118.html}
}

@inproceedings{xieHybridLRUCachingScheme2016,
  title = {Hybrid-{{LRU Caching Scheme}} for {{PDRAM Hybrid Memory Architecture}} in {{Cloud Computing}}},
  doi = {10.1109/IIKI.2016.22},
  abstract = {This in recent years, the mixed memory (PDRAM) which is consisted of the phase-change memory (PRAM) and the conventional memory (DRAM) has drawn a lot of attention from both industry and academia. It is a promising alternative to replace the traditional memory architecture. PDRAM has good characteristics as large capacity, good stability and non-volatile, while PRAM has disadvantages of a limited life and large access latency. Traditional policies are not sufficient to be directly applied to the PDRAM memory architecture. They cannot adapt to the new features of the hybrid architecture because of their undifferentiated operation, extensive use of PRAM can cause performance degradation and shorten the life of PDRAM memory. This paper proposed Hybrid-LRU caching scheme to address these problems. Our scheme mainly uses the cache consistency address resolution mode to distinguish between different physical mediums in PDRAM, and then take different actions depending on different physical mediums. We have taken a variety of tradition policies like LRU, FIFO, RANDOM, CFLRU to make experimental comparisons. The experimental results have shown that our scheme can reduce the PRAM utilization rate of 11.8\% and improve the performance by 4.6\%, energy consumption of write and read operation can be reduced up to 88.2\%.},
  eventtitle = {2016 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}} ({{IIKI}})},
  booktitle = {2016 {{International Conference}} on {{Identification}}, {{Information}} and {{Knowledge}} in the {{Internet}} of {{Things}} ({{IIKI}})},
  date = {2016-10},
  pages = {14-21},
  keywords = {performance,cache storage,DRAM chips,memory architecture,Memory architecture,cache consistency,cache policy,cloud computing,energy,Energy consumption,Flash memories,hybrid architecture,hybrid memory architecture,hybrid-LRU caching scheme,Material storage,Nonvolatile memory,PDRAM hybrid memory architecture,PDRAM memory architecture,phase change memory,Phase change random access memory},
  author = {Xie, H. and Jia, G. and Han, G. and Wan, J. and Ren, Y. and Huang, J. and You, X.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GITHWZQI\\Xie et al. - 2016 - Hybrid-LRU Caching Scheme for PDRAM Hybrid Memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C9I9C53Q\\8281166.html}
}

@article{liuControllerArchitectureLowPower2017,
  title = {Controller {{Architecture}} for {{Low}}-{{Power}}, {{Low}}-{{Latency DRAM With Built}}-in {{Cache}}},
  volume = {34},
  issn = {2168-2356},
  doi = {10.1109/MDAT.2016.2524445},
  abstract = {Memory wall is a critical issue for many today's electronic systems. Tiered latency DRAM with asymmetric bit lines was proposed to optimize the power and latency. This paper proposes a controller architecture for the tiered latency DRAM in which the small array is operated like a cache.},
  number = {2},
  journaltitle = {IEEE Design Test},
  date = {2017-04},
  pages = {69-78},
  keywords = {cache storage,memory hierarchy,Delays,DRAM chips,Random access memory,DRAM,Microprocessors,Transistors,3D DRAM,asymmetric bit lines,built-in cache,built-in cache DRAM (BC-DRAM),controller architecture,controllers,DRAM controller,electronic systems,low-power electronics,low-power low-latency DRAM,memory wall,tiered latency DRAM,tiered latency DRAM (TL-DRAM)},
  author = {Liu, Z. and Shih, H. and Lin, B. and Wu, C.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DG9BL29C\\Liu et al. - 2017 - Controller Architecture for Low-Power, Low-Latency.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\L272EZ4H\\7397924.html}
}

@inproceedings{chenMALRUMisspenaltyAware2017,
  title = {{{MALRU}}: {{Miss}}-Penalty Aware {{LRU}}-Based Cache Replacement for Hybrid Memory Systems},
  doi = {10.23919/DATE.2017.7927151},
  shorttitle = {{{MALRU}}},
  abstract = {Current DRAM based memory systems face the scalability challenges in terms of storage density, power, and cost. Hybrid memory architecture composed of emerging Non-Volatile Memory (NVM) and DRAM is a promising approach to large-capacity and energy-efficient main memory. However, hybrid memory systems pose a new challenge to on-chip cache management due to the asymmetrical penalty of memory access to DRAM and NVM in case of cache misses. Cache hit rate is no longer an effective metric for evaluating memory access performance in hybrid memory systems. Current cache replacement policies that aim to improve cache hit rate are not efficient either. In this paper, we take into account the asymmetry of cache miss penalty on DRAM and NVM, and advocate a more general metric, Average Memory Access Time (AMAT), to evaluate the performance of hybrid memories. We propose a miss penalty-aware LRU-based (MALRU) cache replacement policy for hybrid memory systems. MALRU is aware of the source (DRAM or NVM) of missing blocks and prevents high-latency NVM blocks as well as low-latency DRAM blocks with good temporal locality from being evicted. Experimental results show that MALRU improves system performance against LRU and the state-of-the-art HAP policy by up to 20.4\% and 11.7\% (11.1\% and 5.7\% on average), respectively.},
  eventtitle = {Design, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}}), 2017},
  booktitle = {Design, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}}), 2017},
  date = {2017-03},
  pages = {1086-1091},
  keywords = {Benchmark testing,cache storage,DRAM chips,temporal locality,Random access memory,memory architecture,hybrid memory architecture,Nonvolatile memory,AMAT metric,average memory access time metric,cache miss penalty asymmetry,DRAM based memory systems,hybrid memory systems,Hybrid power systems,large-capacity energy-efficient main-memory,Layout,MALRU cache replacement policy,Mathematical model,Measurement,miss-penalty aware LRU-based cache replacement,nonvolatile memory,NVM,on-chip cache management,performance evaluation},
  author = {Chen, D. and Jin, H. and Liao, X. and Liu, H. and Guo, R. and Liu, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LBGSBKZV\\Chen et al. - 2017 - MALRU Miss-penalty aware LRU-based cache replacem.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VVAR3IGB\\7927151.html}
}

@inproceedings{tsaiJengaSoftwaredefinedCache2017,
  title = {Jenga: {{Software}}-Defined Cache Hierarchies},
  doi = {10.1145/3079856.3080214},
  shorttitle = {Jenga},
  abstract = {Caches are traditionally organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, since working sets settle at the smallest (i.e., fastest and most energy-efficient) level they fit in. However, rigid hierarchies also add overheads, because each level adds latency and energy even when it does not fit the working set. These overheads are expensive on emerging systems with heterogeneous memories, where the differences in latency and energy across levels are small. Significant gains are possible by specializing the hierarchy to applications. We propose Jenga, a reconfigurable cache hierarchy that dynamically and transparently specializes itself to applications. Jenga builds virtual cache hierarchies out of heterogeneous, distributed cache banks using simple hardware mechanisms and an OS runtime. In contrast to prior techniques that trade energy and bandwidth for performance (e.g., dynamic bypassing or prefetching), Jenga eliminates accesses to unwanted cache levels. Jenga thus improves both performance and energy efficiency. On a 36-core chip with a 1 GB DRAM cache, Jenga improves energy-delay product over a combination of state-of-the-art techniques by 23\% on average and by up to 85\%.},
  eventtitle = {2017 {{ACM}}/{{IEEE}} 44th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  booktitle = {2017 {{ACM}}/{{IEEE}} 44th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  date = {2017-06},
  pages = {652-665},
  keywords = {Organizations,Hardware,Software,Bandwidth,Multicore processing,cache storage,DRAM chips,Cache,Random access memory,multiprocessing systems,System-on-chip,memory architecture,DRAM cache,energy-delay product,heterogeneous distributed cache banks,heterogeneous memories,Heterogeneous memories,Hierarchy,Jenga,latency energy,memory size 1.0 GByte,NUCA,Partitioning,reconfigurable cache hierarchy,rigid hierarchy,simple hardware mechanisms,slower memories,storage management,unwanted cache levels,virtual cache hierarchies},
  author = {Tsai, P. and Beckmann, N. and Sanchez, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QZDZ8SK8\\Tsai et al. - 2017 - Jenga Software-defined cache hierarchies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZTP33ABZ\\8192509.html}
}

@inproceedings{subramanianClosedOpenDRAM2018,
  title = {Closed yet {{Open DRAM}}: {{Achieving Low Latency}} and {{High Performance}} in {{DRAM Memory Systems}}},
  doi = {10.1109/DAC.2018.8465817},
  shorttitle = {Closed yet {{Open DRAM}}},
  abstract = {DRAM memory access is a critical performance bottleneck. To access one cache block, an entire row needs to be sensed and amplified, data restored into the bitcells and the bitlines precharged, incurring high latency. Isolating the bitlines and sense amplifiers after activation enables reads and precharges to happen in parallel. However, there are challenges in achieving this isolation. We tackle these challenges and propose an effective scheme, simultaneous read and precharge (SRP), to isolate the sense amplifiers and bitlines and serve reads and precharges in parallel. Our detailed architecture and circuit simulations demonstrate that our simultaneous read and precharge (SRP) mechanism is able to achieve an 8.6\% performance benefit over baseline, while reducing sense amplifier idle power by 30\%, as compared to prior work, over a wide range of workloads.},
  eventtitle = {2018 55th {{ACM}}/{{ESDA}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  booktitle = {2018 55th {{ACM}}/{{ESDA}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  date = {2018-06},
  pages = {1-6},
  keywords = {Organizations,cache storage,History,Timing,DRAM chips,Random access memory,Parallel processing,circuit simulation,Memory management,Transistors,memory architecture,access one cache block,amplifier idle power,amplifiers,circuit simulations,critical performance bottleneck,DRAM memory access,DRAM memory systems,high performance,isolation,open DRAM,sense amplifiers,simultaneous read and precharge mechanism,SRAM chips,SRP},
  author = {Subramanian, L. and Vaidyanathan, K. and Nori, A. and Subramoney, S. and Karnik, T. and Wang, H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9XHCYX32\\Subramanian et al. - 2018 - Closed yet Open DRAM Achieving Low Latency and Hi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H6EUQWEM\\8465817.html}
}

@inproceedings{modgilImprovingPerformanceChip2018,
  title = {Improving the {{Performance}} of {{Chip Multiprocessor}} by {{Delayed Write Drain}} and {{Prefetcher}} Based {{Memory Scheduler}}},
  doi = {10.1109/ICECA.2018.8474846},
  abstract = {To improve the performance and energy consumption of chip multiprocessor (CMP) system, memory request serving latencies should be minimized. These latencies can be minimized by scheduling appropriate memory command at appropriate time. This paper proposes a scheduler that reduces latency related to serving memory read requests by delaying switching into write drain mode when memory traffic is not heavy and write queue is not full. Memory reads are more important to handle than memory writes for system's performance. Further precharge and activate operations are performed using constant stride prefetcher. In idle memory cycles the scheduler issues row precharge commands using cache prefetching technique based on Global History Buffer. Authors in [1] have used stride detector and Global History Buffer based speculative precharges and activates, but they treat memory reads and memory writes equally. Whereas, proposed scheduler in this paper prioritizes reads over writes for better system performance. Our evaluations show that proposed scheduling policy significantly outperforms previous schedulers [1], [2] in varied multicore environments in terms of performance as well as energy consumption. Across a wide range of workloads based on PARSEC benchmark suite, proposed policy improves systems performance by 2.51\%, on 2-core, 0.012\% on 4-core environment in comparison to scheduler proposed in [1].},
  eventtitle = {2018 {{Second International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  booktitle = {2018 {{Second International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  date = {2018-03},
  pages = {1864-1869},
  keywords = {Conferences,cache storage,History,Prefetching,scheduling,DRAM chips,Random access memory,DRAM,Memory management,energy consumption,multiprocessing systems,Energy consumption,Aerospace electronics,cache prefetching technique,chip multiprocessor system,CMP,constant stride prefetcher,delayed write drain,delaying switching,Global History Buffer,idle memory cycles,Memory Access Scheduler,memory command,memory request,memory traffic,Performance,precharge commands,Prefetcher,prefetcher based memory scheduler,scheduling policy,systems performance,write drain mode},
  author = {Modgil, A. and Sehgal, V. K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YNRWI8GF\\Modgil and Sehgal - 2018 - Improving the Performance of Chip Multiprocessor b.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2AHCGKDF\\8474846.html}
}

@article{shinMcDRAMLowLatency2018,
  title = {{{McDRAM}}: {{Low Latency}} and {{Energy}}-{{Efficient Matrix Computations}} in {{DRAM}}},
  volume = {37},
  issn = {0278-0070},
  doi = {10.1109/TCAD.2018.2857044},
  shorttitle = {{{McDRAM}}},
  abstract = {We propose a novel memory architecture for in-memory computation called McDRAM, where DRAM dies are equipped with a large number of multiply accumulate (MAC) units to perform matrix computation for neural networks. By exploiting high internal memory bandwidth and reducing offchip memory accesses, McDRAM realizes both low latency and energy efficient computation. In our experiments, we obtained the chip layout based on the state-of-the-art memory, LPDDR4 where McDRAM is equipped with 2048 MACs in a single chip package with a small area overhead (4.7\%). Compared with the state-ofthe-art accelerator, TPU and the power-efficient GPU, Nvidia P4, McDRAM offers 9.5× and 14.4× speedup, respectively, in the case that the large-scale MLPs and RNNs adopt the batch size of 1. McDRAM also gives 2.1× and 3.7× better computational efficiency in TOPS/W than TPU and P4, respectively, for the large batches.},
  number = {11},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  date = {2018-11},
  pages = {2613-2622},
  keywords = {Bandwidth,DRAM chips,Random access memory,Decoding,Memory management,memory architecture,computational efficiency,Dynamic memory,energy efficient computation,energy-efficient matrix computations,high internal memory bandwidth,in-memory computation,matrix computation,McDRAM,multiply accumulate units,neural nets,Neural networks,neural networks (NNs),Performance evaluation,power-efficient GPU,processing in memory},
  author = {Shin, H. and Kim, D. and Park, E. and Park, S. and Park, Y. and Yoo, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QBFSJIT\\Shin et al. - 2018 - McDRAM Low Latency and Energy-Efficient Matrix Co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JFGECBAF\\8493536.html}
}

@inproceedings{wangReducingDRAMLatency2018,
  title = {Reducing {{DRAM Latency}} via {{Charge}}-{{Level}}-{{Aware Look}}-{{Ahead Partial Restoration}}},
  doi = {10.1109/MICRO.2018.00032},
  abstract = {Long DRAM access latency is a major bottleneck for system performance. In order to access data in DRAM, a memory controller (1) activates (i.e., opens) a row of DRAM cells in a cell array, (2) restores the charge in the activated cells back to their full level, (3) performs read and write operations to the activated row, and (4) precharges the cell array to prepare for the next activation. The restoration operation is responsible for a large portion (up to 43.6\%) of the total DRAM access latency. We find two frequent cases where the restoration operations performed by DRAM do not need to fully restore the charge level of the activated DRAM cells, which we can exploit to reduce the restoration latency. First, DRAM rows are periodically refreshed (i.e., brought back to full charge) to avoid data loss due to charge leakage from the cell. The charge level of a DRAM row that will be refreshed soon needs to be only partially restored, providing just enough charge so that the refresh can correctly detect the cells' data values. Second, the charge level of a DRAM row that will be activated again soon can be only partially restored, providing just enough charge for the activation to correctly detect the data value. However, partial restoration needs to be done carefully: for a row that will be activated again soon, restoring to only the minimum possible charge level can undermine the benefits of complementary mechanisms that reduce the activation time of highly-charged rows. To enable effective latency reduction for both activation and restoration, we propose charge-level-aware look-ahead partial restoration (CAL). CAL consists of two key components. First, CAL accurately predicts the next access time, which is the time between the current restoration operation and the next activation of the same row. Second, CAL uses the predicted next access time and the next refresh time to reduce the restoration time, ensuring that the amount of partial charge restoration is enough to maintain the benefits of reducing the activation time of a highly-charged row. We implement CAL fully in the memory controller, without any changes to the DRAM module. Across a wide variety of applications, we find that CAL improves the average performance of an 8-core system by 14.7\%, and reduces average DRAM energy consumption by 11.3\%.},
  eventtitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2018-10},
  pages = {298-311},
  keywords = {Timing,Arrays,DRAM chips,Capacitors,DRAM-latency,memory controller,DRAM latency reduction,System performance,activation-latency,CAL,charge-level-aware look-ahead partial restoration,data access time,DRAM cells array activation,DRAM energy consumption,partial-restoration,restoration-latency},
  author = {Wang, Y. and Tavakkol, A. and Orosa, L. and Ghose, S. and Ghiasi, N. Mansouri and Patel, M. and Kim, J. S. and Hassan, H. and Sadrosadati, M. and Mutlu, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AGBUL9LY\\8574549.html}
}

@inproceedings{choTamingKillerMicrosecond2018,
  title = {Taming the {{Killer Microsecond}}},
  doi = {10.1109/MICRO.2018.00057},
  abstract = {Modern applications require access to vast datasets at low latencies. Emerging memory technologies can enable faster access to significantly larger volumes of data than what is possible today. However, these memory technologies have a significant caveat: their random access latency falls in a range that cannot be effectively hidden using current hardware and software latency-hiding techniques-namely, the microsecond range. Finding the root cause of this "Killer Microsecond" problem, is the subject of this work. Our goal is to answer the critical question of why existing hardware and software cannot hide microsecond-level latencies, and whether drastic changes to existing platforms are necessary to utilize microsecond-latency devices effectively. We use an FPGA-based microsecond-latency device emulator, a carefully-crafted microbenchmark, and three open-source data-intensive applications to show that existing systems are indeed incapable of effectively hiding such latencies. However, after uncovering the root causes of the problem, we show that simple changes to existing systems are sufficient to support microsecond-latency devices. In particular, we show that by replacing on-demand memory accesses with prefetch requests followed by fast user-mode context switches (to increase access-level parallelism) and enlarging hardware queues that track in-flight accesses (to accommodate many parallel accesses), conventional architectures can effectively hide microsecond-level latencies, and approach the performance of DRAM-based implementations of the same applications. In other words, we show that successful usage of microsecond-level devices is not predicated on drastically new hardware and software architectures.},
  eventtitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2018-10},
  pages = {627-640},
  keywords = {Hardware,cache storage,Prefetching,Servers,software architecture,DRAM chips,Random access memory,memory architecture,performance evaluation,storage management,Performance evaluation,access-level parallelism,Data intensive applications,DRAM-based implementations,Emerging storage,field programmable gate arrays,FPGA,FPGA-based microsecond-latency device emulator,hardware architectures,Kernel,killer microsecond problem,Killer microseconds,memory technologies,microsecond range,microsecond-level devices,microsecond-level latencies,on-demand memory accesses,open-source data-intensive applications,software architectures,software latency-hiding techniques,track in-flight accesses},
  author = {Cho, S. and Suresh, A. and Palit, T. and Ferdman, M. and Honarmand, N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\T6D97TH7\\Cho et al. - 2018 - Taming the Killer Microsecond.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9NEN53JX\\8574574.html}
}

@inproceedings{hassanOffChipMemoryLatency2018,
  title = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}: {{Is DDR DRAM Really}} the {{Best Option}}?},
  doi = {10.1109/RTSS.2018.00062},
  shorttitle = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}},
  abstract = {Predictable execution time upon accessing shared memories in multi-core real-time systems is a stringent requirement. A plethora of existing works focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR DRAMs), or redesigning its memory to provide predictable memory behavior. In this paper, we show that DDR DRAMs by construction suffer inherent limitations associated with achieving such predictability. These limitations lead to 1) highly variable access latencies that fluctuate based on various factors such as access patterns and memory state from previous accesses, and 2) overly pessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some real-time systems that mandate a strict predictable performance with tight timing constraints. Targeting these systems, we promote an alternative off-chip memory solution that is based on the emerging Reduced Latency DRAM (RLDRAM) protocol, and propose a predictable memory controller (RLDC) managing accesses to this memory. Comparing with the state-of-the-art predictable DDR controllers, the proposed solution provides up to 11× less timing variability and 6.4× reduction in the worst case memory latency.},
  eventtitle = {2018 {{IEEE Real}}-{{Time Systems Symposium}} ({{RTSS}})},
  booktitle = {2018 {{IEEE Real}}-{{Time Systems Symposium}} ({{RTSS}})},
  date = {2018-12},
  pages = {495-505},
  keywords = {Protocols,Delays,real-time systems,DRAM chips,Random access memory,DRAM,multiprocessing systems,alternative off-chip memory solution,Data transfer,DDR DRAM,double data rate dynamic random access memories,Latency Analysis,Memory,multicore real-time systems,off-chip memory latency,pessimistic latency bounds,Predictability,predictable execution time,predictable memory controller,Real-time systems,Real-Time Systems,reduced latency DRAM protocol,RLDRAM protocol,state-of-the-art predictable DDR controllers,Task analysis,tight timing constraints,variable access latencies},
  author = {Hassan, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NAAUU4UL\\Hassan - 2018 - On the Off-Chip Memory Latency of Real-Time System.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7SPVFUU6\\8603238.html}
}

@inproceedings{kimSolarDRAMReducingDRAM2018,
  title = {Solar-{{DRAM}}: {{Reducing DRAM Access Latency}} by {{Exploiting}} the {{Variation}} in {{Local Bitlines}}},
  doi = {10.1109/ICCD.2018.00051},
  shorttitle = {Solar-{{DRAM}}},
  abstract = {DRAM latency is a major bottleneck for many applications in modern computing systems. In this work, we rigorously characterize the effects of reducing DRAM access latency on 282 state-of-the-art LPDDR4 DRAM modules. As found in prior work on older DRAM generations (DDR3), we show that regions of LPDDR4 DRAM modules can be accessed with latencies that are significantly lower than manufacturer-specified values without causing failures. We present novel data that 1) further supports the viability of such latency reduction mechanisms and 2) exposes a variety of new cases in which access latencies can be effectively reduced. Using our observations, we propose a new low-cost mechanism, Solar-DRAM, that 1) identifies failure-prone regions of DRAM at reduced latency and 2) robustly reduces average DRAM access latency while maintaining data correctness, by issuing DRAM requests with reduced access latencies to non-failure-prone DRAM regions. We evaluate Solar-DRAM on a wide variety of multi-core workloads and show that for 4-core homogeneous workloads, Solar-DRAM provides an average (maximum) system performance improvement of 4.31\% (10.87\%) compared to using the default fixed DRAM access latency.},
  eventtitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  booktitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  date = {2018-10},
  pages = {282-291},
  keywords = {Organizations,Timing,DRAM chips,Random access memory,Capacitors,Decoding,Transistors,Memory,average DRAM access,DRAM access latency,DRAM generations,DRAM requests,DRAM-Characterization,DRAM-Latency,failure analysis,latency reduction mechanisms,local bitlines,LPDDR4,LPDDR4 DRAM modules,Memory-Controllers,nonfailure-prone DRAM regions,Process-Variation,reduced access latencies,Reliability,Solar-DRAM},
  author = {Kim, J. and Patel, M. and Hassan, H. and Mutlu, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FY5PMU3Y\\Kim et al. - 2018 - Solar-DRAM Reducing DRAM Access Latency by Exploi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3PI38KHX\\8615701.html}
}

@inproceedings{leeTieredlatencyDRAMLow2013,
  title = {Tiered-Latency {{DRAM}}: {{A}} Low Latency and Low Cost {{DRAM}} Architecture},
  doi = {10.1109/HPCA.2013.6522354},
  shorttitle = {Tiered-Latency {{DRAM}}},
  abstract = {The capacity and cost-per-bit of DRAM have historically scaled to satisfy the needs of increasingly large and complex computer systems. However, DRAM latency has remained almost constant, making memory latency the performance bottleneck in today's systems. We observe that the high access latency is not intrinsic to DRAM, but a trade-off made to decrease cost-per-bit. To mitigate the high area overhead of DRAM sensing structures, commodity DRAMs connect many DRAM cells to each sense-amplifier through a wire called a bitline. These bitlines have a high parasitic capacitance due to their long length, and this bitline capacitance is the dominant source of DRAM latency. Specialized low-latency DRAMs use shorter bitlines with fewer cells, but have a higher cost-per-bit due to greater sense-amplifier area overhead. In this work, we introduce Tiered-Latency DRAM (TL-DRAM), which achieves both low latency and low cost-per-bit. In TL-DRAM, each long bitline is split into two shorter segments by an isolation transistor, allowing one segment to be accessed with the latency of a short-bitline DRAM without incurring high cost-per-bit. We propose mechanisms that use the low-latency segment as a hardware-managed or software-managed cache. Evaluations show that our proposed mechanisms improve both performance and energy-efficiency for both single-core and multi-programmed workloads.},
  eventtitle = {2013 {{IEEE}} 19th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  booktitle = {2013 {{IEEE}} 19th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  date = {2013-02},
  pages = {615-626},
  keywords = {cache storage,power aware computing,Timing,DRAM chips,Computer architecture,Capacitors,Transistors,memory architecture,performance evaluation,bitline,Capacitance,complex computer systems,DRAM capacity,DRAM cost-per-bit,DRAM sensing structures,energy conservation,energy-efficiency,hardware-managed cache,high parasitic capacitance,isolation transistor,low-latency low-cost DRAM architecture,memory latency,multiprogrammed workloads,multiprogramming,single-core workloads,software-managed cache,tiered-latency DRAM,TL-DRAM,transistors},
  author = {Lee, D. and Kim, Y. and Seshadri, V. and Liu, J. and Subramanian, L. and Mutlu, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\85DI4VUK\\Lee et al. - 2013 - Tiered-latency DRAM A low latency and low cost DR.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RWMTV3ES\\6522354.html}
}

@inproceedings{hameedAdaptiveCacheManagement2013,
  title = {Adaptive Cache Management for a Combined {{SRAM}} and {{DRAM}} Cache Hierarchy for Multi-Cores},
  doi = {10.7873/DATE.2013.030},
  abstract = {On-chip DRAM caches may alleviate the memory bandwidth problem in future multi-core architectures through reducing off-chip accesses via increased cache capacity. For memory intensive applications, recent research has demonstrated the benefits of introducing high capacity on-chip L4-DRAM as Last-Level-Cache between L3-SRAM and off-chip memory. These multi-core cache hierarchies attempt to exploit the latency benefits of L3-SRAM and capacity benefits of L4-DRAM caches. However, not taking into consideration the cache access patterns of complex applications can cause inter-core DRAM interference and inter-core cache contention. In this paper, we contest to re-architect existing cache hierarchies by proposing a hybrid cache architecture, where the Last-Level-Cache is a combination of SRAM and DRAM caches. We propose an adaptive DRAM placement policy in response to the diverse requirements of complex applications with different cache access behaviors. It reduces inter-core DRAM interference and inter-core cache contention in SRAM/DRAM-based hybrid cache architectures: increasing the harmonic mean instruction-per-cycle throughput by 23.3\% (max. 56\%) and 13.3\% (max. 35.1\%) compared to state-of-the-art.},
  eventtitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  booktitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  date = {2013-03},
  pages = {77-82},
  keywords = {Radiation detectors,Interference,Arrays,System-on-chip,Phase change random access memory,Magnetic cores},
  author = {Hameed, F. and Bauer, L. and Henkel, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I7JXN7A2\\Hameed et al. - 2013 - Adaptive cache management for a combined SRAM and .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FM2S7BRN\\6513476.html}
}

@inproceedings{changReevaluatingLatencyClaims2013,
  title = {Reevaluating the Latency Claims of {{3D}} Stacked Memories},
  doi = {10.1109/ASPDAC.2013.6509675},
  abstract = {In recent years, 3D technology has been a popular area of study that has allowed researchers to explore a number of novel computer architectures. One of the more popular topics is that of integrating 3D main memory dies below the computing die and connecting them with through-silicon vias (TSVs). This is assumed to reduce off-chip main memory access latencies by roughly 45\% to 60\%. Our detailed circuit-level models, however, demonstrate that this latency reduction from the TSVs is significantly less. In this paper, we present these models, compare 2D and 3D main memory latencies, and show that the reduction in latency from using 3D main memory to be no more than 2.4 ns. We also show that although the wider I/O bus width enabled by using TSVs increases performance, it may do so with an increase in power consumption. Although TSVs consume less power per bit transfer than off-chip metal interconnects (11.2 times less power per bit transfer), TSVs typically use considerably more bits and may result in a net increase in power due to the large number of bits in the memory I/O bus. Our analysis shows that although a 3D memory hierarchy exploiting a wider memory bus can increase performance, this performance increase may not justify the net increase in power consumption.},
  eventtitle = {2013 18th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP}}-{{DAC}})},
  booktitle = {2013 18th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP}}-{{DAC}})},
  date = {2013-01},
  pages = {657-662},
  keywords = {Benchmark testing,DRAM chips,Random access memory,Memory management,3D stacked memories,computer architectures,Integrated circuit interconnections,latency claims,power consumption,Solid modeling,Three-dimensional displays,three-dimensional integrated circuits,through-silicon vias,Through-silicon vias},
  author = {Chang, D. W. and Byun, G. and Kim, H. and Ahn, M. and Ryu, S. and Kim, N. S. and Schulte, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2F9MCVTP\\Chang et al. - 2013 - Reevaluating the latency claims of 3D stacked memo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3VDJGLZ8\\6509675.html}
}

@inproceedings{desaiProcessVariationAware2012,
  title = {Process Variation Aware {{DRAM}} Design Using Block Based Adaptive Body Biasing Algorithm},
  doi = {10.1109/ISQED.2012.6187503},
  abstract = {Large dense structures like DRAMs are particularly susceptible to process variation, which can lead to variable latencies in different memory arrays. However, very little work exists on variation studies in the DRAM. This is due to the fact that DRAMs were traditionally placed off-chip and their latency changes due to process variation did not impact the overall processor performance. However, emerging technology trends like three dimensional integration, use of sophisticated memory controllers and continued scaling of technology nodes, substantially reduces DRAM access latency. Hence future technology nodes will see widespread adoption of embedded DRAMs. This makes process variation a critical upcoming challenge in DRAMs that must be addressed in current and forthcoming technology generations. In this paper, we present techniques for modeling the effect of random as well as spatial variation in large DRAM array structures. We use sensitivity based gate level process variation models combined with statistical timing analysis to estimate the impact of process variation on the DRAM performance and leakage power. We also propose a simulated annealing based Vth assignment algorithm using adaptive body biasing to improve the yield of DRAM structures. Applying our algorithm on a 1GB DRAM array, we report an average of 10.3\% improvement in the DRAM yield. To the best of our knowledge, ours is the first technique to model the impact of process variation on large scale DRAM arrays.},
  eventtitle = {Thirteenth {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  booktitle = {Thirteenth {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  date = {2012-03},
  pages = {255-261},
  keywords = {Correlation,Algorithm design and analysis,DRAM chips,Random access memory,DRAM,memory controllers,Mathematical model,Adaptive body bias,block based adaptive body biasing algorithm,Delay,Integrated circuit modeling,Logic gates,memory arrays,Process Variation,process variation aware DRAM design,sensitivity based gate level process variation models,simulated annealing,simulated annealing based Vth assignment algorithm,statistical analysis,statistical timing analysis},
  author = {Desai, S. and Roy, S. and Chakraborty, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XLSSMPGY\\Desai et al. - 2012 - Process variation aware DRAM design using block ba.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3KL3ADMV\\6187503.html}
}

@inproceedings{liHighperformanceDRAMController2011,
  title = {A High-Performance {{DRAM}} Controller Based on Multi-Core System through Instruction Prefetching},
  doi = {10.1109/ICECC.2011.6066295},
  abstract = {In this paper, we propose a cost-effective way to improve the performance of DRAM based on multi-core system. A novel DRAM controller with instruction prefecthing mechanism is introduced. The controller dynamically selects Open Page(OP) or Close Page(CP) policy by getting some information of future accesses in advance. This DRAM controller with dynamic policy based on instruction prefetching(DP\_BIF), can provide DRAM the lowest possible latency without increasing too many areas of chip when compared with the controller only with OP policy or CP policy. The analysis of the simulation results show that the access latency of the DRAM memory can be improved nearly 10.4\%, and the throughput of the DRAM is also increased nearly 10.2\% by adopting the DP\_BIF policy.},
  eventtitle = {2011 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Control}} ({{ICECC}})},
  booktitle = {2011 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Control}} ({{ICECC}})},
  date = {2011-09},
  pages = {1220-1223},
  keywords = {Registers,Bandwidth,Prefetching,Random access memory,multicore system,multiprocessing systems,DRAM controller,storage management,Delay,close page policy,dynamic policy,high-performance DRAM controller,instruction prefecthing,instruction prefetching,open page policy,Process control,random-access storage,Throughput},
  author = {Li, K. and Guang, Q. and Lei, L. and Peng, Y. and Shi, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XPAYD58J\\Li et al. - 2011 - A high-performance DRAM controller based on multi-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\Y9H52EIK\\6066295.html}
}

@inproceedings{zhangLaxityAwareMemoryAccess2011,
  title = {A {{Laxity}}-{{Aware Memory Access Scheduler}} for {{High Performance Multimedia SoC}}},
  doi = {10.1109/CIT.2011.13},
  abstract = {Nowadays high-performance multimedia SoC design always integrates a variety of function units (FU) into a single chip and these FUs impose great stress on the shared memory system. To improve the memory system utilization and meet a wide range of bandwidth and latency requirements of these FUs, a well-designed memory scheduler that takes the quality-of-service (QoS) into account must be adopted. In this paper, a laxity-aware memory scheduler that can adaptively measure the laxity of each memory access task is proposed. Known the laxity of each memory access task, the proposed memory scheduler can guarantee the necessary bandwidth within a certain time interval, which is crucial to the performance and user experience of multimedia SoCs. Compared to previous proposed memory scheduling algorithms based on bandwidth allocating, the laxity-aware memory scheduler can obtain 14.5\% decrease in memory access latency while preserving high DRAM data bus utilization.},
  eventtitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Computer}} and {{Information Technology}}},
  booktitle = {2011 {{IEEE}} 11th {{International Conference}} on {{Computer}} and {{Information Technology}}},
  date = {2011-08},
  pages = {603-608},
  keywords = {Bandwidth,scheduling,System-on-a-chip,Random access memory,Memory management,bandwidth allocation,bandwidth requirement,DRAM data bus utilization,Graphics processing unit,high performance multimedia SoC,latency requirement,laxity-aware memory access scheduler,memory scheduling algorithm,Multimedia communication,multimedia systems,quality of service,Quality of service,quality-of-service,shared memory system,shared memory systems,system-on-chip},
  author = {Zhang, G. and Jiang, Y. and Wang, W. and Su, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J7YQ3AHG\\Zhang et al. - 2011 - A Laxity-Aware Memory Access Scheduler for High Pe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6N9GT9WY\\6036832.html}
}

@article{stuecheliCoordinatingDRAMLastLevelCache2011,
  title = {Coordinating {{DRAM}} and {{Last}}-{{Level}}-{{Cache Policies}} with the {{Virtual Write Queue}}},
  volume = {31},
  issn = {0272-1732},
  doi = {10.1109/MM.2010.102},
  abstract = {To alleviate bottlenecks in this era of many-core architectures, the authors propose a virtual write queue to expand the memory controller's scheduling window through visibility of cache behavior. Awareness of the physical main memory layout and a focus on writes can shorten both read and write average latency, reduce memory power consumption, and improve overall system performance.},
  number = {1},
  journaltitle = {IEEE Micro},
  date = {2011-01},
  pages = {90-98},
  keywords = {Bandwidth,Program processors,cache storage,Processor scheduling,DRAM chips,cache,Random access memory,DRAM,multiprocessing systems,system performance,last-level cache,memory bandwidth,performance evaluation,Memory,Cache memory,cache replacement,cache write-back,DRAM page-mode,DRAM parameters,last level cache policy,many core architecture,memory controller scheduling,memory scheduling,processor scheduling,Queueing analysis,virtual write queue},
  author = {Stuecheli, J. and Kaseridis, D. and Daly, D. and Hunter, H. and John, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R33ZGLEH\\Stuecheli et al. - 2011 - Coordinating DRAM and Last-Level-Cache Policies wi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2T3837LK\\5661752.html}
}

@inproceedings{maLowlatencySDRAMController2010,
  title = {Low-Latency {{SDRAM}} Controller for Shared Memory in {{MPSoC}}},
  doi = {10.1109/ICSICT.2010.5667736},
  abstract = {In a memory structure shared by multiple processors based on Multiprocessor Systems on Chip (MPSoC), the efficiency of memory bus access becomes the bottleneck of the overall system efficiency. This paper presents a low-latency SDARM controller structure integrated in MPSoC, which controls the off-chip SDRAM memory. Consecutive same row optimization and odd-even bank optimization are used to eliminate precharge time and active to read/write execution in memory access. Burst mode supported by data transmit block improves the efficiency of the memory bus. Simulation results show that memory performance improves maximally by 56\% compared to pre-optimized, making it meet the high throughput requirements of shared-memory controller in MPSoC.},
  eventtitle = {2010 10th {{IEEE International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology}}},
  booktitle = {2010 10th {{IEEE International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology}}},
  date = {2010-11},
  pages = {321-323},
  keywords = {Conferences,Timing,DRAM chips,Memory management,Optimization,Mathematical model,shared memory systems,system-on-chip,burst mode,Classification algorithms,data transmit block,low-latency SDRAM controller structure,memory bus access,memory structure,multiprocessor systems on chip,odd-even bank optimization,off-chip SDRAM memory,same row optimization,SDRAM,shared-memory controller},
  author = {Ma, P. and Zhao, J. and Li, K. and Zhu, L. and Shi, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TRLP7B57\\Ma et al. - 2010 - Low-latency SDRAM controller for shared memory in .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FBGVYY2P\\5667736.html}
}

@inproceedings{wei-chetsengOptimalSchedulingMinimize2010,
  title = {Optimal Scheduling to Minimize Non-Volatile Memory Access Time with Hardware Cache},
  doi = {10.1109/VLSISOC.2010.5642609},
  abstract = {In power and size sensitive embedded systems, flash memory and phase change memory are replacing DRAM as the main memory. Unfortunately, these technologies are limited by their endurance and long write latencies. To minimize the main memory access time, we optimally schedule tasks by an ILP formulation that can be generally applied to other main memory technologies, including DRAM. We also present a heuristic, Wander Scheduling, to solve larger instances in a reasonable amount of time. Our experimental results show that when compared with list scheduling, Wander Scheduling can reduce memory access times by an average of 40.73\% and increase the lifetime of flash and phase change memory by 82.56\%.},
  eventtitle = {2010 18th {{IEEE}}/{{IFIP International Conference}} on {{VLSI}} and {{System}}-on-{{Chip}}},
  booktitle = {2010 18th {{IEEE}}/{{IFIP International Conference}} on {{VLSI}} and {{System}}-on-{{Chip}}},
  date = {2010-09},
  pages = {131-136},
  keywords = {cache storage,Optimal scheduling,DRAM chips,DRAM,Scheduling,Nonvolatile memory,phase change memory,processor scheduling,Ash,embedded system,flash memories,flash memory,hardware cache,Law,nonvolatile memory access time minimization,optimal scheduling,phase change memories,Schedules,wander scheduling},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EVVH9Z7G\\Wei-Che Tseng et al. - 2010 - Optimal scheduling to minimize non-volatile memory.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HJP9WQ5Y\\5642609.html}
}

@inproceedings{taozhangCustomizedDesignDRAM2010,
  title = {A Customized Design of {{DRAM}} Controller for On-Chip {{3D DRAM}} Stacking},
  doi = {10.1109/CICC.2010.5617465},
  abstract = {To address the “memory wall” challenge, on-chip memory stacking has been proposed as a promising solution. The stacking memory adopts three-dimensional (3D) IC technology, which leverages through-silicon-vias (TSVs) to connect layers, to dramatically reduce the access latency and improve the bandwidth without the constraint of I/O pins. To demonstrate the feasibility of 3D memory stacking, this paper introduces a customized 3D Double-Data-Rate (DDR) SDRAM controller design, which communicates with DRAM layers by TSVs. In addition, we propose a parallel access policy to further improve the performance. The 3D DDR controller is integrated in a 3D stacking System-on-Chip (SoC) architecture, where a high-bandwidth 3D DRAM chip is stacked on the top. The 3D SoC is divided into two logic layers with each having an area of 2.5 × 5.0mm2, with a 3-layer 2Gb DRAM stacking. The whole chip has been fabricated in Chartered 130nm low-power process and Tezzaron's 3D bonding technology. The simulation result shows that the on-chip DRAM controller can run as fast as 133MHz and provide 4.25GB/s data bandwidth in a single channel and 8.5GB/s with parallel access policy.},
  eventtitle = {{{IEEE Custom Integrated Circuits Conference}} 2010},
  booktitle = {{{IEEE Custom Integrated Circuits Conference}} 2010},
  date = {2010-09},
  pages = {1-4},
  keywords = {Registers,Bandwidth,logic design,System-on-a-chip,DRAM chips,Random access memory,Computer architecture,system-on-chip,3D bonding technology,3D DDR controller,3D DRAM chip,3D IC technology,3D memory stacking,3D SoC,3D stacking system-on-chip architecture,access latency,customized 3D double-data-rate,customized design,data bandwidth,DRAM layers,logic layers,memory wall challenge,on-chip 3D DRAM stacking,on-chip memory stacking,parallel access policy,SDRAM controller design,SoC architecture,Stacking,stacking memory,Three dimensional displays,through-silicon-vias,TSV},
  author = {{Tao Zhang} and {Kui Wang} and {Yi Feng} and {Xiaodi Song} and {Lian Duan} and Xie, Y. and {Xu Cheng} and {Youn-Long Lin}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ILDYCDTB\\Tao Zhang et al. - 2010 - A customized design of DRAM controller for on-chip.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TXIARCL7\\5617465.html}
}

@inproceedings{akessonClassificationAnalysisPredictable2010,
  title = {Classification and {{Analysis}} of {{Predictable Memory Patterns}}},
  doi = {10.1109/RTCSA.2010.35},
  abstract = {The verification complexity of real-time requirements in embedded systems grows exponentially with the number of applications, as resource sharing prevents independent verification using simulation-based approaches. Formal verification is a promising alternative, although its applicability is limited to systems with predictable hardware and software. SDRAM memories are common examples of essential hardware components with unpredictable timing behavior, typically preventing use of formal approaches. A predictable SDRAM controller has been proposed that provides guarantees on bandwidth and latency by dynamically scheduling memory patterns, which are statically computed sequences of SDRAM commands. However, the proposed patterns become increasingly inefficient as memories become faster, making them unsuitable for DDR3 SDRAM. This paper extends the memory pattern concept in two ways. Firstly, we introduce a burst count parameter that enables patterns to have multiple SDRAM bursts per bank, which is required for DDR3 memories to be used efficiently. Secondly, we present a classification of memory pattern sets into four categories based on the combination of patterns that cause worst-case bandwidth and latency to be provided. Bounds on bandwidth and latency are derived that apply to all pattern types and burst counts, as opposed to the single case covered by earlier work. Experimental results show that these extensions are required to support the most efficient pattern sets for many use-cases. We also demonstrate that the burst count parameter increases efficiency in presence of large requests and enables a wider range of real-time requirements to be satisfied.},
  eventtitle = {2010 {{IEEE}} 16th {{International Conference}} on {{Embedded}} and {{Real}}-{{Time Computing Systems}} and {{Applications}}},
  booktitle = {2010 {{IEEE}} 16th {{International Conference}} on {{Embedded}} and {{Real}}-{{Time Computing Systems}} and {{Applications}}},
  date = {2010-08},
  pages = {367-376},
  keywords = {embedded systems,Bandwidth,Real time systems,Timing,DRAM chips,memory controller,Memory management,timing,Clocks,SDRAM,burst count,classification,DDR3 SDRAM,dynamic scheduling,dynamically scheduling memory patterns,formal verification,memory patterns,predictability,predictable hardware,predictable memory patterns,predictable software,real-time requirements,SDRAM memories,Switches,timing behavior},
  author = {Akesson, B. and Jr, W. Hayes and Goossens, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KGXWW4BB\\Akesson et al. - 2010 - Classification and Analysis of Predictable Memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZBWX45L8\\5591843.html}
}

@inproceedings{luoDesignRealizationOptimized2010,
  title = {Design and {{Realization}} of an {{Optimized Memory Access Scheduler}}},
  volume = {2},
  doi = {10.1109/CSO.2010.81},
  abstract = {Memory Wall is a bottleneck of enhancing the performance of computer system, and appearance of multiprocessors (CMPs) makes it more. How to reduce Memory Access Latency is a critical issue we have to deal with. Memory controller is difficult to optimize, the controller needs to obey all DRAM timing constraints to provide correct functionality. State-of -the-art DDR2 SDRAM chips often have a large number of timing constraints that must be obeyed when scheduling commands, for instance, over 50 timing constrains. We have made deep research on optimized memory access scheduling. In order to efficiently utilize the bandwidth and reduce the latency, Memory Access Scheduling optimization adapts the characters of DRAM to reschedule the memory access. By studying effective data bar which is generated by Genetic Algorithm, we mine four rules. So we just use these four rules to schedule in Memory Access Controller. The results of experiment show that compared with FR-FCFS (first-ready first-come first-serve) scheduling strategy, the rule based algorithm improves the performance of scheduling and the ideal speedup is near 1.5 times. The best speedup of the test of spec2000 is 1.467, and the worst speedup is 1.078.},
  eventtitle = {2010 {{Third International Joint Conference}} on {{Computational Science}} and {{Optimization}}},
  booktitle = {2010 {{Third International Joint Conference}} on {{Computational Science}} and {{Optimization}}},
  date = {2010-05},
  pages = {288-292},
  keywords = {Bandwidth,Design optimization,Processor scheduling,Scheduling algorithm,Timing,DRAM chips,Random access memory,memory controller,timing,memory access latency,multiprocessing systems,Delay,processor scheduling,SDRAM,Constraint optimization,data mining,DDR2,DDR2 SDRAM chips,first-ready first-come first-serve scheduling,FR-FCFS scheduling strategy,genetic algorithm,genetic algorithm (GA),genetic algorithms,Genetic algorithms,memory access controller,memory access scheduling,multiprocessors,optimized memory access scheduler,rule based algorithm,scheduling commands},
  author = {Luo, L. and He, H. and Liao, C. and Dou, Q. and Xu, W.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\72D9UFMK\\Luo et al. - 2010 - Design and Realization of an Optimized Memory Acce.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\44V4AJRA\\5533016.html}
}

@inproceedings{liuMultibankMemoryAccess2010,
  title = {Multi-Bank Memory Access Scheduler and Scalability},
  volume = {2},
  doi = {10.1109/ICCET.2010.5485729},
  abstract = {With the progress of semiconductor manufacture techniques and the development of processor architecture, the gap between processor and DRAM speed is becoming larger and larger, memory bandwidth is now the primary bottleneck of improving computer system performance. Modern DRAM provide several independent memory banks, according to this character, we present a virtual channel based memory access scheduler, and least wait time and read-fist schedule approach. This approach significantly reduce observed main memory access latency and improve the effective memory bandwidth.},
  eventtitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  booktitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  date = {2010-04},
  pages = {V2-723-V2-727},
  keywords = {Bandwidth,Processor scheduling,DRAM chips,Random access memory,Computer architecture,memory access latency,memory bandwidth,System performance,storage management,processor scheduling,memory access scheduling,Computer aided manufacturing,computer system performance,DRAM memory system,DRAM speed,Job shop scheduling,Manufacturing processes,multibank memory access scheduler,Scalability,Semiconductor device manufacture,semiconductor manufacture technique,virtual channel,virtual channel based memory access scheduler},
  author = {Liu, D. and Pan, G. and Xie, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FFS6FDZQ\\Liu et al. - 2010 - Multi-bank memory access scheduler and scalability.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5FFBJM2U\\5485729.html}
}

@inproceedings{panImprovingVLIWProcessor2009,
  title = {Improving {{VLIW Processor Performance Using Three}}-{{Dimensional}} ({{3D}}) {{DRAM Stacking}}},
  doi = {10.1109/ASAP.2009.11},
  abstract = {This work studies the potential of using emerging 3D integration to improve embedded VLIW computing system. We focus on the 3D integration of one VLIW processor die with multiple high-capacity DRAM dies. Our proposed memory architecture employs 3D stacking technology to bond one die containing several processing clusters to multiple DRAM dies for a primary memory. The 3D technology also enables wide low-latency buses between clusters and memory and enable the latency of 3D DRAM L2 cache comparable to 2D SRAM L2 cache. These enable it to replace the 2D SRAM L2 cache with 3D DRAM L2 cache. The die area for 2D SRAM L2 cache can be re-allocated to additional clusters that can improve the performance of the system. From the simulation results, we find 3D stacking DRAM main memory can improve the system performance by 10\% 80\% than 2D off-chip DRAM main memory depending on different benchmarks. Also, for a similar logic die area, a four clusters system with 3D DRAM L2 cache and 3D DRAM main memory outperforms a two clusters system with 2D SRAM L2 cache and 3D DRAM main memory by about 10\%.},
  eventtitle = {2009 20th {{IEEE International Conference}} on {{Application}}-Specific {{Systems}}, {{Architectures}} and {{Processors}}},
  booktitle = {2009 20th {{IEEE International Conference}} on {{Application}}-Specific {{Systems}}, {{Architectures}} and {{Processors}}},
  date = {2009-07},
  pages = {38-45},
  keywords = {cache storage,DRAM chips,Random access memory,Computer architecture,Parallel processing,multiprocessing systems,memory architecture,System performance,3D DRAM,SRAM chips,Delay,Stacking,2D SRAM L2 cache,3D DRAM L2 cache,3D DRAM stacking,Digital signal processing,DSP,Embedded computing,embedded VLIW computing system,instruction sets,low-latency buses,parallel architectures,parallel machines,USA Councils,VLIW},
  author = {Pan, Y. and Zhang, T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BUZIPI6T\\Pan and Zhang - 2009 - Improving VLIW Processor Performance Using Three-D.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\48AKA72V\\5200008.html}
}

@inproceedings{hadkeDesignEvaluationOptical2008,
  title = {Design and Evaluation of an Optical {{CPU}}-{{DRAM}} Interconnect},
  doi = {10.1109/ICCD.2008.4751906},
  abstract = {We present OCDIMM (Optically Connected DIMM), a CPU-DRAM interface that uses multiwavelength optical interconnects. We show that OCDIMM is more scalable and offers higher bandwidth and lower latency than FBDIMM (Fully-Buffered DIMM), a state-of-the-art electrical alternative. Though OCDIMM is more power efficient than FBDIMM, we show that ultimately the total power consumption in the memory subsystem is a key impediment to scalability and thus to achieving truly balanced computing systems in the terascale era.},
  eventtitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  booktitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  date = {2008-10},
  pages = {492-497},
  keywords = {Bandwidth,Multicore processing,DRAM chips,Clocks,Energy consumption,Delay,Scalability,balanced computing,CPU-DRAM interface,Frequency,fully-buffered DIMM,memory subsystem,Optical computing,optical CPU-DRAM interconnect,Optical design,optical interconnections,Optical interconnections,total power consumption},
  author = {Hadke, A. and Benavides, T. and Amirtharajah, R. and Farrens, M. and Akella, V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DGPSV7V7\\Hadke et al. - 2008 - Design and evaluation of an optical CPU-DRAM inter.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SBIIRJH9\\4751906.html}
}

@inproceedings{chegeniDesignHighSpeed2007,
  title = {Design of a {{High Speed}}, {{Low Latency}} and {{Low Power Consumption DRAM Using}} Two-Transistor {{Cell Structure}}},
  doi = {10.1109/ICECS.2007.4511203},
  abstract = {This paper presents a new structure of DRAM, using two-transistor cell. The most important advantages of this structure are: a) High speed read, write and refresh operation b) low data access latency c) low power consumption compared to other structures d) each write/refresh operation can be carried out just in one cycle and e) no need to special process and compatible with standard digital process.},
  eventtitle = {2007 14th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  booktitle = {2007 14th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  date = {2007-12},
  pages = {1167-1170},
  keywords = {Laboratories,DRAM chips,Random access memory,DRAM,Microprocessors,Energy consumption,Capacitance,power consumption,Delay,Circuits,data access latency,Distributed power generation,Microelectronics,two-transistor cell structure,Voltage,write-refresh operation},
  author = {Chegeni, A. and Hadidi, K. and Khoei, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7EYIFN9L\\Chegeni et al. - 2007 - Design of a High Speed, Low Latency and Low Power .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DZNH26XH\\4511203.html}
}

@inproceedings{chienLowLatencyMemory2007,
  title = {A {{Low Latency Memory Controller}} for {{Video Coding Systems}}},
  doi = {10.1109/ICME.2007.4284874},
  abstract = {The dynamic memory controller plays an important role in system-on-a-chip (SoC) designs to provide enough memory bandwidth through external memory for DSP and multimedia processing. However, the overhead cycles in accessing the data located in external memory have much influence on the SoC performance. In this paper, we propose a low latency memory controller with AHB interface to reduce the overhead cycles for the SDR memory access in the SoC designs. Through the pre-calculated addresses of impending transfers, two memory control schemes, i.e. Burst terminates Burst (BTB) and Anticipative Row Activation (ARA), are used to reduce the latency of SDR memory access. The experimental results show that the proposed memory controller reduces the memory bandwidth by 33\% in a typical MPEG-4 video decoding system.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  date = {2007-07},
  pages = {1211-1214},
  keywords = {Bandwidth,Costs,System-on-a-chip,DRAM chips,Decoding,Scheduling,SRAM chips,Delay,system-on-chip,anticipative row activation,burst terminates burst,Control systems,dynamic memory controller,low latency memory controller,MPEG 4 Standard,MPEG-4 video decoding system,Multimedia systems,SoC design,video coding,Video coding,video coding systems},
  author = {Chien, C. and Wang, C. and Lin, C. and Hsieh, T. and Chu, Y. and Guo, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LFPHEKY6\\Chien et al. - 2007 - A Low Latency Memory Controller for Video Coding S.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3AHHJWJJ\\4284874.html}
}

@inproceedings{linDRAMLevelPrefetchingFullyBuffered2007,
  title = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}: {{Design}}, {{Performance}} and {{Power Saving}}},
  doi = {10.1109/ISPASS.2007.363740},
  shorttitle = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}},
  abstract = {We have studied DRAM-level prefetching for the fully buffered DIMM (FB-DIMM) designed for multi-core processors. FB-DIMM has a unique two-level interconnect structure, with FB-DIMM channels at the first-level connecting the memory controller and advanced memory buffers (AMBs); and DDR2 buses at the second-level connecting the AMBs with DRAM chips. We propose an AMB prefetching method that prefetches memory blocks from DRAM chips to AMBs. It utilizes the redundant bandwidth between the DRAM chips and AMBs but does not consume the crucial channel bandwidth. The proposed method fetches K memory blocks of L2 cache block sizes around the demanded block, where K is a small value ranging from two to eight. The method may also reduce the DRAM power consumption by merging some DRAM precharges and activations. Our cycle-accurate simulation shows that the average performance improvement is 16\% for single-core and multi-core workloads constructed from memory-intensive SPEC2000 programs with software cache prefetching enabled; and no workload has negative speedup. We have found that the performance gain comes from the reduction of idle memory latency and the improvement of channel bandwidth utilization. We have also found that there is only a small overlap between the performance gains from the AMB prefetching and the software cache prefetching. The average of estimated power saving is 15\%},
  eventtitle = {2007 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  booktitle = {2007 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  date = {2007-04},
  pages = {94-104},
  keywords = {Bandwidth,Multicore processing,Prefetching,DRAM chips,Random access memory,dynamic random access memory,memory controller,Energy consumption,storage management,fully-buffered DIMM,channel bandwidth utilization,DRAM chip,DRAM power consumption,DRAM-level prefetching,dual in-line memory module,idle memory latency,interconnect structure,Joining processes,L2 cache block,memory block,Merging,multicore processor,Performance gain,power saving,Process design,redundant bandwidth,software cache prefetching,Software performance,SPEC2000 program,storage management chips},
  author = {Lin, J. and Zheng, H. and Zhu, Z. and Zhang, Z. and David, H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2GHPXF54\\Lin et al. - 2007 - DRAM-Level Prefetching for Fully-Buffered DIMM De.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BQCS9QVC\\4211026.html}
}

@inproceedings{shaoBurstSchedulingAccess2007,
  title = {A {{Burst Scheduling Access Reordering Mechanism}}},
  doi = {10.1109/HPCA.2007.346206},
  abstract = {Utilizing the nonuniform latencies of SDRAM devices, access reordering mechanisms alter the sequence of main memory access streams to reduce the observed access latency. Using a revised M5 simulator with an accurate SDRAM module, the burst scheduling access reordering mechanism is proposed and compared to conventional in order memory scheduling as well as existing academic and industrial access reordering mechanisms. With burst scheduling, memory accesses to the same rows of the same banks are clustered into bursts to maximize bus utilization of the SDRAM device. Subject to a static threshold, memory reads are allowed to preempt ongoing writes for reduced read latency, while qualified writes are piggybacked at the end of bursts to exploit row locality in writes and prevent write queue saturation. Performance improvements contributed by read preemption and write piggybacking are identified. Simulation results show that burst scheduling reduces the average execution time of selected SPEC CPU2000 benchmarks by 21\% over conventional bank in order memory scheduling. Burst scheduling also outperforms Intel's patented out of order memory scheduling and the row hit access reordering mechanism by 11\% and 6\% respectively},
  eventtitle = {2007 {{IEEE}} 13th {{International Symposium}} on {{High Performance Computer Architecture}}},
  booktitle = {2007 {{IEEE}} 13th {{International Symposium}} on {{High Performance Computer Architecture}}},
  date = {2007-02},
  pages = {285-294},
  keywords = {Bandwidth,Processor scheduling,scheduling,Computational modeling,Out of order,DRAM chips,storage management,Delay,SDRAM,access latency,Job shop scheduling,Aerospace industry,burst scheduling access reordering mechanism,bus utilization,M5 simulator,main memory access streams,order memory scheduling,Read-write memory,SDRAM devices,Space exploration,SPEC CPU2000 benchmarks,system buses},
  author = {Shao, J. and Davis, B. T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZV8GWYP9\\Shao and Davis - 2007 - A Burst Scheduling Access Reordering Mechanism.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZWPBSKJ6\\4147669.html}
}

@inproceedings{stankovicDRAMControllerComplete2005,
  title = {{{DRAM Controller}} with a {{Complete Predictor}}: {{Preliminary Results}}},
  volume = {2},
  doi = {10.1109/℡SKS.2005.1572183},
  shorttitle = {{{DRAM Controller}} with a {{Complete Predictor}}},
  abstract = {In the arsenal of solutions for computer memory system performance improvement, predictors have gained an increasing role in the past years. They enable hiding the latencies when accessing cache or main memory. Recently the technique of using temporal parameters of cache memory accesses and tag patterns observing has been applied by some authors for prediction of data prefetching. In this paper a possibility of applying analog techniques on controlling DRAM rows opening/closing, is being researched. Obtained results confirm such a possibility, in a form of a complete predictor, which predicts not only when to close the currently open row but also which is the next row to be opened. Using such a predictor can decrease the average DRAM latency, which is very important in many areas, including telecommunications},
  eventtitle = {℡{{SIKS}} 2005 - 2005 Uth {{International Conference}} on {{Telecommunication}} in {{ModernSatellite}}, {{Cable}} and {{Broadcasting Services}}},
  booktitle = {℡{{SIKS}} 2005 - 2005 Uth {{International Conference}} on {{Telecommunication}} in {{ModernSatellite}}, {{Cable}} and {{Broadcasting Services}}},
  date = {2005-09},
  pages = {593-596},
  keywords = {Prefetching,DRAM chips,Random access memory,DRAM,System performance,predictor,DRAM controller,Delay,Cache memory,analog techniques,computer memory system,data prefetching,Digital images,latency,policy,Table lookup,Telecommunication buffers,Telecommunication control},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QT2ZCLBE\\Stankovic and Milenkovic - 2005 - DRAM Controller with a Complete Predictor Prelimi.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9CM8S23P\\1572183.html}
}

@article{liuBridgingProcessormemoryPerformance2005,
  title = {Bridging the Processor-Memory Performance Gap with {{3D IC}} Technology},
  volume = {22},
  issn = {0740-7475},
  doi = {10.1109/MDT.2005.134},
  abstract = {Microprocessor performance has been improving at roughly 60\% per year. Memory access times, however, have improved by less than 10\% per year. The resulting gap between logic and memory performance has forced microprocessor designs toward complex and power-hungry architectures that support out-of-order and speculative execution. Moreover, processors have been designed with increasingly large cache hierarchies to hide main memory latency. This article examines how 3D IC technology can improve interactions between the processor and memory. Our work examines the performance of a single-core, single-threaded processor under representative work loads. We have shown that reducing memory latency by bringing main memory on chip gives us near-perfect performance. Three-dimensional IC technology can provide the much needed bandwidth without the cost, design complexity, and power issues associated with a large number of off-chip pins. The principal challenge remains the demonstration of a highly manufacturable 3D IC technology with high yield and low cost.},
  number = {6},
  journaltitle = {IEEE Design Test of Computers},
  date = {2005-11},
  pages = {556-564},
  keywords = {Bandwidth,Costs,microprocessor chips,cache storage,Out of order,DRAM chips,Microprocessors,memory architecture,Delay,3D IC technology,Process design,integrated circuit technology,Logic design,main memory on chip,Manufacturing,memory access,microprocessor design,microprocessor performance,Pins,processor-memory performance,single-threaded processor,Three-dimensional integrated circuits,three-dimensional integration 3-D ICs microprocessor cache design stream prefetching embedded DRAM},
  author = {Liu, C. C. and Ganusov, I. and Burtscher, M. and {Sandip Tiwari}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PLDS6YDF\\Liu et al. - 2005 - Bridging the processor-memory performance gap with.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7AKAYTWA\\1541918.html}
}

@inproceedings{miuraMemoryControllerThat2005,
  title = {A Memory Controller That Reduces Latency of Cached {{SDRAM}}},
  doi = {10.1109/ISCAS.2005.1465819},
  abstract = {The proposed controller has two main control schemes, address-alignment control and dummy-cache control. These two schemes cooperatively control cached SDRAM to reduce its latency. Testing of the controller using benchmark programs demonstrated that latency was reduced 25\% and execution time was reduced 13\% compared to those of a sense-amplifier cache controller for standard SDRAM. The proposed controller requires 9.2 Kgates at a supply voltage of 1.8 V and an operating frequency of 133 MHz.},
  eventtitle = {2005 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  booktitle = {2005 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  date = {2005-05},
  pages = {5250-5253 Vol. 5},
  keywords = {Laboratories,Benchmark testing,cache storage,DRAM chips,Random access memory,memory controller,integrated circuit design,Delay,Cache memory,SDRAM,Frequency,1.8 V,133 MHz,address-alignment control,benchmark programs,cached SDRAM latency reduction,Centralized control,dummy-cache control,Operational amplifiers,sense-amplifier cache controller,standard SDRAM,synchronous DRAM,Voltage control},
  author = {Miura, S. and Akiyama, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QFVCVC2L\\Miura and Akiyama - 2005 - A memory controller that reduces latency of cached.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NEDWVNM9\\1465819.html}
}

@article{mutluRunaheadExecutionEffective2003,
  title = {Runahead Execution: {{An}} Effective Alternative to Large Instruction Windows},
  volume = {23},
  issn = {0272-1732},
  doi = {10.1109/MM.2003.1261383},
  shorttitle = {Runahead Execution},
  abstract = {An instruction window that can tolerate latencies to DRAM memory is prohibitively complex and power hungry. To avoid having to build such large windows, runahead execution uses otherwise-idle clock cycles to achieve an average 22 percent performance improvement for processors with instruction windows of contemporary sizes. This technique incurs only a small hardware cost and does not significantly increase the processor's complexity.},
  number = {6},
  journaltitle = {IEEE Micro},
  date = {2003-11},
  pages = {20-25},
  keywords = {Hardware,Registers,Costs,Microarchitecture,Out of order,Random access memory,Clocks,Energy consumption,storage management,Delay,instruction sets,computer architecture,DRAM memory,instruction windows,Retirement,runahead execution},
  author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y. N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4UCPZSDD\\Mutlu et al. - 2003 - Runahead execution An effective alternative to la.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NJL46HQQ\\1261383.html}
}

@inproceedings{tzu-chiehlinQualityawareMemoryController2003,
  title = {Quality-Aware Memory Controller for Multimedia Platform {{SoC}}},
  doi = {10.1109/SIPS.2003.1235691},
  abstract = {The ongoing advancements in VLSI technology allow SoC design to integrate heterogeneous control and computing functions into a single chip. On the other hand, the pressures of area and cost lead to the requirement for a single, shared off-chip DRAM memory subsystem. To satisfy different memory access requirements (for latency and bandwidth) of these heterogeneous functions to this kind of DRAM memory subsystem, a quality-aware memory controller is important. The paper presents an efficient memory controller that contains a quality-aware scheduler and a configurable DRAM memory interface socket to achieve high DRAM utilization while still meeting different requirements for bandwidth and latency. Simulation results show that the latency of the latency-sensitive data flow can be reduced to 50\%, and the memory bandwidths can be precisely allocated to bandwidth-sensitive data flows with a high degree of control.},
  eventtitle = {2003 {{IEEE Workshop}} on {{Signal Processing Systems}} ({{IEEE Cat}}. {{No}}.{{03TH8682}})},
  booktitle = {2003 {{IEEE Workshop}} on {{Signal Processing Systems}} ({{IEEE Cat}}. {{No}}.{{03TH8682}})},
  date = {2003-08},
  pages = {328-333},
  keywords = {Bandwidth,Costs,Processor scheduling,scheduling,System-on-a-chip,Random access memory,storage management,Delay,random-access storage,bandwidth allocation,Quality of service,system-on-chip,Control systems,memory access,bandwidth-sensitive data flows,Communication system control,Design engineering,interface socket,latency-sensitive data flow,memory bandwidth allocation,multimedia platform SoC,off-chip DRAM memory subsystem,quality-aware memory controller,quality-aware scheduler,VLSI technology},
  author = {{Tzu-Chieh Lin} and {Kun-Bin Lee} and {Chein-Wei Jen}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VN6NF4GS\\Tzu-Chieh Lin et al. - 2003 - Quality-aware memory controller for multimedia pla.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\93XMK69I\\1235691.html}
}

@inproceedings{seong-ilparkHistorybasedMemoryMode2003,
  title = {History-Based Memory Mode Prediction for Improving Memory Performance},
  volume = {5},
  doi = {10.1109/ISCAS.2003.1206226},
  abstract = {To increase the bandwidth of synchronous memories that are widely adopted for high performance memory systems, a predictive mode control scheme is proposed in this paper. Memory latency can be reduced by effectively managing the states of banks. The local access history of each bank is considered to predict the memory mode. Experimental results show that the proposed scheme, at the cost of negligible area overhead, reduces the memory latency by 19.0\% over the conventional scheme that always keeps the memory in idle state.},
  eventtitle = {Proceedings of the 2003 {{International Symposium}} on {{Circuits}} and {{Systems}}, 2003. {{ISCAS}} '03.},
  booktitle = {Proceedings of the 2003 {{International Symposium}} on {{Circuits}} and {{Systems}}, 2003. {{ISCAS}} '03.},
  date = {2003-05},
  pages = {V-V},
  keywords = {Bandwidth,Costs,History,System-on-a-chip,DRAM chips,Random access memory,Decoding,Memory management,integrated circuit design,Delay,SDRAM,Control systems,latency,area overhead,bandwidth,history-based memory mode prediction,idle state,local access history,memory mode,memory performance,predictive mode control scheme,synchronous memories},
  author = {{Seong-Il Park} and {In-Cheol Park}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2VUA6ZWA\\Seong-Il Park and In-Cheol Park - 2003 - History-based memory mode prediction for improving.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HYY2MUEJ\\1206226.html}
}

@inproceedings{ekanayakeAsynchronousDRAMDesign2003,
  title = {Asynchronous {{DRAM}} Design and Synthesis},
  doi = {10.1109/ASYNC.2003.1199177},
  abstract = {We present the design of a high performance on-chip pipelined asynchronous DRAM suitable for use in a microprocessor cache. Although traditional DRAM structures suffer from long access latency and even longer cycle times, our design achieves a simulated core sub-nanosecond latency and a respectable cycle time of 4.8 ns in a standard 0.25 /spl mu/m logic process. We also show how the cycle time penalty can be overcome by using pipelined interleaved banks with quasi-delay insensitive asynchronous control circuits. We can thus approach the performance of SRAM, which is typically used for caches, while still benefiting from the smaller area footprint of DRAM.},
  eventtitle = {Ninth {{International Symposium}} on {{Asynchronous Circuits}} and {{Systems}}, 2003. {{Proceedings}}.},
  booktitle = {Ninth {{International Symposium}} on {{Asynchronous Circuits}} and {{Systems}}, 2003. {{Proceedings}}.},
  date = {2003-05},
  pages = {174-183},
  keywords = {Laboratories,microprocessor chips,cache storage,logic design,System-on-a-chip,Random access memory,Microprocessors,integrated circuit design,Delay,random-access storage,Logic design,Design engineering,0.25 micron,4.8 ns,asynchronous circuits,asynchronous control circuits,asynchronous DRAM design,Banking,Circuit simulation,cycle time penalty,High performance computing,long access latency,microprocessor cache,on-chip asynchronous DRAM,pipeline processing,pipelined asynchronous DRAM,pipelined interleaved banks,quasi-delay insensitive control circuits},
  author = {Ekanayake, V. N. and Manohar, R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G6U57F85\\Ekanayake and Manohar - 2003 - Asynchronous DRAM design and synthesis.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FH333QQZ\\1199177.html}
}

@inproceedings{frittsMultilevelMemoryPrefetching2002,
  title = {Multi-Level Memory Prefetching for Media and Stream Processing},
  volume = {2},
  doi = {10.1109/ICME.2002.1035522},
  abstract = {This paper presents a multi-level memory prefetch hierarchy for media and stream processing applications. Two major bottlenecks in the performance of multimedia and network applications are long memory latencies and limited off-chip processor bandwidth. Aggressive prefetching can be used to mitigate the memory latency problem, but overly aggressive prefetching may overload the limited external processor bandwidth. To accommodate both problems, we propose multilevel memory prefetching. The multi-level organization enables conservative prefetching on-chip and more aggressive prefetching off-chip. The combination provides aggressive prefetching while minimally impacting off-chip bandwidth, enabling more efficient memory performance for media and stream processing. This paper presents preliminary results for multi-level memory prefetching, which show that combining prefetching at the L1 and DRAM memory levels provides the most effective prefetching with minimal extra bandwidth.},
  eventtitle = {Proceedings. {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  booktitle = {Proceedings. {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  date = {2002-08},
  pages = {101-104 vol.2},
  keywords = {Computer networks,Computer science,Bandwidth,Prefetching,Random access memory,buffer storage,Delay,DRAM memory,bandwidth,memory performance,Application software,L1 memory,media processing,memory latencies,multi-level memory prefetch hierarchy,multi-level memory prefetching,multi-level organization,multimedia applications,multimedia communication,multimedia computing,Multimedia computing,network applications,off-chip processor bandwidth,on-chip,Personal digital assistants,stream processing,Streaming media},
  author = {Fritts, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BNDPUWHS\\Fritts - 2002 - Multi-level memory prefetching for media and strea.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YKCMRBR7\\1035522.html}
}

@inproceedings{solihinUsingUserlevelMemory2002,
  title = {Using a User-Level Memory Thread for Correlation Prefetching},
  doi = {10.1109/ISCA.2002.1003576},
  abstract = {This paper introduces the idea of using a user-level memory thread (ULMT) for correlation prefetching. In this approach, a user thread runs on a general-purpose processor in main memory, either in the memory controller chip or in a DRAM chip. The thread performs correlation prefetching in software, sending the prefetched data into the L2 cache of the main processor. This approach requires minimal hardware beyond the memory processor: the correlation table is a software data structure that resides in the main memory, while the main processor only needs a few modifications to its L2 cache so that it can accept incoming prefetches. In addition, the approach has wide usability, as it can effectively prefetch even for irregular applications. Finally, it is very flexible, as the prefetching algorithm can be customized by the user on an application basis. Our simulation results show that, through a new design of the correlation table and prefetching algorithm, our scheme delivers good results. Specifically, nine mostly-irregular applications show an average speedup of 1.32. Furthermore, our scheme works well in combination with a conventional processor-side sequential prefetcher, in which case the average speedup increases to 1.46. Finally, by exploiting the customization of the prefetching algorithm, we increase the average speed up to 1.53.},
  eventtitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  booktitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  date = {2002-05},
  pages = {171-182},
  keywords = {Hardware,Prefetching,memory architecture,storage management,Software performance,Application software,correlation methods,correlation prefetching,correlation table,customization,data structure prefetching,Data structures,Engines,Graphics,memory access latencies,Proposals,Usability,user-level memory thread,Yarn},
  author = {Solihin, Y. and {Jaejin Lee} and Torrellas, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\455NZ3WH\\Solihin et al. - 2002 - Using a user-level memory thread for correlation p.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZFDFIZ4\\1003576.html}
}

@inproceedings{wei-fenlinReducingDRAMLatencies2001,
  title = {Reducing {{DRAM}} Latencies with an Integrated Memory Hierarchy Design},
  doi = {10.1109/HPCA.2001.903272},
  abstract = {In this paper we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half of its time stalling for L2 misses. Large cache blocks can improve performance, but only when coupled with wide memory channels. DRAM address mappings also affect performance significantly. We evaluate an aggressive prefetch unit integrated with the L2 cache and memory, controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 43\% speedup across 10 of the 26 SPEC2000 benchmarks, without degrading performance an the others. With eight Rambus channels, these ten benchmarks improve to within 10\% of the performance of a perfect L2 cache.},
  eventtitle = {Proceedings {{HPCA Seventh International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  booktitle = {Proceedings {{HPCA Seventh International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  date = {2001-01},
  pages = {301-312},
  keywords = {Computer science,performance,cache storage,Prefetching,Random access memory,Clocks,memory architecture,performance evaluation,Delay,Frequency,Banking,High performance computing,benchmarks,cache blocks,Degradation,DRAM accesses,Dynamic scheduling,integrated memory hierarchy,next-generation memory system,performance gap,Rambus channels},
  author = {{Wei-Fen Lin} and Reinhardt, S. K. and Burger, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QZ59WSBZ\\Wei-Fen Lin et al. - 2001 - Reducing DRAM latencies with an integrated memory .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EUUYIX7N\\903272.html}
}

@article{zhangCachedDRAMILP2001,
  title = {Cached {{DRAM}} for {{ILP}} Processor Memory Access Latency Reduction},
  volume = {21},
  issn = {0272-1732},
  doi = {10.1109/40.946676},
  abstract = {Cached DRAM adds a small cache onto a DRAM chip to reduce average DRAM access latency. The authors compare cached DRAM with other advanced DRAM techniques for reducing memory access latency in instruction-level-parallelism processors.},
  number = {4},
  journaltitle = {IEEE Micro},
  date = {2001-07},
  pages = {22-32},
  keywords = {Bandwidth,cache storage,DRAM chips,Random access memory,Parallel processing,Capacitors,memory architecture,DRAM access latency,Delay,SDRAM,parallel architectures,cached DRAM,Computer aided instruction,Concurrent computing,ILP,Impedance,instruction-level-parallelism processors,Interleaved codes,latency reduction,processor memory access latency},
  author = {Zhang, Z. and Zhu, Z. and Zhang, X.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VH8GNL7D\\Zhang et al. - 2001 - Cached DRAM for ILP processor memory access latenc.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3MRLLL4U\\946676.html}
}

@inproceedings{stankovicTechniquesPerformanceImprovements2001,
  title = {Some Techniques for Performance Improvements of {{DRAMs}} in Multimedia Applications},
  volume = {2},
  doi = {10.1109/℡SKS.2001.955891},
  abstract = {DRAM memory performance is critical, factor in many multimedia applications. Some techniques, which improve DRAM memory performance, are proposed in this paper. These are, first, combined strategies of opening and closing DRAM pages, and second, address remapping in DRAM memory referencing. Simulations we have done showed some improvements in latency of memory references. Implied requirements on DRAM controller are also discussed.},
  eventtitle = {5th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}. ℡{{SIKS}} 2001. {{Proceedings}} of {{Papers}} ({{Cat}}. {{No}}.{{01EX517}})},
  booktitle = {5th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}. ℡{{SIKS}} 2001. {{Proceedings}} of {{Papers}} ({{Cat}}. {{No}}.{{01EX517}})},
  date = {2001-09},
  pages = {794-797 vol.2},
  keywords = {Bandwidth,DRAM chips,Random access memory,Microprocessors,storage allocation,Clocks,memory architecture,DRAM controller,Delay,multimedia systems,SDRAM,Digital signal processing,latency,synchronous DRAM,multimedia applications,Data structures,address remapping,closing pages,combined strategies,DC generators,DRAM memory performance,Image processing,memory referencing,multiple concurrent transactions,opening pages,performance improvement techniques,row buffers,SimpleScalar Tool Set},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BVRE73SK\\Stankovic and Milenkovic - 2001 - Some techniques for performance improvements of DR.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JXMQTIFM\\955891.html}
}

@inproceedings{rixnerMemoryAccessScheduling2000,
  title = {Memory Access Scheduling},
  doi = {10.1145/339647.339668},
  abstract = {The bandwidth and latency of a memory system are strongly dependent on the manner in which accesses interact with the "3-D" structure of banks, rows, and columns characteristic of contemporary DRAM chips. There is nearly an order of magnitude difference in bandwidth between successive references to different columns within a row and different rows within a bank. This paper introduces memory access scheduling, a technique that improves the performance of a memory system by reordering memory references to exploit locality within the 3-D memory structure. Conservative reordering, in which the first ready reference in a sequence is performed, improves bandwidth by 40\% for traces from five media benchmarks. Aggressive reordering, in which operations are scheduled to optimize memory bandwidth, improves bandwidth by 93\% for the same set of applications. Memory access scheduling is particularly important for media processors where it enables the processor to make the most efficient use of scarce memory bandwidth.},
  eventtitle = {Proceedings of 27th {{International Symposium}} on {{Computer Architecture}} ({{IEEE Cat}}. {{No}}.{{RS00201}})},
  booktitle = {Proceedings of 27th {{International Symposium}} on {{Computer Architecture}} ({{IEEE Cat}}. {{No}}.{{RS00201}})},
  date = {2000-06},
  pages = {128-138},
  keywords = {Laboratories,Bandwidth,Processor scheduling,Out of order,DRAM chips,Random access memory,storage management,Delay,processor scheduling,memory access scheduling,latency,bandwidth,Streaming media,Arithmetic,memory system,Permission,Pipeline processing},
  author = {Rixner, S. and Dally, W. J. and Kapasi, U. J. and Mattson, P. and Owens, J. D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ATEENGHW\\Rixner et al. - 2000 - Memory access scheduling.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2EZMNPJD\\854384.html}
}

@inproceedings{yabeNextGenerationChanneledDRAM2000,
  title = {A next Generation Channeled-{{DRAM}} Architecture with Direct Background-Operation and Delayed Channel-Replacement Techniques},
  doi = {10.1109/VLSIC.2000.852864},
  abstract = {As processor performance is reaching the level of executing a single instruction in 1 ns, long memory latencies have become a critical problem, because a single memory access could stall the execution of hundreds of instructions. A recently announced channeled-DRAM approaches this problem by integrating a small low-latency buffer, called "channels", in front of a DRAM core in order to reduce the effective memory latency. Since the channels can provide intrinsically faster access than that of a bare DRAM core when they hit, key considerations in this architecture become (1) how to achieve high channel hit rates and (2) how to reduce the channel-miss latencies. Since channeled-DRAMs rely on an external memory controller to handle all the channel management, design of the memory controller heavily dominates the first issue. In this paper, we propose two novel techniques for reducing the channel-miss latencies: direct background operation and delayed channel replacement. We examined these techniques in a future 256-Mb DRAM with a 200-MHz double-data-rate (DDR) synchronous interface. Both SPICE simulation results (that show channel-miss latency reduction) and system-level simulation results (that reveal system-level performance improvement) are presented.},
  eventtitle = {2000 {{Symposium}} on {{VLSI Circuits}}. {{Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{00CH37103}})},
  booktitle = {2000 {{Symposium}} on {{VLSI Circuits}}. {{Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{00CH37103}})},
  date = {2000-06},
  pages = {108-111},
  keywords = {Laboratories,Bandwidth,Prefetching,Delays,DRAM chips,Random access memory,Memory management,Memory architecture,memory latency,Delay,memory access,Graphics,1 ns,200 MHz,256 Mbit,channel hit rates,channel management,channel-miss latencies,channeled-DRAM architecture,delayed channel replacement,direct background operation,double-data-rate synchronous interface,DRAM core,external memory controller,Large scale integration,low-latency buffer,National electric code,processor performance,Silicon,SPICE simulation,system-level performance improvement,system-level simulation},
  author = {Yabe, Y. and Nakamura, N. and Aimoto, Y. and Motomura, M. and Matsui, Y. and Adakura, Y.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TZKGEQH3\\Yabe et al. - 2000 - A next generation channeled-DRAM architecture with.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VG4NHJ46\\852864.html}
}

@inproceedings{cuppuPerformanceComparisonContemporary1999,
  title = {A Performance Comparison of Contemporary {{DRAM}} Architectures},
  doi = {10.1109/ISCA.1999.765953},
  abstract = {In response to the growing gap between memory access time and processor speed, DRAM manufacturers have created several new DRAM architectures. This paper presents a simulation-based performance study of a representative group, each evaluated in a small system organization. These small-system organizations correspond to workstation-class computers and use on the order of IO DRAM chips. The study covers Fast Page Mode, Extended Data Out, Synchronous, Enhanced Synchronous, Synchronous Link, Rambus, and Direct Rambus designs. Our simulations reveal several things: (a) current advanced DRAM technologies are attacking the memory bandwidth problem but not the latency problem; (b) bus transmission speed will soon become a primary factor limiting memory-system performance; (c) the post-12 address stream still contains significant locality, though it varies from application to application; and (d) as we move to wider buses, row access time becomes more prominent, making it important to investigate techniques to exploit the available locality to decrease access time.},
  eventtitle = {Proceedings of the 26th {{International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{99CB36367}})},
  booktitle = {Proceedings of the 26th {{International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{99CB36367}})},
  date = {1999-05},
  pages = {222-233},
  keywords = {Bandwidth,Costs,Out of order,DRAM chips,Random access memory,Computer architecture,Capacitors,performance evaluation,Delay,parallel architectures,Pins,contemporary DRAM architectures,digital simulation,Jacobian matrices,memory access time,memory-system performance,performance comparison,processor speed,simulation-based performance study,simulations,small-system organizations,Time measurement,workstation-class computers},
  author = {Cuppu, V. and Jacob, B. and Davis, B. and Mudge, T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\96RK28Z9\\Cuppu et al. - 1999 - A performance comparison of contemporary DRAM arch.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NKYR8DCJ\\765953.html}
}

@inproceedings{carterImpulseBuildingSmarter1999,
  title = {Impulse: Building a Smarter Memory Controller},
  doi = {10.1109/HPCA.1999.744334},
  shorttitle = {Impulse},
  abstract = {Impulse is a new memory system architecture that adds two important features to a traditional memory controller. First, Impulse supports application-specific optimizations through configurable physical address remapping. By remapping physical addresses, applications control how their data is accessed and cached, improving their cache and bus utilization. Second, Impulse supports prefetching at the memory controller, which can hide much of the latency of DRAM accesses. In this paper we describe the design of the Impulse architecture, and show how an Impulse memory system can be used to improve the performance of memory-bound programs. For the NAS conjugate gradient benchmark, Impulse improves performance by 67\%. Because it requires no modification to processor, cache, or bus designs, Impulse can be adopted in conventional systems. In addition to scientific applications, we expect that Impulse will benefit regularly strided memory-bound applications of commercial importance, such as database and multimedia programs.},
  eventtitle = {Proceedings {{Fifth International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  booktitle = {Proceedings {{Fifth International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  date = {1999-01},
  pages = {70-79},
  keywords = {Computer science,Bandwidth,performance,cache storage,Prefetching,Sparse matrices,Random access memory,Microprocessors,memory controller,memory architecture,data access,Delay,multimedia computing,application-specific optimization,bus design,cache design,Cities and towns,configurable physical address remapping,conjugate gradient methods,data caching,database management systems,database programs,Databases,DRAM access latency hiding,Electronic switching systems,Impulse memory system architecture,memory-bound program performance,multimedia programs,NAS conjugate gradient benchmark,prefetching,processor design,scientific applications},
  author = {Carter, J. and Hsieh, W. and Stoller, L. and Swanson, M. and {Lixin Zhang} and Brunvand, E. and Davis, A. and {Chen-Chi Kuo} and Kuramkote, R. and Parker, M. and Schaelicke, L. and Tateyama, T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YBBYXM6M\\Carter et al. - 1999 - Impulse building a smarter memory controller.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8HLH9ZUF\\744334.html}
}

@article{wielWhenCachesAren1997,
  title = {When Caches Aren't Enough: Data Prefetching Techniques},
  volume = {30},
  issn = {0018-9162},
  doi = {10.1109/2.596622},
  shorttitle = {When Caches Aren't Enough},
  abstract = {With data prefetching, memory systems call data into the cache before the processor needs it, thereby reducing memory-access latency. Using the most suitable techniques is critical to maximizing data prefetching's effectiveness. The authors review three popular prefetching techniques: software-initiated prefetching, sequential hardware-initiated prefetching, and prefetching via reference prediction tables.},
  number = {7},
  journaltitle = {Computer},
  date = {1997-07},
  pages = {23-30},
  keywords = {Hardware,cache storage,Prefetching,Processor scheduling,DRAM chips,cache,memory systems,Random access memory,memory architecture,Delay,Cache memory,Read-write memory,Application software,data prefetching techniques,Fabrication,memory-access latency,reference prediction tables,sequential hardware-initiated prefetching,software-initiated prefetching},
  author = {Wiel, S. P. Vander and Lilja, D. J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XS2UJFYW\\Wiel and Lilja - 1997 - When caches aren't enough data prefetching techni.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HGEQ8KMX\\596622.html}
}

@inproceedings{pattersonIntelligentRAMIRAM1997,
  title = {Intelligent {{RAM}} ({{IRAM}}): Chips That Remember and Compute},
  doi = {10.1109/ISSCC.1997.585348},
  shorttitle = {Intelligent {{RAM}} ({{IRAM}})},
  abstract = {It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.},
  eventtitle = {1997 {{IEEE International Solids}}-{{State Circuits Conference}}. {{Digest}} of {{Technical Papers}}},
  booktitle = {1997 {{IEEE International Solids}}-{{State Circuits Conference}}. {{Digest}} of {{Technical Papers}}},
  date = {1997-02},
  pages = {224-225},
  keywords = {Computer science,Bandwidth,microprocessor chips,Random access memory,Microprocessors,memory bandwidth,memory architecture,memory latency,Delay,random-access storage,Switches,Read-write memory,adjustable memory width,board space,chip yield,Electronics industry,high-speed interface,integrated circuit yield,integrated memory circuits,intelligent RAM,IRAM,Logic,memory retention rates,merged chip,metal layers,system power,Vector processors},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7W5BM63I\\Patterson et al. - 1997 - Intelligent RAM (IRAM) chips that remember and co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KI8AD45T\\585348.html}
}

@inproceedings{yamauchiHierarchicalMultibankDRAM1997,
  title = {The Hierarchical Multi-Bank {{DRAM}}: A High-Performance Architecture for Memory Integrated with Processors},
  doi = {10.1109/ARVLSI.1997.634862},
  shorttitle = {The Hierarchical Multi-Bank {{DRAM}}},
  abstract = {A microprocessor integrated with DRAM on the same die has the potential to improve system performance by reducing the memory latency and improving the memory bandwidth. However a high performance microprocessor will typically send more accesses than the DRAM can handle due to the long cycle time of the embedded DRAM, especially in applications with significant memory requirements. A multi-bank DRAM can hide the long cycle time by allowing the DRAM to process multiple accesses in parallel, but it will incur a significant area penalty and will therefore restrict the density of the embedded DRAM main memory. In this paper we propose a hierarchical multi-bank DRAM architecture to achieve high system performance with a minimal area penalty. In this architecture, the independent memory banks are each divided into many semi-independent subbanks that share I/O and decoder resources. A hierarchical multi-bank DRAM with 4 main banks each composed of 32 subbanks occupies approximately the same area as a conventional 4 bank DRAM while performing like a 32 bank one-up to 65\% better than a conventional 4 bank DRAM when integrated with a single-chip multiprocessor.},
  eventtitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  booktitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  date = {1997-09},
  pages = {303-319},
  keywords = {Laboratories,Bandwidth,microprocessor chips,DRAM chips,Random access memory,Computer architecture,Microprocessors,memory architecture,Memory architecture,System performance,Delay,High performance computing,decoder resources,embedded DRAM,hierarchical multi-bank DRAM,high-performance architecture,microprocessor,multiple accesses,processor integrated memory,Ultra large scale integration},
  author = {Yamauchi, T. and Hammond, L. and Olukotun, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9JSHHHNP\\Yamauchi et al. - 1997 - The hierarchical multi-bank DRAM a high-performan.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\P6T3597Y\\634862.html}
}

@inproceedings{veidenbaumDecoupledAccessDRAM1997,
  title = {Decoupled Access {{DRAM}} Architecture},
  doi = {10.1109/IWIA.1997.670415},
  abstract = {This paper discusses an approach to reducing memory latency in future systems. It focuses on systems where a single chip DRAM/processor will not be feasible even in 10 years, e.g. systems requiring a large memory and/or many CPU's. In such systems a solution needs to be found to DRAM latency and bandwidth as well as to inter-chip communication. Utilizing the projected advances in chip I/O bandwidth we propose to implement a decoupled access-execute processor where the access processor is placed in memory. A program is compiled to run as a computational process and several access processes with the latter executing in the DRAM processors. Instruction set extensions are discussed to support this paradigm. Using multi-level branch prediction the access processor stays ahead of the execute processor and keeps the latter supplied with data. The system reduces latency by moving address computation to memory and thus avoiding sending address to memory by the computational processor. This and the fetch-ahead capabilities of the access processor are combined with multiple DRAM "streaming" to improve performance. DRAM caching is assumed to be used to assist in this as well.},
  eventtitle = {Proceedings {{Innovative Architecture}} for {{Future Generation High}}-{{Performance Processors}} and {{Systems}}},
  booktitle = {Proceedings {{Innovative Architecture}} for {{Future Generation High}}-{{Performance Processors}} and {{Systems}}},
  date = {1997-10},
  pages = {94-103},
  keywords = {Computer science,Bandwidth,Very large scale integration,Random access memory,Computer architecture,Clocks,memory architecture,Delay,parallel architectures,access processor,Access protocols,chip I/O bandwidth,Circuit noise,Circuit synthesis,decoupled access-execute processor,DRAM caching,execute processor,multi-level branch prediction,multiple DRAM,reducing memory latency,single chip DRAM},
  author = {Veidenbaum, A. V. and Gallivan, K. A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\A9ZXBAFX\\Veidenbaum and Gallivan - 1997 - Decoupled access DRAM architecture.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XGK3SY5D\\670415.html}
}

@inproceedings{nowatzykMissingMemoryWall1996,
  title = {Missing the {{Memory Wall}}: {{The Case}} for {{Processor}}/{{Memory Integration}}},
  doi = {10.1109/ISCA.1996.10008},
  shorttitle = {Missing the {{Memory Wall}}},
  abstract = {Current high performance computer systems use complex, large superscalar CPUs that interface to the main memory through a hierarchy of caches and interconnect systems. These CPU-centric designs invest a lot of power and chip area to bridge the widening gap between CPU and main memory speeds. Yet, many large applications do not operate well on these systems and are limited by the memory subsystem performance.This paper argues for an integrated system approach that uses less-powerful CPUs that are tightly integrated with advanced memory technologies to build competitive systems with greatly reduced cost and complexity. Based on a design study using the next generation 0.25µm, 256Mbit dynamic random-access memory (DRAM) process and on the analysis of existing machines, we show that processor memory integration can be used to build competitive, scalable and cost-effective MP systems.We present results from execution driven uni- and multi-processor simulations showing that the benefits of lower latency and higher bandwidth can compensate for the restrictions on the size and complexity of the integrated processor. In this system, small direct mapped instruction caches with long lines are very effective, as are column buffer data caches augmented with a victim cache.},
  eventtitle = {23rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}}'96)},
  booktitle = {23rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}}'96)},
  date = {1996-05},
  pages = {90-90},
  keywords = {Bridges,Random access memory,Delay,High performance computing,Application software,Analytical models,backward error recovery,coherence protocol,Computer interfaces,Cost benefit analysis,fault-tolerance,Paper technology,Power system interconnection,Scalable Shared Memory Multiprocessors},
  author = {Nowatzyk, A. and {Fong Pong} and Saulsbury, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\39SLDQLP\\Nowatzyk et al. - 1996 - Missing the Memory Wall The Case for ProcessorMe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TB95MCA6\\1563038.html}
}

@inproceedings{alexanderDistributedPrefetchbufferCache1996,
  title = {Distributed Prefetch-Buffer/Cache Design for High Performance Memory Systems},
  doi = {10.1109/HPCA.1996.501191},
  abstract = {Microprocessor execution speeds are improving at a rate of 50\%-80\% per year while DRAM access times are improving at a much lower rate of 5\%-10\% per year. Computer systems are rapidly approaching the point at which overall system performance is determined not by the speed of the CPU but by the memory system speed. We present a high performance memory system architecture that overcomes the growing speed disparity between high performance microprocessors and current generation DRAMs. A novel prediction and prefetching technique is combined with a distributed cache architecture to build a high performance memory system. We use a table based prediction scheme with a prediction cache to prefetch data from the on-chip DRAM array to an on-chip SRAM prefetch buffer. By prefetching data we are able to hide the large latency associated with DRAM access and cycle times. Our experiments show that with a small (32 KB) prediction cache we can get an effective main memory access time that is close to the access time of larger secondary caches.},
  eventtitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  booktitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  date = {1996-02},
  pages = {254-263},
  keywords = {Computer science,Hardware,Bandwidth,cache storage,Prefetching,Random access memory,Microprocessors,Clocks,memory architecture,System performance,Delay,cache design,distributed cache architecture,distributed prefetch buffer design,DRAM access times,Gears,high performance memory systems,memory system architecture,memory system speed,prefetching technique,SRAM,table based prediction scheme},
  author = {Alexander, T. and Kedem, G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DUGPXIEJ\\Alexander and Kedem - 1996 - Distributed prefetch-buffercache design for high .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2W8RQXMC\\501191.html}
}

@article{bolandPredictingPrecludingProblems1994,
  title = {Predicting and Precluding Problems with Memory Latency},
  volume = {14},
  issn = {0272-1732},
  doi = {10.1109/40.296166},
  abstract = {By examining the rate at which successive generations of processor and DRAM cycle times have been diverging over time, we can track the latency problem of computer memory systems. Our research survey starts with the fundamentals of single-level caches and moves to the need for multilevel cache hierarchies. We look at some of the current techniques for boosting cache performance, especially compiler-based methods for code restructuring and instruction and data prefetching. These two areas will likely yield improvements for a much larger domain of applications in the future.{$<>$}},
  number = {4},
  journaltitle = {IEEE Micro},
  date = {1994-08},
  pages = {59-67},
  keywords = {Prefetching,DRAM chips,Random access memory,Microprocessors,buffer storage,Clocks,memory architecture,memory latency,Delay,instruction prefetching,Throughput,data prefetching,Application software,Boosting,code restructuring,compiler-based methods,Content addressable storage,DRAM cycle times,Microcomputers,multilevel cache hierarchies,single-level caches},
  author = {Boland, K. and Dollas, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3R5U6GDQ\\Boland and Dollas - 1994 - Predicting and precluding problems with memory lat.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JA6WJ3MG\\296166.html}
}

@inproceedings{gustafssonMalardalenWCETBenchmarks2010,
  location = {{Brussels, Belgium}},
  title = {The {{Mälardalen WCET Benchmarks}} -- {{Past}}, {{Present}} and {{Future}}},
  eventtitle = {10th {{International Workshop}} on {{Worst}}-{{Case Execution Time Analysis}} ({{WCET}}'2010)},
  booktitle = {Proceedings 10th {{International Workshop}} on {{Worst}}-{{Case Execution Time Analysis}} ({{WCET}}'2010)},
  publisher = {{OCG}},
  date = {2010-07},
  pages = {137-147},
  author = {Gustafsson, Jan and Betts, Adam and Ermedahl, Andreas and Lisper, Björn},
  editor = {Lisper, Björn}
}

@misc{watermanRISCVInstructionSet2019,
  langid = {english},
  title = {The {{RISC}}-{{V Instruction Set Manual}}},
  url = {https://riscv.org/specifications/},
  publisher = {{RISC-V Foundation}},
  urldate = {2019-07-26},
  date = {2019-06-08},
  editor = {Waterman, Andrew and Asanovic, Krste},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H9R784BP\\Waterman et al. - Volume I Unprivileged ISA.pdf}
}

@article{gautschiNearThresholdRISCVCore2017,
  langid = {english},
  title = {Near-{{Threshold RISC}}-{{V Core With DSP Extensions}} for {{Scalable IoT Endpoint Devices}}},
  volume = {25},
  issn = {1063-8210, 1557-9999},
  url = {http://ieeexplore.ieee.org/document/7864441/},
  doi = {10.1109/TVLSI.2017.2654506},
  abstract = {Endpoint devices for Internet-of-Things not only need to work under extremely tight power envelope of a few milliwatts, but also need to be ﬂexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold (NT) operation can achieve higher energy efﬁciency, and the performance scalability can be gained through parallelism. In this paper, we describe the design of an open-source RISC-V processor core speciﬁcally designed for NT operation in tightly coupled multicore clusters. We introduce instruction extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure toward the shared-memory hierarchy. For typical data-intensive sensor processing workloads, the proposed core is, on average, 3.5× faster and 3.2× more energy efﬁcient, thanks to a smart L0 buffer to reduce cache access contentions and support for compressed instructions. Single Instruction Multiple Data extensions, such as dot products, and a built-in L0 storage further reduce the shared-memory accesses by 8× reducing contentions by 3.2×. With four NT-optimized cores, the cluster is operational from 0.6 to 1.2 V, achieving a peak efﬁciency of 67 MOPS/mW in a low-cost 65-nm bulk CMOS technology. In a low-power 28-nm FD-SOI process, a peak efﬁciency of 193 MOPS/mW (40 MHz and 1 mW) can be achieved.},
  number = {10},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  urldate = {2019-07-26},
  date = {2017-10},
  pages = {2700-2713},
  author = {Gautschi, Michael and Schiavone, Pasquale Davide and Traber, Andreas and Loi, Igor and Pullini, Antonio and Rossi, Davide and Flamand, Eric and Gurkaynak, Frank K. and Benini, Luca},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5TKEB9LZ\\Gautschi et al. - 2017 - Near-Threshold RISC-V Core With DSP Extensions for.pdf}
}

@online{OpenSourceHardware,
  title = {Open {{Source Hardware Association}} - {{Homepage}}},
  url = {https://www.oshwa.org/},
  journaltitle = {Open Source Hardware Association},
  urldate = {2019-07-25}
}

@article{ponugotiEnablingOntheFlyHardware2019,
  langid = {english},
  title = {Enabling {{On}}-the-{{Fly Hardware Tracing}} of {{Data Reads}} in {{Multicores}}},
  volume = {18},
  issn = {15399087},
  url = {http://dl.acm.org/citation.cfm?doid=3340300.3322642},
  doi = {10.1145/3322642},
  number = {4},
  journaltitle = {ACM Transactions on Embedded Computing Systems},
  urldate = {2019-07-25},
  date = {2019-06-10},
  pages = {1-27},
  author = {Ponugoti, Mounika and Milenkovic, Aleksandar},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4E6A5JFS\\Ponugoti and Milenkovic - 2019 - Enabling On-the-Fly Hardware Tracing of Data Reads.pdf}
}

@misc{NiosIIProcessor2019,
  title = {Nios {{II Processor Reference Guide}}},
  publisher = {{Intel}},
  date = {2019-07-01},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G9PF6HA3\\Nios II Processor Reference Guide.pdf}
}

@misc{CoreSightBaseSystem18,
  langid = {english},
  title = {{{CoreSight Base System Architecture}}},
  publisher = {{ARM}},
  date = {0018-07-23},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JN5U2XCE\\CoreSight Base System Architecture.pdf}
}

@article{leeWhenPrefetchingWorks2012,
  langid = {english},
  title = {When {{Prefetching Works}}, {{When It Doesn}}’t, and {{Why}}},
  volume = {9},
  issn = {15443566},
  url = {http://dl.acm.org/citation.cfm?doid=2133382.2133384},
  doi = {10.1145/2133382.2133384},
  number = {1},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  urldate = {2019-07-25},
  date = {2012-03-01},
  pages = {1-29},
  author = {Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HCFV4W26\\Lee et al. - 2012 - When Prefetching Works, When It Doesn’t, and Why.pdf}
}

@inproceedings{ozawaCacheMissHeuristics1995,
  title = {Cache Miss Heuristics and Preloading Techniques for General-Purpose Programs},
  doi = {10.1109/MICRO.1995.476832},
  abstract = {Previous research on hiding memory latencies has tended to focus on regular numerical programs. This paper presents a latency-hiding compiler technique that is applicable to general-purpose C programs. By assuming a lock-up free cache and instruction score-boarding, our technique 'preloads' the data that are likely to cause a cache-miss before they are used, and thereby hiding the cache miss latency. We have developed simple compiler heuristics to identify load instructions that are likely to cause a cache-miss. Experimentation with a set of SPEC92 benchmarks shows that our heuristics are successful in identifying 85\% of cache misses. We have also developed an algorithm that flexibly schedules the selected load instruction and instructions that use the loaded data to hide memory latency. Our simulation suggests that our technique is successful in hiding memory latency and improves the overall performance.},
  eventtitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  booktitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  date = {1995-11},
  pages = {243-248},
  keywords = {Laboratories,Program processors,performance,cache storage,Processor scheduling,Scheduling algorithm,Computational modeling,cache miss,storage management,Delay,C language,C programs,compiler,compiler heuristics,Computer simulation,general-purpose programs,instruction score-boarding,latency-hiding,Load flow analysis,load instructions,lock-up free cache,preloading,program compilers,SPEC92 benchmarks,Testing},
  author = {Ozawa, T. and Kimura, Y. and Nishizaki, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LMZYV6S3\\Ozawa et al. - 1995 - Cache miss heuristics and preloading techniques fo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LKPBV58X\\476832.html}
}

@inproceedings{limayeWorkloadCharacterizationSPEC2018,
  location = {{Belfast}},
  title = {A {{Workload Characterization}} of the {{SPEC CPU2017 Benchmark Suite}}},
  isbn = {978-1-5386-5010-3},
  url = {https://ieeexplore.ieee.org/document/8366949/},
  doi = {10.1109/ISPASS.2018.00028},
  eventtitle = {2018 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  booktitle = {2018 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  publisher = {{IEEE}},
  urldate = {2019-07-25},
  date = {2018-04},
  pages = {149-158},
  author = {Limaye, Ankur and Adegbija, Tosiron},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TJR2M8LS\\Limaye and Adegbija - 2018 - A Workload Characterization of the SPEC CPU2017 Be.pdf}
}

@inproceedings{bieniaPARSECBenchmarkSuite2008,
  langid = {english},
  location = {{Toronto, Ontario, Canada}},
  title = {The {{PARSEC}} Benchmark Suite: Characterization and Architectural Implications},
  isbn = {978-1-60558-282-5},
  url = {http://portal.acm.org/citation.cfm?doid=1454115.1454128},
  doi = {10.1145/1454115.1454128},
  shorttitle = {The {{PARSEC}} Benchmark Suite},
  abstract = {This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on highperformance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip trafﬁc. The benchmark suite has been made available to the public.},
  eventtitle = {The 17th International Conference},
  booktitle = {Proceedings of the 17th International Conference on {{Parallel}} Architectures and Compilation Techniques - {{PACT}} '08},
  publisher = {{ACM Press}},
  urldate = {2019-07-25},
  date = {2008},
  pages = {72},
  author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\77XP977J\\Bienia et al. - 2008 - The PARSEC benchmark suite characterization and a.pdf}
}

@patent{lewchukPrefetchingDataUsing2000,
  title = {Prefetching Data Using Profile of Cache Misses from Earlier Code Executions},
  url = {https://patents.google.com/patent/US6047363A/en},
  number = {6047363A},
  type = {patentus},
  urldate = {2019-07-24},
  date = {2000-04-04},
  keywords = {cache,addresses,code sequence,miss,profile},
  author = {Lewchuk, W. Kurt},
  holder = {{Advanced Micro Devices Inc}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YFSUN2LR\\Lewchuk - 2000 - Prefetching data using profile of cache misses fro.pdf}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004,
  langid = {english},
  location = {{Huntsville, Alabama}},
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  isbn = {978-1-58113-870-2},
  url = {http://portal.acm.org/citation.cfm?doid=986537.986601},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  eventtitle = {The 42nd Annual {{Southeast}} Regional Conference},
  booktitle = {Proceedings of the 42nd Annual {{Southeast}} Regional Conference on   - {{ACM}}-{{SE}} 42},
  publisher = {{ACM Press}},
  urldate = {2019-07-24},
  date = {2004},
  pages = {267},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9A3TBW67\\Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf}
}

@article{pattersonCaseIntelligentRAM1997,
  title = {A Case for Intelligent {{RAM}}},
  volume = {17},
  issn = {02721732},
  url = {http://ieeexplore.ieee.org/document/592312/},
  doi = {10.1109/40.592312},
  number = {2},
  journaltitle = {IEEE Micro},
  urldate = {2019-07-22},
  date = {1997},
  pages = {34-44},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WMAV5QJG\\Patterson et al. - 1997 - A case for intelligent RAM.pdf}
}

@article{furiaModelingTimeComputing2010,
  langid = {english},
  title = {Modeling Time in Computing: {{A}} Taxonomy and a Comparative Survey},
  volume = {42},
  issn = {03600300},
  url = {http://portal.acm.org/citation.cfm?doid=1667062.1667063},
  doi = {10.1145/1667062.1667063},
  shorttitle = {Modeling Time in Computing},
  number = {2},
  journaltitle = {ACM Computing Surveys},
  urldate = {2019-07-24},
  date = {2010-02-01},
  pages = {1-59},
  author = {Furia, Carlo A. and Mandrioli, Dino and Morzenti, Angelo and Rossi, Matteo},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UWY2P8D5\\Furia et al. - 2010 - Modeling time in computing A taxonomy and a compa.pdf}
}

@article{whithamTimePredictableOutofOrderExecution2010,
  langid = {english},
  title = {Time-{{Predictable Out}}-of-{{Order Execution}} for {{Hard Real}}-{{Time Systems}}},
  volume = {59},
  issn = {0018-9340},
  url = {http://ieeexplore.ieee.org/document/5467051/},
  doi = {10.1109/TC.2010.109},
  abstract = {Superscalar out-of-order CPU designs can achieve higher performance than simpler in-order designs through exploitation of instruction-level parallelism in software. However, these CPU designs are often considered to be unsuitable for hard real-time systems because of the difficulty of guaranteeing the worst-case execution time (WCET) of software. This paper proposes and evaluates modifications for a superscalar out-of-order CPU core to allow instruction-level parallelism to be exploited without sacrificing time predictability and support for WCET analysis. Experiments using the M5 O3 CPU simulator show that WCETs can be two-four times smaller than those obtained using an idealized in-order CPU design, as instruction-level parallelism is exploited without compromising timing safety.},
  number = {9},
  journaltitle = {IEEE Transactions on Computers},
  urldate = {2019-07-23},
  date = {2010-09},
  pages = {1210-1223},
  author = {Whitham, Jack and Audsley, Neil},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\56ZI6LUR\\Whitham and Audsley - 2010 - Time-Predictable Out-of-Order Execution for Hard R.pdf}
}

@article{wilkesSlaveMemoriesDynamic1965,
  langid = {english},
  title = {Slave {{Memories}} and {{Dynamic Storage Allocation}}},
  volume = {EC-14},
  issn = {0367-7508},
  url = {http://ieeexplore.ieee.org/document/4038419/},
  doi = {10.1109/PGEC.1965.264263},
  number = {2},
  journaltitle = {IEEE Transactions on Electronic Computers},
  urldate = {2019-07-22},
  date = {1965-04},
  pages = {270-271},
  author = {Wilkes, M. V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SERKATC8\\Wilkes - 1965 - Slave Memories and Dynamic Storage Allocation.pdf}
}

@book{przybylskiCacheMemoryHierarchy1990,
  langid = {english},
  location = {{San Mateo, Calif}},
  title = {Cache and Memory Hierarchy Design: A Performance-Directed Approach},
  isbn = {978-1-55860-136-9},
  shorttitle = {Cache and Memory Hierarchy Design},
  pagetotal = {223},
  publisher = {{Morgan Kaufmann Publishers}},
  date = {1990},
  keywords = {Cache memory,Memory hierarchy (Computer science)},
  author = {Przybylski, Steven A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GJ4LFRQ6\\Przybylski - 1990 - Cache and memory hierarchy design a performance-d.pdf}
}

@book{hennessyComputerArchitectureQuantitative2019,
  location = {{Cambridge, MA}},
  title = {Computer {{Architecture}}: A {{Quantitative Approach}}},
  edition = {6th},
  isbn = {978-0-12-811905-1},
  shorttitle = {Computer Architecture},
  pagetotal = {1},
  publisher = {{Morgan Kaufmann Publishers}},
  date = {2019},
  keywords = {Computer architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JV6VPGZ7\\Hennessy - Computer Architecture A Quantitative Approach.pdf}
}

@article{wilkesMemoryGapFuture2001,
  langid = {english},
  title = {The Memory Gap and the Future of High Performance Memories},
  volume = {29},
  issn = {01635964},
  url = {http://portal.acm.org/citation.cfm?doid=373574.373576},
  doi = {10.1145/373574.373576},
  number = {1},
  journaltitle = {ACM SIGARCH Computer Architecture News},
  urldate = {2019-07-22},
  date = {2001-03-01},
  pages = {2-7},
  author = {Wilkes, Maurice V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\85ND6GEF\\Wilkes - 2001 - The memory gap and the future of high performance .pdf}
}

@inproceedings{deutschMassiveSignalTracing2014,
  title = {Massive Signal Tracing Using On-Chip {{DRAM}} for in-System Silicon Debug},
  doi = {10.1109/TEST.2014.7035363},
  abstract = {Silicon debug is a major challenge due to continuously increasing design complexity. Traditional debug methods using signal tracing suffer from the limited capacity of on-chip trace buffers that only allow for signal observation during a short time window. We propose a low-cost debug architecture for massive signal tracing in ICs that integrate fast DRAM, such as 2D-ICs with embedded DRAM or 3D-stacked ICs with wide-I/O DRAM dies. The key idea is to use available on-chip DRAM for trace-data storage, which results in a significant increase of the observation window compared to traditional methods that use trace buffers. During a debug session, the entire observation window is divided into intervals and a signature is calculated for each observed interval using a multiple-input signature register. At run time, intervals containing erroneous bits are identified by comparing their signature with pre-calculated “golden” signatures that are stored in the DRAM a priori. Only failing intervals including their time stamp are stored into DRAM, which allows for a more efficient use of the memory, resulting in a larger observation window. The proposed method does not require multiple iterations or intermediate processing steps, hence it can be used during functional testing with minimum time overhead associated with the upload of golden signatures and the download of stored debug data to external equipment. We have created a Verilog RTL model for the proposed architecture, synthesized it using a 45 nm CMOS library, and verified its functionality by simulation. The results show that the observation window can be increased by orders of magnitude compared to prior work at comparable hardware cost.},
  eventtitle = {2014 {{International Test Conference}}},
  booktitle = {2014 {{International Test Conference}}},
  date = {2014-10},
  pages = {1-10},
  keywords = {Registers,Bandwidth,DRAM chips,Random access memory,Clocks,System-on-chip,Silicon,2D-IC,3D-stacked IC,buffer circuits,Buffer storage,CMOS,CMOS integrated circuits,elemental semiconductors,hardware description languages,in-system silicon debug,logic testing,multiple-input signature register,on-chip DRAM,on-chip trace buffers t,Si,signal tracing,silicon,size 45 nm,trace-data storage,Verilog RTL model},
  author = {Deutsch, S. and Chakrabarty, K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDASUK38\\Deutsch and Chakrabarty - 2014 - Massive signal tracing using on-chip DRAM for in-s.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZGZSZ7T\\7035363.html}
}

@inproceedings{huApplicationsOnchipTrace2007,
  title = {Applications of {{On}}-Chip {{Trace}} on {{Debugging Embedded Processor}}},
  volume = {1},
  doi = {10.1109/SNPD.2007.227},
  abstract = {On-chip trace system records run-time information of the embedded processor with special hardware to overcome the obstacle of non-intrusive debug and optimization of traditional techniques. This paper explores the mechanism, characteristics and applications of on-chip trace technique with TraceDo, an on-chip trace system of a multi-core SoC. The functions and structures of TraceDo are introduced, the working process of path trace is explained and the trace applications with two cases of debug and optimization are also discussed.},
  eventtitle = {Eighth {{ACIS International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}}, and {{Parallel}}/{{Distributed Computing}} ({{SNPD}} 2007)},
  booktitle = {Eighth {{ACIS International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}}, and {{Parallel}}/{{Distributed Computing}} ({{SNPD}} 2007)},
  date = {2007-07},
  pages = {140-145},
  keywords = {embedded systems,Hardware,Software debugging,Embedded system,microprocessor chips,Network-on-a-chip,Real time systems,Software tools,System-on-a-chip,performance evaluation,embedded system,Application software,Concurrent computing,debugging embedded processor,Distributed computing,multicore SoC,on-chip trace system,software reliability,TraceDo},
  author = {Hu, X. and Chen, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QQYXBLJF\\Hu and Chen - 2007 - Applications of On-chip Trace on Debugging Embedde.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9PGPS2UP\\4287490.html}
}

@inproceedings{chamskiTracebasedRuntimeAnalysis2010,
  title = {Trace-Based Runtime Analysis of Embedded Real-Time Systems},
  abstract = {Execution tracing is one of the key techniques for analyzing and validating the operation of embedded products. After reviewing several approaches to the runtime behavior analysis of embedded systems, we present the experience gained in developing a range of high-bandwidth communications devices combining multiple wireless and wired link technologies. In particular, all cases studies are based on actual product development.},
  eventtitle = {Proceedings of the 17th {{International Conference Mixed Design}} of {{Integrated Circuits}} and {{Systems}} - {{MIXDES}} 2010},
  booktitle = {Proceedings of the 17th {{International Conference Mixed Design}} of {{Integrated Circuits}} and {{Systems}} - {{MIXDES}} 2010},
  date = {2010-06},
  pages = {117-120},
  keywords = {Linux,embedded systems,Hardware,Monitoring,Program processors,Kernel,code optimization,embedded devices,embedded product,embedded real time system,execution tracing,high bandwidth communication device,monitoring,multiple wireless technology,Probes,product development,profiling,real time,system monitoring,systems analysis,telecommunication links,trace based runtime analysis,tracing,wired link technology},
  author = {Chamski, Z. and Borzęcki, M. and Świercz, B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9B7DX9RL\\Chamski et al. - 2010 - Trace-based runtime analysis of embedded real-time.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RBTRBMBM\\5551285.html}
}

@article{milenkovicCachesPredictorsRealTime2011,
  title = {Caches and {{Predictors}} for {{Real}}-{{Time}}, {{Unobtrusive}}, and {{Cost}}-{{Effective Program Tracing}} in {{Embedded Systems}}},
  volume = {60},
  issn = {0018-9340},
  doi = {10.1109/TC.2010.146},
  abstract = {The increasing complexity of modern embedded computer systems makes software development and system verification the most critical steps in system development. To expedite verification and program debugging, chip manufacturers increasingly consider hardware infrastructure for program debugging and tracing, including logic to capture and filter traces, buffers to store traces, and a trace port through which the trace is read by the debug tools. In this paper, we introduce a new approach to capture and compress program execution traces in hardware. The proposed trace compressor encompasses two cost-effective structures, a stream descriptor cache, and a last stream predictor. Information about the program flow is translated into a sequence of hit and miss events in these structures, thus dramatically reducing the number of bits that need to be sent out of the chip. We evaluate the efficiency of the proposed mechanism by measuring the trace port bandwidth on a set of benchmark programs. Our mechanism requires only 0.15 bits/instruction/CPU on average on the trace port, which is a sixfold improvement over state-of-the-art commercial solutions. The trace compressor requires an on-chip area that is equivalent to one third of a 1 kilobyte cache and it allows for continual and unobtrusive program tracing in real time.},
  number = {7},
  journaltitle = {IEEE Transactions on Computers},
  date = {2011-07},
  pages = {992-1005},
  keywords = {embedded systems,Hardware,Bandwidth,Program processors,program debugging,program verification,cache storage,system-on-chip,Silicon,program diagnostics,buffers,chip manufacturer,Complexity theory,Compression technologies,cost-effective program tracing,data compression,debug tools,Debugging,Detectors,hardware infrastructure,last stream predictor,on-chip area,program execution trace capture,program execution trace compression,program flow information,real time and embedded systems,real-time program tracing,software development,stream descriptor cache,system development,system verification,testing and debugging,trace port bandwidth,tracing.,unobtrusive program tracing},
  author = {Milenkovic, A. and Uzelac, V. and Milenkovic, M. and Burtscher, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SAFXJA9L\\Milenkovic et al. - 2011 - Caches and Predictors for Real-Time, Unobtrusive, .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FGLE5U6P\\5487509.html}
}

@inproceedings{linExtractingMemorylevelParallelism2013,
  title = {Extracting Memory-Level Parallelism through Reconfigurable Hardware Traces},
  doi = {10.1109/ReConFig.2013.6732290},
  abstract = {This paper proposes a new FPGA-based embedded computer architecture, which focuses on how to construct an application-specific memory access network capable of extracting the maximum amount of memory-level parallelism on a per-application basis. Specifically, through performing dynamic memory analysis and utilizing the capabilities of modern FPGA devices: abundant distributed block RAMs and programmability, the proposed reconfigurable architecture synthesizes highly efficient accelerators that enable parallelized memory accesses, and therefore accomplish effective data orchestration by maximally extracting the target application's instruction, loop and memory-level parallelism. To validate our proposed architecture, we implemented a baseline embedded processor platform, a conventional CPU +accelerator with a centralized single memory, and a prototype based on Xilinx MicroBlaze technology. Our experimental results have shown that on average for 5 benchmark applications from SPEC2006 and MiBench [1], our proposed architecture achieves 8.6 times speedup compared to the baseline embedded processor platform and 1.7 times speedup compared to a conventional CPU+accelcrator platform. More interestingly, the proposed platform achieves more than 40\% reduction in energy-delay product compared to a conventional CPU+accelerator with a centralized memory.},
  eventtitle = {2013 {{International Conference}} on {{Reconfigurable Computing}} and {{FPGAs}} ({{ReConFig}})},
  booktitle = {2013 {{International Conference}} on {{Reconfigurable Computing}} and {{FPGAs}} ({{ReConFig}})},
  date = {2013-12},
  pages = {1-8},
  keywords = {embedded systems,Hardware,Field programmable gate arrays,microprocessor chips,Computer architecture,Parallel processing,memory architecture,energy-delay product,Performance evaluation,field programmable gate arrays,random-access storage,reconfigurable architectures,Acceleration,application-specific memory access network,baseline embedded processor platform,centralized memory,centralized single memory,Coherence,conventional CPU +accelerator,data orchestration,distributed block RAM,dynamic memory analysis,FPGA devices,FPGA-based embedded computer architecture,memory-level parallelism,MiBench,parallelized memory accesses,programmability,reconfigurable architecture,reconfigurable hardware traces,SPEC2006,Xilinx MicroBlaze technology},
  author = {Lin, M. and Cheng, S. and Wawrzynek, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EN68BP3W\\Lin et al. - 2013 - Extracting memory-level parallelism through reconf.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\86XMHZG2\\6732290.html}
}

@inproceedings{deckerOnlineAnalysisDebug2018,
  title = {Online Analysis of Debug Trace Data for Embedded Systems},
  doi = {10.23919/DATE.2018.8342124},
  abstract = {Modern multi-core Systems-on-Chip (SoC) provide very high computational power. On the downside, they are hard to debug and it is often very difficult to understand what is going on in these chips because of the limited observability inside the SoC. Chip manufacturers try to compensate this difficulty by providing highly compressed trace data from the individual cores. In the past, the common way to deal with this data was storing it for later offline analysis, which severely limits the time span that can be observed. In this contribution, we present an FPGA-based solution that is able to process the trace data in real-time, enabling continuous observation of the state of a core. Moreover, we discuss applications enabled by this technology.},
  eventtitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  booktitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  date = {2018-03},
  pages = {851-856},
  keywords = {Instruments,embedded systems,Hardware,Monitoring,Runtime,Software,Bandwidth,Field programmable gate arrays,program debugging,SoC,multiprocessing systems,field programmable gate arrays,system-on-chip,Chip manufacturers,continuous observation,debug trace data,FPGA-based solution,high computational power,highly compressed trace data,individual cores,modern multicore Systems-on-Chip,offline analysis,online analysis,time span},
  author = {Decker, N. and Dreyer, B. and Gottschling, P. and Hochberger, C. and Lange, A. and Leucker, M. and Scheffel, T. and Wegener, S. and Weiss, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WWBGHZM7\\Decker et al. - 2018 - Online analysis of debug trace data for embedded s.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W5G8AMFU\\8342124.html}
}

@inproceedings{lamichhaneNonintrusiveProgramTracing2018,
  title = {Non-Intrusive Program Tracing of Non-Preemptive Multitasking Systems Using Power Consumption},
  doi = {10.23919/DATE.2018.8342184},
  abstract = {System tracing, runtime monitoring, execution reconstruction are useful techniques for protecting the safety and integrity of systems. Furthermore, with time-aware or overhead-aware techniques being available, these techniques can also be used to monitor and secure production systems. As operating systems gain in popularity, even in deeply embedded systems, these techniques face the challenge to support multitasking. In this paper, we propose a novel non-intrusive technique, which efficiently reconstructs the execution trace of non-preemptive multitasking system by observing power consumption characteristics. Our technique uses the control-flow graph (CFG) of the application program to identify the most likely block of code that the system is executing at any given point in time. For the purpose of the experimental evaluation, we first instrument the source code to obtain power consumption information for each basic block, which is used as the training data for our Dynamic Time Warping and k-Nearest Neighbours (k-NN) classifier. Once the system is trained, this technique is used to identify live code-block execution (LCBE). We show that the technique can reconstruct the execution flow of programs in a multi-tasking environment with high accuracy.},
  eventtitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  booktitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  date = {2018-03},
  pages = {1147-1150},
  keywords = {Multitasking,Instruments,embedded systems,Monitoring,Runtime,Embedded software,program debugging,Scheduling,multiprogramming,power consumption,program diagnostics,Debugging,CFG,control-flow graph,deeply embedded systems,Dynamic Time Warping,execution reconstruction,execution trace,k-Nearest Neighbours classifier,k-NN,LCBE,live code-block execution,multi-tasking environment,nearest neighbour methods,nonintrusive program tracing,nonpreemptive multitasking system,novel nonintrusive technique,Operating system,operating systems,operating systems (computers),overhead-aware techniques,pattern classification,power consumption characteristics,power consumption information,Power demand,Power tracing,production systems,runtime monitoring,Runtime monitoring,safety,security of data,source code (software),System tracing,Time series analysis,useful techniques},
  author = {Lamichhane, K. and Moreno, C. and Fischmeister, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LL32ETJ8\\Lamichhane et al. - 2018 - Non-intrusive program tracing of non-preemptive mu.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YKAJ3DWV\\8342184.html}
}

@inproceedings{wangRealTimeCache2013,
  title = {Real {{Time Cache Performance Analyzing}} for {{Multi}}-Core {{Parallel Programs}}},
  doi = {10.1109/CSC.2013.11},
  abstract = {Modern processors mostly use cache to hide the memory access latency, so cache performance is very important to application program. A detailed cache performance analysis will provide programmers a clear view of their program behaviors, which can help them to identify the performance bottleneck and to optimize the source code. As the chip industry turn to integrate multiple cores into one chip, multi-core/many-core processor becomes the new approach to maintain the Moor's Law. Therefore, Parallel programs turn to be more important even in the personal computers. In parallel programs, the interaction between tasks is the source of bugs and errors and is hard to handling for most of programmers. The detailed cache behaviors will greatly helpful to the programmer to find the errors and optimize the programs. However, the existing cache performance analysis tools, due to the limitations of the hardware performance counters they depend on to get data, cannot get as much data as we expected. Those tools cannot reveal the program routines characteristics on shared cache and the source of cache misses with limited metrics on cache misses. In this paper, we propose a method to obtain and analysis real time cache performance with binary instrumentation and cache emulation. We instrument the parallel program while it is running, and get the trace data about memory access. Then we transport the trace data to an carefully configured cache emulation module to get the detailed cache behavior information. The emulation module can not only get more information than hardware performance counter but also can be configured to simulate different target hardware environment. Additionally, we use the performance data to form a group of cache performance metrics which can intuitively help programmers to optimize their codes. The accuracy of this method is demonstrated by comparing the summary result with the hardware performance counter. Finally, we design an cache performance analysis tool named CC-Analyzer for parallel programs. Comparing with the existing technologies, CC-Analyzer is able to analyze the cause of cache misses and gather much more performance statistics when the parallel program is running on different cache architectures.},
  eventtitle = {2013 {{International Conference}} on {{Cloud}} and {{Service Computing}}},
  booktitle = {2013 {{International Conference}} on {{Cloud}} and {{Service Computing}}},
  date = {2013-11},
  pages = {16-23},
  keywords = {Instruments,Hardware,Radiation detectors,Program processors,cache storage,memory access latency,multiprocessing systems,Measurement,multicore processor,application program,binary instrumentation,cache architectures,cache behavior information,cache misses,cache performance analysis tools,cache performance metrics,CC-Analyzer,chip industry,configured cache emulation module,data tracing,detailed cache performance analysis,Emulation,hardware performance counter,limited metrics,many-core processor,modern processors,Moor's law,multicore parallel programs,multiple cores,parallel programming,Performance analysis,performance statistics,personal computers,program behaviors,program routines characteristics,real time cache performance,shared cache,source code},
  author = {Wang, R. and Gao, Y. and Zhang, G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DWRQN76V\\Wang et al. - 2013 - Real Time Cache Performance Analyzing for Multi-co.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6AZKHWY5\\6693173.html}
}

@article{uzelacUsingBranchPredictors2014,
  title = {Using {{Branch Predictors}} and {{Variable Encoding}} for {{On}}-the-{{Fly Program Tracing}}},
  volume = {63},
  issn = {0018-9340},
  doi = {10.1109/TC.2012.267},
  abstract = {Unobtrusive capturing of program execution traces in real-time is crucial for debugging many embedded systems. However, tracing even limited program segments is often cost-prohibitive, requiring wide trace ports and large on-chip trace buffers. This paper introduces a new cost-effective technique for capturing and compressing program execution traces on-the-fly. It relies on branch predictor-like structures in the trace module and corresponding software modules in the debugger to significantly reduce the number of events that need to be streamed out of the target system. Coupled with an effective variable encoding scheme that adapts to changing program patterns, our technique requires merely 0.029 bits per instruction of trace port bandwidth, providing a 34-fold improvement over the commercial state-of-the-art and a five-fold improvement over academic proposals, at the low cost of under 5,000 logic gates.},
  number = {4},
  journaltitle = {IEEE Transactions on Computers},
  date = {2014-04},
  pages = {1008-1020},
  keywords = {embedded systems,Hardware,Radiation detectors,Bandwidth,Program processors,program debugging,Real-time systems,program diagnostics,tracing,Compression technologies,Debugging,real time and embedded systems,testing and debugging,branch predictor-like structures,cost-effective technique,logic gates,on-the-fly program tracing,program execution traces,program patterns,software modules,variable encoding},
  author = {Uzelac, V. and Milenković, A. and Milenković, M. and Burtscher, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZW8AG3D4\\Uzelac et al. - 2014 - Using Branch Predictors and Variable Encoding for .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R2PAJVVR\\6342940.html}
}

@inproceedings{morenoNonintrusiveProgramTracing2013,
  location = {{New York, NY, USA}},
  title = {Non-Intrusive {{Program Tracing}} and {{Debugging}} of {{Deployed Embedded Systems Through Side}}-Channel {{Analysis}}},
  isbn = {978-1-4503-2085-6},
  url = {http://doi.acm.org/10.1145/2491899.2465570},
  doi = {10.1145/2491899.2465570},
  abstract = {One of the hardest aspects of embedded software development is that of debugging, especially when faulty behavior is observed at the production or deployment stage. Non-intrusive observation of the system's behavior is often insufficient to infer the cause of the problem and identify and fix the bug. In this work, we present a novel approach for non-intrusive program tracing aimed at assisting developers in the task of debugging embedded systems at deployment or production stage, where standard debugging tools are usually no longer available. The technique is rooted in cryptography, in particular the area of side-channel attacks. Our proposed technique expands the scope of these cryptographic techniques so that we recover the sequence of operations from power consumption observations (power traces). To this end, we use digital signal processing techniques (in particular, spectral analysis) combined with pattern recognition techniques to determine blocks of source code being executed given the observed power trace. One of the important highlights of our contribution is the fact that the system works on a standard PC, capturing the power traces through the recording input of the sound card. Experimental results are presented and confirm that the approach is viable.},
  booktitle = {Proceedings of the 14th {{ACM SIGPLAN}}/{{SIGBED Conference}} on {{Languages}}, {{Compilers}} and {{Tools}} for {{Embedded Systems}}},
  series = {{{LCTES}} '13},
  publisher = {{ACM}},
  urldate = {2019-01-30},
  date = {2013},
  pages = {77--88},
  keywords = {embedded systems,debugging,tracing,side-channel analysis,simple power analysis},
  author = {Moreno, Carlos and Fischmeister, Sebastian and Hasan, M. Anwar},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XVYA3LR2\\Moreno et al. - 2013 - Non-intrusive Program Tracing and Debugging of Dep.pdf}
}

@article{uzelacHardwareBasedLoadValue2013,
  title = {Hardware-{{Based Load Value Trace Filtering}} for {{On}}-the-{{Fly Debugging}}},
  volume = {12},
  issn = {1539-9087},
  url = {http://doi.acm.org/10.1145/2465787.2465799},
  doi = {10.1145/2465787.2465799},
  abstract = {Capturing program and data traces during program execution unobtrusively on-the-fly is crucial in debugging and testing of cyber-physical systems. However, tracing a complete program unobtrusively is often cost-prohibitive, requiring large on-chip trace buffers and wide trace ports. This article describes a new hardware-based load data value filtering technique called Cache First-access Tracking. Coupled with an effective variable encoding scheme, this technique achieves a significant reduction of load data value traces, from 5.86 to 56.39 times depending on the data cache size, thus enabling cost-effective, unobtrusive on-the-fly tracing and debugging.},
  issue = {2s},
  journaltitle = {ACM Trans. Embed. Comput. Syst.},
  urldate = {2019-01-30},
  date = {2013-05},
  pages = {97:1--97:18},
  keywords = {Debugging,variable encoding,load value filtering,program tracing,software debugger,trace compression,trace module},
  author = {Uzelac, Vladimir and Milenković, Aleksandar},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\3VWT5UG7\\Uzelac and Milenković - 2013 - Hardware-Based Load Value Trace Filtering for On-t.pdf}
}

@inproceedings{huImplementationOnChipDynamic2006,
  title = {Implementation of {{On}}-{{Chip Dynamic Trace}} for {{Microprocessors}}},
  doi = {10.1109/ICSICT.2006.306226},
  abstract = {This paper presents an on-chip dynamic trace design method which is very useful for the real-time dynamic trace of the program executing process of embedded microprocessors. We introduce an on-chip tracer (OCT) module to fulfill this dynamic tracing task by setting watchpoints at some locations of user program in order to trace and output the information on these spots. As a result, debugger can find out program errors and monitor or reconstruct the executing route of user program. This OCT module needs only a few registers rather than a special on-chip trace buffer (OCTB) which is an essential component for traditional trace methods. Finally, the results of simulation prove that the on-chip dynamic trace debugging is accomplishable with the help of this OCT module},
  eventtitle = {2006 8th {{International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology Proceedings}}},
  booktitle = {2006 8th {{International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology Proceedings}}},
  date = {2006-10},
  pages = {1429-1431},
  keywords = {Laboratories,embedded systems,Monitoring,Registers,microprocessor chips,program debugging,System-on-a-chip,Microprocessors,Debugging,Design methodology,dynamic tracing,embedded microprocessors,Information analysis,on-chip trace buffer,on-chip tracer,Power generation,Signal processing,user program},
  author = {Hu, Y. and Xiong, B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9FIZJEG8\\Hu and Xiong - 2006 - Implementation of On-Chip Dynamic Trace for Microp.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QI364PK\\4098430.html}
}

@inproceedings{penttinenRunTimeDebuggingMonitoring2006,
  title = {Run-{{Time Debugging}} and {{Monitoring}} of {{FPGA Circuits Using Embedded Microprocessor}}},
  doi = {10.1109/DDECS.2006.1649598},
  abstract = {Field programmable gate arrays (FPGAs) provide a fast and flexible hardware for embedded control systems and signal processing. Despite this, tracing and monitoring of internal signals is awkward. FPGA vendors provide their own tools to solve the debugging problems but they are not sufficient for real time monitoring. Instead, these signal tracing tools are good especially for tracing timing issues. This paper presents a method to monitor the internal signals of FPGA circuits by using an embedded microprocessor. The efficiency of this method is demonstrated with an FPGA-based active magnetic bearing control hardware},
  eventtitle = {2006 {{IEEE Design}} and {{Diagnostics}} of {{Electronic Circuits}} and Systems},
  booktitle = {2006 {{IEEE Design}} and {{Diagnostics}} of {{Electronic Circuits}} and Systems},
  date = {2006-04},
  pages = {147-148},
  keywords = {Hardware,Monitoring,Runtime,Field programmable gate arrays,microprocessor chips,Timing,Microprocessors,field programmable gate arrays,Circuits,Control systems,logic testing,Debugging,Array signal processing,automatic test equipment,embedded control systems,embedded microprocessor,FPGA circuits,run-time debugging,run-time monitoring,signal processing},
  author = {Penttinen, A. and Jastrzebski, R. and Pollanen, R. and Pyrhonen, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QLPTTALE\\Penttinen et al. - 2006 - Run-Time Debugging and Monitoring of FPGA Circuits.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5TAWRU5B\\1649598.html}
}

@inproceedings{cilkuImprovingPerformanceSinglePath2017,
  title = {Improving {{Performance}} of {{Single}}-{{Path Code}} through a {{Time}}-{{Predictable Memory Hierarchy}}},
  doi = {10.1109/ISORC.2017.17},
  abstract = {Deriving the Worst-Case Execution Time (WCET) of a task is a challenging process, especially for processor architectures that use caches, out-of-order pipelines, and speculative execution. Despite existing contributions to WCET analysis for these complex architectures, there are open problems. The single-path code generation overcomes these problems by generating time-predictable code that has a single execution trace. However, the simplicity of this approach comes at the cost of longer execution times. This paper addresses performance improvements for single-path code. We propose a time-predictable memory hierarchy with a prefetcher that exploits the predictability of execution traces in single-path code to speed up code execution. The new memory hierarchy reduces both the cache-miss penalty time and the cache-miss rate on the instruction cache. The benefit of the approach is demonstrated through benchmarks that are executed on an FPGA implementation.},
  eventtitle = {2017 {{IEEE}} 20th {{International Symposium}} on {{Real}}-{{Time Distributed Computing}} ({{ISORC}})},
  booktitle = {2017 {{IEEE}} 20th {{International Symposium}} on {{Real}}-{{Time Distributed Computing}} ({{ISORC}})},
  date = {2017-05},
  pages = {76-83},
  keywords = {Hardware,Pipelines,cache storage,Prefetching,Computer architecture,FPGA,Real-time systems,Process control,prefetching,program compilers,worst-case execution time,cache-miss penalty time,cache-miss rate,single-path code,single-path code generation,time-predictable memory hierarchy,Time-predictable memory hierarchy,WCET analysis},
  author = {Cilku, B. and Puffitsch, W. and Prokesch, D. and Schoeberl, M. and Puschner, P.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZXRZLXGQ\\Cilku et al. - 2017 - Improving Performance of Single-Path Code through .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R7Q7ARXP\\7964873.html}
}

@inproceedings{cilkuTimePredictableInstructionCacheArchitecture2015,
  location = {{Auckland, New Zealand}},
  title = {A {{Time}}-{{Predictable Instruction}}-{{Cache Architecture}} That {{Uses Prefetching}} and {{Cache Locking}}},
  isbn = {978-1-4673-7709-6},
  url = {http://ieeexplore.ieee.org/document/7160126/},
  doi = {10.1109/ISORCW.2015.58},
  eventtitle = {2015 {{IEEE International Symposium}} on {{Object}}/{{Component}}/{{Service}}-{{Oriented Real}}-{{Time Distributed Computing Workshops}} ({{ISORCW}})},
  booktitle = {2015 {{IEEE International Symposium}} on {{Object}}/{{Component}}/{{Service}}-{{Oriented Real}}-{{Time Distributed Computing Workshops}}},
  publisher = {{IEEE}},
  urldate = {2019-01-30},
  date = {2015-04},
  pages = {74-79},
  author = {Cilku, Bekim and Prokesch, Daniel and Puschner, Peter},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G8M4Y82U\\Cilku et al. - 2015 - A Time-Predictable Instruction-Cache Architecture .pdf}
}

@inproceedings{shoukryProactiveSchedulingContent2014,
  title = {Proactive Scheduling for Content Pre-Fetching in Mobile Networks},
  doi = {10.1109/ICC.2014.6883756},
  abstract = {The global adoption of smart phones has raised major concerns about a potential surge in the wireless traffic due to the excessive demand on multimedia services. This ever increasing demand is projected to cause significant congestions and degrade the quality of service for network users. In this paper, we develop a proactive caching framework that utilizes the predictability of the mobile user behavior to offload predictable traffic through the WiFi networks ahead of time. First, we formulate the proactive scheduling problem with the objective of maximizing the user-content hit ratio subject to constrains stemming from the user behavioral models. Second, we propose a quadratic-complexity (in the number of slots per day) greedy, yet, high performance heuristic algorithm that pinpoints the best download slot for each content item to attain maximal hit ratio. We confirm the merits of the proposed scheme based on the traces of a real dataset leveraging a large number of smart phone users who consistently utilized our framework for two months.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  booktitle = {2014 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  date = {2014-06},
  pages = {2848-2854},
  keywords = {Batteries,scheduling,storage management,quality of service,behavioral models,content pre-fetching,Content pre-fetching,Data models,greedy algorithms,high performance heuristic algorithm,IEEE 802.11 Standards,maximal hit ratio,Mobile communication,Mobile computing,mobile networks,mobile radio,mobile user behavior predictability,multimedia services,offload predictable traffic,proactive caching framework,proactive scheduling problem,quadratic-complexity,smart phone user traces,smart phones,Smart phones,telecommunication traffic,traffic offloading,WiFi networks,Wireless communication,wireless LAN,wireless traffic},
  author = {Shoukry, O. and ElMohsen, M. A. and Tadrous, J. and Gamal, H. E. and ElBatt, T. and Wanas, N. and Elnakieb, Y. and Khairy, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9KVCQZEL\\Shoukry et al. - 2014 - Proactive scheduling for content pre-fetching in m.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4WIAE3B5\\6883756.html}
}

@article{changPARCNovelOS2018,
  langid = {english},
  title = {{{PARC}}: {{A}} Novel {{OS}} Cache Manager},
  volume = {48},
  issn = {1097-024X},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2633},
  doi = {10.1002/spe.2633},
  shorttitle = {{{PARC}}},
  abstract = {To boost input-output performance, operating systems employ a kernel-managed caching space called the buffer cache or page cache. Given the limited size of a buffer cache, an effective cache manager is required to decide which blocks should be evicted from the cache. Previous cache managers use historical information to make replacement decisions. However, existing approaches are unable to maximize performance since they rely on limited historical information. Motivated by the limitations of existing solutions, this paper proposes a novel manager called the Pattern-assisted Adaptive Recency Caching (PARC) manager. PARC simultaneously uses the historical information of recency, frequency, and access patterns to estimate the locality strengths of blocks and, upon a cache miss, evicts the block with the least strength. Specifically, PARC exploits the reference regularities exhibited in past input-output behaviors to actively and rapidly adapt the recency and frequency information of blocks so as to precisely distinguish blocks with long- and short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache manager is similar, PARC always outperforms existing solutions.},
  number = {12},
  journaltitle = {Software: Practice and Experience},
  urldate = {2019-01-30},
  date = {2018},
  pages = {2193-2222},
  keywords = {operating systems,buffer cache,cache manager,page cache,replacement algorithms},
  author = {Chang, Hsung-Pin and Chiang, Cheng-Pang},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6AILZJZF\\Chang and Chiang - 2018 - PARC A novel OS cache manager.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7ZJDPIJI\\spe.html}
}

@inproceedings{changAdaptiveBufferCache2016,
  title = {An {{Adaptive Buffer Cache Management Scheme}}},
  doi = {10.1109/ICS.2016.0033},
  abstract = {Previous cache replacement algorithms utilize the access history information to make replacement decisions. However, they fail to deliver utmost performance since the history information exploited is incomplete. Motivated by the limitations of existing algorithms, this paper proposes a novel replacement scheme, called the Pattern-assisted Adaptive Recency Caching (PARC). PARC simultaneously utilizes the history information of recency, frequency, and access patterns to estimate the locality strength and to select the victim block. Specifically, PARC exploits the reference regularities exhibited in past behaviors, including looping or sequential references, to actively and rapidly adapt the recency and frequency information of blocks so as to exactly distill blocks with long-term utility from those with only short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache replacement algorithm is similar, PARC always outperforms the least recently used (LRU) scheme and other existing cache replacement algorithms.},
  eventtitle = {2016 {{International Computer Symposium}} ({{ICS}})},
  booktitle = {2016 {{International Computer Symposium}} ({{ICS}})},
  date = {2016-12},
  pages = {124-127},
  keywords = {Radiation detectors,cache storage,History,Prediction algorithms,Classification algorithms,buffer cache,page cache,replacement algorithms,adaptive buffer cache management scheme,cache replacement algorithm,cache replacement algorithms,Disk drives,file systems,memory management,Optimized production technology,pattern-assisted adaptive recency caching,random workloads,Robustness},
  author = {Chang, H. and Chiang, C. and Yu, Y.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4SBWTBUH\\Chang et al. - 2016 - An Adaptive Buffer Cache Management Scheme.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PL67HAC7\\7858455.html}
}

@inproceedings{ponugotiExploitingCacheCoherence2016,
  title = {Exploiting Cache Coherence for Effective On-the-Fly Data Tracing in Multicores},
  doi = {10.1109/ICCD.2016.7753295},
  abstract = {Software testing and debugging of modern embedded computer systems become increasingly a challenging task due to growing hardware and software complexity, increased integration and miniaturization, and ever tightening time-to-market. To find software bugs faster, developers often rely on on-chip trace and debug resources. However, these resources offer limited visibility of the system, increase the system cost, and do not scale well with a growing number of processor cores. This paper introduces a new hardware/software mechanism for capturing and filtering load data value traces in multicores that enables a complete reconstruction of a parallel program execution. The proposed mechanism exploits data caches and cache coherence protocol states to minimize the number of trace events that are necessary to stream out of the target platform to the software debugger. The mechanism relies on a single trace bit per data cache block, thus minimizing the cost of hardware implementation. Our experimental evaluation explores the effectiveness of the proposed technique by measuring the trace port bandwidth as a function of the cache size and the number of processor cores. The results show that the proposed mechanism significantly reduces the required trace port bandwidth when compared to the Nexus-like load data value tracing. Depending on data cache size, the improvements range from 9.9 to 23.5 times for single cores and from 18.6 to 37.3 times for octa cores.},
  eventtitle = {2016 {{IEEE}} 34th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  booktitle = {2016 {{IEEE}} 34th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  date = {2016-10},
  pages = {312-319},
  keywords = {Software,Bandwidth,Benchmark testing,Multicore processing,Nexus-like load data value tracing,on-chip trace,program testing,Real-time embedded systems,software complexity,software testing,program debugging,Clocks,Complexity theory,Debugging,trace port bandwidth,cache coherence protocol,cache size,Compression,data cache size,data handling,debug resources,Debugging aids,hardware implementation cost minimization,hardware-software mechanism,minimisation,on-the-fly data tracing,processor cores,software bugs,software debugging,software developers,trace event number minimization,Tracing},
  author = {Ponugoti, M. and Milenković, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SJ2T9ZVS\\Ponugoti and Milenković - 2016 - Exploiting cache coherence for effective on-the-fl.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZWPU8FQ3\\7753295.html}
}

@article{singhResourceThroughputAware2016,
  langid = {english},
  title = {Resource and {{Throughput Aware Execution Trace Analysis}} for {{Efficient Run}}-{{Time Mapping}} on {{MPSoCs}}},
  volume = {35},
  issn = {0278-0070, 1937-4151},
  url = {http://ieeexplore.ieee.org/document/7128364/},
  doi = {10.1109/TCAD.2015.2446938},
  abstract = {There have been several efforts on run-time mapping of applications on multiprocessor-systems-on-chip. These traditional efforts perform either on-the-ﬂy processing or use design-time analyzed results. However, on-the-ﬂy processing often leads to low-quality mappings, and design-time analysis becomes computationally costly for large-size problems and require huge storage for large number of applications. In this paper, we present a novel run-time mapping approach, where identiﬁcation of an efﬁcient mapping for a use-case is done by the online execution trace analysis of the active applications. The trace analysis facilitates for fast identiﬁcation of the mapping while optimizing for the system resource usage and throughput of the active applications, leading to reduced energy consumption as well. By rapidly identifying the efﬁcient mapping at run-time, the proposed approach overcomes the mappings’ exploration time bottleneck for large-size problems and their storage overhead problem when compared to the traditional approaches. Our experiments show that on average the exploration time to identify the mapping is reduced 14× when compared to stateof-the-art approaches and storage overhead is reduced by 92\%. Additionally, energy and resource savings are achieved along with identiﬁcation of high-quality mapping.},
  number = {1},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  urldate = {2019-01-28},
  date = {2016-01},
  pages = {72-85},
  author = {Singh, Amit Kumar and Shafique, Muhammad and Kumar, Akash and Henkel, Jorg},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\D2ED9PFG\\Singh et al. - 2016 - Resource and Throughput Aware Execution Trace Anal.pdf}
}

@inproceedings{liTracebasedAnalysisMethodology2016,
  langid = {english},
  title = {Trace-Based {{Analysis Methodology}} of {{Program Flash Contention}} in {{Embedded Multicore Systems}}},
  isbn = {978-3-9815370-7-9},
  url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7459304},
  doi = {10.3850/9783981537079_0442},
  abstract = {Contention for shared resources is a major performance issue in multicore systems. In embedded multicore microcontrollers, contentions of program ﬂash accesses have a signiﬁcant performance impact, because the ﬂash access has a large latency compared to a core clock cycle. Therefore, the detection and analysis of program ﬂash contentions are necessary to remedy this situation. With a lack of existing tools being able to fulﬁll this task, a novel post-processing analysis methodology is proposed in this paper to acquire the information of program ﬂash contentions in detail based on the non-intrusive trace. This information can be utilized to improve the overall performance and particularly to enhance the real-time performance of speciﬁc threads or functions for hard real-time multicore systems.},
  eventtitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  booktitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  publisher = {{Research Publishing Services}},
  urldate = {2019-01-28},
  date = {2016},
  pages = {199-204},
  author = {Li, Lin and Mayer, Albrecht},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QLMYXHQ\\Li and Mayer - 2016 - Trace-based Analysis Methodology of Program Flash .pdf}
}

@book{pattersonComputerOrganizationDesign2018,
  location = {{Cambridge, Massachusetts}},
  title = {Computer Organization and Design: The Hardware/Software Interface},
  edition = {RISC-V edition},
  isbn = {978-0-12-812275-4},
  shorttitle = {Computer Organization and Design},
  abstract = {The new RISC-V Edition of Computer Organization and Design features the RISC-V open source instruction set architecture, the first open source architecture designed to be used in modern computing environments such as cloud computing, mobile devices, and other embedded systems. With the post-PC era now upon us, Computer Organization and Design moves forward to explore this generational change with examples, exercises, and material highlighting the emergence of mobile computing and the Cloud. Updated content featuring tablet computers, Cloud infrastructure, and the x86 (cloud computing) and ARM (mobile computing devices) architectures is included},
  pagetotal = {565},
  publisher = {{Morgan Kaufmann Publishers, an imprint of Elsevier}},
  date = {2018},
  keywords = {Computer interfaces,Computer engineering,Computer organization},
  author = {Patterson, David A. and Hennessy, John L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4Q6FYX39\\Computer-Organization-and-Design-The-Hardware-Software-Interface-RISC-V-Edition-.pdf},
  note = {OCLC: ocn993666159}
}

@article{gortFormalanalysisbasedTraceComputation2012,
  title = {Formal-Analysis-Based Trace Computation for Post-Silicon Debug},
  volume = {20},
  doi = {10.1109/TVLSI.2011.2166416},
  abstract = {This paper presents a post-silicon debug methodology that provides a means to rewind, or backspace, a chip from a known crash state using a combination of on-chip real-time data collection and off-chip formal analysis methods. A complete debug flow is presented that considers practical considerations such as area, on-chip non-determinism and signal propagation delay. This flow, along with a low-overhead breakpoint circuit, allows for state-accurate breakpointing capabilities without the need to monitor the entire state of the chip. The flow and associated hardware was tested using a hardware prototype, which consists of an OpenRISC processor instrumented with the debug hardware connected to a PC running the formal verification algorithms. Traces hundreds of cycles long were obtained using the methodology presented in this paper. © 2011 IEEE.},
  number = {11},
  journaltitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  date = {2012},
  pages = {1997-2010},
  keywords = {Formal analysis,hardware breakpoint,post-silicon debug,silicon debug,validation},
  author = {Gort, M. and De, Paula and Kuan, J.J.W. and Aamodt, T.M. and Hu, A.J. and Wilton, S.J.E. and Yang, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EUFIZGBK\\display.html}
}

@inproceedings{caoPostSiliconTraceAnalysis2017,
  title = {A {{Post}}-{{Silicon Trace Analysis Approach}} for {{System}}-on-{{Chip Protocol Debug}}},
  doi = {10.1109/ICCD.2017.35},
  abstract = {Reconstructing system-level behavior from silicon traces is a critical problem in post-silicon validation of System-on-Chip designs. Current industrial practice in this area is primarily manual, depending on collaborative insights of the architects, designers, and validators. This paper presents a trace analysis approach that exploits architectural models of system-level protocols to reconstruct design behavior from partially observed silicon traces in the presence of ambiguous and noisy data. The output of the approach is a set of all potential interpretations of a system's internal execution abstracted to system-level protocols. To support the trace analysis approach, a companion trace signal selection framework guided by system-level protocols is also presented, and its impacts on the complexity and accuracy of the analysis approach are discussed. That approach and the framework have been evaluated on a multi-core System-on-Chip prototype that implements a set of common industrial system-level protocols.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Design}} ({{ICCD}})},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Design}} ({{ICCD}})},
  date = {2017-11},
  pages = {177-184},
  keywords = {Protocols,integrated circuit testing,software architecture,System-on-chip,integrated circuit design,system-on-chip,formal verification,Silicon,silicon,protocols,companion trace signal selection framework,Computer bugs,computer debugging,industrial system-level protocols,IP networks,Observability,partially observed silicon traces,post-silicon trace analysis approach,post-silicon validation,reconstructing system-level behavior,system-on-chip designs,system-on-chip protocol debug,system-on-chip prototype,Universal Serial Bus},
  author = {Cao, Y. and Zheng, H. and Palombo, H. and Ray, S. and Yang, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CC4BEFJK\\Cao et al. - 2017 - A Post-Silicon Trace Analysis Approach for System-.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HEQE2UEE\\8119208.html}
}

@article{delshadtehraniNileProgrammableMonitoring2018,
  title = {Nile: {{A Programmable Monitoring Coprocessor}}},
  volume = {17},
  issn = {1556-6056},
  doi = {10.1109/LCA.2017.2784416},
  shorttitle = {Nile},
  abstract = {Researchers widely employ hardware performance counters (HPCs) as well as debugging and profiling tools in processors for monitoring different events such as cache hits, cache misses, and branch prediction statistics during the execution of programs. The collected information can be used for power, performance, and thermal management of the system as well as detecting anomalies or malicious behavior in the software. However, monitoring new or complex events using HPCs and existing tools is a challenging task because HPCs only provide a fixed pool of raw events to monitor. To address this challenge, we propose the implementation of a programmable hardware monitor in a complete system framework including the hardware monitor architecture and its interface with an in-order single-issue RISC-V processor as well as an operating system. As a proof of concept, we demonstrate how to programmatically implement a shadow stack using our hardware monitor and how the programmed shadow stack detects stack buffer overflow attacks. Our hardware monitor design incurs a 26 percent power overhead and a 15 percent area overhead over an unmodified RISC-V processor. Our programmed shadow stack has less than 3 percent performance overhead in the worst case.},
  number = {1},
  journaltitle = {IEEE Computer Architecture Letters},
  date = {2018-01},
  pages = {92-95},
  keywords = {Linux,Hardware,Monitoring,Coprocessors,security,debugging,Program processors,cache storage,performance evaluation,operating systems (computers),cache misses,branch prediction statistics,cache hits,complete system framework,complex events,coprocessors,fixed pool,Hardware coprocessor,hardware monitor architecture,hardware monitor design,hardware performance counters,HPCs,malicious behavior,Nile,operating system,Pattern matching,performance overhead,power overhead,profiling tools,programmable hardware,programmable hardware monitor,programmable monitoring coprocessor,programmed shadow stack,raw events,reduced instruction set computing,Rockets,shadow stack,single-issue RISC-V processor,stack buffer overflow attack,stack buffer overflow attacks,thermal management,unmodified RISC-V processor},
  author = {Delshadtehrani, L. and Eldridge, S. and Canakci, S. and Egele, M. and Joshi, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5CLZ8NEA\\Delshadtehrani et al. - 2018 - Nile A Programmable Monitoring Coprocessor.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\634735H9\\8219379.html}
}

@thesis{garsideRealtimePrefetchingSharedmemory2015,
  location = {{York, UK}},
  title = {Real-Time {{Prefetching On Shared}}-Memory {{Multi}}-Core {{Systems}}},
  url = {http://etheses.whiterose.ac.uk/10711/1/thesis.pdf},
  abstract = {In recent years, there has been a growing trend towards using multi-core processors
in real-time systems to cope with the rising computation requirements of
real-time tasks. Coupled with this, the rising memory requirements of these tasks
pushes demand beyond what can be provided by small, private on-chip caches, requiring
the use of larger, slower off-chip memories such as DRAM. Due to the cost,
power requirements and complexity of these memories, they are typically shared
between all of the tasks within the system.
In order for the execution time of these tasks to be bounded, the response time
of the memory and the interference from other tasks also needs to be bounded.
While there is a great amount of current research on bounding this interference,
one popular method is to effectively partition the available memory bandwidth
between the processors in the system. Of course, as the number of processors
increases, so does the worst-case blocking, and worst-case blocking times quickly
increase with the number of processors.
It is difficult to further optimise the arbitration scheme; instead, this scaling problem
needs to be approached from another angle. Prefetching has previously been
shown to improve the execution time of tasks by speculatively issuing memory
accesses ahead of time for items which may be useful in the near future, although
these prefetchers are typically not used in real-time systems due to their unpredictable
nature. Instead, this work presents a framework by which a prefetcher
can be safely used alongside a composable memory arbiter, a predictable prefetching
scheme, and finally a method by which this predictable prefetcher can be used
to improve the worst-case execution time of a running task.},
  pagetotal = {185},
  institution = {{The University of York}},
  type = {Doctoral},
  urldate = {2018-06-25},
  date = {2015-07},
  author = {Garside, Jamie},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WZNZW7PQ\\thesis.pdf}
}

@inproceedings{thakkar3DWizNovelHigh2014,
  title = {{{3D}}-{{Wiz}}: {{A}} Novel High Bandwidth, Optically Interfaced {{3D DRAM}} Architecture with Reduced Random Access Time},
  doi = {10.1109/ICCD.2014.6974654},
  shorttitle = {{{3D}}-{{Wiz}}},
  abstract = {This paper introduces 3D-Wiz, which is a high bandwidth, low latency, optically interfaced 3D DRAM architecture with fine grained data organization and activation. 3D-Wiz integrates sub-bank level 3D partitioning of the data array to enable fine-grained activation and greater memory parallelism. A novel method of routing the internal memory bus using TSVs and fan-out buffers enables 3D-Wiz to use smaller dimension subarrays without significant area overhead. This in turn reduces the random access latency and activation-precharge energy. 3D-Wiz demonstrates access latency of 19.5ns and row cycle time of 25ns. It yields per access activation energy and precharge energy of 0.78nJ and 0.62nJ respectively with 42.5\% area efficiency. 3D-Wiz yields the best latency and energy consumption values per access among other well-known 3D DRAM architectures. Experimental results with PARSEC benchmarks indicate that 3D-Wiz achieves 38.8\% improvement in performance, 81.1\% reduction in power consumption, and 77.1\% reduction in energy-delay product (EDP) on average over 3D DRAM architectures from prior work.},
  eventtitle = {2014 {{IEEE}} 32nd {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  booktitle = {2014 {{IEEE}} 32nd {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  date = {2014-10},
  pages = {1-7},
  keywords = {Bandwidth,Arrays,DRAM chips,Random access memory,energy-delay product,Three-dimensional displays,three-dimensional integrated circuits,Through-silicon vias,TSV,3D-Wiz,access activation energy,activation-precharge energy,dimension subarrays,EDP,efficiency 42.5 percent,energy consumption value,fan-out buffers,fine-grained data organization,high-bandwidth 3D DRAM architecture,internal memory bus routing,low-latency 3D DRAM architecture,memory parallelism,optically-interfaced 3D DRAM architecture,PARSEC benchmarks,Photonics,precharge energy,random access latency,reduced random access time,row cycle time,subbank level 3D partitioning,time 19.5 ns,time 25 ns},
  author = {Thakkar, I. G. and Pasricha, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\J6D4QVDS\\Thakkar and Pasricha - 2014 - 3D-Wiz A novel high bandwidth, optically interfac.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\IIEGCK6U\\6974654.html}
}

@inproceedings{aroraCompositeDataPrefetcher2014,
  title = {A Composite Data {{Prefetcher}} Framework for Multilevel Caches},
  doi = {10.1109/ICACCI.2014.6968442},
  abstract = {The increasing difference between the Processor speed and the DRAM performance have led to the assertive need to hide memory latency and reduce memory access time. It is noticed that the Processor remains stalled on memory references. Data Prefetching is a technique that fetches that next instruction's data parallel to the current instruction execution in a typical Processor-Cache-DRAM system. A Prefetcher anticipates a cache miss that might take place in the next instruction and fetches the data before the actual memory reference. The goal of prefetching is to reduce as many cache misses as possible. In this paper we present a detailed summary of the different prefetching techniques, and implement a composite prefetcher prototype that employs the techniques of Sequential, Stride and Distance Prefetching.},
  eventtitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  date = {2014-09},
  pages = {1827-1833},
  keywords = {Educational institutions,Hardware,cache storage,History,Prefetching,DRAM chips,Random access memory,dynamic random access memory,cache miss,multilevel cache,storage management,Global History Buffer,memory latency,data prefetching,memory access time,processor speed,Arbitrary Stride Prefetching,Average Memory Access Time,composite data prefetcher framework,distance prefetching,Distance Prefetching,DRAM performance,Dynamic Read Only Memory,Markov processes,processor-cache-DRAM system,sequential prefetching,stride prefetching},
  author = {Arora, H. and Banerjee, S. and Davina, V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HHVXUMNB\\Arora et al. - 2014 - A composite data Prefetcher framework for multilev.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZQXL5GK\\6968442.html}
}

@inproceedings{krishnapillaiRankSwitchingOpenRowDRAM2014,
  title = {A {{Rank}}-{{Switching}}, {{Open}}-{{Row DRAM Controller}} for {{Time}}-{{Predictable Systems}}},
  doi = {10.1109/ECRTS.2014.37},
  abstract = {We introduce ROC, a Rank-switching, Open-row Controller for Double Data Rate Dynamic RAM (DDR DRAM). ROC is optimized for mixed-criticality multicore systems using modern DDR devices: compared to existing real-time memory controllers, it provides significantly lower worst case latency bounds for hard real-time tasks and supports throughput-oriented optimizations for soft real-time applications. The key to improved performance is an innovative rank-switching mechanism which hides the latency of write-read transitions in DRAM devices without requiring unpredictable request reordering. We further employ open row policy to take advantage of the data caching mechanism (row buffering) in each device. ROC provides complete timing isolation between hard and soft tasks and allows for compositional timing analysis over the number of cores and memory ranks in the system. We implemented and synthesized the ROC back end in Verilog RTL, and evaluated its performance on both synthetic tasks and a set of representative benchmarks.},
  eventtitle = {2014 26th {{Euromicro Conference}} on {{Real}}-{{Time Systems}}},
  booktitle = {2014 26th {{Euromicro Conference}} on {{Real}}-{{Time Systems}}},
  date = {2014-07},
  pages = {27-38},
  keywords = {microcontrollers,cache storage,Delays,DRAM chips,Random access memory,Memory management,Clocks,multiprocessing systems,DDR DRAM,Real-time systems,Switches,optimisation,compositional timing analysis,data caching mechanism,double data rate dynamic RAM,mixed-criticality multicore systems,rank-switching open-row DRAM controller,ROC,row buffering,throughput-oriented optimizations,time-predictable systems,Verilog RTL,worst case latency bounds,write-read transition latency},
  author = {Krishnapillai, Y. and Wu, Z. P. and Pellizzoni, R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\K5UD57JB\\Krishnapillai et al. - 2014 - A Rank-Switching, Open-Row DRAM Controller for Tim.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GRGK4BQ6\\6932587.html}
}

@inproceedings{huangATCacheReducingDRAM2014,
  title = {{{ATCache}}: {{Reducing DRAM}} Cache Latency via a Small {{SRAM}} Tag Cache},
  doi = {10.1145/2628071.2628089},
  shorttitle = {{{ATCache}}},
  abstract = {3D-stacking technology has enabled the option of embedding a large DRAM onto the processor. Prior works have proposed to use this as a DRAM cache. Because of its large size (a DRAM cache can be in the order of hundreds of megabytes), the total size of the tags associated with it can also be quite large (in the order of tens of megabytes). The large size of the tags has created a problem. Should we maintain the tags in the DRAM and pay the cost of a costly tag access in the critical path? Or should we maintain the tags in the faster SRAM by paying the area cost of a large SRAM for this purpose? Prior works have primarily chosen the former and proposed a variety of techniques for reducing the cost of a DRAM tag access. In this paper, we first establish (with the help of a study) that maintaining the tags in SRAM, because of its smaller access latency, leads to overall better performance. Motivated by this study, we ask if it is possible to maintain tags in SRAM without incurring high area overhead. Our key idea is simple. We propose to cache the tags in a small SRAM tag cache - we show that there is enough spatial and temporal locality amongst tag accesses to merit this idea. We propose the ATCache which is a small SRAM tag cache. Similar to a conventional cache, the ATCache caches recently accessed tags to exploit temporal locality; it exploits spatial locality by prefetching tags from nearby cache sets. In order to avoid the high miss latency and cache pollution caused by excessive prefetching, we use a simple technique to throttle the number of sets prefetched. Our proposed ATCache (which consumes 0.4\% of overall tag size) can satisfy over 60\% of DRAM cache tag accesses on average.},
  eventtitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  booktitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  date = {2014-08},
  pages = {51-60},
  keywords = {Design,cache storage,Prefetching,Servers,DRAM chips,Pollution,Random access memory,Compounds,DRAM cache,storage management,SRAM chips,Performance,3D-stacking technology,ATCache,cache pollution,DRAM cache latency,Media,prefetching tags,small SRAM tag cache,Systems architecture},
  author = {Huang, C. and Nagarajan, V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5JV8LE9V\\Huang and Nagarajan - 2014 - ATCache Reducing DRAM cache latency via a small S.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SIJII4GP\\7855888.html}
}

@article{nairEvolutionMemoryArchitecture2015,
  title = {Evolution of {{Memory Architecture}}},
  volume = {103},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2435018},
  abstract = {Computer memories continue to serve the role that they first served in the electronic discrete variable automatic computer (EDVAC) machine documented by John von Neumann, namely that of supplying instructions and operands for calculations in a timely manner. As technology has made possible significantly larger and faster machines with multiple processors, the relative distance in processor cycles of this memory has increased considerably. Microarchitectural techniques have evolved to share this memory across ever-larger systems of processors with deep cache hierarchies and have managed to hide this latency for many applications, but are proving to be expensive and energy-inefficient for newer types of problems working on massive amounts of data. New paradigms include scale-out systems distributed across hundreds and even thousands of nodes, in-memory databases that keep data in memory much longer than the duration of a single task, and near-data computation, where some of the computation is off-loaded to the location of the data to avoid wasting energy in the movement of data. This paper provides a historical perspective on the evolution of memory architecture, and suggests that the requirements of new problems and new applications are likely to fundamentally change processor and system architecture away from the currently established von Neumann model.},
  number = {8},
  journaltitle = {Proceedings of the IEEE},
  date = {2015},
  pages = {1331-1345},
  keywords = {read,Registers,cache storage,memory hierarchy,Random access memory,Computer architecture,Memory management,memory architecture,Flash memories,flash memory,Approximate memories,computer memory architecture,data location,data movement,deep-cache hierarchies,disk,dynamic random access memory (DRAM),EDVAC machine,electronic discrete variable automatic computer machine,in-memory databases,Information processing,main memory,microarchitectural techniques,multiple processors,near-data processing,non-von Neumann architectures,processing-in-memory,processor cycles,relative distance,scale-out systems,storage-class memory,von Neumann architecture},
  author = {Nair, R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9ER7W34I\\Nair - 2015 - Evolution of Memory Architecture.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RYZCC6CF\\7151782.html}
}

@inproceedings{weiExploitingProgramSemantics2015,
  title = {Exploiting {{Program Semantics}} to {{Place Data}} in {{Hybrid Memory}}},
  doi = {10.1109/PACT.2015.10},
  abstract = {Large-memory applications like data analytics and graph processing benefit from extended memory hierarchies, and hybrid DRAM/NVM (non-volatile memory) systems represent an attractive means by which to increase capacity at reasonable performance/energy tradeoffs. Compared to DRAM, NVMs generally have longer latencies and higher energies for writes, which makes careful data placement essential for efficient system operation. Data placement strategies that resort to monitoring all data accesses and migrating objects to dynamically adjust data locations incur high monitoring overhead and unnecessary memory copies due to mispredicted migrations. We find that program semantics (specifically, global access characteristics) can effectively guide initial data placement with respect to memory types, which, in turn, makes run-time migration more efficient. We study a combined offline/online placement scheme that uses access profiling information to place objects statically and then selectively monitors run-time behaviors to optimize placements dynamically. We present a software/hardware cooperative framework, 2PP, and evaluate it with respect to state-of-the-art migratory placement, finding that it improves performance by an average of 12.1\%. Furthermore, 2PP improves energy efficiency by up to 51.8\%, and by an average of 18.4\%. It does so by reducing run-time monitoring and migration overheads.},
  eventtitle = {2015 {{International Conference}} on {{Parallel Architecture}} and {{Compilation}} ({{PACT}})},
  booktitle = {2015 {{International Conference}} on {{Parallel Architecture}} and {{Compilation}} ({{PACT}})},
  date = {2015-10},
  pages = {163-173},
  keywords = {Organizations,Monitoring,DRAM chips,Random access memory,DRAM,Nonvolatile memory,storage management,2PP,access profiling information,computer centres,data center,Data mining,data placement strategy,energy efficiency,hybird memory systems,hybrid memory,nonvolatile memory system,NVM system,offline/online placement scheme,Phase change materials,program semantics,Semantics,software/hardware cooperative framework},
  author = {Wei, W. and Jiang, D. and McKee, S. A. and Xiong, J. and Chen, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZJ69QVFR\\Wei et al. - 2015 - Exploiting Program Semantics to Place Data in Hybr.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9NUV2RPH\\7429303.html}
}

@inproceedings{seshadriGatherScatterDRAMInDRAM2015,
  title = {Gather-{{Scatter DRAM}}: {{In}}-{{DRAM}} Address Translation to Improve the Spatial Locality of Non-Unit Strided Accesses},
  doi = {10.1145/2830772.2830820},
  shorttitle = {Gather-{{Scatter DRAM}}},
  abstract = {Many data structures (e.g., matrices) are typically accessed with multiple access patterns. Depending on the layout of the data structure in physical address space, some access patterns result in non-unit strides. In existing systems, which are optimized to store and access cache lines, non-unit strided accesses exhibit low spatial locality. Therefore, they incur high latency, and waste memory bandwidth and cache space. We propose the Gather-Scatter DRAM (GS-DRAM) to address this problem. We observe that a commodity DRAM module contains many chips. Each chip stores a part of every cache line mapped to the module. Our idea is to enable the memory controller to access multiple values that belong to a strided pattern from different chips using a single read/write command. To realize this idea, GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips. Second, instead of sending a separate address to each chip, GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module. Based on the pattern ID, each chip independently computes the address of the value to be accessed. The cache line returned by the module contains different values of the strided pattern gathered from different chips. We show that this approach enables GS-DRAM to achieve near-ideal memory bandwidth and cache utilization for many common access patterns. We design an end-to-end system to exploit GS-DRAM. Our evaluations show that 1) for in-memory databases, GS-DRAM obtains the best of the row store and the column store layouts, in terms of both performance and energy, and 2) for matrix-matrix multiplication, GS-DRAM seamlessly enables SIMD optimizations and outperforms the best tiled layout. Our framework is general, and can benefit many modern data-intensive applications.},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2015-12},
  pages = {267-280},
  keywords = {Servers,DRAM chips,DRAM,memory controller,Performance,in-memory databases,cache line,cache utilization,Caches,commodity module,data structures,data-intensive applications,Energy,explixit near-ideal memory bandwidth,gather-scatter DRAM,GS-DRAM,In-memory databases,matrix multiplication,matrix-matrix multiplication,Memory bandwidth,multiple access patterns,nonunit strided accesses,parallel processing,pattern ID,physical address space,SIMD,SIMD optimizations,single read command,single write command,spatial locality improvement,Strided accesses},
  author = {Seshadri, V. and Mullins, T. and Boroumand, A. and Mutlu, O. and Gibbons, P. B. and Kozuch, M. A. and Mowry, T. C.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KICKZTBD\\Seshadri et al. - 2015 - Gather-Scatter DRAM In-DRAM address translation t.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YGS6DMYZ\\7856604.html}
}

@inproceedings{waslyHidingMemoryLatency2014,
  title = {Hiding Memory Latency Using Fixed Priority Scheduling},
  doi = {10.1109/RTAS.2014.6925992},
  abstract = {Modern embedded platforms contain a variety of physical resources, such as caches, interconnects, main memory, etc., which the processor must access during the execution of a task. We argue that processor task execution and accesses to physical resources should be co-scheduled in real-time systems to predictably hide resource access latency. In particular, in this work we focus on co-scheduling task execution and accesses to main memory to hide DRAM access latency. Since modern systems implement DMA controllers that can be operated independently of processor execution, this allows us to hide memory transfer latency by scheduling DMA transfer in parallel with processor execution. The main contribution of this paper is a dynamic scheduling algorithm for a set of sporadic real-time tasks that efficiently co-schedules processor and DMA execution to hide memory transfer latency. The proposed algorithm can be applied to either uniprocessor or partitioned multiprocessor systems. We demonstrate that we improve processor utilization significantly compared to existing scratchpad and cache management systems.},
  eventtitle = {2014 {{IEEE}} 19th {{Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  booktitle = {2014 {{IEEE}} 19th {{Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  date = {2014-04},
  pages = {75-86},
  keywords = {Interference,Algorithm design and analysis,cache storage,real-time systems,DRAM chips,Memory management,multiprocessing systems,Real-time systems,DRAM access latency,memory latency,processor scheduling,Schedules,Time factors,main memory,cache management systems,coscheduling task execution,DMA controllers,DMA execution,DMA transfer scheduling,embedded platforms,fixed priority scheduling,interconnects,memory transfer latency,partitioned multiprocessor systems,physical resources,processor task execution,Scheduling algorithms,scratchpad management systems,uniprocessor},
  author = {Wasly, S. and Pellizzoni, R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QEZCWNCP\\Wasly and Pellizzoni - 2014 - Hiding memory latency using fixed priority schedul.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HQ8M63W6\\6925992.html}
}

@inproceedings{luImprovingDRAMLatency2015,
  title = {Improving {{DRAM}} Latency with Dynamic Asymmetric Subarray},
  doi = {10.1145/2830772.2830827},
  abstract = {The evolution of DRAM technology has been driven by capacity and bandwidth during the last decade. In contrast, DRAM access latency stays relatively constant and is trending to increase. Much efforts have been devoted to tolerate memory access latency but these techniques have reached the point of diminishing returns. Having shorter bitline and wordline length in a DRAM device will reduce the access latency. However by doing so it will impact the array efficiency. In the mainstream market, manufacturers are not willing to trade capacity for latency. Prior works had proposed hybrid-bitline DRAM design to overcome this problem. However, those methods are either intrusive to the circuit and layout of the DRAM design, or there is no direct way to migrate data between the fast and slow levels. In this paper, we proposed a novel asymmetric DRAM with capability to perform low cost data migration between subarrays. Having this design we determined a simple management mechanism and explored many management related policies. We showed that with this new design and our simple management technique we could achieve 7.25\% and 11.77\% performance improvement in single- and multi-programming workloads, respectively, over a system with traditional homogeneous DRAM. This gain is above 80\% of the potential performance gain of a system based on a hypothetical DRAM which is made out of short bitlines entirely.},
  eventtitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  date = {2015-12},
  pages = {255-266},
  keywords = {Delays,DRAM chips,Computer architecture,Microprocessors,Capacitors,Transistors,memory access latency,performance evaluation,DRAM access latency,performance improvement,asymmetric DRAM,DRAM device,DRAM technology,dynamic asymmetric subarray,hybrid-bitline DRAM design,low cost data migration,multiprogramming workloads,single-programming workloads,wordline length},
  author = {Lu, S. and Lin, Y. and Yang, C.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\S5PNHIBT\\Lu et al. - 2015 - Improving DRAM latency with dynamic asymmetric sub.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GIW27Z6W\\7856603.html}
}

@inproceedings{chatterjeeManagingDRAMLatency2014,
  title = {Managing {{DRAM Latency Divergence}} in {{Irregular GPGPU Applications}}},
  doi = {10.1109/SC.2014.16},
  abstract = {Memory controllers in modern GPUs aggressively reorder requests for high bandwidth usage, often interleaving requests from different warps. This leads to high variance in the latency of different requests issued by the threads of a warp. Since a warp in a SIMT architecture can proceed only when all of its memory requests are returned by memory, such latency divergence causes significant slowdown when running irregular GPGPU applications. To solve this issue, we propose memory scheduling mechanisms that avoid inter-warp interference in the DRAM system to reduce the average memory stall latency experienced by warps. We further reduce latency divergence through mechanisms that coordinate scheduling decisions across multiple independent memory channels. Finally we show that carefully orchestrating the memory scheduling policy can achieve low average latency for warps, without compromising bandwidth utilization. Our combined scheme yields a 10.1\% performance improvement for irregular GPGPU workloads relative to a throughput-optimized GPU memory controller.},
  eventtitle = {{{SC}} '14: {{Proceedings}} of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  booktitle = {{{SC}} '14: {{Proceedings}} of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  date = {2014-11},
  pages = {128-139},
  keywords = {Bandwidth,Instruction sets,DRAM chips,Random access memory,memory controllers,Parallel processing,Memory management,memory requests,storage management chips,average memory stall latency,bandwidth utilization,DRAM system,graphics processing units,Graphics processing units,high bandwidth usage,independent memory channels,interleaving requests,interwarp interference,irregular GPGPU applications,latency divergence,memory scheduling mechanisms,memory scheduling policy,scheduling decisions,SIMT architecture,throughput-optimized GPU memory controller},
  author = {Chatterjee, N. and O'Connor, M. and Loh, G. H. and Jayasena, N. and Balasubramonia, R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\576J8EEP\\Chatterjee et al. - 2014 - Managing DRAM Latency Divergence in Irregular GPGP.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NUT6FVPY\\7012998.html}
}

@inproceedings{choiMultipleCloneRow2015,
  title = {Multiple {{Clone Row DRAM}}: {{A}} Low Latency and Area Optimized {{DRAM}}},
  doi = {10.1145/2749469.2750402},
  shorttitle = {Multiple {{Clone Row DRAM}}},
  abstract = {Several previous works have changed DRAM bank structure to reduce memory access latency and have shown performance improvement. However, changes in the area-optimized DRAM bank can incur large area-overhead. To solve this problem, we propose Multiple Clone Row DRAM (MCR-DRAM), which uses existing DRAM bank structure without any modification.},
  eventtitle = {2015 {{ACM}}/{{IEEE}} 42nd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  booktitle = {2015 {{ACM}}/{{IEEE}} 42nd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  date = {2015-06},
  pages = {223-234},
  keywords = {DRAM chips,memory access latency,area optimized DRAM,bank structure,low latency DRAM,MCR-DRAM,multiple clone row DRAM},
  author = {Choi, J. and Shin, W. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ATDZYEJF\\Choi et al. - 2015 - Multiple Clone Row DRAM A low latency and area op.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DEIKM2ER\\7284068.html}
}

@inproceedings{shinNUATNonuniformAccess2014,
  title = {{{NUAT}}: {{A}} Non-Uniform Access Time Memory Controller},
  doi = {10.1109/HPCA.2014.6835956},
  shorttitle = {{{NUAT}}},
  abstract = {With rapid development of micro-processors, off-chip memory access becomes a system bottleneck. DRAM, a main memory in most computers, has concentrated only on capacity and bandwidth for decades to achieve high performance computing. However, DRAM access latency should also be considered to keep the development trend in multi-core era. Therefore, we propose NUAT which is a new memory controller focusing on reducing memory access latency without any modification of the existing DRAM structure. We only exploit DRAM's intrinsic phenomenon: electric charge variation in DRAM cell capacitors. Given the cost-sensitive DRAM market, it is a big advantage in terms of actual implementation. NUAT gives a score to every memory access request and the request with the highest score obtains a priority. For scoring, we introduce two new concepts: Partitioned Bank Rotation (PBR) and PBR Page Mode (PPM). First, PBR is a mechanism that draws information of access speed from refresh timing and position; the request which has faster access speed gains higher score. Second, PPM selects a better page mode between open- and close-page modes based on the information from PBR. Evaluations show that NUAT decreases memory access latency significantly for various environments.},
  eventtitle = {2014 {{IEEE}} 20th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  booktitle = {2014 {{IEEE}} 20th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  date = {2014-02},
  pages = {464-475},
  keywords = {microcontrollers,Timing,DRAM chips,Random access memory,Capacitors,DRAM cell capacitors,electric charge variation,DRAM access latency,Abstracts,capacitors,cost-sensitive DRAM market,Lead,memory access latency reduction,memory access request,microprocessor development,nonuniform access time memory controller,NUAT,off-chip memory access,partitioned bank rotation,PBR page mode,PPM,Sensors,Standards,system bottleneck},
  author = {Shin, W. and Yang, J. and Choi, J. and Kim, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\C25T4M26\\Shin et al. - 2014 - NUAT A non-uniform access time memory controller.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8RTQX2Q5\\6835956.html}
}

@inproceedings{hameedReducingLatencySRAM2014,
  title = {Reducing Latency in an {{SRAM}}/{{DRAM}} Cache Hierarchy via a Novel {{Tag}}-{{Cache}} Architecture},
  doi = {10.1145/2593069.2593197},
  abstract = {Memory speed has become a major performance bottleneck as more and more cores are integrated on a multi-core chip. The widening latency gap between high speed cores and memory has led to the evolution of multi-level SRAM/DRAM cache hierarchies that exploit the latency benefits of smaller caches (e.g. private L1 and L2 SRAM caches) and the capacity benefits of larger caches (e.g. shared L3 SRAM and shared L4 DRAM cache). The main problem of employing large L3/L4 caches is their high tag lookup latency. To solve this problem, we introduce the novel concept of small and low latency SRAM/DRAM Tag-Cache structures that can quickly determine whether an access to the large L3/L4 caches will be a hit or a miss. The performance of the proposed Tag-Cache architecture depends upon the Tag-Cache hit rate and to improve it we propose a novel Tag-Cache insertion policy and a DRAM row buffer mapping policy that reduce the latency of memory requests. For a 16-core system, this improves the average harmonic mean instruction per cycle throughput of latency sensitive applications by 13.3\% compared to state-of-the-art.},
  eventtitle = {2014 51st {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  booktitle = {2014 51st {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  date = {2014-06},
  pages = {1-6},
  keywords = {Organizations,Monitoring,Radiation detectors,Multicore processing,Program processors,cache storage,Arrays,DRAM chips,Random access memory,SRAM chips,average harmonic mean instruction,DRAM row buffer mapping policy,high speed cores,high tag lookup latency,multi-core chip,private L1 SRAM caches,private L2 SRAM caches,shared L3 SRAM cache,shared L4 DRAM cache,SRAM/DRAM cache hierarchy,tag-cache architecture,tag-cache insertion policy,widening latency gap},
  author = {Hameed, F. and Bauer, L. and Henkel, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9RFWJJ2X\\Hameed et al. - 2014 - Reducing latency in an SRAMDRAM cache hierarchy v.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H4LE6X34\\6881364.html}
}

@inproceedings{oRowbufferDecouplingCase2014,
  title = {Row-Buffer Decoupling: {{A}} Case for Low-Latency {{DRAM}} Microarchitecture},
  doi = {10.1109/ISCA.2014.6853230},
  shorttitle = {Row-Buffer Decoupling},
  abstract = {Modern DRAM devices for the main memory are structured to have multiple banks to satisfy ever-increasing throughput, energy-efficiency, and capacity demands. Due to tight cost constraints, only one row can be buffered (opened) per bank and actively service requests at a time, while the row must be deactivated (closed) before a new row is stored into the row buffers. Hasty deactivation unnecessarily re-opens rows for otherwise row-buffer hits while hindsight accompanies the deactivation process on the critical path of accessing data for row-buffer misses. The time to (de)activate a row is comparable to the time to read an open row while applications are often sensitive to DRAM latency. Hence, it is critical to make the right decision on when to close a row. However, the increasing number of banks per DRAM device over generations reduces the number of requests per bank. This forces a memory controller to frequently predict when to close a row due to a lack of information on future requests, while the dynamic nature of memory access patterns limits the prediction accuracy. In this paper, we propose a novel DRAM microarchitecture that can eliminate the need for any prediction. First, we identify that precharging the bitlines dominates the deactivate time, while sense amplifiers that work as a row buffer are physically coupled with the bitlines such that a single command precharges both bitlines and sense amplifiers simultaneously. By decoupling the bitlines from the row buffers using isolation transistors, the bitlines can be precharged right after a row becomes activated. Therefore, only the sense amplifiers need to be precharged for a miss in most cases, taking an order of magnitude shorter time than the conventional deactivation process. Second, we show that this row-buffer decoupling enables internal DRAM μ-operations to be separated and recombined, which can be exploited by memory controllers to make the main memory system more energy efficient. Our experiments demonstrate that row-buffer decoupling improves the geometric mean of the instructions per cycle and MIPS2/W by 14\% and 29\%, respectively, for memory-intensive SPEC CPU2006 applications.},
  eventtitle = {2014 {{ACM}}/{{IEEE}} 41st {{International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  booktitle = {2014 {{ACM}}/{{IEEE}} 41st {{International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  date = {2014-06},
  pages = {337-348},
  keywords = {Microarchitecture,DRAM chips,Random access memory,memory controller,Memory management,Transistors,buffer storage,Capacitance,memory system,Buffer storage,memory access patterns,energy efficiency,deactivation process,DRAM latency,isolation transistors,low-latency DRAM microarchitecture,memory-intensive SPEC CPU2006 applications,modern DRAM devices,row-buffer decoupling,row-buffer misses},
  author = {O, S. and Son, Y. H. and Kim, N. S. and Ahn, J. H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F2TEIUI5\\O et al. - 2014 - Row-buffer decoupling A case for low-latency DRAM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GPVT642Y\\6853230.html}
}

@inproceedings{armejachTidyCacheImproving2015,
  title = {Tidy {{Cache}}: {{Improving Data Placement}} in {{Die}}-{{Stacked DRAM Caches}}},
  doi = {10.1109/SBAC-PAD.2015.23},
  shorttitle = {Tidy {{Cache}}},
  abstract = {Die-stacked DRAM caches are likely to become available in mainstream chips in the near future. DRAM caches are typically used as a last level shared cache behind the traditional hierarchy of on-chip SRAM caches. However, its internal organization differs from traditional caches as it is based on DRAM technology that provides significantly diverse access latencies depending on the state of its internal structures. Accesses that hit in the row-buffer require only one DRAM command and are significantly faster than those that require closing the row-buffer to load a new row to read from. Prior work has focused on maximizing row-buffer locality while maintaining high cache hit ratios. However, past designs do not consider performance problems that may arise due to interleaved accesses from different applications that compete for the shared DRAM resources, nor the different access patterns and locality characteristics that each of these applications may have. In this paper, we first identify performance pathologies that are specific to DRAM caches which arise due to the interference caused by interleaved accesses from multiple cores. We then propose Tidy Cache, a novel DRAM cache design that is able to ameliorate these performance pathologies by dynamically adapting the replacement policy for demanded data. Our performance evaluation results show that our design outperforms the state-of-the-art by 9.2\% for multi-programmed SPEC workloads and by 16.7\% for a set of TPC-H queries, mainly due to significantly better cache miss ratios.},
  eventtitle = {2015 27th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  booktitle = {2015 27th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  date = {2015-10},
  pages = {65-73},
  keywords = {Organizations,Bandwidth,Interference,cache storage,DRAM chips,cache,Random access memory,DRAM,System-on-chip,performance evaluation,SRAM chips,Proposals,3D stacking,access latencies,cache miss ratios,data placement improvement,data replacement policy,die-stacked DRAM caches,DRAM cache design,DRAM command,interleaved access,internal structures,multiprogrammed SPEC workloads,on-chip SRAM caches,Pathology,row-buffer,shared DRAM resources,Tidy Cache,TPC-H queries},
  author = {Armejach, A. and Cristal, A. and Unsal, O. S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UFPC3JNF\\Armejach et al. - 2015 - Tidy Cache Improving Data Placement in Die-Stacke.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5ZEB2AQ8\\7379835.html}
}

@inproceedings{jevdjicUnisonCacheScalable2014,
  title = {Unison {{Cache}}: {{A Scalable}} and {{Effective Die}}-{{Stacked DRAM Cache}}},
  doi = {10.1109/MICRO.2014.51},
  shorttitle = {Unison {{Cache}}},
  abstract = {Recent research advocates large die-stacked DRAM caches in many core servers to break the memory latency and bandwidth wall. To realize their full potential, die-stacked DRAM caches necessitate low lookup latencies, high hit rates and the efficient use of off-chip bandwidth. Today's stacked DRAM cache designs fall into two categories based on the granularity at which they manage data: block-based and page-based. The state-of-the-art block-based design, called Alloy Cache, collocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches. We introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache's approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14\% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.},
  eventtitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  booktitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  date = {2014-12},
  pages = {25-37},
  keywords = {Organizations,Bandwidth,cache storage,Servers,DRAM chips,Random access memory,DRAM,DRAM access,Resource management,System-on-chip,Metals,SRAM chips,memory latency,caches,3D die stacking,Alloy Cache approach,bandwidth wall,block-based data management,die-stacked DRAM cache,footprint cache design,lookup latencies,manycore servers,memory,multigigabyte stacked DRAM caches,off-chip bandwidth,off-chip traffic,on-chip tag storage,page granularity,page-based data management,page-based DRAM caches,page-sized cache allocation units,paged storage,server workloads,servers,SRAM-based tags,stacked-DRAM capacities,Unison cache},
  author = {Jevdjic, D. and Loh, G. H. and Kaynak, C. and Falsafi, B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XUD7AKME\\Jevdjic et al. - 2014 - Unison Cache A Scalable and Effective Die-Stacked.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YZW8T95W\\7011375.html}
}

@inproceedings{dingWCETCentricDynamicInstruction2014,
  title = {{{WCET}}-{{Centric}} Dynamic Instruction Cache Locking},
  doi = {10.7873/DATE.2014.040},
  abstract = {Cache locking is an effective technique to improve timing predictability in real-time systems. In static cache locking, the locked memory blocks remain unchanged throughout the program execution. Thus static locking may not be effective for large programs where multiple memory blocks are competing for few cache lines available for locking. In comparison, dynamic cache locking overcomes cache space limitation through time-multiplexing of locked memory blocks. Prior dynamic locking technique partitions the program into regions and takes independent locking decisions for each region. We propose a flexible loop-based dynamic cache locking approach. We not only select the memory blocks to be locked but also the locking points (e.g., loop level). We judiciously allow memory blocks from the same loop to be locked at different program points for WCET improvement. We design a constraint-based approach that incorporates a global view to decide on the number of locking slots at each loop entry point and then select the memory blocks to be locked for each loop. Experimental evaluation shows that our dynamic cache locking approach achieves substantial improvement of WCET compared to prior techniques.},
  eventtitle = {2014 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  booktitle = {2014 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  date = {2014-03},
  pages = {1-6},
  keywords = {Educational institutions,Benchmark testing,cache storage,program execution,real-time systems,Timing,timing predictability,worst-case execution time,Abstracts,cache lines,constraint-based approach,Electronic mail,flexible loop-based dynamic cache locking approach,independent locking decisions,locked memory blocks,locking points,loop entry point,multiple memory blocks,Nickel,program points,Resilience,time-multiplexing,WCET-centric dynamic instruction cache locking},
  author = {Ding, H. and Liang, Y. and Mitra, T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CS7DG2N6\\Ding et al. - 2014 - WCET-Centric dynamic instruction cache locking.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QBFPXBZG\\6800241.html}
}

@inproceedings{puautWCETCentricSoftwarecontrolledInstruction2006,
  langid = {english},
  location = {{Dresden, Germany}},
  title = {{{WCET}}-{{Centric Software}}-Controlled {{Instruction Caches}} for {{Hard Real}}-{{Time Systems}}},
  isbn = {978-0-7695-2619-5},
  url = {http://ieeexplore.ieee.org/document/1647740/},
  doi = {10.1109/ECRTS.2006.32},
  abstract = {Cache memories have been extensively used to bridge the gap between high speed processors and relatively slower main memories. However, they are sources of predictability problems because of their dynamic and adaptive behavior, and thus need special attention to be used in hard real-time systems. A lot of progress has been achieved in the last ten years to statically predict worst-case execution times (WCETs) of tasks on architectures with caches. However, cache-aware WCET analysis techniques are not always applicable due to the lack of documentation of hardware manuals concerning the cache replacement policies. Moreover, they tend to be pessimistic with some cache replacement policies (e.g. random replacement policies) [6]. Lastly, caches are sources of timing anomalies in dynamically scheduled processors [13] (a cache miss may in some cases result in a shorter execution time than a hit).},
  eventtitle = {18th {{Euromicro Conference}} on {{Real}}-{{Time Systems}} ({{ECRTS}}'06)},
  booktitle = {18th {{Euromicro Conference}} on {{Real}}-{{Time Systems}} ({{ECRTS}}'06)},
  publisher = {{IEEE}},
  urldate = {2019-08-02},
  date = {2006},
  pages = {217-226},
  author = {Puaut, I.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U6HITHT4\\Puaut - 2006 - WCET-Centric Software-controlled Instruction Cache.pdf}
}

@inproceedings{falkCompiletimeDecidedInstruction2007,
  langid = {english},
  location = {{Salzburg, Austria}},
  title = {Compile-Time Decided Instruction Cache Locking Using Worst-Case Execution Paths},
  isbn = {978-1-59593-824-4},
  url = {http://portal.acm.org/citation.cfm?doid=1289816.1289853},
  doi = {10.1145/1289816.1289853},
  abstract = {Caches are notorious for their unpredictability. It is diﬃcult or even impossible to predict if a memory access results in a deﬁnite cache hit or miss. This unpredictability is highly undesired for real-time systems. The Worst-Case Execution Time (WCET) of a software running on an embedded processor is one of the most important metrics during real-time system design. The WCET depends to a large extent on the total amount of time spent for memory accesses. In the presence of caches, WCET analysis must always assume a memory access to be a cache miss if it can not be guaranteed that it is a hit. Hence, WCETs for cached systems are imprecise due to the overestimation caused by the caches.},
  eventtitle = {The 5th {{IEEE}}/{{ACM}} International Conference},
  booktitle = {Proceedings of the 5th {{IEEE}}/{{ACM}} International Conference on {{Hardware}}/Software Codesign and System Synthesis  - {{CODES}}+{{ISSS}} '07},
  publisher = {{ACM Press}},
  urldate = {2019-08-02},
  date = {2007},
  pages = {143},
  author = {Falk, Heiko and Plazar, Sascha and Theiling, Henrik},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZV9QCPZ5\\Falk et al. - 2007 - Compile-time decided instruction cache locking usi.pdf}
}

@inproceedings{zhengWCETAwareDynamic2014,
  langid = {english},
  location = {{Edinburgh, United Kingdom}},
  title = {{{WCET}}: Aware Dynamic Instruction Cache Locking},
  isbn = {978-1-4503-2877-7},
  url = {http://dl.acm.org/citation.cfm?doid=2597809.2597820},
  doi = {10.1145/2597809.2597820},
  shorttitle = {{{WCET}}},
  abstract = {Caches are widely used in embedded systems to bridge the increasing speed gap between processors and off-chip memory. However, caches make it signiﬁcantly harder to compute the WCET( Worst Case Execution Time) of a task. To alleviate this problem, cache locking has been proposed. We investigate the I-cache locking problem, and propose a WCET-aware, min-cut based dynamic instruction cache locking approach for reducing the WCET of a single task. We have implemented our approach and compared it with the two state-of-the-art cache locking approaches by using a set of benchmarks from the MRTC benchmark suite. The experimental results show that our approach achieves the average improvements of 41\%, 15\% and 7\% over the partial locking approach for the 256B, 512B and 1KB caches, respectively, and 7\%, 18\% and 17\% over the longest path based dynamic locking approach for the 256B, 512B and 1KB caches, respectively.},
  eventtitle = {The 2014 {{SIGPLAN}}/{{SIGBED}} Conference},
  booktitle = {Proceedings of the 2014 {{SIGPLAN}}/{{SIGBED}} Conference on {{Languages}}, Compilers and Tools for Embedded Systems - {{LCTES}} '14},
  publisher = {{ACM Press}},
  urldate = {2019-08-02},
  date = {2014},
  pages = {53-62},
  author = {Zheng, Wenguang and Wu, Hui},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BYJ974RM\\Zheng and Wu - 2014 - WCET aware dynamic instruction cache locking.pdf}
}

@inproceedings{dingWCETcentricPartialInstruction2012,
  title = {{{WCET}}-Centric Partial Instruction Cache Locking},
  abstract = {Caches play an important role in embedded systems by bridging the performance gap between high speed processors and slow memory. At the same time, caches introduce imprecision in Worst-case Execution Time (WCET) estimation due to unpredictable access latencies. Modern embedded processors often include cache locking mechanism for better timing predictability. As the cache contents are statically known, memory access latencies are predictable, leading to precise WCET estimate. Moreover, by carefully selecting the memory blocks to be locked, WCET estimate can be reduced compared to cache modeling without locking. Existing static instruction cache locking techniques strive to lock the entire cache to minimize the WCET. We observe that such aggressive locking mechanisms may have negative impact on the overall WCET as some memory blocks with predictable access behavior get excluded from the cache. We introduce a partial cache locking mechanism that has the flexibility to lock only a fraction of the cache. We judiciously select the memory blocks for locking through accurate cache modeling that determines the impact of the decision on the program WCET. Our synergistic cache modeling and locking mechanism achieves substantial reduction in WCET for a large number of embedded benchmark applications.},
  eventtitle = {{{DAC Design Automation Conference}} 2012},
  booktitle = {{{DAC Design Automation Conference}} 2012},
  date = {2012-06},
  pages = {412-420},
  keywords = {embedded systems,Benchmark testing,Program processors,microprocessor chips,cache storage,Estimation,Timing,memory access latency,Analytical models,timing predictability,worst-case execution time,WCET,Abstracts,benchmark testing,cache contents,Concrete,embedded benchmark applications,embedded processor,high speed processors,memory block selection,Partial Cache Locking,timing circuits,unpredictable access latency,WCET estimation,WCET minimization,WCET-centric partial instruction cache locking},
  author = {Ding, H. and Liang, Y. and Mitra, T.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6UMR5TJR\\Ding et al. - 2012 - WCET-centric partial instruction cache locking.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AV4W9RK9\\6241540.html}
}

@article{mittalSurveyTechniquesCache2016,
  title = {A {{Survey}} of {{Techniques}} for {{Cache Locking}}},
  volume = {21},
  issn = {1084-4309},
  url = {http://doi.acm.org/10.1145/2858792},
  doi = {10.1145/2858792},
  abstract = {Cache memory, although important for boosting application performance, is also a source of execution time variability, and this makes its use difficult in systems requiring worst-case execution time (WCET) guarantees. Cache locking is a promising approach for simplifying WCET estimation and providing predictability, and hence, several commercial processors provide ability for locking cache. However, cache locking also has several disadvantages (e.g., extra misses for unlocked blocks, complex algorithms required for selection of locking contents) and hence, a careful management is required to realize the full potential of cache locking. In this article, we present a survey of techniques proposed for cache locking. We categorize the techniques into several groups to underscore their similarities and differences. We also discuss the opportunities and obstacles in using cache locking. We hope that this article will help researchers gain insight into cache locking schemes and will also stimulate further work in this area.},
  number = {3},
  journaltitle = {ACM Trans. Des. Autom. Electron. Syst.},
  urldate = {2019-08-02},
  date = {2016-05},
  pages = {49:1--49:24},
  keywords = {classification,CPU,cache locking,cache partitioning,GPU,hard real-time system,multitasking,Review,worst-case execution time (WCET)},
  author = {Mittal, Sparsh},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5VYJUFSS\\Mittal - 2016 - A Survey of Techniques for Cache Locking.pdf}
}

@inproceedings{pandaSurveyReplacementStrategies2016,
  langid = {english},
  location = {{Mangalore, India}},
  title = {A Survey on Replacement Strategies in Cache Memory for Embedded Systems},
  isbn = {978-1-5090-1623-5},
  url = {http://ieeexplore.ieee.org/document/7806218/},
  doi = {10.1109/DISCOVER.2016.7806218},
  abstract = {Cache is one of the most power-consuming components in computer architecture. Power reduction in cache can be achieved by reducing miss rate, miss penalty, latency per access, and power consumption per access. The power reduction can also be achieved by shutting down unused part of the cache, by allowing not so recently used cache banks to sleep, reconfiguring the cache for specific application and various combinations of one or more of these. The cache hit depends on the cache size, associativity and the cache line size. Replacement strategies in associative mapping schemes play an important role in cache hit rate performance. This survey paper proposes a classification of these strategies with detailed discussion on their advantages and disadvantages.},
  eventtitle = {2016 {{IEEE Distributed Computing}}, {{VLSI}}, {{Electrical Circuits}} and {{Robotics}} ({{DISCOVER}})},
  booktitle = {2016 {{IEEE Distributed Computing}}, {{VLSI}}, {{Electrical Circuits}} and {{Robotics}} ({{DISCOVER}})},
  publisher = {{IEEE}},
  urldate = {2019-08-02},
  date = {2016-08},
  pages = {12-17},
  author = {Panda, Parag and Patil, Geeta and Raveendran, Biju},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LLZFK7ZJ\\Panda et al. - 2016 - A survey on replacement strategies in cache memory.pdf}
}

@inproceedings{jeongOptimalReplacementsCaches1999,
  location = {{New York, NY, USA}},
  title = {Optimal {{Replacements}} in {{Caches}} with {{Two Miss Costs}}},
  isbn = {978-1-58113-124-6},
  url = {http://doi.acm.org/10.1145/305619.305636},
  doi = {10.1145/305619.305636},
  booktitle = {Proceedings of the {{Eleventh Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  series = {{{SPAA}} '99},
  publisher = {{ACM}},
  urldate = {2019-08-02},
  date = {1999},
  pages = {155--164},
  author = {Jeong, Jaeheon and Dubois, Michel},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XZBS3D8B\\Jeong and Dubois - 1999 - Optimal Replacements in Caches with Two Miss Costs.pdf},
  venue = {Saint Malo, France}
}

@article{soCacheOperationsMRU1988,
  title = {Cache Operations by {{MRU}} Change},
  volume = {37},
  issn = {0018-9340},
  doi = {10.1109/12.2208},
  abstract = {The performance of set associative caches is analyzed. The method used is to group the cache lines into regions according to their positions in the replacement stacks of a cache, and then to observe how the memory access of a CPU is distributed over these regions. Results from the preserved CPU traces show that the memory accesses are heavily concentrated on the most recently used (MRU) region in the cache. The concept of MRU change is introduced; the idea is to use the event that the CPU accesses a non-MRU line to approximate the time the CPU is changing its working set. The concept is shown to be useful in many aspects of cache design and performance evaluation, such as comparison of various replacement algorithms, improvement of prefetch algorithms, and speedup of cache simulation.{$<>$}},
  number = {6},
  journaltitle = {IEEE Transactions on Computers},
  date = {1988-06},
  pages = {700-709},
  keywords = {Central Processing Unit,performance,Algorithm design and analysis,Very large scale integration,Prefetching,Computational modeling,content-addressable storage,performance evaluation,storage management,Memory,memory access,Microcomputers,CPU,replacement algorithms,cache simulation,Cache storage,most recently used,MRU change,prefetch algorithms,set associative caches,virtual storage},
  author = {So, K. and Rechtschaffen, R. N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\STZIW7R2\\So and Rechtschaffen - 1988 - Cache operations by MRU change.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\N4EDTW82\\2208.html}
}

@article{aguilarCoherenceReplacementProtocolWeb2006,
  title = {A {{Coherence}}-{{Replacement Protocol For Web Proxy Cache Systems}}},
  volume = {28},
  issn = {1206-212X},
  url = {https://doi.org/10.1080/1206212X.2006.11441783},
  doi = {10.1080/1206212X.2006.11441783},
  abstract = {As World Wide Web usage has grown dramatically in recent years, so has the recognition that web caches (especially proxy caches) will have an important role in reducing server loads, client request latencies, and network traffic. In this paper, we propose an adaptive cache coherence-replacement scheme for web proxy cache systems that is based on several criteria about the system and applications, with the objective of optimizing the distributed cache system performance. Our coherence-replacement scheme assigns a replacement priority value to each cache block according to a set of criteria for deciding which block to remove. The goal is to provide an effective utilization of the distributed cache memory and a good application performance.},
  number = {1},
  journaltitle = {International Journal of Computers and Applications},
  urldate = {2019-08-05},
  date = {2006-01-01},
  pages = {12-18},
  keywords = {coherency techniques,replacement techniques,Web caching},
  author = {Aguilar, J. and Leiss, E. L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PJVR3TZ5\\Aguilar and Leiss - 2006 - A Coherence-Replacement Protocol For Web Proxy Cac.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6LJTW773\\1206212X.2006.html}
}

@inproceedings{kampeSelfCorrectingLRUReplacement2004,
  title = {Self-{{Correcting LRU Replacement Policies}}},
  abstract = {With wider associativity the replacement algorithm becomes critical. Although LRU makes many good replacement decisions, the wide performance gap between OPT, the optimum off-line algorithm, and LRU suggests that LRU still makes too many mistakes. Self-correcting},
  booktitle = {In {{Proceedings}} of the 1st {{Conference}} on {{Computing Frontiers}}},
  date = {2004},
  pages = {181--191},
  author = {Kampe, Martin and Stenstrom, Per and Dubois, Michel},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7TKWKSHY\\Kampe et al. - 2004 - Self-Correcting LRU Replacement Policies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GKRS54DW\\summary.html}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004a,
  location = {{New York, NY, USA}},
  title = {Performance {{Evaluation}} of {{Cache Replacement Policies}} for the {{SPEC CPU2000 Benchmark Suite}}},
  isbn = {978-1-58113-870-2},
  url = {http://doi.acm.org/10.1145/986537.986601},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions.Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50\%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.},
  booktitle = {Proceedings of the {{42Nd Annual Southeast Regional Conference}}},
  series = {{{ACM}}-{{SE}} 42},
  publisher = {{ACM}},
  urldate = {2019-08-05},
  date = {2004},
  pages = {267--272},
  keywords = {performance evaluation,cache memory,replacement policy},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I66AQ9LF\\Al-Zoubi et al. - 2004 - Performance Evaluation of Cache Replacement Polici.pdf},
  venue = {Huntsville, Alabama}
}

@inproceedings{makiDatareplacecontrolledCacheMemory1999,
  title = {A Data-Replace-Controlled Cache Memory System and Its Performance Evaluations},
  volume = {1},
  doi = {10.1109/TENCON.1999.818453},
  abstract = {We present a novel cache memory system to reduce cache miss ratio. It enables the cache to lock or release the data in it by software controls. The paper describes its overall hardware and programming directions and also shows performance evaluations. The results show that the computer with this system can reduce the cache misses by up to 60.9\% and can execute faster than a computer with a conventional cache.},
  eventtitle = {Proceedings of {{IEEE}}. {{IEEE Region}} 10 {{Conference}}. {{TENCON}} 99. '{{Multimedia Technology}} for {{Asia}}-{{Pacific Information Infrastructure}}' ({{Cat}}. {{No}}.{{99CH37030}})},
  booktitle = {Proceedings of {{IEEE}}. {{IEEE Region}} 10 {{Conference}}. {{TENCON}} 99. '{{Multimedia Technology}} for {{Asia}}-{{Pacific Information Infrastructure}}' ({{Cat}}. {{No}}.{{99CH37030}})},
  date = {1999-09},
  pages = {471-474 vol.1},
  keywords = {Hardware,programming,cache storage,Prefetching,Pollution,Computer architecture,Microprocessors,performance evaluation,storage management,multiprogramming,Cache memory,Control systems,computer architecture,Degradation,cache miss ratio,conventional cache,Data engineering,data-replace-controlled cache memory system,Information systems,performance evaluations,software control},
  author = {Maki, N. and Hoson, K. and Ishida, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PLGIE3UM\\Maki et al. - 1999 - A data-replace-controlled cache memory system and .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DFNBN9X2\\818453.html}
}

@inproceedings{jainSoftwareassistedCacheReplacement2001,
  langid = {english},
  title = {Software-Assisted Cache Replacement Mechanisms for Embedded Systems},
  isbn = {978-0-7803-7247-4},
  url = {http://ieeexplore.ieee.org/document/968607/},
  doi = {10.1109/ICCAD.2001.968607},
  abstract = {We address the problem of improving cache predictability and performance in embedded systems through the use of softwareassisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e., effectively make the element the least recently used element, or keep that cache element, i.e., the element will never be evicted.},
  publisher = {{IEEE}},
  urldate = {2019-08-05},
  date = {2001},
  pages = {119-126},
  author = {Jain, P. and Devadas, S. and Engels, D. and Rudolph, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VSPKXH5W\\Jain et al. - 2001 - Software-assisted cache replacement mechanisms for.pdf}
}

@inproceedings{zhenlinwangUsingCompilerImprove2002,
  title = {Using the Compiler to Improve Cache Replacement Decisions},
  doi = {10.1109/PACT.2002.1106018},
  abstract = {Memory performance is increasingly determining microprocessor performance and technology trends are exacerbating this problem. Most architectures use set-associative caches with LRU replacement policies to combine fast access with relatively low miss rates. To improve replacement decisions in set-associative caches, we develop a new set of compiler algorithms that predict which data will and will not be reused and provide these hints to the architecture. We prove that the hints either match or improve hit rates over LRU. We describe a practical one-bit cache-line tag implementation of our algorithm, called evict-me. On a cache replacement, the architecture will replace a line for which the evict-me bit is set, or if none is set, it will use the LRU bits. We implement our compiler analysis and its output in the Scale compiler. On a variety of scientific programs, using the evict-me algorithm in both the level 1 and 2 caches improves simulated cycle times by up to 34\% over the LRU policy by increasing hit rates. In addition, a combination of simple hardware prefetching and evict-me works together to further improve performance.},
  eventtitle = {Proceedings.{{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  booktitle = {Proceedings.{{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  date = {2002-09},
  pages = {199-208},
  keywords = {Computer science,Hardware,cache storage,Microarchitecture,Prefetching,Bridges,Prediction algorithms,Computer architecture,Microprocessors,Clocks,content-addressable storage,performance evaluation,storage management,Delay,microprocessor performance,memory performance,program compilers,cache replacement decisions,evict-me,hardware prefetching,hit rates,LRU replacement policies,one-bit cache-line tag implementation,program compiler,Scale compiler,scientific programs,set-associative cache},
  author = {{Zhenlin Wang} and McKinley, K. S. and Rosenberg, A. L. and Weems, C. C.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZR6QY6PF\\Zhenlin Wang et al. - 2002 - Using the compiler to improve cache replacement de.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DFHI92ZY\\1106018.html}
}

@inproceedings{smaragdakisGeneralAdaptiveReplacement2004,
  langid = {english},
  title = {General Adaptive Replacement Policies},
  isbn = {978-1-58113-945-7},
  url = {http://portal.acm.org/citation.cfm?doid=1029873.1029887},
  doi = {10.1145/1029873.1029887},
  abstract = {We propose a general scheme for creating adaptive replacement policies with good performance and strong theoretical guarantees. Speciﬁcally, we show how to combine any two existing replacement policies so that the resulting policy provably can never perform worse than either of the original policies by more than a small factor. To show that our scheme performs very well with real application data, we derive a virtual memory replacement policy that adapts between LRU, loop detection, LFU, and MRU-like replacement. The resulting policy often performs better than all of the policies it adapts over, as well as two other hand-tuned adaptive policies from the recent literature.},
  publisher = {{ACM Press}},
  urldate = {2019-08-05},
  date = {2004},
  pages = {108},
  author = {Smaragdakis, Yannis},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8HFTAD4R\\Smaragdakis - 2004 - General adaptive replacement policies.pdf}
}

@inproceedings{alghazoSFLRUCacheReplacement2004,
  title = {{{SF}}-{{LRU}} Cache Replacement Algorithm},
  doi = {10.1109/MTDT.2004.1327979},
  abstract = {In this paper we propose a replacement algorithm, SF-LRU (second chance-frequency - least recently used) that combines the LRU (least recently used) and the LFU (least frequently used) using the second chance concept. A comprehensive comparison is made between our algorithm and both LRU and LFU algorithms. Experimental results show that the SF-LRU significantly reduces the number of cache misses compared the other two algorithms. Simulation results show that our algorithm can provide a maximum value of approximately 6.3\% improvement in the miss ratio over the LRU algorithm in data cache and approximately 9.3\% improvement in miss ratio in instruction cache. This performance improvement is attributed to the fact that our algorithm provides a second chance to the block that may be deleted according to LRU's rules. This is done by comparing the frequency of the block with the block next to it in the set.},
  eventtitle = {Records of the 2004 {{International Workshop}} on {{Memory Technology}}, {{Design}} and {{Testing}}, 2004.},
  booktitle = {Records of the 2004 {{International Workshop}} on {{Memory Technology}}, {{Design}} and {{Testing}}, 2004.},
  date = {2004-08},
  pages = {19-24},
  keywords = {Costs,cache storage,History,Bridges,Clocks,System performance,Energy consumption,low-power electronics,performance improvement,Delay,Cache memory,Frequency,Application software,cache replacement algorithm,cache misses reduction,data cache,instruction cache,LFU,low power cache,LRU rules,miss ratio improvement,second chance concept,second chance-frequency - least recently used,SF-LRU},
  author = {Alghazo, J. and Akaaboune, A. and Botros, N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\QAY5LIZV\\Alghazo et al. - 2004 - SF-LRU cache replacement algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HU2EYP6D\\1327979.html}
}

@article{reinekeRandomizedCachesConsidered2014,
  langid = {english},
  title = {Randomized {{Caches Considered Harmful}} in {{Hard Real}}-{{Time Systems}}},
  volume = {Vol 1},
  url = {http://ojs.dagstuhl.de/index.php/lites/article/view/LITES-v001-i001-a003},
  doi = {10.4230/lites-v001-i001-a003},
  abstract = {We investigate the suitability of caches with randomized placement and replacement in the context of hard real-time systems. Such caches have been claimed to drastically reduce the amount of information required by static worst-case execution time (WCET) analysis, and to be an enabler for measurement-based probabilistic timing analysis. We refute these claims and conclude that with prevailing static and measurement-based analysis techniques caches with deterministic placement and least-recently-used replacement are preferable over randomized ones.},
  journaltitle = {Leibniz Transactions on Embedded Systems},
  urldate = {2019-08-05},
  date = {2014-06-10},
  pages = {No 1 (2014)-},
  author = {Reineke, Jan},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SNUCVCYD\\Reineke - 2014 - Randomized Caches Considered Harmful in Hard Real-.pdf}
}

@inproceedings{milenkovicPerformanceEvaluationMemory2003,
  title = {A Performance Evaluation of Memory Hierarchy in Embedded Systems},
  doi = {10.1109/SSST.2003.1194606},
  abstract = {The increasing speed gap between processors and memory makes the design of memory hierarchy one of the critical issues in general purpose embedded systems. As memory requirements for embedded applications grow, especially in emerging area of handheld multimedia devices, cache memories become crucial for providing high performance and reducing power. This paper describes a performance evaluation of typical cache design issues such as cache size and organization, block size, and replacement policy. The evaluation is done using simulation tools for architectural exploration based on ARM instruction set and MiBench benchmark suite. Our performance evaluation includes monitoring of dynamic cache behavior, since embedded systems designers are interested not only in the total number of cache misses, but also in the number of cache misses throughout application execution.},
  eventtitle = {Proceedings of the 35th {{Southeastern Symposium}} on {{System Theory}}, 2003.},
  booktitle = {Proceedings of the 35th {{Southeastern Symposium}} on {{System Theory}}, 2003.},
  date = {2003-03},
  pages = {427-431},
  keywords = {embedded systems,Monitoring,Costs,Embedded system,cache storage,memory hierarchy,Computer architecture,Clocks,memory architecture,performance evaluation,Cache memory,Embedded computing,Performance gain,Design engineering,cache misses,cache size,replacement policy,ARM instruction set,block size,cache memories,cache organization,Computer industry,dynamic behavior,handheld multimedia devices,MiBench benchmark suite,simulation tools},
  author = {Milenkovic, A. and Milenkovic, M. and Barnes, N.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WI5X4Q2N\\Milenkovic et al. - 2003 - A performance evaluation of memory hierarchy in em.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KF8Q7DSM\\1194606.html}
}

@inproceedings{soryaniPerformanceEvaluationCache2007,
  title = {Performance {{Evaluation}} of {{Cache Memory Organizations}} in {{Embedded Systems}}},
  doi = {10.1109/ITNG.2007.150},
  abstract = {The tremendous rise in microprocessor technology has offered high speed processors and has increased the processor-memory speed gap dramatically. On the other hand, real-time embedded systems often have a hard deadline to complete their instructions. Consequently, the design of cache memory hierarchy is a critical issue in embedded systems. This paper describes a simulation-based performance evaluation of typical cache design issues in embedded systems such as using split caches for data and instruction versus unified cache for data and instruction, cache size and associativity and replacement policy. The evaluation is done using SimpleScalar simulation tools based on its Alpha version. We select some benchmarks for this study based on some previous researches about the clustering of SPEC CPU2000 benchmark suite. The contribution of this work is identifying important parameters for cache design in general-purpose embedded systems. Our results show that the Pseudo LRU techniques for cache replacement, such as MRU can approximate LRU with much lower complexity for a wide variety of cache sizes and degree of associativities},
  eventtitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  booktitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  date = {2007-04},
  pages = {1045-1050},
  keywords = {embedded systems,Hardware,Costs,Embedded system,Real time systems,cache storage,Computer architecture,Microprocessors,integrated circuit design,performance evaluation,Cache memory,Embedded computing,High performance computing,Optimized production technology,cache memory hierarchy design,cache memory organization,real-time embedded systems,split caches,unified cache},
  author = {Soryani, M. and Sharifi, M. and Rezvani, M. H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HMGSFECC\\Soryani et al. - 2007 - Performance Evaluation of Cache Memory Organizatio.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8RWY5RDL\\4151842.html}
}

@article{altmeyerStaticProbabilisticTiming2015,
  langid = {english},
  title = {Static Probabilistic Timing Analysis for Real-Time Systems Using Random Replacement Caches},
  volume = {51},
  issn = {1573-1383},
  url = {https://doi.org/10.1007/s11241-014-9218-4},
  doi = {10.1007/s11241-014-9218-4},
  abstract = {In this paper, we investigate static probabilistic timing analysis (SPTA) for single processor real-time systems that use a cache with an evict-on-miss random replacement policy. We show that previously published formulae for the probability of a cache hit can produce results that are optimistic and unsound when used to compute probabilistic worst-case execution time (pWCET) distributions. We investigate the correctness, optimality, and precision of different approaches to SPTA for random replacement caches. We prove that one of the previously published formulae for the probability of a cache hit is optimal with respect to the limited information (reuse distance and cache associativity) that it uses. We derive an alternative formulation that makes use of additional information in the form of the number of distinct memory blocks accessed (the stack distance). This provides a complementary lower bound that can be used together with previously published formula to obtain more accurate analysis. We improve upon this joint approach by using extra information about cache contention. To investigate the precision of various approaches to SPTA, we introduce a simple exhaustive method that computes a precise pWCET distribution, albeit at the cost of exponential complexity. We integrate this precise approach, applied to small numbers of frequently accessed memory blocks, with imprecise analysis of other memory blocks, to form a combined approach that improves precision, without significantly increasing complexity. The performance of the various approaches are compared on benchmark programs. We also make comparisons against deterministic analysis of the least recently used replacement policy.},
  number = {1},
  journaltitle = {Real-Time Systems},
  shortjournal = {Real-Time Syst},
  urldate = {2019-08-05},
  date = {2015-01-01},
  pages = {77-123},
  keywords = {WCET analysis,Random cache replacement,Static probabilistic timing analysis,Timing verification},
  author = {Altmeyer, Sebastian and Cucu-Grosjean, Liliana and Davis, Robert I.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7B73XNLS\\Altmeyer et al. - 2015 - Static probabilistic timing analysis for real-time.pdf}
}

@article{limCacheReplacementPolicy2010,
  title = {A Cache Replacement Policy to Reduce Cache Miss Rate for Multiprocessor Architecture},
  volume = {7},
  doi = {10.1587/elex.7.850},
  abstract = {In this paper, a new cache replacement policy named Selection Alternative Replacement (SAR), which minimizes shared cache miss rate in chip multi-processor architecture, is proposed. A variety of cache replacement policies have been used for minimizing the cache misses. However, replacing cache items which have high utilization leads to additional cache misses. SAR policy stores the labels of discarded cache items and uses stored information to prevent additional cache misses. The results of experiments show that the SAR policy decreases cache miss rate by 6.01\% averagely and enhances instruction per cycle by 7.01\% averagely compared with the conventional pseudo least recently used policy.},
  number = {12},
  journaltitle = {IEICE Electronics Express},
  date = {2010},
  pages = {850-855},
  keywords = {cache miss rate,cache replacement policy,instruction per cycle,least frequently used policy},
  author = {Lim, Ho and Kim, Jaehwan and Chong, Jong-wha},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LECGWPPK\\Lim et al. - 2010 - A cache replacement policy to reduce cache miss ra.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2SMXMQS6\\ja.html}
}

@article{beladyStudyReplacementAlgorithms1966,
  title = {A Study of Replacement Algorithms for a Virtual-Storage Computer},
  volume = {5},
  issn = {0018-8670},
  doi = {10.1147/sj.52.0078},
  abstract = {One of the basic limitations of a digital computer is the size of its available memory.1In most cases, it is neither feasible nor economical for a user to insist that every problem program fit into memory. The number of words of information in a program often exceeds the number of cells (i.e., word locations) in memory. The only way to solve this problem is to assign more than one program word to a cell. Since a cell can hold only one word at a time, extra words assigned to the cell must be held in external storage. Conventionally, overlay techniques are employed to exchange memory words and external-storage words whenever needed; this, of course, places an additional planning and coding burden on the programmer. For several reasons, it would be advantageous to rid the programmer of this function by providing him with a “virtual” memory larger than his program. An approach that permits him to use a sufficiently large address range can accomplish this objective, assuming that means are provided for automatic execution of the memory-overlay functions.},
  number = {2},
  journaltitle = {IBM Systems Journal},
  date = {1966},
  pages = {78-101},
  author = {Belady, L. A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YP53KECD\\Belady - 1966 - A study of replacement algorithms for a virtual-st.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EGMDU6VS\\5388441.html}
}

@inproceedings{quinonesUsingRandomizedCaches2009,
  langid = {english},
  title = {Using {{Randomized Caches}} in {{Probabilistic Real}}-{{Time Systems}}},
  isbn = {978-0-7695-3724-5},
  url = {http://ieeexplore.ieee.org/document/5161509/},
  doi = {10.1109/ECRTS.2009.30},
  abstract = {While hardware caches are generally effective at improving application performance, they greatly complicate performance prediction. Slight changes in memory layout or data access patterns can lead to large and systematic increases in cache misses, degrading performance. In the worst case, these misses can effectively render the cache useless. These pathological cases, or “cache risk patterns”, are difﬁcult to predict, test or debug, and their presence limits the usefulness of caches in safety critical real-time systems, especially in hard real-time environments.},
  publisher = {{IEEE}},
  urldate = {2019-08-06},
  date = {2009-07},
  pages = {129-138},
  author = {Quiñones, Eduardo and Berger, Emery D. and Bernat, Guillem and Cazorla, Francisco J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JQHCGV28\\Quiñones et al. - 2009 - Using Randomized Caches in Probabilistic Real-Time.pdf}
}

@inproceedings{olanrewajuStudyPerformanceEvaluation2016,
  title = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms: {{A}} Review},
  doi = {10.1109/PDGC.2016.7913185},
  shorttitle = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms},
  abstract = {Cache Replacement Policies play a significant and contributory role in the context of determining the effectiveness of cache memory cells. It has also become one of the major key features for efficient memory management from the technological aspect. Hence, owing to the existing critical computing systems, it has become too essential to attain faster processing of executable instructions under any adverse situations. In the current scenario, the state of art processors such as Intel multi-core processors for application specific integrated circuits, usually employ various cache replacement policies such as Least Recently Used (LRU) and Pseudo LRU (pLRU), Round Robin, etc. However, fewer amounts of existing research works are found till date to utter about explicit performance issues associated with the conventional cache replacement algorithms. Therefore, the proposed study intended to carry out a performance evaluation to explore the design space of conventional cache replacement policies under SPEC CPU2000 benchmark suite. It initiates and configures the experimental Simple Scalar toolbox prototype on a wide range of cache sizes. The experimental outcomes obtained from the benchmark suite show that PLRU outperforms the conventional LRU concerning computational complexity and a variety of cache blocks organization.},
  eventtitle = {2016 {{Fourth International Conference}} on {{Parallel}}, {{Distributed}} and {{Grid Computing}} ({{PDGC}})},
  booktitle = {2016 {{Fourth International Conference}} on {{Parallel}}, {{Distributed}} and {{Grid Computing}} ({{PDGC}})},
  date = {2016-12},
  pages = {550-556},
  keywords = {Context,Benchmark testing,cache storage,performance evaluation,storage management,Performance evaluation,Cache memory,Space exploration,replacement algorithms,cache replacement algorithms,memory management,cache memory,cache blocks organization,cache memory cells,cache replacement policies,Decision support systems,Heuristic algorithms,Simple Scalar toolbox prototype,SPEC CPU2000 benchmark suite},
  author = {Olanrewaju, R. F. and Baba, A. and Khan, B. U. I. and Yaacob, M. and Azman, A. W. and Mir, M. S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XDIQGC4I\\Olanrewaju et al. - 2016 - A study on performance evaluation of conventional .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6QTGL6JS\\7913185.html}
}

@article{smithCacheMemories1982,
  title = {Cache {{Memories}}},
  volume = {14},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/356887.356892},
  doi = {10.1145/356887.356892},
  number = {3},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2019-08-06},
  date = {1982-09},
  pages = {473--530},
  author = {Smith, Alan Jay},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JALV7R5I\\Smith - 1982 - Cache Memories.pdf}
}

@inproceedings{kumarOverviewModernCache2016,
  title = {An Overview of Modern Cache Memory and Performance Analysis of Replacement Policies},
  doi = {10.1109/ICETECH.2016.7569243},
  abstract = {Memory hierarchy in current generation computers is formed by keeping registers inside, cache on or outside the processor and virtual memory on Hard disk. The principle of locality of reference is used to make memory hierarchy work efficiently. In recent years various advances have been made to improve the cache memory performance on the basis of hit rate, latency, speed, replacement policies and energy consumption. Cache replacement policy is one of the important design parameter which affects the overall processor performance and also become more important with recent technological moves towards highly associative cache. This paper yields a survey of current generation processors on the basis of various factors effecting cache memory performance and throughput. The main focus of this paper is the study and performance analysis of the cache replacement policies on the basis of simulation on several benchmarks.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  booktitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  date = {2016-03},
  pages = {210-214},
  keywords = {Conferences,Organizations,Hardware,Registers,Multicore processing,microprocessor chips,cache storage,memory hierarchy,Clocks,energy consumption,memory architecture,performance evaluation,hit rate,performance improvement,Cache memory,processor performance,benchmark testing,virtual storage,cache memory,LFU,cache replacement policy,associative cache,Cache Performance,current generation processors,design parameter,FIFO,hard disk,Hit rate,LRU,Miss rate,performance analysis,RANDOM,registers,Replacement Policy,virtual memory},
  author = {Kumar, S. and Singh, P. K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4LDS84RN\\Kumar and Singh - 2016 - An overview of modern cache memory and performance.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PEAXWT9X\\7569243.html}
}

@article{karedlaCachingStrategiesImprove1994,
  title = {Caching Strategies to Improve Disk System Performance},
  volume = {27},
  issn = {0018-9162},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  number = {3},
  journaltitle = {Computer},
  date = {1994-03},
  pages = {38-46},
  keywords = {Operating systems,Hardware,Costs,Algorithm design and analysis,buffer storage,System performance,performance evaluation,Delay,Throughput,Control systems,latency,cache replacement algorithms,caching strategies,computer system hierarchy,data throughput,disk rotation speeds,disk system performance,Drives,frequency-based variation,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,least recently used algorithm,magnetic disc storage,random replacement,segmented LRU,solid-state disks,system response time,Terminology},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KLR6PJKM\\Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ETYTQD2H\\268884.html}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004b,
  langid = {english},
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  isbn = {978-1-58113-870-2},
  url = {http://portal.acm.org/citation.cfm?doid=986537.986601},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  publisher = {{ACM Press}},
  urldate = {2019-08-06},
  date = {2004},
  pages = {267},
  author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VPWMJPZB\\Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf}
}

@inproceedings{wei-chetsengPRRLowoverheadCache2012,
  title = {{{PRR}}: {{A}} Low-Overhead Cache Replacement Algorithm for Embedded Processors},
  doi = {10.1109/ASPDAC.2012.6164972},
  shorttitle = {{{PRR}}},
  abstract = {In embedded systems power consumption and area tightly constrain the cache capacity and management logic. Many good cache replacement policies have been proposed in the past, but none approach the performance of the least recently used (LRU) algorithm without incurring high overheads. In fact, many embedded designers consider even pseudo-LRU too complex for their embedded systems processors. In this paper, we propose a new level 1 (L1) data cache replacement algorithm, Protected Round-Robin (PRR) that is simple enough to be incorporated into embedded processors while providing miss rates that are very similar to the miss rates of LRU. Our experiments showed that on average the miss rates of PRR are only 0.22\% higher than the miss rates of LRU on a 32KB, 4-way L1 data cache with 32 byte long cache lines. PRR has miss rates that are on average 4.72\% and 4.66\% lower than random and round-robin replacement algorithms, respectively.},
  eventtitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  booktitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  date = {2012-01},
  pages = {35-40},
  keywords = {embedded systems,Benchmark testing,Multicore processing,Program processors,Algorithm design and analysis,cache storage,Complexity theory,Embedded systems,cache replacement policies,least recently used algorithm,cache capacity,data cache replacement algorithm,embedded systems power consumption,embedded systems processor,low-overhead cache replacement algorithm,LRU algorithm,management logic,miss rates,protected round-robin,pseudoLRU,random algorithm,Round robin,round-robin replacement algorithm},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EMTYUQ72\\Wei-Che Tseng et al. - 2012 - PRR A low-overhead cache replacement algorithm fo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SB2XYXES\\6164972.html}
}

@inproceedings{faresPerformanceEvaluationTraditional2012,
  title = {Performance {{Evaluation}} of {{Traditional Caching Policies}} on a {{Large System}} with {{Petabytes}} of {{Data}}},
  doi = {10.1109/NAS.2012.32},
  abstract = {Caching is widely known to be an effective method for improving I/O performance by storing frequently used data on higher speed storage components. However, most existing studies that focus on caching performance evaluate fairly small files populating a relatively small cache. Few reports are available that detail the performance of traditional cache replacement policies on extremely large caches. Do such traditional caching policies still work effectively when applied to systems with petabytes of data? In this paper, we comprehensively evaluate the performance of several cache policies, which include First-In-First-Out (FIFO), Least Recently Used (LRU) and Least Frequently Used (LFU), on the global satellite imagery distribution application maintained by the U.S. Geological Survey (USGS) Earth Resources Observation and Science Center (EROS). Evidence is presented suggesting traditional caching policies are capable of providing performance gains when applied to large data sets as with smaller data sets. Our evaluation is based on approximately three million real-world satellite images download requests representing global user download behavior since October 2008.},
  eventtitle = {2012 {{IEEE Seventh International Conference}} on {{Networking}}, {{Architecture}}, and {{Storage}}},
  booktitle = {2012 {{IEEE Seventh International Conference}} on {{Networking}}, {{Architecture}}, and {{Storage}}},
  date = {2012-06},
  pages = {227-234},
  keywords = {Computer science,cache storage,Servers,performance evaluation,Performance evaluation,cache replacement policy,least frequently used policy,caching performance evaluation,data petabytes,Earth,Earth Resources Observation and Science Center,EROS,FIFO policy,first-in-first-out policy,frequently used data,global satellite imagery distribution application,global user download behavior,higher speed storage components,I/O performance,least recently used policy,LFU policy,LRU policy,NASA,performance gains,real-world satellite images download requests,Remote sensing,Satellites,traditional caching policy,U.S. Geological Survey,USGS},
  author = {Fares, R. and Romoser, B. and Zong, Z. and Nijim, M. and Qin, X.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RDRSFF3U\\Fares et al. - 2012 - Performance Evaluation of Traditional Caching Poli.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RVYUGNMK\\6310897.html}
}

@inproceedings{rajanEmulatingOptimalReplacement2007,
  title = {Emulating {{Optimal Replacement}} with a {{Shepherd Cache}}},
  doi = {10.1109/MICRO.2007.25},
  abstract = {The inherent temporal locality in memory accesses is filtered out by the L1 cache. As a consequence, an L2 cache with LRU replacement incurs significantly higher misses than the optimal replacement policy (OPT). We propose to narrow this gap through a novel replacement strategy that mimics the replacement decisions of OPT. The L2 cache is logically divided into two components, a Shepherd Cache (SC) with a simple FIFO replacement and a Main Cache (MC) with an emulation of optimal replacement. The SC plays the dual role of caching lines and guiding the replacement decisions in MC. Our proposed organization can cover 40\% of the gap between OPT and LRU for a 2MB cache resulting in 7\% overall speedup. Comparison with the dynamic insertion policy, a victim buffer, a V-Way cache and an LRU based fully associative cache demonstrates that our scheme performs better than all these strategies.},
  eventtitle = {40th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}} 2007)},
  booktitle = {40th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}} 2007)},
  date = {2007-12},
  pages = {445-454},
  keywords = {Computer science,cache storage,History,Microarchitecture,temporal locality,content-addressable storage,Frequency,memory access,Proposals,Emulation,Optimized production technology,associative cache,Automation,Computer science education,dynamic insertion policy,FIFO replacement,L1 cache,L2 cache,main cache,optimal replacement policy,Shepherd cache,Supercomputers},
  author = {Rajan, K. and Ramaswamy, G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WKTJIP29\\4408275.html}
}

@inproceedings{khalidPerformanceEvaluationNew1996,
  title = {Performance Evaluation of a New Cache Replacement Scheme Using {{SPEC}}},
  doi = {10.1109/PCCC.1996.493626},
  abstract = {Presents a new neural network-based algorithm called KORA (Khalid-Obaidat Replacement Algorithm), that uses a backpropagation neural network (BPNN) for the purpose of guiding the line/block replacement decisions in a cache. The KORA algorithm attempts to approximate the replacement decisions made by the optimal scheme (OPT). The key to our algorithm is to identify and subsequently discard the dead lines in cache memories. This allows our algorithm to provide better cache performance as compared to the conventional LRU (least recently used), MRU (most recently used) and FIFO (first-in, first-out) replacement policies. Extensive trace-driven simulations were performed for 30 different cache configurations using different SPEC (Standard Performance Evaluation Corp.) programs. Simulation results have shown that KORA can provide a substantial improvement in the miss ratio over the conventional algorithms. Our work opens up new dimensions for research in the development of new and improved page replacement schemes for virtual memory systems and disk caches.},
  eventtitle = {Conference {{Proceedings}} of the 1996 {{IEEE Fifteenth Annual International Phoenix Conference}} on {{Computers}} and {{Communications}}},
  booktitle = {Conference {{Proceedings}} of the 1996 {{IEEE Fifteenth Annual International Phoenix Conference}} on {{Computers}} and {{Communications}}},
  date = {1996-03},
  pages = {144-150},
  keywords = {Educational institutions,Costs,cache storage,History,performance evaluation,Neural networks,Performance evaluation,Cache memory,High performance computing,Cities and towns,Optimized production technology,paged storage,cache miss ratio,backpropagation,Backpropagation algorithms,backpropagation neural network,block replacement decisions,cache configurations,cache replacement scheme,dead line discarding,discrete event simulation,disk caches,feedforward neural nets,Khalid-Obaidat Replacement Algorithm,KORA algorithm,line replacement decisions,optimal scheme,page replacement schemes,SPEC benchmark programs,Standard Performance Evaluation Corp.,trace-driven simulations,virtual memory systems},
  author = {Khalid, H. and Obaidat, M. S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PKS4BSLC\\493626.html}
}

@inproceedings{ghasemzadehPseudoFIFOArchitectureLRU2005,
  title = {Pseudo-{{FIFO Architecture}} of {{LRU Replacement Algorithm}}},
  doi = {10.1109/INMIC.2005.334496},
  abstract = {Cache replacement algorithms have been widely used in modern computer systems to reduce the number of cache misses. The LRU algorithm has been shown to be an efficient replacement policy in terms of miss rates. However, most of the processors employ a block replacement algorithm which is very simple to implement in hardware or that is an approximation to the true LRU. In this paper, we propose a new implementation of block replacement algorithms in CPU caches by designing the circuitry required to implement an LRU replacement policy in set associative caches. We propose a simple and efficient architecture, Pseudo-FIFO, such that the true LRU replacement algorithm can be implemented without the disadvantages of the traditional implementations. Experimental results show that the Pseudo-FIFO significantly reduces the number of memory cells needed for hardware implementation. Simulation results reveal that our proposed architecture can provide an average value of 26\% improvement in the chip area compared to "reference matrix" and "basic architecture" circuits. Furthermore, it operates about 2.4 times faster than other architectures},
  eventtitle = {2005 {{Pakistan Section Multitopic Conference}}},
  booktitle = {2005 {{Pakistan Section Multitopic Conference}}},
  date = {2005-12},
  pages = {1-7},
  keywords = {Hardware,Central Processing Unit,Algorithm design and analysis,cache storage,Computer architecture,Cache memory,Optical computing,Circuit simulation,High performance computing,cache misses,cache replacement algorithms,set associative caches,replacement policy,Approximation algorithms,basic architecture circuits,block replacement algorithm,chip area,first in first out,least recently used",LRU replacement algorithm,memory cells,Modems,pseudo FIFO architecture,reference matrix},
  author = {Ghasemzadeh, H. and Fatemi, S. O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HFKKRY28\\Ghasemzadeh and Fatemi - 2005 - Pseudo-FIFO Architecture of LRU Replacement Algori.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7QVD44TI\\4133511.html}
}

@inproceedings{guptaPreeminentPairReplacement2009,
  title = {Preeminent Pair of Replacement Algorithms for {{L1}} and {{L2}} Cache for Proxy Server},
  doi = {10.1109/AHICI.2009.5340334},
  abstract = {Access to the internet and WWW is growing extensively, which results in heavy network traffic. To reduce the network traffic proxy server is used. Proxy server reduces the load of server. If the cache replacement algorithm of proxy server's cache is efficient then proxy server will be helpful to reduce the network traffic in more efficient manner. In this paper we are considering proxy server cache to be Level 1 (L1) cache and storage cache of proxy server to be Level 2 (L2) cache. For collecting the real trace CC proxy server is used in an organization. Log of proxy server gives the information of various URLs accessed by various clients with time. For performing experiments various URLs were given a numeric identity. This paper proposes an efficient replacement algorithm on L1 for proxy server. The replacement algorithms taken into consideration are Least Recently Used (LRU), Least Frequently Used (LFU), First In First Out (FIFO). Access Pattern of L1 and L2 are different as if the desired page is not on L1 then it accesses to L2. Thus L1 is having better temporal locality than L2. Thus the replacement algorithm which is giving efficient results for L1 may not be suitable for L2. This paper also proposes a preeminent pair of replacement algorithms for L1 and L2 cache for proxy server.},
  eventtitle = {2009 {{First Asian Himalayas International Conference}} on {{Internet}}},
  booktitle = {2009 {{First Asian Himalayas International Conference}} on {{Internet}}},
  date = {2009-11},
  pages = {1-5},
  keywords = {Network servers,Computer science,Costs,cache storage,Delay,cache replacement algorithm,IP networks,Cache storage,client,heavy network traffic,Internet,least frequently used algorithms,least recently used algorithms,Level 1 cache,Level 1 Cache (L1),Level 2 cache,Level 2 Cache (L2),network servers,network traffic proxy server,Proxy server,proxy server cache,Replacement Algorithm,storage cache,Telecommunication traffic,Uniform resource locators,web access pattern,Web server,World Wide Web},
  author = {Gupta, R. and Tokekar, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KF4B3I2S\\5340334.html}
}

@article{podlipnigSurveyWebCache2003,
  title = {A {{Survey}} of {{Web Cache Replacement Strategies}}},
  volume = {35},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/954339.954341},
  doi = {10.1145/954339.954341},
  abstract = {Web caching is an important technique to scale the Internet. One important performance factor of Web caches is the replacement strategy. Due to specific characteristics of the World Wide Web, there exist a huge number of proposals for cache replacement. This article proposes a classification for these proposals that subsumes prior classifications. Using this classification, different proposals and their advantages and disadvantages are described. Furthermore, the article discusses the importance of cache replacement strategies in modern proxy caches and outlines potential future research topics.},
  number = {4},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2019-08-30},
  date = {2003-12},
  pages = {374--398},
  keywords = {Web caching,replacement strategies},
  author = {Podlipnig, Stefan and Böszörmenyi, Laszlo},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\U8RJRSPT\\Podlipnig and Böszörmenyi - 2003 - A Survey of Web Cache Replacement Strategies.pdf}
}

@inproceedings{juanImprovedMulticoreShared2012,
  title = {An {{Improved Multi}}-Core {{Shared Cache Replacement Algorithm}}},
  doi = {10.1109/DCABES.2012.39},
  abstract = {Many multi-core processors employ a large last-level cache (LLC) shared among the multiple cores. Past research has demonstrated that traditional LRU and its approximation can lead to poor performance and unfairness when the multiple cores compete for the limited LLC capacity, and is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. As the LLC grows in capacity, associativity, the performance gap between the LRU and the theoretical optimal replacement algorithms has widened. In this paper, we propose FLRU (Frequency based LRU) replacement algorithm, which is applied to multi-core shared L2 cache, and it takes the recent access information, partition and the frequency information into consideration. FLRU manages to filter the less reused blocks through dynamic insertion/promotion policy and victim selection strategy to ensure that some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits and to avoid trashing, meanwhile we augment traditional cache partition with victim selection, insertion and promotion policies to manage shared L2 caches.},
  eventtitle = {2012 11th {{International Symposium}} on {{Distributed Computing}} and {{Applications}} to {{Business}}, {{Engineering Science}}},
  booktitle = {2012 11th {{International Symposium}} on {{Distributed Computing}} and {{Applications}} to {{Business}}, {{Engineering Science}}},
  date = {2012-10},
  pages = {13-17},
  keywords = {Educational institutions,Radiation detectors,Runtime,Multicore processing,cache storage,multicore processors,multiprocessing systems,last-level cache,LLC,USA Councils,shared cache,access information,Computers,dynamic insertion-promotion policy,FLRU,frequency based LRU replacement algorithm,frequency information,memory-intensive workloads,multi-core,multicore shared cache replacement algorithm,multicore shared L2 cache,partition information,Partitioning algorithms,replacement,victim selection strategy},
  author = {Juan, F. and Chengyan, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\88TYZ4JD\\Juan and Chengyan - 2012 - An Improved Multi-core Shared Cache Replacement Al.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GPBN84E6\\6385229.html}
}

@article{karedlaCachingStrategiesImprove1994a,
  title = {Caching Strategies to Improve Disk System Performance},
  volume = {27},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  number = {3},
  journaltitle = {Computer},
  date = {1994-03},
  pages = {38-46},
  keywords = {Operating systems,Hardware,Costs,Algorithm design and analysis,buffer storage,System performance,performance evaluation,Delay,Throughput,Control systems,latency,cache replacement algorithms,caching strategies,computer system hierarchy,data throughput,disk rotation speeds,disk system performance,Drives,frequency-based variation,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,least recently used algorithm,magnetic disc storage,random replacement,segmented LRU,solid-state disks,system response time,Terminology},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\MWSHVCFE\\Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FBWVQRK9\\268884.html}
}

@inproceedings{jinhyukyoonBufferCacheManagement2002,
  title = {Buffer Cache Management: Predicting the Future from the Past},
  doi = {10.1109/ISPAN.2002.1004268},
  shorttitle = {Buffer Cache Management},
  abstract = {Efficient and effective management of the buffer cache in the operating system becomes increasingly important as the speed gap between microprocessors and hard disks becomes wider This paper presents different techniques for predicting the future disk access patterns from the access history of each block and the access patterns detected for related blocks. The first part of the paper focuses on a block replacement policy called LRFU (least recently/frequently used) that subsumes the well-known LRU (least recently used) and the LFU (least frequently used) policies. Then, the next part discusses techniques for handling regular references such as sequential and looping references. Finally, the results from both trace-driven simulations and our implementation of the techniques within a real operating system are presented.},
  eventtitle = {Proceedings {{International Symposium}} on {{Parallel Architectures}}, {{Algorithms}} and {{Networks}}. {{I}}-{{SPAN}}'02},
  booktitle = {Proceedings {{International Symposium}} on {{Parallel Architectures}}, {{Algorithms}} and {{Networks}}. {{I}}-{{SPAN}}'02},
  date = {2002-05},
  pages = {105-110},
  keywords = {Operating systems,Computer science,cache storage,History,Microprocessors,Frequency,operating systems (computers),operating system,trace-driven simulations,access history,block replacement policy,buffer cache management,Engineering management,File systems,future disk access pattern prediction,Hard disks,least recently/frequently used policies,looping references,Pattern analysis,Pattern recognition,regular reference handling,sequential references},
  author = {{Jinhyuk Yoon} and {Sang Lyul Min} and {Yookun Cho}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6X9P9FTS\\Jinhyuk Yoon et al. - 2002 - Buffer cache management predicting the future fro.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\26CR2K4N\\1004268.html}
}

@inproceedings{aimtongkhamNovelWebCaching2016,
  title = {A Novel Web Caching Scheme Using Hybrid Least Frequently Used and Support Vector Machine},
  doi = {10.1109/JCSSE.2016.7748932},
  abstract = {The gargantuan uses of web access in various types of applications, such as text, image, audio, and video, across the globe have caused the limitation for service providers to optimally make use of Internet infrastructure. The advance of web proxy/caching has recently been in place to mitigate this phenomenon using the concept of locality and proximity. There exist some traditional caching schemes, such as FIFO, LFU, LRU, and Size, but with key limitations on the precision. On the other hands, soft computing has recently been investigated due to its advantage of high precision. Thus, this paper proposes a novel caching method by integrating these twos. SVM was first used for classification, to divide the caching probability - to be replaced or else. Then, LFU was applied for the actual replacement given new web objects (if cache full); and these are Hybrid LFU-SVM. Its performance is practically confirmed from our intensive evaluation against SVM-LRU and its traditional schemes like LFU and LRU in order of 14\% to 52.3\% and 18\% to 63.2\%, for hit and byte hit rate, respectively, using a standard NLANR dataset.},
  eventtitle = {2016 13th {{International Joint Conference}} on {{Computer Science}} and {{Software Engineering}} ({{JCSSE}})},
  booktitle = {2016 13th {{International Joint Conference}} on {{Computer Science}} and {{Software Engineering}} ({{JCSSE}})},
  date = {2016-07},
  pages = {1-6},
  keywords = {Training,cache storage,Kernel,pattern classification,FIFO,Internet,Artificial neural networks,caching probability,Fuzzy logic,Hybrid Caching,hybrid least frequently used,hybrid LFU-SVM,information retrieval,Internet infrastructure,Least Frequency Used,locality concept,NLANR dataset,proximity concept,Proxy,Radio frequency,Replacement,service providers,Soft Computing,support vector machine,Support Vector Machine,support vector machines,Support vector machines,SVM-LRU,Web access,Web Caching,Web caching scheme,Web objects,Web proxy,Web Proxy},
  author = {Aimtongkham, P. and So-In, C. and Sanguanpong, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\R6UPCWHS\\Aimtongkham et al. - 2016 - A novel web caching scheme using hybrid least freq.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HKWFKRWW\\7748932.html}
}

@inproceedings{hasslingerOptimumCachingLRU2018,
  title = {Optimum Caching versus {{LRU}} and {{LFU}}: {{Comparison}} and Combined Limited Look-Ahead Strategies},
  doi = {10.23919/WIOPT.2018.8362880},
  shorttitle = {Optimum Caching versus {{LRU}} and {{LFU}}},
  abstract = {We compare web caching strategies based on the least recently used (LRU) and the least frequently used (LFU) replacement principles with optimum caching according to Belady's algorithm. The achievable hit rates of the strategies are shown to improve with the exploited knowledge about the request pattern while the computation effort is also increasing. The results give an overview of performance tradeoffs in the whole relevant range for web caching with Zipf request pattern. In a second part, we study a combined approach of the optimum strategy for a limited look-ahead with LRU, LFU or other non-predictive methods. We evaluate the hit rate gain depending on the extent of the look-ahead for request traces and for the independent reference model (IRM) via simulation and derive an analytic confirmation of the observed behaviour. It is shown that caching for video streaming can benefit from the proposed look-ahead technique, when replacement decisions can be partly revised due to new requests being encountered during long lasting content updates.},
  eventtitle = {2018 16th {{International Symposium}} on {{Modeling}} and {{Optimization}} in {{Mobile}}, {{Ad Hoc}}, and {{Wireless Networks}} ({{WiOpt}})},
  booktitle = {2018 16th {{International Symposium}} on {{Modeling}} and {{Optimization}} in {{Mobile}}, {{Ad Hoc}}, and {{Wireless Networks}} ({{WiOpt}})},
  date = {2018-05},
  pages = {1-6},
  keywords = {Benchmark testing,cache storage,Delays,Indexes,hit rate,Streaming media,Analytical models,Markov processes,LFU,LRU,Internet,achievable hit rates,Belady algorithm,Belady's algorithm,computation effort,exploited knowledge,hit rate gain,least frequently used (LFU),least frequently used replacement principles,least recently used (LRU),least recently used replacement principles,look-ahead strategies,look-ahead technique,optimum caching,optimum strategy,request traces,Shape,simulation,Web cache strategies,web caching strategies,Zipf distributed requests,Zipf request pattern},
  author = {Hasslinger, G. and Heikkinen, J. and Ntougias, K. and Hasslinger, F. and Hohlfeld, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\Y9PCC3ME\\Hasslinger et al. - 2018 - Optimum caching versus LRU and LFU Comparison and.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ACDTG5HE\\8362880.html}
}

@inproceedings{hasslingerComparingWebCache2018,
  title = {Comparing {{Web Cache Implementations}} for {{Fast O}}(1) {{Updates Based}} on {{LRU}}, {{LFU}} and {{Score Gated Strategies}}},
  doi = {10.1109/CAMAD.2018.8514951},
  abstract = {To be applicable to high user request workloads, web caching strategies benefit from low implementation and update effort. In this regard, the Least Recently Used (LRU) replacement principle is a simple and widely-used method. Despite its popularity, LRU has deficits in the achieved hit rate performance and cannot consider transport and network optimization criteria for selecting content to be cached. As a result, many alternatives have been proposed in the literature, which improve the cache performance at the cost of higher complexity. In this work, we evaluate the implementation complexity and runtime performance of LRU, Least Frequently Used (LFU), and score based strategies in the class of fast O(1) updates with constant effort per request. We implement Window LFU (W-LFU) within this class and show that O(1) update effort can be achieved. We further compare fast update schemes of Score Gated LRU and new Score Gated Polling (SGP). SGP is simpler than LRU and provides full flexibility for arbitrary score assessment per data object as information basis for performance optimization regarding network cost and quality measures.},
  eventtitle = {2018 {{IEEE}} 23rd {{International Workshop}} on {{Computer Aided Modeling}} and {{Design}} of {{Communication Links}} and {{Networks}} ({{CAMAD}})},
  booktitle = {2018 {{IEEE}} 23rd {{International Workshop}} on {{Computer Aided Modeling}} and {{Design}} of {{Communication Links}} and {{Networks}} ({{CAMAD}})},
  date = {2018-09},
  pages = {1-7},
  keywords = {Conferences,cache storage,Computational modeling,hit rate,Logic gates,Data structures,Complexity theory,Internet,least frequently used (LFU),least recently used (LRU),simulation,Web cache strategies,web caching strategies,achieved hit rate performance,arbitrary score assessment,cache performance,fast O(1) updates,fast updating,high user request workloads,hypermedia,implementation,information technology,least recently used replacement principle,LRU replacement principle,Microsoft Windows,network optimization criteria,performance optimization,runtime performance,score gated LRU,score gated polling,score gated polling (SGP),Score Gated strategies,transport protocols,web cache implementations,window LFU,Windows},
  author = {Hasslinger, G. and Ntougias, K. and Hasslinger, F. and Hohlfeld, O.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GBPKV9NX\\Hasslinger et al. - 2018 - Comparing Web Cache Implementations for Fast O(1) .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\Q74QF62L\\8514951.html}
}

@inproceedings{samieeWRPWeightingReplacement2008,
  title = {{{WRP}}: {{Weighting Replacement Policy}} to {{Improve Cache Performance}}},
  doi = {10.1109/CSA.2008.61},
  shorttitle = {{{WRP}}},
  abstract = {As the performance gap between memory systems and processors has increased, virtual memory management plays an important role in system performance. Different caching policies have different effects on the system performance. This paper studies an adaptive replacement policy which has low overhead on system and is easy to implement. Simulations show that our algorithm performs better than least-recently-used (LRU) and least-frequently-used (LFU). In addition, it performs similarly to LRU in worst cases.},
  eventtitle = {International {{Symposium}} on {{Computer Science}} and Its {{Applications}}},
  booktitle = {International {{Symposium}} on {{Computer Science}} and Its {{Applications}}},
  date = {2008-10},
  pages = {38-41},
  keywords = {Radiation detectors,Algorithm design and analysis,cache storage,Computational modeling,Memory management,System performance,paged storage,virtual memory management,Computers,Adaptive systems,cache performance improvement,least-frequently-used algorithm,least-recently-used algorithm,processor system,weighting adaptive page replacement policy},
  author = {Samiee, K. and Rad, G. R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WXHTTMWG\\Samiee and Rad - 2008 - WRP Weighting Replacement Policy to Improve Cache.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GUB5F8K4\\4654057.html}
}

@inproceedings{liCRFPNovelAdaptive2008,
  title = {{{CRFP}}: {{A Novel Adaptive Replacement Policy Combined}} the {{LRU}} and {{LFU Policies}}},
  doi = {10.1109/CIT.2008.Workshops.22},
  shorttitle = {{{CRFP}}},
  abstract = {A variety of cache replacement algorithms have been proposed and applied in different situations, in which the LRU (least recently used) and LFU (least frequently used) replacement policies are two of the most popular policies. However, most real systems donpsilat consider obtaining a maximized throughput by switching between the two policies in response to the access pattern. In this paper, we propose a novel adaptive replacement policy that combined the LRU and LFU Policies (CRFP); CRFP is self-tuning and can switch between different cache replacement policies adaptively and dynamically in response to the access pattern changes.Conducting simulations with a variety of file access patterns and a wide range of buffer size, we show that the CRFP outperforms other algorithms in many cases and performs the best in most of these cases.},
  eventtitle = {2008 {{IEEE}} 8th {{International Conference}} on {{Computer}} and {{Information Technology Workshops}}},
  booktitle = {2008 {{IEEE}} 8th {{International Conference}} on {{Computer}} and {{Information Technology Workshops}}},
  date = {2008-07},
  pages = {72-79},
  keywords = {Laboratories,Conferences,cache storage,Throughput,Switches,Frequency,cache replacement algorithms,Data engineering,LFU,LRU,Replacement Policy,LFU policy,LRU policy,Computers,adaptive replacement policy,Bismuth,Caching,CRFP,Database systems,file access patterns,Information technology,least frequently used replacement policy,least recently used replacement policy},
  author = {Li, Z. and Liu, D. and Bi, H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\26J4WQQQ\\Li et al. - 2008 - CRFP A Novel Adaptive Replacement Policy Combined.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TIWG2ELW\\4568482.html}
}

@inproceedings{lindemannEvaluatingImpactDifferent2002,
  title = {Evaluating the Impact of Different Document Types on the Performance of Web Cache Replacement Schemes},
  doi = {10.1109/DSN.2002.1029017},
  abstract = {In this paper, we present a comprehensive performance study of least recently used and least frequently used with dynamic aging as traditional replacement schemes as well as for the newly proposed schemes greedy dual size and greedy dual. The goal of our study constitutes the understanding how these replacement schemes deal with different web document types. Using trace-driven simulation, we present curves plotting the hit rate and byte hit rate broken down for image, HTML, multi media, and application documents. The presented results show for the first workload that under the packet cost model Greedy Dual outperforms the other schemes both in terms of hit rate and byte hit rate for image, HTML, and multi media documents. However, the advantages of Greedy Dual diminish when the workload contains more distinct multi media documents and a larger number of requests to multi media documents.},
  eventtitle = {Proceedings {{International Conference}} on {{Dependable Systems}} and {{Networks}}},
  booktitle = {Proceedings {{International Conference}} on {{Dependable Systems}} and {{Networks}}},
  date = {2002-06},
  pages = {717-726},
  keywords = {Computer science,performance evaluation,Streaming media,Performance analysis,discrete event simulation,Internet,Aging,algorithm theory,Cost function,Digital audio players,document types,dynamic aging,greedy dual size,HTML,information resources,Page description languages,performance study,Spine,trace-driven simulation,Web cache replacement schemes},
  author = {Lindemann, C. and Waldhorst, O. P.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\52RDCJFQ\\Lindemann and Waldhorst - 2002 - Evaluating the impact of different document types .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HU833HVI\\1029017.html}
}

@article{dongheeleeLRFUSpectrumPolicies2001,
  title = {{{LRFU}}: A Spectrum of Policies That Subsumes the Least Recently Used and Least Frequently Used Policies},
  volume = {50},
  doi = {10.1109/TC.2001.970573},
  shorttitle = {{{LRFU}}},
  number = {12},
  journaltitle = {IEEE Transactions on Computers},
  date = {2001-12},
  pages = {1352-1361},
  keywords = {History,Bridges,Frequency,Databases,File systems,Hard disks},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Jong-Hun Kim} and Noh, S. H. and {Sang Lyul Min} and {Yookun Cho} and {Chong Sang Kim}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VV9J789A\\Donghee Lee et al. - 2001 - LRFU a spectrum of policies that subsumes the lea.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\K8TTHISS\\970573.html}
}

@article{wangLRLRUPACSOrientedIntelligent2019,
  title = {{{LR}}-{{LRU}}: {{A PACS}}-{{Oriented Intelligent Cache Replacement Policy}}},
  volume = {7},
  doi = {10.1109/ACCESS.2019.2913961},
  shorttitle = {{{LR}}-{{LRU}}},
  abstract = {An intelligent cache replacement policy suitable for picture archiving and communication systems (PACS) was proposed in this work. By combining the logistic regression (LR) algorithm with the classic least recently used (LRU) cache replacement policy, we have created a new intelligent cache replacement policy called LR-LRU. The LR-LRU policy is unlike conventional cache replacement policies, which are solely dependent on the intrinsic properties of the cached items. Our PACS-oriented LRLRU algorithm identifies the variables that affect file access probabilities by mining medical data. The LR algorithm is then used to model the future access probabilities of the cached items, thus improving cache performance. In addition, ℓ1-regularization was used to reduce the absolute values of the variables' coefficients. This screens some variables that have little influence on the model by causing their coefficients to approach zero, which achieves the effect of screening the variables. Finally, a simulation experiment was performed using the trace-driven simulation method. It was shown that the ℓ1-regularized LR model is superior to the LR and ℓ2-regularized LR models. The LR-LRU cache algorithm significantly improves PACS cache performance when compared to conventional cache replacement policies, such as LRU, LFU, SIZE, GDF, and GDSF.},
  journaltitle = {IEEE Access},
  date = {2019},
  pages = {58073-58084},
  keywords = {cache storage,Prediction algorithms,Computer architecture,Predictive models,probability,cache replacement policy,cache replacement policies,LRU,Biomedical imaging,cached items,communication systems,hybrid storage,ℓ1-regularized LR model,least recently used cache replacement policy,logistic regression,logistic regression algorithm,Logistics,LR-LRU cache algorithm,Machine learning algorithms,PACS,PACS cache performance,PACS-oriented intelligent cache replacement policy,PACS-oriented LRLRU algorithm,picture archiving,Picture archiving and communication systems,regression analysis},
  author = {Wang, Y. and Yang, Y. and Han, C. and Ye, L. and Ke, Y. and Wang, Q.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I5UZ9CEK\\Wang et al. - 2019 - LR-LRU A PACS-Oriented Intelligent Cache Replacem.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UHKI5MXL\\8703131.html}
}

@inproceedings{santhanakrishnanGoalorientedSelftuningCaching2004,
  title = {A Goal-Oriented Self-Tuning Caching Algorithm},
  doi = {10.1109/PCCC.2004.1395012},
  abstract = {The contribution of this paper is a novel approach to adaptivity that combines alternatives rather than selecting one among alternatives. Using only three, homogenous, cache replacement algorithms, GD-GhOST were able to provide a cache replacement policy that requires no tuning or user-intervention beyond the initial selection of the performance criteria to be optimized. Overall, at its worst observed performance GD-GhOST was within approximately 1\% of the best policy's miss ratio, and at its best, GD-GhOST reduced byte miss rates by well over 50\%.},
  eventtitle = {{{IEEE International Conference}} on {{Performance}}, {{Computing}}, and {{Communications}}, 2004},
  booktitle = {{{IEEE International Conference}} on {{Performance}}, {{Computing}}, and {{Communications}}, 2004},
  date = {2004-04},
  pages = {311-312},
  keywords = {Computer science,cache storage,Measurement,Performance evaluation,Frequency,Testing,cache replacement policy,Automation,Internet,Adaptive algorithm,goal-oriented self-tuning caching algorithm,greedy dual size caching algorithm,Tuning},
  author = {Santhanakrishnan, G. and Amer, A. and Chrysanthis, P. K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GHAILCAJ\\Santhanakrishnan et al. - 2004 - A goal-oriented self-tuning caching algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AJ5ZVYBU\\1395012.html}
}

@inproceedings{robinsonDataCacheManagement1990,
  location = {{New York, NY, USA}},
  title = {Data {{Cache Management Using Frequency}}-Based {{Replacement}}},
  isbn = {978-0-89791-359-1},
  url = {http://doi.acm.org/10.1145/98457.98523},
  doi = {10.1145/98457.98523},
  abstract = {We propose a new frequency-based replacement algorithm for managing caches used for disk blocks by a file system, database management system, or disk control unit, which we refer to here as data caches. Previously, LRU replacement has usually been used for such caches. We describe a replacement algorithm based on the concept of maintaining reference counts in which locality has been “factored out”. In this algorithm replacement choices are made using a combination of reference frequency and block age. Simulation results based on traces of file system and I/O activity from actual systems show that this algorithm can offer up to 34\% performance improvement over LRU replacement, where the improvement is expressed as the fraction of the performance gain achieved between LRU replacement and the theoretically optimal policy in which the reference string must be known in advance. Furthermore, the implementation complexity and efficiency of this algorithm is comparable to one using LRU replacement.},
  booktitle = {Proceedings of the 1990 {{ACM SIGMETRICS Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  series = {{{SIGMETRICS}} '90},
  publisher = {{ACM}},
  urldate = {2019-09-12},
  date = {1990},
  pages = {134--142},
  author = {Robinson, John T. and Devarakonda, Murthy V.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2LPDE7P8\\Robinson and Devarakonda - 1990 - Data Cache Management Using Frequency-based Replac.pdf},
  venue = {Univ. of Colorado, Boulder, Colorado, USA}
}

@inproceedings{mekhielMultiLevelCacheMost29,
  langid = {english},
  location = {{Honolulu, Hawaii}},
  title = {Multi-{{Level Cache With Most Frequently Used Policy}}: {{A New Concept In Cache Design}}},
  isbn = {1-880843-14-5},
  abstract = {The number of unique references in any program represents a small part of the total number of requested references. The unique references (small part of the total requested references) consist of two types: unique references that are used several times (most frequently used references) and unique references that are used once (least frequently used references).},
  eventtitle = {Computer {{Applications}} in {{Industry}} and {{Engineering}} ({{CAINE}}-95), 8th {{Int}}'l. {{Conference}}},
  booktitle = {Proceedings of the {{ISCA}} 8th {{International Conference}}},
  date = {0029-11/1995-12-01},
  pages = {5},
  author = {Mekhiel, Nagi N},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\I4ZD4QTM\\Mekhiel - Multi-Level Cache With Most Frequently Used Policy.pdf}
}

@article{abdelfattahLeastRecentlyFive2012,
  langid = {english},
  title = {Least {{Recently Plus Five Least Frequently Replacement Policy}} ({{LR}}+{{5LF}})},
  volume = {9},
  abstract = {In this paper, we present a new block replacement policy in which we proposed a new efficient algorithm for combining two important policies Least Recently Used (LRU) and Least Frequently Used (LFU). The implementation of the proposed policy is simple. It requires limited calculations to determine the victim block. We proposed our models to implement LRU and LFU policies. The new policy gives each block in cache two weighing values corresponding to LRU and LFU policies. Then a simple algorithm is used to get the overall value for each block. A comprehensive comparison is made between our Policy and LRU, First In First Out (FIFO), V-WAY, and Combined LRU and LFU (CRF) policies. Experimental results show that the LR+5LF replacement policy significantly reduces the number of cache misses. We modified simple scalar simulator version 3 under Linux Ubuntu 9.04 and we used speccpu2000 benchmark to simulate this policy. The results of simulations showed, that giving higher weighing to LFU policy gives this policy best performance characteristics over other policies. Substantial improvement on miss rate was achieved on instruction level 1 cache and at level 2 cache memory.},
  number = {1},
  journaltitle = {The International Arab Journal of Information Technology},
  date = {2012},
  pages = {6},
  author = {AbdelFattah, Adwan and Samra, Aiman Abu},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\8VMXNYT5\\AbdelFattah and Samra - 2012 - Least Recently Plus Five Least Frequently Replacem.pdf}
}

@inproceedings{khanDecoupledDynamicCache2012,
  title = {Decoupled Dynamic Cache Segmentation},
  doi = {10.1109/HPCA.2012.6169030},
  abstract = {The least recently used (LRU) replacement policy performs poorly in the last-level cache (LLC) because temporal locality of memory accesses is filtered by first and second level caches. We propose a cache segmentation technique that dynamically adapts to cache access patterns by predicting the best number of not-yet-referenced and already-referenced blocks in the cache. This technique is independent from the LRU policy so it can work with less expensive replacement policies. It can automatically detect when to bypass blocks to the CPU with no extra overhead. In a 2MB LLC single-core processor with a memory intensive subset of SPEC CPU 2006 benchmarks, it outperforms LRU replacement on average by 5.2\% with not-recently-used (NRU) replacement and on average by 2.2\% with random replacement. The technique also complements existing shared cache partitioning techniques. Our evaluation with 10 multi-programmed workloads shows that this technique improves performance of an 8MB LLC four-core system on average by 12\%, with a random replacement policy requiring only half the space of the LRU policy.},
  eventtitle = {{{IEEE International Symposium}} on {{High}}-{{Performance Comp Architecture}}},
  booktitle = {{{IEEE International Symposium}} on {{High}}-{{Performance Comp Architecture}}},
  date = {2012-02},
  pages = {1-12},
  keywords = {Resistance,Runtime,Benchmark testing,microprocessor chips,cache storage,System-on-a-chip,last-level cache,Proposals,least recently used replacement policy,2MB LLC single-core processor,8MB LLC four-core system,already-referenced block prediction,cache access patterns,Decision trees,decoupled dynamic cache segmentation,Electronics packaging,LRU replacement,memory access temporal locality,memory intensive subset,not-recently-used replacement,not-yet-referenced block prediction,off-chip memory latency mitigation,on-chip caches,shared cache partitioning techniques,SPEC CPU 2006 benchmarks},
  author = {Khan, S. M. and Wang, Z. and Jiménez, D. A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\B7WXJSBQ\\Khan et al. - 2012 - Decoupled dynamic cache segmentation.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\92KZJS84\\6169030.html}
}

@article{qureshiSetDuelingControlledAdaptiveInsertion2008,
  title = {Set-{{Dueling}}-{{Controlled Adaptive Insertion}} for {{High}}-{{Performance Caching}}},
  volume = {28},
  doi = {10.1109/MM.2008.14},
  abstract = {The commonly used LRU replacement policy causes thrashing for memory- intensive workloads. A simple mechanism that dynamically changes the insertion policy used by LRU replacement reduces cache misses by 21 percent and requires a total storage overhead of less than 2 bytes.},
  number = {1},
  journaltitle = {IEEE Micro},
  date = {2008-01},
  pages = {91-98},
  keywords = {Hardware,cache storage,Bridges,cache,System performance,storage management,Data structures,Proposals,Cache storage,memory-intensive workloads,replacement,Art,Filters,high-performance caching,insertion,LRU replacement policy,set dueling,set sampling,set-dueling-controlled adaptive insertion,thrashing},
  author = {Qureshi, M. K. and Jaleel, A. and Patt, Y. N. and Steely, S. C. and Emer, J.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NTP2QE89\\Qureshi et al. - 2008 - Set-Dueling-Controlled Adaptive Insertion for High.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DYNNZ8PJ\\4460516.html}
}

@article{duongSCOREScoreBasedMemory2010,
  langid = {english},
  title = {{{SCORE}}: {{A Score}}-{{Based Memory Cache Replacement Policy}}},
  url = {https://hal.inria.fr/inria-00492956},
  shorttitle = {{{SCORE}}},
  abstract = {We propose SCORE, a novel adaptive cache replacement policy, which uses a score system to select a cache line to replace. Results show that SCORE o®ers low over-all miss rates on SPEC CPU2006 benchmarks, and provides an average IPC that is 4.9\% higher than LRU and 7.4\% higher than LIP.},
  urldate = {2019-09-12},
  date = {2010-06-20},
  author = {Duong, Nam and Cammarota, Rosario and Zhao, Dali and Kim, Taesu and Veidenbaum, Alex},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VBA6ZPP7\\Duong et al. - 2010 - SCORE A Score-Based Memory Cache Replacement Poli.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZLYYHHS8\\inria-00492956.html}
}

@inproceedings{liLocalityDrivenDynamicGPU2015,
  location = {{New York, NY, USA}},
  title = {Locality-{{Driven Dynamic GPU Cache Bypassing}}},
  isbn = {978-1-4503-3559-1},
  url = {http://doi.acm.org/10.1145/2751205.2751237},
  doi = {10.1145/2751205.2751237},
  abstract = {This paper presents novel cache optimizations for massively parallel, throughput-oriented architectures like GPUs. L1 data caches (L1 D-caches) are critical resources for providing high-bandwidth and low-latency data accesses. However, the high number of simultaneous requests from single-instruction multiple-thread (SIMT) cores makes the limited capacity of L1 D-caches a performance and energy bottleneck, especially for memory-intensive applications. We observe that the memory access streams to L1 D-caches for many applications contain a significant amount of requests with low reuse, which greatly reduce the cache efficacy. Existing GPU cache management schemes are either based on conditional/reactive solutions or hit-rate based designs specifically developed for CPU last level caches, which can limit overall performance. To overcome these challenges, we propose an efficient locality monitoring mechanism to dynamically filter the access stream on cache insertion such that only the data with high reuse and short reuse distances are stored in the L1 D-cache. Specifically, we present a design that integrates locality filtering based on reuse characteristics of GPU workloads into the decoupled tag store of the existing L1 D-cache through simple and cost-effective hardware extensions. Results show that our proposed design can dramatically reduce cache contention and achieve up to 56.8\% and an average of 30.3\% performance improvement over the baseline architecture, for a range of highly-optimized cache-unfriendly applications with minor area overhead and better energy efficiency. Our design also significantly outperforms the state-of-the-art CPU and GPU bypassing schemes (especially for irregular applications), without generating extra L2 and DRAM level contention.},
  booktitle = {Proceedings of the 29th {{ACM}} on {{International Conference}} on {{Supercomputing}}},
  series = {{{ICS}} '15},
  publisher = {{ACM}},
  urldate = {2019-09-12},
  date = {2015},
  pages = {67--77},
  keywords = {cache bypassing,gpu architecture optimization,locality},
  author = {Li, Chao and Song, Shuaiwen Leon and Dai, Hongwen and Sidelnik, Albert and Hari, Siva Kumar Sastry and Zhou, Huiyang},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CJNEDWI2\\Li et al. - 2015 - Locality-Driven Dynamic GPU Cache Bypassing.pdf},
  venue = {Newport Beach, California, USA}
}

@inproceedings{zhangDivideandconquerBubbleReplacement2009,
  location = {{New York, NY, USA}},
  title = {Divide-and-Conquer: {{A Bubble Replacement}} for {{Low Level Caches}}},
  isbn = {978-1-60558-498-0},
  url = {http://doi.acm.org/10.1145/1542275.1542291},
  doi = {10.1145/1542275.1542291},
  shorttitle = {Divide-and-Conquer},
  abstract = {The widely used LRU replacement policy suffers from the following problems. First, LRU does not exploit fre-quency information of cache accesses. Second, LRU may experience cache thrashing when access to cache exhibits cyclic patterns and the cache capacity is less than the working set. Finally, LRU is expensive to implement in hardware. We propose a bubble replacement for low-level caches, where cache blocks in one set are arranged in a queue for replacement determination. An incoming block enters the queue from the bottom and exchanges its posi-tion with the block above when the block hits, therefore, both recency and frequency information of a program are exploited. A victim block can be chosen from either the bottom or the top block of the queue, which is controlled by a single-bit set-hit flag per set. Choosing the bottom block as the victim makes the bubble replacement resistant to less frequently used blocks from polluting the cache while choosing the top block as the victim makes the bub-ble replacement adaptable to changes in the working set. We also propose to divide the blocks in a cache set into groups where each group implements the bubble replace-ment (we name it the DC-Bubble) to resolve the problems of the bubble replacement. The victim block is chosen ran-domly from the bottom block of each group. The proposed DC-Bubble reduces the average MPKI of the baseline 1MB 16-way L2 cache by 14\%, bridges 47\% of the gap between LRU and the OPT, reduces the storage require-ment by 61\% and simplifies the circuit design compared to LRU.},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  series = {{{ICS}} '09},
  publisher = {{ACM}},
  urldate = {2019-09-12},
  date = {2009},
  pages = {80--89},
  keywords = {cache replacement policy,divide-and-conquer,high-performance computing},
  author = {Zhang, Chuanjun and Xue, Bing},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NXVXF6HB\\Zhang and Xue - 2009 - Divide-and-conquer A Bubble Replacement for Low L.pdf},
  venue = {Yorktown Heights, NY, USA}
}

@inproceedings{dasArbitrationCacheReplacements2016,
  title = {An Arbitration on Cache Replacements Based on Frequency — {{Recency}} Product Values},
  doi = {10.1109/VLSI-SATA.2016.7593031},
  abstract = {Evolving an efficient cache replacement policy has been a challenge since the last few decades. LRU (Least Recently Used) and LFU (Least Frequently Used) cache replacement techniques and a variety of their combinations were the most sought after. This paper proposes a new combination of the LRU and LFU in such a style that the time and complexity to replace moves below the current benchmarks. Here, a frequency-recency product value is computed which dictates the cache replacement arbitration. It out performs the existing methods by a significant reduction in computational overhead.},
  eventtitle = {2016 {{International Conference}} on {{VLSI Systems}}, {{Architectures}}, {{Technology}} and {{Applications}} ({{VLSI}}-{{SATA}})},
  booktitle = {2016 {{International Conference}} on {{VLSI Systems}}, {{Architectures}}, {{Technology}} and {{Applications}} ({{VLSI}}-{{SATA}})},
  date = {2016-01},
  pages = {1-6},
  keywords = {Organizations,Radiation detectors,Very large scale integration,cache storage,Indexes,Cache memory,Complexity theory,replacement policy,LFU,LRU,frequency,least frequently used cache replacement techniques,least recently used cache replacement techniques,LFRU,Loading,minima,recency,set-associative mapping},
  author = {Das, S. and Banerjee, A.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\62NDP6DC\\Das and Banerjee - 2016 - An arbitration on cache replacements based on freq.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\H22QFACA\\7593031.html}
}

@article{tianEffectivenessbasedAdaptiveCache2014,
  title = {An Effectiveness-Based Adaptive Cache Replacement Policy},
  volume = {38},
  issn = {0141-9331},
  url = {http://www.sciencedirect.com/science/article/pii/S014193311300197X},
  doi = {10.1016/j.micpro.2013.11.011},
  abstract = {Belady’s optimal cache replacement policy is an algorithm to work out the theoretical minimum number of cache misses, but the rationale behind it was too simple. In this work, we revisit the essential function of caches to develop an underlying analytical model. We argue that frequency and recency are the only two affordable attributes of cache history that can be leveraged to predict a good replacement. Based on those two properties, we propose a novel replacement policy, the Effectiveness-Based Replacement policy (EBR) and a refinement, Dynamic EBR (D-EBR), which combines measures of recency and frequency to form a rank sequence inside each set and evict blocks with lowest rank. To evaluate our design, we simulated all 30 applications from SPEC CPU2006 for uni-core system and a set of combinations for 4-core systems, for different cache sizes. The results show that EBR achieves an average miss rate reduction of 12.4\%. With the help of D-EBR, we can tune the weight ratio between ‘frequency’ and ‘recency’ dynamically. D-EBR can nearly double the miss reduction achieved by EBR alone. In terms of hardware overhead, EBR requires half the hardware overhead of real LRU and even compared with Pseudo LRU the overhead is modest.},
  number = {1},
  journaltitle = {Microprocessors and Microsystems},
  shortjournal = {Microprocessors and Microsystems},
  urldate = {2019-09-12},
  date = {2014-02-01},
  pages = {98-111},
  keywords = {Performance,Cache memory,Configurable cache,Micro architecture,Scan resistance,Thrashing},
  author = {Tian, Geng and Liebelt, Michael},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\EWQW8W3A\\Tian and Liebelt - 2014 - An effectiveness-based adaptive cache replacement .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FEIKKXWD\\S014193311300197X.html}
}

@inproceedings{jainSoftwareassistedCacheReplacement2001a,
  title = {Software-Assisted Cache Replacement Mechanisms for Embedded Systems},
  doi = {10.1109/ICCAD.2001.968607},
  abstract = {We address the problem of improving cache predictability and performance in embedded systems through the use of software-assisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e. effectively make the element the least recently used element, or keep that cache element, i.e. the element will never be evicted. We prove basic theorems that provide conditions under which kill and keep instructions can be inserted into program code, such that the resulting performance is guaranteed to be as good as or better than the original program run using the standard LRU policy. We developed a compiler algorithm based on the theoretical results that, given an arbitrary program, determines when to perform software-assisted replacement, i.e., when to insert either a kill or keep instruction. Empirical evidence is provided that shows that performance and predictability (worst-case performance) can be improved for many programs.},
  eventtitle = {{{IEEE}}/{{ACM International Conference}} on {{Computer Aided Design}}. {{ICCAD}} 2001. {{IEEE}}/{{ACM Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{01CH37281}})},
  booktitle = {{{IEEE}}/{{ACM International Conference}} on {{Computer Aided Design}}. {{ICCAD}} 2001. {{IEEE}}/{{ACM Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{01CH37281}})},
  date = {2001-11},
  pages = {119-126},
  keywords = {Laboratories,Computer science,embedded systems,Hardware,Program processors,Embedded software,Embedded system,cache storage,System-on-a-chip,Random access memory,storage management,Software performance,program compilers,LRU policy,cache performance,cache element,cache element keep instructions,cache element kill instructions,cache predictability,cache replacement decision,Code standards,compiler algorithm,program code,software controlled state information,software instructions,software-assisted cache replacement mechanisms,software-assisted replacement,software-assisted replacement mechanisms,worst-case performance},
  author = {Jain, P. and Devadas, S. and Engels, D. and Rudolph, L.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9WPFYKQR\\Jain et al. - 2001 - Software-assisted cache replacement mechanisms for.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X9AT93S3\\968607.html}
}

@inproceedings{zhangTagbasedCacheReplacement2010,
  title = {A Tag-Based Cache Replacement},
  doi = {10.1109/ICCD.2010.5647602},
  abstract = {Conventional cache replacement policies use access information of each cache block for replacement decisions. We observe that there are many identical tags across different cache sets because programs exhibit spatial locality. The number of different tags in cache memory is significantly less than the total number of cache blocks in a cache. We propose a tag-based replacement that uses access frequency and recency of tags instead of cache blocks for the replacement decision. The tag-based replacement reduces the average miss rate of the baseline 1MB L2 cache by 15\% over conventional LRU with 95\% status bits reduction over conventional LRU. The performance improvement of a processor using the tag-based replacement is up to 40\% with an average of 4.5\% over LRU.},
  eventtitle = {2010 {{IEEE International Conference}} on {{Computer Design}}},
  booktitle = {2010 {{IEEE International Conference}} on {{Computer Design}}},
  date = {2010-10},
  pages = {92-97},
  keywords = {Hardware,Radiation detectors,Benchmark testing,cache storage,Indexes,Cache memory,cache memory,LRU,Art,access frequency,Frequency conversion,processor performance improvement,tag-based cache replacement},
  author = {Zhang, C. and Xue, B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W9SHQL3V\\Zhang and Xue - 2010 - A tag-based cache replacement.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\28FVPR5F\\5647602.html}
}

@inproceedings{subhaAlgorithmBufferCache2009,
  title = {An {{Algorithm}} for {{Buffer Cache Management}}},
  doi = {10.1109/ITNG.2009.100},
  abstract = {This paper proposes an algorithm for buffer cache management with prefetching. The buffer cache contains two units, the main cache unit and prefetch unit. The sizes of both the units are fixed. The total sizes of both the units are a constant. Blocks are fetched in one block look ahead prefetch principle. The block placement and replacement policies are defined. The replacement strategy depends on the most recently accessed block and the defined miss counts of the blocks. FIFO algorithm is used for the prefetch unit. The proposed algorithm is compared with W2 R algorithm for sequential and random input. For sequential input, the performance is comparable with that of W2 R algorithm. For random input, the proposed algorithm performs better than W2 R by 9\%.},
  eventtitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}},
  booktitle = {2009 {{Sixth International Conference}} on {{Information Technology}}: {{New Generations}}},
  date = {2009-04},
  pages = {889-893},
  keywords = {cache storage,Prefetching,data access,prefetching,replacement policy,buffer cache management,File systems,Information technology,block placement,Conference management,DBMS,FIFO algorithm,prefetch unit,Technology management},
  author = {Subha, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AS4PWXHS\\Subha - 2009 - An Algorithm for Buffer Cache Management.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WIYT5DQ5\\5070736.html}
}

@inproceedings{zhangPACPLRUCacheReplacement2011,
  title = {{{PAC}}-{{PLRU}}: {{A Cache Replacement Policy}} to {{Salvage Discarded Predictions}} from {{Hardware Prefetchers}}},
  doi = {10.1109/CCGrid.2011.27},
  shorttitle = {{{PAC}}-{{PLRU}}},
  abstract = {Cache replacement policy plays an important role in guaranteeing the availability of cache blocks, reducing miss rates, and improving applications' overall performance. However, recent research efforts on improving replacement policies require either significant additional hardware or major modifications to the organization of the existing cache. In this study, we propose the PAC-PLRU cache replacement policy. PAC-PLRU not only utilizes but also judiciously salvages the prediction information discarded from a widely-adopted stride prefetcher. The main idea behind PAC-PLRU is utilizing the prediction results generated by the existing stride prefetcher and preventing these predicted cache blocks from being replaced in the near future. Experimental results show that leveraging the PAC-PLRU with a stride prefetcher reduces the average L2 cache miss rate by 91\% over a baseline system with only PLRU policy, and by 22\% over a system using PLRU with an unconnected stride prefetcher. Most importantly, PAC-PLRU only requires minor modifications to existing cache architecture to get these benefits. The proposed PAC-PLRU policy is promising in fostering the connection between prefetching and replacement policies, and have a lasting impact on improving the overall cache performance.},
  eventtitle = {2011 11th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  booktitle = {2011 11th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Grid Computing}}},
  date = {2011-05},
  pages = {265-274},
  keywords = {Hardware,cache storage,Filtering,Microarchitecture,Prefetching,hardware prefetchers,memory architecture,memory wall,Mathematical model,computer architecture,cache blocks,cache replacement policy,Binary trees,cache architecture,high-performance processors,L2 cache miss rate,PAC-PLRU,salvage discarded predictions,stride prefetcher},
  author = {Zhang, K. and Wang, Z. and Chen, Y. and Zhu, H. and Sun, X.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2FSSUDHL\\Zhang et al. - 2011 - PAC-PLRU A Cache Replacement Policy to Salvage Di.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\LI5DFS7J\\5948617.html}
}

@inproceedings{kimLowoverheadHighperformanceUnified2000,
  location = {{Berkeley, CA, USA}},
  title = {A {{Low}}-Overhead {{High}}-Performance {{Unified Buffer Management Scheme That Exploits Sequential}} and {{Looping References}}},
  url = {http://dl.acm.org/citation.cfm?id=1251229.1251238},
  abstract = {In traditional file system implementations, the Least Recently Used (LRU) block replacement scheme is widely used to manage the buffer cache due to its simplicity and adaptability. However, the LRU scheme exhibits performance degradations because it does not make use of reference regularities such as sequential and looping references. In this paper, we present a Unified Buffer Management (UBM) scheme that exploits these regularities and yet, is simple to deploy. The UBM scheme automatically detects sequential and looping references and stores the detected blocks in separate partitions of the buffer cache. These partitions are managed by appropriate replacement schemes based on their detected patterns. The allocation problem among the divided partitions is also tackled with the use of the notion of marginal gains. In both trace-driven simulation experiments and experimental studies using an actual implementation in the FreeBSD operating system, the performance gains obtained through the use of this scheme are substantial. The results show that the hit ratios improve by as much as 57.7\% (with an average of 29.2\%) and the elapsed times are reduced by as much as 67.2\% (with an average of 28.7\%) compared to the LRU scheme for the workloads we used.},
  booktitle = {Proceedings of the 4th {{Conference}} on {{Symposium}} on {{Operating System Design}} \& {{Implementation}} - {{Volume}} 4},
  series = {{{OSDI}}'00},
  publisher = {{USENIX Association}},
  urldate = {2019-09-12},
  date = {2000},
  author = {Kim, Jong Min and Choi, Jongmoo and Kim, Jesung and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun and Kim, Chong Sang},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4HJFLF7N\\Kim et al. - 2000 - A Low-overhead High-performance Unified Buffer Man.pdf},
  venue = {San Diego, California}
}

@inproceedings{xiangLessReusedFilter2009,
  location = {{New York, NY, USA}},
  title = {Less {{Reused Filter}}: {{Improving L2 Cache Performance}} via {{Filtering Less Reused Lines}}},
  isbn = {978-1-60558-498-0},
  url = {http://doi.acm.org/10.1145/1542275.1542290},
  doi = {10.1145/1542275.1542290},
  shorttitle = {Less {{Reused Filter}}},
  abstract = {The L2 cache is commonly managed using LRU policy. For workloads that have a working set larger than L2 cache, LRU behaves poorly, resulting in a great number of less reused lines that are never reused or reused for few times. In this case, the cache performance can be improved through retaining a portion of working set in cache for a period long enough. Previous schemes approach this by bypassing never reused lines. Nevertheless, severely constrained by the number of never reused lines, sometimes they deliver no benefit due to the lack of never reused lines. This paper proposes a new filtering mechanism that filters out the less reused lines rather than just never reused lines. The extended scope of bypassing provides more opportunities to fit the working set into cache. This paper also proposes a Less Reused Filter (LRF), a separate structure that precedes L2 cache, to implement the above mechanism. LRF employs a reuse frequency predictor to accurately identify the less reused lines from incoming lines. Meanwhile, based on our observation that most less reused lines have a short life span, LRF places the filtered lines into a small filter buffer to fully utilize them, avoiding extra misses. Our evaluation, for 24 SPEC 2000 benchmarks, shows that augmenting a 512KB LRU-managed L2 cache with a LRF having 32KB filter buffer reduces the average MPKI by 27.5\%, narrowing the gap between LRU and OPT by 74.4\%.},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  series = {{{ICS}} '09},
  publisher = {{ACM}},
  urldate = {2019-09-12},
  date = {2009},
  pages = {68--79},
  keywords = {cache filtering,less reused line},
  author = {Xiang, Lingxiang and Chen, Tianzhou and Shi, Qingsong and Hu, Wei},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\DYWMAUSB\\Xiang et al. - 2009 - Less Reused Filter Improving L2 Cache Performance.pdf},
  venue = {Yorktown Heights, NY, USA}
}

@article{guptaManagingBufferCache2012,
  langid = {english},
  title = {Managing {{Buffer Cache}} by {{Block Access Pattern}}},
  volume = {9},
  abstract = {As buffer cache is used to overcome the speed gap between processor and storage devices, performance of buffer cache is a deciding factor in verifying the system performance. Need of improved buffer cache hit ratio and inabilities of the Least Recent Used replacement algorithm inspire the development of the proposed algorithm. Data reuse and program locality are the basis for determining the cache performance. The proposed algorithm determines the temporal locality by detecting the access patterns in the program context from which the I/O request are issued, identified by the program counter signature, and the files to which the I/O request are addressed. For accurate pattern detection and enhanced cache performance re-reference behavior exploited in the cache block are associated with unique signature. Use of multiple caching policies is supported by the proposed algorithm so that the cache under that pattern can be best utilized.},
  number = {6},
  date = {2012},
  pages = {7},
  author = {Gupta, Reetu and Shrawankar, Urmila},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\F4ESVHUJ\\Gupta and Shrawankar - 2012 - Managing Buffer Cache by Block Access Pattern.pdf}
}

@article{albericioExploitingReuseLocality2013,
  title = {Exploiting {{Reuse Locality}} on {{Inclusive Shared Last}}-Level {{Caches}}},
  volume = {9},
  issn = {1544-3566},
  url = {http://doi.acm.org/10.1145/2400682.2400697},
  doi = {10.1145/2400682.2400697},
  abstract = {Optimization of the replacement policy used for Shared Last-Level Cache (SLLC) management in a Chip-MultiProcessor (CMP) is critical for avoiding off-chip accesses. Temporal locality, while being exploited by first levels of private cache memories, is only slightly exhibited by the stream of references arriving at the SLLC. Thus, traditional replacement algorithms based on recency are bad choices for governing SLLC replacement. Recent proposals involve SLLC replacement policies that attempt to exploit reuse either by segmenting the replacement list or improving the rereference interval prediction. On the other hand, inclusive SLLCs are commonplace in the CMP market, but the interaction between replacement policy and the enforcement of inclusion has barely been discussed. After analyzing that interaction, this article introduces two simple replacement policies exploiting reuse locality and targeting inclusive SLLCs: Least Recently Reused (LRR) and Not Recently Reused (NRR). NRR has the same implementation cost as NRU, and LRR only adds one bit per line to the LRU cost. After considering reuse locality and its interaction with the invalidations induced by inclusion, the proposals are evaluated by simulating multiprogrammed workloads in an 8-core system with two private cache levels and an SLLC. LRR outperforms LRU by 4.5\% (performing better in 97 out of 100 mixes) and NRR outperforms NRU by 4.2\% (performing better in 99 out of 100 mixes). We also show that our mechanisms outperform rereference interval prediction, a recently proposed SLLC replacement policy and that similar conclusions can be drawn by varying the associativity or the SLLC size.},
  number = {4},
  journaltitle = {ACM Trans. Archit. Code Optim.},
  urldate = {2019-09-12},
  date = {2013-01},
  pages = {38:1--38:19},
  keywords = {Replacement policy,shared resources management},
  author = {Albericio, Jorge and Ibáñez, Pablo and Viñals, Víctor and Llabería, Jose María},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\G8LAASED\\Albericio et al. - 2013 - Exploiting Reuse Locality on Inclusive Shared Last.pdf}
}

@inproceedings{ghasemzadehModifiedPseudoLRU2006,
  title = {Modified Pseudo {{LRU}} Replacement Algorithm},
  doi = {10.1109/ECBS.2006.52},
  abstract = {Although the LRU replacement algorithm has been widely used in cache memory management, it is well-known for its inability to be easily implemented in hardware. Most of primary caches employ a simple block replacement algorithm like pseudo LRU to avoid the disadvantages of a complex hardware design. In this paper, we propose a novel block replacement scheme, MPLRU (modified pseudo LRU), by exploiting second chance concept in pseudo LRU algorithm. A comprehensive comparison is made between our algorithm and both true LRU and other conventional schemes such as FIFO, random and pseudo LRU. Experimental results show that MPLRU significantly reduces the number of cache misses compared to the other algorithms. Simulation results reveal that in average our algorithm can provide a value of 8.52\% improvement on the miss ratio compared to the pseudo LRU algorithm. Moreover, it provides 7.93\% and 11.57\%performance improvement compared to FIFO and random replacement policies respectively},
  eventtitle = {13th {{Annual IEEE International Symposium}} and {{Workshop}} on {{Engineering}} of {{Computer}}-{{Based Systems}} ({{ECBS}}'06)},
  booktitle = {13th {{Annual IEEE International Symposium}} and {{Workshop}} on {{Engineering}} of {{Computer}}-{{Based Systems}} ({{ECBS}}'06)},
  date = {2006-03},
  pages = {6 pp.-376},
  keywords = {Hardware,Costs,Algorithm design and analysis,cache storage,Computational modeling,Memory management,Delay,Cache memory,High performance computing,FIFO,least recently used algorithm,Engineering management,block replacement scheme,cache memory management,first in first out algorithm,modified pseudo LRU replacement algorithm,random LRU,Random number generation},
  author = {Ghasemzadeh, H. and Mazrouee, S. and Kakoee, M. R.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\UKBMBFKC\\Ghasemzadeh et al. - 2006 - Modified pseudo LRU replacement algorithm.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ENN8RYAP\\1607387.html}
}

@inproceedings{subramanianAdaptiveCachesEffective2006a,
  location = {{Washington, DC, USA}},
  title = {Adaptive {{Caches}}: {{Effective Shaping}} of {{Cache Behavior}} to {{Workloads}}},
  isbn = {978-0-7695-2732-1},
  url = {https://doi.org/10.1109/MICRO.2006.7},
  doi = {10.1109/MICRO.2006.7},
  shorttitle = {Adaptive {{Caches}}},
  abstract = {We present and evaluate the idea of adaptive processor cache management. Specifically, we describe a novel and general scheme by which we can combine any two cache management algorithms (e.g., LRU, LFU, FIFO, Random) and adaptively switch between them, closely tracking the locality characteristics of a given program. The scheme is inspired by recent work in virtual memory management at the operating system level, which has shown that it is possible to adapt over two replacement policies to provide an aggregate policy that always performs within a constant factor of the better component policy. A hardware implementation of adaptivity requires very simple logic but duplicate tag structures. To reduce the overhead, we use partial tags, which achieve good performance with a small hardware cost. In particular, adapting between LRU and LFU replacement policies on an 8-way 512KB L2 cache yields a 12.7\% improvement in average CPI on applications that exhibit a non-negligible L2 miss ratio. Our approach increases total cache storage by 4.0\%, but it still provides slightly better performance than a conventional 10-way setassociative 640KB cache which requires 25\% more storage.},
  booktitle = {Proceedings of the 39th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  series = {{{MICRO}} 39},
  publisher = {{IEEE Computer Society}},
  urldate = {2019-09-12},
  date = {2006},
  pages = {385--396},
  author = {Subramanian, Ranjith and Smaragdakis, Yannis and Loh, Gabriel H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7W7HCXBR\\Subramanian et al. - 2006 - Adaptive Caches Effective Shaping of Cache Behavi.pdf}
}

@inproceedings{dongheeleeImplementationPerformanceEvaluation1997,
  title = {Implementation and Performance Evaluation of the {{LRFU}} Replacement Policy},
  doi = {10.1109/EMSCNT.1997.658446},
  abstract = {Recently, a new block replacement policy called the LRFU (Least Recently/Frequently Used) policy was proposed that subsumes both the LRU and LFU policies, and provides a spectrum of replacement policies between them. We describe an implementation of the LRFU replacement policy in the FreeBSD 2.1.5 and present a performance evaluation of the implementation using the SPEC SDET benchmark. The results show that the new policy gives up to a 30\% performance improvement over the LRU block replacement policy.},
  eventtitle = {Proceedings 23rd {{Euromicro Conference New Frontiers}} of {{Information Technology}} - {{Short Contributions}} -},
  booktitle = {Proceedings 23rd {{Euromicro Conference New Frontiers}} of {{Information Technology}} - {{Short Contributions}} -},
  date = {1997-09},
  pages = {106-111},
  keywords = {Operating systems,Very large scale integration,cache storage,History,Random access memory,performance evaluation,performance improvement,Frequency,Databases,block replacement policy,FreeBSD,Least Recently/Frequently Used policy,LFU policies,LRFU replacement policy,LRU block replacement policy,SPEC SDET benchmark},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Honggi Choe} and {Sam H. Noh} and {Sang Lyul Min} and {Yookun Cho}},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X45T4U8Y\\Donghee Lee et al. - 1997 - Implementation and performance evaluation of the L.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\GEJN9C9I\\658446.html}
}

@inproceedings{jainBackFutureLeveraging2016,
  title = {Back to the {{Future}}: {{Leveraging Belady}}'s {{Algorithm}} for {{Improved Cache Replacement}}},
  doi = {10.1109/ISCA.2016.17},
  shorttitle = {Back to the {{Future}}},
  abstract = {Belady's algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady's algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efficient, as we introduce a new method of efficiently simulating Belady's behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4\%, as opposed to 6.2\% for the previous state-of-the-art. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0\%, compared to 12.0\% for the previous state-of-the-art.},
  eventtitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  date = {2016-06},
  pages = {78-89},
  keywords = {Hardware,Benchmark testing,cache storage,History,Prediction algorithms,LLC,replacement state,cache replacement,Optimized production technology,Cache replacement,Belady algorithm,Art,4-core system,Belady behavior,Belady's Algorithm,cache accesses,Marine vehicles,memory-intensive subset,sampling techniques,tag array},
  author = {Jain, A. and Lin, C.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KG9345TE\\Jain and Lin - 2016 - Back to the Future Leveraging Belady's Algorithm .pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XIN4JXM5\\7551384.html}
}

@article{zhuRACERobustAdaptive2008,
  title = {{{RACE}}: {{A Robust Adaptive Caching Strategy}} for {{Buffer Cache}}},
  volume = {57},
  doi = {10.1109/TC.2007.70788},
  shorttitle = {{{RACE}}},
  abstract = {Although many block replacement algorithms for buffer caches have been proposed to address the well-known drawbacks of the LRU algorithm, they are not robust and cannot maintain a consistent performance improvement over all workloads. This paper proposes a novel and simple replacement scheme, called the Robust Adaptive buffer Cache management schemE (RACE), which differentiates the locality of I/O streams by actively detecting access patterns that are inherently exhibited in two correlated spaces, that is, the discrete block space of program contexts from which I/O requests are issued and the continuous block space within files to which I/O requests are addressed. This scheme combines the global I/O regularities of an application and the local I/O regularities of individual files that are accessed in that application to accurately estimate the locality strength, which is crucial in deciding which blocks are to be replaced upon a cache miss. Through comprehensive simulations on 10 real-application traces, RACE is shown to have higher hit ratios than LRU and all other state-of-the-art cache management schemes studied in this paper.},
  number = {1},
  journaltitle = {IEEE Transactions on Computers},
  date = {2008-01},
  pages = {25-40},
  keywords = {Training,Algorithm design and analysis,cache storage,Accuracy,Memory management,Classification algorithms,Robustness,block replacement algorithm,File systems,access pattern detection,Buffering,correlated space,discrete block space,file system,Input/output,Main memory,RACE,robust adaptive buffer cache management scheme,Stability analysis},
  author = {Zhu, Y. and Jiang, H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\S3PBXCYC\\Zhu and Jiang - 2008 - RACE A Robust Adaptive Caching Strategy for Buffe.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\AZR2W7Y4\\4358240.html}
}

@inproceedings{zhangCacheReplacementPolicy2010,
  title = {A {{Cache Replacement Policy Using Adaptive Insertion}} and {{Re}}-Reference {{Prediction}}},
  doi = {10.1109/SBAC-PAD.2010.21},
  abstract = {Previous research shows that LRU replacement policy is not efficient when applications exhibit a distant re-reference interval. Recently proposed RRIP policy improves performance for such workloads. However, RRIP lacks of access recency information, which may confuse the replacement policy to make accurate prediction. Consequently, RRIP is not robust for recency-friendly workloads. This paper proposes an Adaptive Insertion and Re-reference Prediction (AI-RRP) policy which evicts data based on both re-reference prediction value and the access recency information. To make the replacement policy more adaptive across different workloads and different phases during execution, Dynamic AI-RRP (DAI-RRP) is proposed which adjusts the insertion position and prediction value for different access patterns. Simulation results show DAI-RRP reduces CPI over LRU and Dynamic RRIP by an average of 8.3\% and 4.1\% respectively on a single-core processor with a 1MB 16-way set last-level cache (LLC). Evaluations on quad-core CMP with a 4MB shared LLC show that DAI-RRP outperforms LRU and Dynamic RRIP (DRRIP) on the weighted speedup metric by an average of 13.2\% and 26.7\% respectively. Furthermore, compred to LRU, DAI-RRP requires similar hardware, or even less hardware for high-associativity cache.},
  eventtitle = {2010 22nd {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}},
  booktitle = {2010 22nd {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}},
  date = {2010-10},
  pages = {95-102},
  keywords = {Hardware,Radiation detectors,Registers,cache storage,Proposals,Robustness,cache replacement policy,Simulation,Electronics packaging,access pattern,Adaptive Insertion,adaptive insertion and re-reference prediction,Cache Replacement,last level cache,Set Dueling,Shared Cache,single core processor},
  author = {Zhang, X. and Li, C. and Wang, H. and Wang, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WIVN9I7B\\Zhang et al. - 2010 - A Cache Replacement Policy Using Adaptive Insertio.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\X9QNV5M9\\5644963.html}
}

@inproceedings{guTheoryPotentialLRUMRU2011,
  location = {{New York, NY, USA}},
  title = {On the {{Theory}} and {{Potential}} of {{LRU}}-{{MRU Collaborative Cache Management}}},
  isbn = {978-1-4503-0263-0},
  url = {http://doi.acm.org/10.1145/1993478.1993485},
  doi = {10.1145/1993478.1993485},
  abstract = {The goal of cache management is to maximize data reuse. Collaborative caching provides an interface for software to communicate access information to hardware. In theory, it can obtain optimal cache performance. In this paper, we study a collaborative caching system that allows a program to choose different caching methods for its data. As an interface, it may be used in arbitrary ways, sometimes optimal but probably suboptimal most times and even counter productive. We develop a theoretical foundation for collaborative caches to show the inclusion principle and the existence of a distance metric we call LRU-MRU stack distance. The new stack distance is important for program analysis and transformation to target a hierarchical collaborative cache system rather than a single cache configuration. We use 10 benchmark programs to show that optimal caching may reduce the average miss ratio by 24\%, and a simple feedback-driven compilation technique can utilize collaborative cache to realize 50\% of the optimal improvement.},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Management}}},
  series = {{{ISMM}} '11},
  publisher = {{ACM}},
  urldate = {2019-09-12},
  date = {2011},
  pages = {43--54},
  keywords = {cache replacement algorithm,bipartite cache,collaborative caching,lru,mru,opt},
  author = {Gu, Xiaoming and Ding, Chen},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\SXHBKDQ4\\Gu and Ding - 2011 - On the Theory and Potential of LRU-MRU Collaborati.pdf},
  venue = {San Jose, California, USA}
}

@inproceedings{chenSSARCShortSightedAdaptive2009,
  title = {{{SSARC}}: {{The Short}}-{{Sighted Adaptive Replacement Cache}}},
  doi = {10.1109/HPCC.2009.82},
  shorttitle = {{{SSARC}}},
  abstract = {As the performance gap between disks and processors continues to increase, dozens of cache replacement policies come up to handle the problem. Unfortunately, most of the policies are static. Nimrod Megiddo etc put forward a low overhead adaptive policy called ARC. It outperforms most of the static policies in most situations. But, ARC adapts itself to the workloads by the feedback of the missed pages. It hasn 't carried out the adaption before missed pages are discovered. We propose a high performance adaptive replacement policy. It adapts itself to the workloads by the feedback of the hit pages, so, it is more sensitive to the changes of the workloads than ARC. As the policy stares at the tails of the queues regardless of other pages, we name the policy as short-sighted adaptive replacement policy. The ARC usually regrets for the missed pages and wishes to rescue the neighborhood of them. However, SSARC endeavors to protect the would-be-reused pages from being replaced aggressively. So, it outperforms ARC in most situations. We compared SSARC with LRU, 2Q and ARC. The trace-driven experiments represent that SSARC gains higher performance.},
  eventtitle = {2009 11th {{IEEE International Conference}} on {{High Performance Computing}} and {{Communications}}},
  booktitle = {2009 11th {{IEEE International Conference}} on {{High Performance Computing}} and {{Communications}}},
  date = {2009-06},
  pages = {551-556},
  keywords = {Computer science,cache storage,Cache,Delay,Performance gain,High performance computing,Feedback,cache replacement policies,adaptive,low overhead adaptive policy,Protection,short-sighted adaptive replacement cache,short-sighted adaptive replacement policy,SSARC,storge,Tail},
  author = {Chen, Z. and Xiao, N. and Liu, F. and Zhao, Y.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NVRT2KCP\\Chen et al. - 2009 - SSARC The Short-Sighted Adaptive Replacement Cach.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9ZX3IEXM\\5167043.html}
}

@inproceedings{altmanNovelMethodologyUsing1993,
  langid = {english},
  title = {A {{Novel Methodology}} Using {{Genetic Algorithms}} for the {{Design}} of {{Caches}} and {{Cache Replacement Policy}}},
  isbn = {1-55860-299-2},
  eventtitle = {5th {{International Conference}} on {{Genetic Algorithms}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Genetic Algorithms}}},
  publisher = {{Morgan Kaufmann Publishers}},
  date = {1993},
  pages = {392-399},
  author = {Altman, Erik R and Agarwal, Vinod K and Gao, Guang R},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\339B4AMH\\Altman et al. - A Novel Methodology using Genetic Algorithms for t.pdf}
}

@incollection{doughertyOptimizingIntegratedApplication2011,
  location = {{Berlin, Heidelberg}},
  title = {Optimizing {{Integrated Application Performance}} with {{Cache}}-{{Aware Metascheduling}}},
  volume = {7045},
  isbn = {978-3-642-25105-4 978-3-642-25106-1},
  url = {http://link.springer.com/10.1007/978-3-642-25106-1_2},
  booktitle = {On the {{Move}} to {{Meaningful Internet Systems}}: {{OTM}} 2011},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-09-12},
  date = {2011},
  pages = {432-450},
  author = {Dougherty, Brian and White, Jules and Kegley, Russell and Preston, Jonathan and Schmidt, Douglas C. and Gokhale, Aniruddha},
  editor = {Meersman, Robert and Dillon, Tharam and Herrero, Pilar and Kumar, Akhil and Reichert, Manfred and Qing, Li and Ooi, Beng-Chin and Damiani, Ernesto and Schmidt, Douglas C. and White, Jules and Hauswirth, Manfred and Hitzler, Pascal and Mohania, Mukesh},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YZHPCD67\\Dougherty et al. - 2011 - Optimizing Integrated Application Performance with.pdf}
}

@inproceedings{anandkumarHybridCacheReplacement2014,
  title = {A Hybrid Cache Replacement Policy for Heterogeneous Multi-Cores},
  doi = {10.1109/ICACCI.2014.6968209},
  abstract = {Future generation computer architectures are endeavoring to achieve high performance without compromise on energy efficiency. In a multiprocessor system, cache miss degrades the performance as the miss penalty scales by an exponential factor across a shared memory system when compared to general purpose processors. This instigates the need for an efficient cache replacement scheme to cater to the data needs of underlying functional units in case of a cache miss. Minimal cache miss improves resource utilization and reduces data movement across the core which in turn contributes to a high performance and lesser power dissipation. Existing replacement policies has several issues when implemented in a heterogeneous multi-core system. The commonly used LRU replacement policy does not offer optimal performance for applications with high dependencies. Motivated by the limitations of the existing algorithms, we propose a hybrid cache replacement policy which combines Least Recently Used (LRU) and Least Frequently Used (LFU) replacement policies. Each cache block has two weighing values corresponding to LRU and LFU policies and a cumulative weight is calculated using these two values. Conducting simulations over wide range of cache sizes and associativity, we show that our proposed approach has shown increased cache hit to miss ratio when compared with LRU and other conventional cache replacement policies.},
  eventtitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  date = {2014-09},
  pages = {594-599},
  keywords = {cache storage,cache miss,performance evaluation,shared memory system,shared memory systems,data movement,energy efficiency,LFU policy,Time-frequency analysis,multi-core,least frequently used replacement policy,least recently used replacement policy,resource allocation,LRU replacement policy,Cache Replacement,future generation computer architecture,heterogeneous multicore system,heterogeneous multicores,hybrid cache replacement policy,Libraries,multiprocessor system,power dissipation,resource utilization,weighing values},
  author = {AnandKumar, K. M. and S, A. and Ganesh, D. and Christy, M. S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JQI5WCDH\\AnandKumar et al. - 2014 - A hybrid cache replacement policy for heterogeneou.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\T8CJAVC8\\6968209.html}
}

@inproceedings{kelwadeReputationBasedCache2017,
  title = {Reputation Based Cache Management Policy for Performance Improvement},
  doi = {10.1109/ISS1.2017.8389236},
  abstract = {As the performance gap between CPU and main memory speed increases, memory subsystem design more critical. Caches are important part of modern memory hierarchies. Cache performance is important in computer system. As the cache size is limited it should be properly utilized. After the cache block is used by an application it should be moved from cache to main memory so that another block required by other application can use its space [2]. Now a day's multi-level cache are used everywhere to provide better cache performance. As the block which is moved from cache memory to main memory may be required in future, so it is kept in lower level cache in the cache hierarchy. Therefore the cost of accessing the block from main memory is minimized. Many approaches like LRU-K, LFU, LRFU, PROMOTE, DEMOTE are used to increase cache performance by promoting or demoting a memory block into the cache depending upon its latest history [1] [21]. Some are used for single level cache and some for hierarchical cache. The drawback of existing multi-level cache management techniques is that it uses hints of level L1 only. It does not provide sufficient information about the history of the cache block at all previous cache levels. Therefore, a hybrid cache management policy is proposed in this paper which takes its replacement decision based on the multiple level cache level. In this paper, a hybrid cache management policy is proposed which constitute the feature of two existing algorithms PEOMOTE and DEMTE that gives better hit ratio than other existing policies.},
  eventtitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  booktitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  date = {2017-12},
  pages = {582-587},
  keywords = {Conferences,cache storage,storage management,memory block,cache memory,recency,Cache performance,demote,DEMTE algorithms,hierarchical cache,hierarchical systems,hints,hybrid cache management policy,memory subsystem design,modern memory hierarchies,Multi-level cache,multilevel cache management,multiple level cache level,PEOMOTE algorithms,promote,reputation based cache management policy,single level cache},
  author = {Kelwade, K. and Sahu, S. and Kawade, G. and Korde, N. and Upadhye, S. and Motghare, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\XXVPEPZ4\\Kelwade et al. - 2017 - Reputation based cache management policy for perfo.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\4CQIK3WJ\\8389236.html}
}

@article{jungCacheReplacementPolicy2013,
  langid = {kor},
  title = {Cache Replacement Policy Based on Dynamic Counter for High Performance Processor},
  volume = {50},
  issn = {2287-5026},
  url = {http://www.koreascience.or.kr/article/JAKO201315463253488.page},
  doi = {10.5573/ieek.2013.50.4.052},
  abstract = {Cache Replacement Policy Based on Dynamic Counter for High Performance Processor Cache memory;Cache replacement policy;LRU;Zero reuse line;Counter based policy; Replacement policy is one of the key factors determining the effectiveness of a cache. The LRU replacement policy has remained the standard for caches for many years. However, the traditional LRU has ineffective performance in zero-reuse line intensive workloads, although it performs well in high temporal locality workloads. To address this problem, We propose a new replacement policy; DCR(Dynamic Counter based Replacement) policy. A temporal locality of workload dynamically changes across time and DCR policy is based on the detection of these changing. DCR policy improves cache miss rate over a traditional LRU policy, by as much as 2.7\% at maximum and 0.47\% at average.},
  number = {4},
  journaltitle = {Journal of the Institute of Electronics and Information Engineers},
  urldate = {2019-09-12},
  date = {2013},
  pages = {52-58},
  author = {Jung, Do Young and Lee, Yong Surk},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\B8DYCACE\\Jung and Lee - 2013 - Cache Replacement Policy Based on Dynamic Counter .pdf}
}

@article{chooDIGDegreeInterreference2006,
  title = {{{DIG}}: {{Degree}} of Inter-Reference Gap for a Dynamic Buffer Cache Management},
  volume = {176},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025505000551},
  doi = {10.1016/j.ins.2005.01.018},
  shorttitle = {{{DIG}}},
  abstract = {The effectiveness of the buffer cache replacement is critical to the performance of I/O systems. In this paper, we propose a degree of inter-reference gap (DIG) based block replacement scheme. This scheme keeps the simplicity of the least recently used (LRU) scheme and does not depend on the detection of access regularities. The proposed scheme is based on the low inter-reference recency set (LIRS) scheme, which is currently known to be very effective. However, the proposed scheme employs several history information items whereas the LIRS scheme uses only one history information item. The overhead of the proposed scheme is almost negligible. To evaluate the performance of the proposed scheme, the comprehensive trace-driven computer simulation is used in general access patterns. Our simulation results show that the cache hit ratio (CHR) in the proposed scheme is improved as much as 65.3\% (with an average of 26.6\%) compared to the LRU for the same workloads, and up to 6\% compared to the LIRS in multi3 trace.},
  number = {8},
  journaltitle = {Information Sciences},
  shortjournal = {Information Sciences},
  urldate = {2019-09-12},
  date = {2006-04-22},
  pages = {1032-1044},
  keywords = {Buffer cache management,Cache hit ratio,Degree of inter-reference gap,Least recently used,Low inter-reference recency set},
  author = {Choo, Hyunseung and Lee, Young Jae and Yoo, Seong-Moo},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\TP762LZU\\Choo et al. - 2006 - DIG Degree of inter-reference gap for a dynamic b.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6VZZ28PM\\S0020025505000551.html}
}

@article{consuegraAnalyzingAdaptiveCache2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.07624},
  primaryClass = {cs},
  title = {Analyzing {{Adaptive Cache Replacement Strategies}}},
  url = {http://arxiv.org/abs/1503.07624},
  abstract = {Adaptive Replacement Cache (ARC) and CLOCK with Adaptive Replacement (CAR) are state-of-the- art "adaptive" cache replacement algorithms invented to improve on the shortcomings of classical cache replacement policies such as LRU, LFU and CLOCK. By separating out items that have been accessed only once and items that have been accessed more frequently, both ARC and CAR are able to control the harmful effect of single-access items flooding the cache and pushing out more frequently accessed items. Both ARC and CAR have been shown to outperform their classical and popular counterparts in practice. Both algorithms are complex, yet popular. Even though they can be treated as online algorithms with an "adaptive" twist, a theoretical proof of the competitiveness of ARC and CAR remained unsolved for over a decade. We show that the competitiveness ratio of CAR (and ARC) has a lower bound of N + 1 (where N is the size of the cache) and an upper bound of 18N (4N for ARC). If the size of cache offered to ARC or CAR is larger than the one provided to OPT, then we show improved competitiveness ratios. The important implication of the above results are that no "pathological" worst-case request sequences exist that could deteriorate the performance of ARC and CAR by more than a constant factor as compared to LRU.},
  urldate = {2019-09-12},
  date = {2015-03-26},
  keywords = {Computer Science - Data Structures and Algorithms},
  author = {Consuegra, Mario E. and Martinez, Wendy A. and Narasimhan, Giri and Rangaswami, Raju and Shao, Leo and Vietri, Giuseppe},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\N3EN9VJN\\Consuegra et al. - 2015 - Analyzing Adaptive Cache Replacement Strategies.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RGS6Z6ZU\\1503.html}
}

@inproceedings{sreedharanCacheReplacementPolicy2017,
  title = {A Cache Replacement Policy Based on Re-Reference Count},
  doi = {10.1109/ICICCT.2017.7975173},
  abstract = {The cache replacement policy is a major factor which determines the effectiveness of memory hierarchy. The replacement policy affects both the hit rate and the access latency of the cache. It decides the cache block to be replaced to give room for the incoming block. The replacement policy has to be chosen in such a way that the cache misses are reduced. Last level cache misses causes hundreds of stall cycles due to the need for main memory access. So last level cache misses are given more priority over L1 cache misses. The traditional cache replacement policy used is Least Recently Used (LRU) policy. LRU policy favors workloads having cyclic access pattern which fit in cache, but it exhibit thrashing behavior for memory-intensive workloads that does not fit in the available cache. Hence, many replacement policies were proposed to improve the miss rate for last level caches while maintaining low hardware overhead and minimum design changes. Here a novel replacement policy which is a variation of LRU Insertion policy (LIP) based on re-reference count is proposed. The promotion policy in LIP is modified to implement the new policy which is based on the re-reference count. The proposed replacement policy was implemented and performance comparisons of the replacement policies were done on Gem5 simulator using cpu2006 benchmarks. Under this policy, the memory intensive workload mcf attains 64\% improvement in L2 cache miss rate over LRU policy.},
  eventtitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  booktitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  date = {2017-03},
  pages = {129-134},
  keywords = {Computer science,Conferences,Algorithm design and analysis,cache storage,memory hierarchy,Arrays,Memory management,hit rate,cache replacement policy,LRU,least recently used policy,LRU policy,L1 cache,Electronics packaging,cache access latency,cache block,Cache Hit Rate,cpu2006 benchmarks,cyclic access pattern,Gem5 simulator,last-level cache miss rate,Lips,low-hardware overhead,LRU insertion policy,memory intensive workload,Promotion Policy,re-reference count,Re-reference count,thrashing behavior},
  author = {Sreedharan, S. and Asokan, S.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\WDMF4RQQ\\Sreedharan and Asokan - 2017 - A cache replacement policy based on re-reference c.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FJV9V2XT\\7975173.html}
}

@article{vermaEnhancedApproachCache2017,
  langid = {english},
  title = {Enhanced {{Approach}} for {{Cache Behaviour}} and {{Performance Evaluation}}},
  abstract = {In previous research on cache replacement policies, have seen that a block usually becomes dead after some uses. By classifying the dead block and discarding them early, there are various policies have been used but still there are number of problems such as cache pollution, high cache miss rate and cache overhead existed which degrades the system performance. In this paper, a new approach for cache replacement policy has introduced which reduces the cache miss rate and cache overhead by using shared L2 level cache. In terms of less miss rate and good hit rate there is need to deploy effective replacement policies. Cache replacement techniques is one of the crucial design parameter that affects the overall processor performance and also become more important with current growing technological moves in the direction of high associative cache .},
  journaltitle = {International Journal of Advanced Research in Computer Science},
  date = {2017},
  pages = {5},
  author = {Verma, Manish Kumar and Singh, Dr P K},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ESS8VV6A\\Verma and Singh - 2017 - Enhanced Approach for Cache Behaviour and Performa.pdf}
}

@inproceedings{zhuSurveyComputerSystem,
  langid = {english},
  location = {{Las Vegas, Nevada}},
  title = {A {{Survey}} on {{Computer System Memory Management}}},
  abstract = {Computer memory is central to the operation of a modern computer system; it stores data or program instructions on a temporary or permanent basis for use in a computer. In this paper, various memory management and optimization techniques to improve computer performance are reviewed, such as the hardware design of the memory organization, the memory management algorithms and optimization techniques, and some hardware and software memory optimization techniques.},
  eventtitle = {2012 {{World Congress}} in {{Computer Science}}, {{Computer Engineering}} \& {{Applied Computing}}},
  booktitle = {{{WorldComp}} 2012 {{Proceedings}}},
  pages = {7},
  author = {Zhu, Qi},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZI42W2KX\\Zhu - A Survey on Computer System Memory Management.pdf}
}

@inproceedings{liLFCKCacheReplacement2004,
  langid = {english},
  title = {{{LFC}}-{{K Cache Replacement Algorithm}} for {{Grid Index Information Service}} ({{GIIS}})},
  isbn = {978-3-540-30208-7},
  abstract = {Traditional cache replacement algorithms are not easily applicable to a dynamic and heterogeneous environment. Moreover, the frequently used hit-ratio and byte-hit ratio are not appropriate measures in grid applications, because non-uniformity of the resource object sizes and non-uniformity cost of cache misses in resource information traffic. In this paper, we propose a Least Frequently Cost cache replacement algorithm based on at most K backward references, LFC-K. We define average retrieval cost ratio (ARCR), as the cost saved by using a cache divided by the total retrieval cost if no cache was used. We compare performance of LFC-K with other caching algorithms using ARCR, hit-ratio and byte-hit ratio as performance metrics. Our experimental results indicate that LFC-2 outperforms LRU, LFU and LFU-2.},
  booktitle = {Grid and {{Cooperative Computing}} - {{GCC}} 2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer Berlin Heidelberg}},
  date = {2004},
  pages = {795-798},
  author = {Li, Dong and Huang, Linpeng and Li, Minglu},
  editor = {Jin, Hai and Pan, Yi and Xiao, Nong and Sun, Jianhua},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\IZYTHNMM\\Li et al. - 2004 - LFC-K Cache Replacement Algorithm for Grid Index I.pdf}
}

@inproceedings{wangLCRCDependencyAwareCache2018,
  title = {{{LCRC}}: {{A Dependency}}-{{Aware Cache Management Policy}} for {{Spark}}},
  doi = {10.1109/BDCloud.2018.00140},
  shorttitle = {{{LCRC}}},
  abstract = {Memory is a constrained resource for in-memory big data computing systems. Efficient memory management plays a pivotal role in performance improvement for these systems. However, simple history-based cache replacement strategies, such as Least Recently Used (LRU), usually have poor performance when applied in cluster applications, due to their lack of data dependency knowledge. Even though Least Reference Count (LRC) can be aware of dependency by giving blocks with bigger reference count high priority to reside in memory. However, these blocks will not be accessed in some stages during their entire life cycle, leading to available memory deterioration. To eliminate this shortcoming, we propose LCRC, a dependency-aware cache management policy that considers both intra-stage and inter-stage dependency. By providing a prefetching mechanism, we can rewrite these inter-stages accessed blocks into memory before its next use. Experiments show that compared with previous methods the proposed mechanism can improve computing performance over 65\%.},
  eventtitle = {2018 {{IEEE Intl Conf}} on {{Parallel Distributed Processing}} with {{Applications}}, {{Ubiquitous Computing Communications}}, {{Big Data Cloud Computing}}, {{Social Computing Networking}}, {{Sustainable Computing Communications}} ({{ISPA}}/{{IUCC}}/{{BDCloud}}/{{SocialCom}}/{{SustainCom}})},
  booktitle = {2018 {{IEEE Intl Conf}} on {{Parallel Distributed Processing}} with {{Applications}}, {{Ubiquitous Computing Communications}}, {{Big Data Cloud Computing}}, {{Social Computing Networking}}, {{Sustainable Computing Communications}} ({{ISPA}}/{{IUCC}}/{{BDCloud}}/{{SocialCom}}/{{SustainCom}})},
  date = {2018-12},
  pages = {956-963},
  keywords = {cache storage,Prefetching,Random access memory,Memory management,storage management,Task analysis,memory management,Semantics,resource allocation,Big Data,Big data; In-memory computing framework; Cache replacement; Prefetching,cluster computing,data dependency knowledge,dependency-aware cache management policy,history-based cache replacement,in-memory Big Data computing systems,inter-stage dependency,inter-stages accessed blocks,LCRC,Least Reference Count,Sparks},
  author = {Wang, B. and Tang, J. and Zhang, R. and Ding, W. and Qi, D.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\P6C3RPXH\\Wang et al. - 2018 - LCRC A Dependency-Aware Cache Management Policy f.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\JR5Q6T64\\8672319.html}
}

@inproceedings{dasRoleCacheReplacement2019,
  langid = {english},
  title = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}: {{A Survey}}},
  isbn = {9789811323720},
  shorttitle = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}},
  abstract = {Cache replacement policies play important roles in efficiently processing the current big data applications. The performance of any high performance computing system is highly depending on the performance of its cache memory. A better replacement policy allows the important blocks to be placed nearer to the core. Hence reduces the overall execution latency and gives better computational efficiency. There are different replacement policies exits. The main difference among these policies is how to select the victim block from the cache such that it can be replaced with another newly fetched block. Non-optimal replacement policy may remove important blocks from the cache when some less important (dead) blocks also present in the cache. Proposing better replacement policy for cache memory is a major research area from last three decades. The most widely used replacement policies used for classical cache memories are Least Recently Used Policy (LRU), Random Replacement Policy or Pseudo-LRU. As the technology advances the technology of cache memory is also changing. For efficient processing of big data based applications today’s computer having high performance computing ability requires larger cache memory. Such larger cache memory makes the task of replacement policies more challenging. In this paper we have done a survey about the innovations done in cache replacement policies to support the efficient processing of big data based applications.},
  booktitle = {Communication, {{Networks}} and {{Computing}}},
  series = {Communications in {{Computer}} and {{Information Science}}},
  publisher = {{Springer Singapore}},
  date = {2019},
  pages = {400-410},
  keywords = {Cache memory,Efficient data processing,Multicore-system,Replacement policies},
  author = {Das, Purnendu},
  editor = {Verma, Shekhar and Tomar, Ranjeet Singh and Chaurasia, Brijesh Kumar and Singh, Vrijendra and Abawajy, Jemal},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\FD9N4ZE3\\Das - 2019 - Role of Cache Replacement Policies in High Perform.pdf}
}

@article{tadaCacheReplacementPolicy2019,
  langid = {english},
  title = {A {{Cache Replacement Policy}} with {{Considering Global Fluctuations}} of {{Priority Values}}},
  volume = {9},
  issn = {2185-2847},
  abstract = {In the high-associativity caches, the hardware overheads of the cache replacement policy become a problem. To avoid this problem, the Adaptive Demotion Policy (ADP) is proposed. The ADP focuses on the priority value demotion at a cache miss, and it can achieve a higher performance compared with conventional cache replacement policies. The ADP can be implemented with small hardware resources, and the priority value update logic can be implemented with a small hardware cost. The ADP can suit for various applications by the appropriate selection of its insertion, promotion and selection policies. If the dynamic selection of the suitable policies for the running application is possible, the performance of the cache replacement policy will be increased. In order to achieve the dynamic selection of the suitable policies, this paper focuses on the global ﬂuctuations of the priority values. At ﬁrst, the cache is partitioned into several partitions. At every cache access, the total of priority values in each partition is calculated. At every set interval, the ﬂuctuations of total priority values in all the partitions are checked, and the information is used to detect the behavior of the application. This paper adapts this mechanism to the ADP, and the adapted cache replacement policy is called the ADP-G. The performance evaluation shows that the ADP-G achieves the MPKI reductions and the IPC improvements, compared to the LRU policy, the RRIP policy and the ADP.},
  number = {2},
  journaltitle = {International Journal of Networking and Computing},
  date = {2019-07},
  pages = {161-170},
  author = {Tada, Jubee},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\CNYU8Q5L\\Tada - A Cache Replacement Policy with Considering Global.pdf}
}

@article{cuiNewHybridApproach2003,
  title = {A {{New Hybrid Approach}} to {{Exploit Localities}}: {{LRFU}} with {{Adaptive Prefetching}}},
  volume = {31},
  issn = {0163-5999},
  url = {http://doi.acm.org/10.1145/974036.974041},
  doi = {10.1145/974036.974041},
  shorttitle = {A {{New Hybrid Approach}} to {{Exploit Localities}}},
  abstract = {This paper reviewed a number of existing methods to exploit the spatial and temporal locality commonly existing in programs, and provided detailed analysis and testing of adaptive prefetching (a method designed to utilize spatial locality) and the least recently and frequently used (LRFU) method (a method designed to utilize temporal locality). The two methods were combined in this work in terms of their exploitation of locality. The comparative studies of the methods were done using real traces, and hit rate was used as an evaluation measure.Results showed that by using adaptive prefetching, the hit rate improved significantly by an average of 11.7\% over the hit rate of LRU in the traces and cache configurations used. It also showed that LRFU consistently gives higher hit rates than LRU, but not by much in the trace files and cache configurations tested. And the X value (a controllable parameter which determines the Weights given to recency and frequency) has to be in a certain range, which is usually narrow, in order to get the best performance for hit rate. Compared to adaptive prefetching and LRU, the hybrid approach of combining adaptive prefetching and LRFU gave a consistently higher hit rate also. But, affected by the performance of LRFU, the improvement in the hit rate by the combination was low.},
  number = {3},
  journaltitle = {SIGMETRICS Perform. Eval. Rev.},
  urldate = {2019-09-16},
  date = {2003-12},
  pages = {37--43},
  author = {Cui, Jike and Samadzadeh, Mansur. H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VVBD52LY\\Cui and Samadzadeh - 2003 - A New Hybrid Approach to Exploit Localities LRFU .pdf}
}

@report{kegleyPredictiveCacheModeling2011,
  langid = {english},
  title = {Predictive {{Cache Modeling}} and {{Analysis}}},
  url = {https://apps.dtic.mil/docs/citations/ADA552968},
  abstract = {This work applied particle swarm heuristic optimization techniques to the problem of finding a near-optimal order in which to schedule tasks in a real-time embedded system in order to minimize cache miss rates experienced by the software. Reducing the number of cache misses is an important component of runtime execution efficiency. We demonstrated runtime reductions of 3-5\% in execution time, significant for embedded systems attempting to add new capability without upgrading hardware. The expectation is that these gains can be improved further by the use of hardware with pseudo-LRU cache behavior.},
  institution = {{LOCKHEED MARTIN AERONAUTICS CO FORT WORTH TX}},
  urldate = {2019-09-16},
  date = {2011-11},
  author = {Kegley, Russell and Preston, Jonathan and Dougherty, Brian and White, Jules and Gokhale, Anirudda},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M5W66TST\\Kegley et al. - 2011 - Predictive Cache Modeling and Analysis.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W6K7EAKC\\ADA552968.html}
}

@article{chhedaMemorySystemsOverview,
  langid = {english},
  title = {Memory {{Systems}}: {{Overview}} and {{Trends}}},
  abstract = {Computer pioneers have correctly predicted that programmers would want unlimited amounts of memory. An economical solution to this desire is the implementation of a Memory Hierarchical System, which takes advantage of locality and cost/performance of memory technologies. As time has gone by, the technology has progressed, bringing about various changes in the way memory systems are built. Memory systems must be ﬂexible enough to accommodate various levels of memory hierarchies, and must be able to emulate an environment with unlimited amount of memory. For more than two decades the main emphasis of memory system designers has been achieving high performance. However, recent market trends and application requirements suggest that other design goals such as low-power, predictability, and ﬂexibility/reconﬁgurability are becoming equally important to consider. This paper gives a comprehensive overview of memory systems with the objective to give any reader a broad overview. Emphasis is put on the various components of a typical memory system of present-day systems and emerging new memory system architecture trends. We focus on emerging memory technologies, system architectures, compiler technology, which are likely to shape the computer industry in the future.},
  pages = {12},
  author = {Chheda, Saurabh and Chittamuru, Jeevan Kumar and Moritz, Csaba Andras},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\NISUVJUW\\Chheda et al. - Memory Systems Overview and Trends.pdf}
}

@article{midorikawaAdaptiveReplacementBased2008,
  title = {On {{Adaptive Replacement Based}} on {{LRU}} with {{Working Area Restriction Algorithm}}},
  volume = {42},
  issn = {0163-5980},
  url = {http://doi.acm.org/10.1145/1453775.1453790},
  doi = {10.1145/1453775.1453790},
  abstract = {Adaptive algorithms are capable of modifying their own behavior through time, depending on the execution characteristics. Recently, we have proposed LRU-WAR, an adaptive replacement algorithm whose objective is to minimize failures detected in LRU policy, preserving its simplicity and low overhead. In this paper, we present our contribution to the study of adaptive replacement algorithms describing their behavior under a number of workloads. Simulations include an analysis of the performance sensibility with the variation of the control parameters and its application in a multiprogrammed environment. In order to address LRU-WAR weakness as a global policy, we also introduce LRU-WARlock. The simulation results show that substantial performance improvements can be obtained.},
  number = {6},
  journaltitle = {SIGOPS Oper. Syst. Rev.},
  urldate = {2019-09-16},
  date = {2008-10},
  pages = {81--92},
  keywords = {LRU,virtual memory,adaptive replacement,demand paging},
  author = {Midorikawa, Edson T. and Piantola, Ricardo L. and Cassettari, Hugo H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\7PLKVN5X\\Midorikawa et al. - 2008 - On Adaptive Replacement Based on LRU with Working .pdf}
}

@article{kiniwaLookaheadSchedulingRequests,
  langid = {english},
  title = {Lookahead {{Scheduling Requests}} for {{Eﬃcient Paging}}},
  pages = {9},
  author = {Kiniwa, Jun and Kameda, Tiko},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\PCLEMN47\\Kiniwa and Kameda - Lookahead Scheduling Requests for Eﬃcient Paging.pdf}
}

@thesis{minUSINGRUNTIMEINFORMATION2005,
  langid = {english},
  title = {{{USING RUNTIME INFORMATION TO IMPROVE MEMORY SYSTEM PERFORMANCE}}},
  url = {https://etd.ohiolink.edu/pg_10?0::NO:10:P10_ACCESSION_NUM:ucin1134043707},
  institution = {{University of Cincinnati}},
  urldate = {2019-09-16},
  date = {2005},
  author = {Min, Rui},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M47J8S5T\\Min - 2005 - USING RUNTIME INFORMATION TO IMPROVE MEMORY SYSTEM.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\VYC68M47\\pg_10.html}
}

@article{kianiRDGCReuseDistanceBased2019,
  langid = {english},
  title = {{{RDGC}}: {{A Reuse Distance}}-{{Based Approach}} to {{GPU Cache Performance Analysis}}},
  volume = {38},
  issn = {2585-8807},
  url = {http://www.cai.sk/ojs/index.php/cai/article/view/2019_2_421},
  shorttitle = {{{RDGC}}},
  abstract = {In the present paper, we propose RDGC, a reuse distance-based performance analysis approach for GPU cache hierarchy. RDGC models the thread-level parallelism in GPUs to generate appropriate cache reference sequence. Further, reuse distance analysis is extended to model the multi-partition/multi-port parallel caches and employed by RDGC to analyze GPU cache memories. RDGC can be utilized for architectural space exploration and parallel application development through providing hit ratios and transaction counts. The results of the present study demonstrate that the proposed model has an average error of 3.72 \% and 4.5 \% (for L1 and L2 hit ratios, respectively). The results also indicate that the slowdown of RDGC is equal to 47 000 times compared to hardware execution, while it is 59 times faster than GPGPU-Sim simulator.},
  number = {2},
  journaltitle = {COMPUTING AND INFORMATICS},
  urldate = {2019-09-16},
  date = {2019-05-31},
  pages = {421-453-453},
  author = {Kiani, Mohsen and Rajabzadeh, Amir},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\6NRI6BU2\\2019_2_421.html}
}

@thesis{faizalbinmohdsharifCacheReplacementAlgorithm2015,
  location = {{Malaysia}},
  title = {Cache {{Replacement Algorithm Using Hierarchical Allocation Scheduling}}},
  institution = {{Universiti Putra Malaysia}},
  type = {Master of Science},
  date = {2015-05},
  author = {Faizal Bin Mohd Sharif, Mohammad},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HTBWDYPP\\153824461.pdf}
}

@article{leeImprovingPrefetchingEffects2008,
  langid = {kor},
  title = {Improving Prefetching Effects by Exploiting Reference Patterns},
  volume = {14},
  issn = {1229-7712},
  url = {http://www.koreascience.or.kr/article/JAKO200814357783707.page},
  abstract = {Improving Prefetching Effects by Exploiting Reference Patterns Prefetching;Reference Pattern; Prefetching is one of widely used techniques to improve performance of I/O. But it has been reported that prefetching can bring adverse result on some reference pattern. This paper proposes a prefet-ching frame that can be adopted on existing prefetching techniques simply. The frame called IPRP (Improving Prefetching Effects by Exploiting Reference Patterns) and detects reference patterns online and control pre-fetching upon the characteristics of the detected pattern. In our experiment, we adopted IPRP on Linux read-ahead prefetching. IPRP could prevent adverse result clearly when Linux read-ahead prefetching increases total execution time about \$40\%\{\textbackslash{}sim\}70\%\$. When Linux read-ahead prefetching could bring some benefit, IPRP with read- ahead performed similar or slightly better benefit on execution time. With this result we could see our IPRP can complement and improve legacy prefetching techniques efficiently.},
  number = {2},
  journaltitle = {Journal of KIISE:Computing Practices and Letters},
  urldate = {2019-09-16},
  date = {2008},
  pages = {226-230},
  author = {Lee, Hyo-Jeong and Doh, In-Hwan and Noh, Sam-H.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\ZD9W9QG3\\Lee et al. - 2008 - Improving Prefetching Effects by Exploiting Refere.pdf}
}

@article{parkWWCLOCKPageReplacement2009,
  langid = {kor},
  title = {WWCLOCK: Page Replacement Algorithm Considering Asymmetric I/O Cost of Flash Memory},
  volume = {15},
  issn = {1229-7712},
  url = {http://www.koreascience.or.kr/article/JAKO200909659865050.page},
  shorttitle = {WWCLOCK},
  abstract = {WWCLOCK: Page Replacement Algorithm Considering Asymmetric I/O Cost of Flash Memory Buffer Cache Replacement Algorithm;Cost-aware;Heterogeneous storage; Flash memories have asymmetric I/O costs for read and write in terms of latency and energy consumption. However, the ratio of these costs is dependent on the type of storage. Moreover, it is becoming more common to use two flash memories on a system as an internal memory and an external memory card. For this reason, buffer cache replacement algorithms should consider I/O costs of device as well as possibility of reference. This paper presents WWCLOCK(Write-Weighted CLOCK) algorithm which directly uses I/O costs of devices along with recency and frequency of cache blocks to selecting a victim to evict from the buffer cache. WWCLOCK can be used for wide range of storage devices with different I/O cost and for systems that are using two or more memory devices at the same time. In addition to this, it has low time and space complexity comparable to CLOCK algorithm. Trace-driven simulations show that the proposed algorithm reduces the total I/O time compared with LRU by 36.2\% on average.},
  number = {12},
  journaltitle = {Journal of KIISE:Computing Practices and Letters},
  urldate = {2019-09-16},
  date = {2009},
  pages = {913-917},
  author = {Park, Jun-Seok and Lee, Eun-Ji and Seo, Hyun-Min and Koh, Kern},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HQRBRVH6\\Park et al. - 2009 - WWCLOCK Page Replacement Algorithm Considering As.pdf}
}

@inproceedings{dybdahlLRUbasedReplacementAlgorithm2006,
  location = {{New York, NY, USA}},
  title = {An {{LRU}}-Based {{Replacement Algorithm Augmented}} with {{Frequency}} of {{Access}} in {{Shared Chip}}-Multiprocessor {{Caches}}},
  isbn = {978-1-59593-568-7},
  url = {http://doi.acm.org/10.1145/1166133.1166139},
  doi = {10.1145/1166133.1166139},
  abstract = {This paper proposes a new replacement algorithm to protect cache lines with potential future reuse from being evicted. In contrast to the recency based approaches used in the past (LRU for example), our algorithm also uses the notion of frequency of access. Instead of evicting the least recently used block, our algorithm identifies among a set of LRU blocks the one that is also least-frequently-used (according to a heuristic) and chooses that as a victim. We have implemented this replacement algorithm in a detailed simulation model of a chip multiprocessor system driven by SPEC2000 benchmarks. We have found that the new scheme improves performance for memory intensive applications. Moreover, as compared to other attempts, our replacement algorithm provides robust improvements across all benchmarks. We have also extended an earlier scheme proposed by Wong and Baer so it is switched off when performance is not improved. Our results show that this makes the scheme much more suitable for CMP configurations.},
  booktitle = {Proceedings of the 2006 {{Workshop}} on {{MEmory Performance}}: {{DEaling}} with {{Applications}}, {{Systems}} and {{Architectures}}},
  series = {{{MEDEA}} '06},
  publisher = {{ACM}},
  urldate = {2019-09-16},
  date = {2006},
  pages = {45--52},
  author = {Dybdahl, Haakon and Stenström, Per and Natvig, Lasse},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\69F9ZYYN\\Dybdahl et al. - 2006 - An LRU-based Replacement Algorithm Augmented with .pdf},
  venue = {Seattle, Washington, USA}
}

@article{olukotunMultilevelOptimizationPipelined1997,
  title = {Multilevel Optimization of Pipelined Caches},
  volume = {46},
  doi = {10.1109/12.628394},
  abstract = {This paper formulates and shows how to solve the problem of selecting the cache size and depth of cache pipelining that maximizes the performance of a given instruction-set architecture. The solution combines trace-driven architectural simulations and the timing analysis of the physical implementation of the cache. Increasing cache size tends to improve performance but this improvement is limited because cache access time increases with its size. This trade-off results in an optimization problem we referred to as multilevel optimization, because it requires the simultaneous consideration of two levels of machine abstraction: the architectural level and the physical implementation level. The introduction of pipelining permits the use of larger caches without increasing their apparent access time, however, the bubbles caused by load and branch delays limit this technique. In this paper we also show how multilevel optimization can be applied to pipelined systems if software- and hardware-based strategies are considered for hiding the branch and load delays. The multilevel optimization technique is illustrated with the design of a pipelined cache for a high clock rate MIPS-based architecture. The results of this design exercise show that, because processors with pipelined caches can have shorter CPU cycle times and larger caches, a significant performance advantage is gained by using two or three pipeline stages to fetch data from the cache. Of course, the results are only optimal for the implementation technologies chosen for the design exercise; other choices could result in quite different optimal designs. The exercise is primarily to illustrate the steps in the design of pipelined caches using multilevel optimization; however, it does exemplify the importance of pipelined caches if high clock rate processors are to achieve high performance.},
  number = {10},
  journaltitle = {IEEE Transactions on Computers},
  date = {1997-10},
  pages = {1093-1102},
  keywords = {cache pipelining,cache storage,clock rate processors,Clocks,Computer architecture,CPU cycle times,Delay effects,Design optimization,discrete event simulation,Gallium arsenide,instruction sets,instruction-set architecture,Logic design,memory architecture,Multichip modules,multilevel optimization,optimization problem,Packaging,performance,Pipeline processing,pipelined caches,pipelining,timing,Timing,timing analysis,trace-driven architectural simulations},
  author = {Olukotun, K. and Mudge, T. N. and Brown, R. B.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\5FS3XDXT\\Olukotun et al. - 1997 - Multilevel optimization of pipelined caches.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\YVAARZIE\\628394.html}
}

@inproceedings{bansalCARClockAdaptive2004,
  location = {{Berkeley, CA, USA}},
  title = {{{CAR}}: {{Clock}} with {{Adaptive Replacement}}},
  url = {http://dl.acm.org/citation.cfm?id=1096673.1096699},
  shorttitle = {{{CAR}}},
  abstract = {CLOCK is a classical cache replacement policy dating back to 1968 that was proposed as a low-complexity approximation to LRU. On every cache hit, the policy LRU needs to move the accessed item to the most recently used position, at which point, to ensure consistency and correctness, it serializes cache hits behind a single global lock. CLOCK eliminates this lock contention, and, hence, can support high concurrency and high throughput environments such as virtual memory (for example, Multics, UNIX, BSD, AIX) and databases (for example, DB2). Unfortunately, CLOCK is still plagued by disadvantages of LRU such as disregard for "frequency", susceptibility to scans, and low performance.As our main contribution, we propose a simple and elegant new algorithm, namely, CLOCK with Adaptive Replacement (CAR), that has several advantages over CLOCK: (i) it is scan-resistant; (ii) it is self-tuning and it adaptively and dynamically captures the "recency" and "frequency" features of a workload; (iii) it uses essentially the same primitives as CLOCK, and, hence, is low-complexity and amenable to a high-concurrency implementation; and (iv) it outperforms CLOCK across a wide-range of cache sizes and workloads. The algorithm CAR is inspired by the Adaptive Replacement Cache (ARC) algorithm, and inherits virtually all advantages of ARC including its high performance, but does not serialize cache hits behind a single global lock. As our second contribution, we introduce another novel algorithm, namely, CAR with Temporal filtering (CART), that has all the advantages of CAR, but, in addition, uses a certain temporal filter to distill pages with long-term utility from those with only short-term utility.},
  booktitle = {Proceedings of the 3rd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  series = {{{FAST}} '04},
  publisher = {{USENIX Association}},
  urldate = {2019-09-16},
  date = {2004},
  pages = {187--200},
  author = {Bansal, Sorav and Modha, Dharmendra S.},
  venue = {San Francisco, CA}
}

@inproceedings{glassAdaptivePageReplacement1997,
  location = {{New York, NY, USA}},
  title = {Adaptive {{Page Replacement Based}} on {{Memory Reference Behavior}}},
  isbn = {978-0-89791-909-8},
  url = {http://doi.acm.org/10.1145/258612.258681},
  doi = {10.1145/258612.258681},
  abstract = {As disk performance continues to lag behind that of memory systems and processors, virtual memory management becomes increasingly important for overall system performance. In this paper we study the page reference behavior of a collection of memory-intensive applications, and propose a new virtual memory page replacement algorithm, SEQ. SEQ detects long sequences of page faults and applies most-recently-used replacement to those sequences. Simulations show that for a large class of applications, SEQ performs close to the optimal replacement algorithm, and significantly better than Least-Recently-Used (LRU). In addition, SEQ performs similarly to LRU for applications that do not exhibit sequential faulting.},
  booktitle = {Proceedings of the 1997 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  series = {{{SIGMETRICS}} '97},
  publisher = {{ACM}},
  urldate = {2019-09-16},
  date = {1997},
  pages = {115--126},
  author = {Glass, Gideon and Cao, Pei},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\HVCBHFN3\\Glass and Cao - 1997 - Adaptive Page Replacement Based on Memory Referenc.pdf},
  venue = {Seattle, Washington, USA}
}

@inproceedings{smaragdakisEELRUSimpleEffective1999,
  location = {{New York, NY, USA}},
  title = {{{EELRU}}: {{Simple}} and {{Effective Adaptive Page Replacement}}},
  isbn = {978-1-58113-083-6},
  url = {http://doi.acm.org/10.1145/301453.301486},
  doi = {10.1145/301453.301486},
  shorttitle = {{{EELRU}}},
  booktitle = {Proceedings of the 1999 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  series = {{{SIGMETRICS}} '99},
  publisher = {{ACM}},
  urldate = {2019-09-16},
  date = {1999},
  pages = {122--133},
  author = {Smaragdakis, Yannis and Kaplan, Scott and Wilson, Paul},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\9Y4K9D4P\\Smaragdakis et al. - 1999 - EELRU Simple and Effective Adaptive Page Replacem.pdf},
  venue = {Atlanta, Georgia, USA}
}

@article{franciscoProceedingsFAST03,
  langid = {english},
  title = {Proceedings of {{FAST}} ’03: 2nd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  abstract = {We consider the problem of cache management in a demand paging scenario with uniform page sizes. We propose a new cache management policy, namely, Adaptive Replacement Cache (ARC), that has several advantages.},
  pages = {17},
  author = {Francisco, San},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\M9LX5DVR\\Francisco - Proceedings of FAST ’03 2nd USENIX Conference on .pdf}
}

@article{qureshiAdaptiveInsertionPolicies,
  langid = {english},
  title = {Adaptive {{Insertion Policies}} for {{High Performance Caching}}},
  abstract = {The commonly used LRU replacement policy is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. For such applications, the majority of lines traverse from the MRU position to the LRU position without receiving any cache hits, resulting in inefﬁcient use of cache space. Cache performance can be improved if some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits.},
  pages = {11},
  author = {Qureshi, Moinuddin K and Jaleel, Aamer and Patt, Yale N and Jr, Simon C Steely and Emer, Joel},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\BHHPMZSD\\Qureshi et al. - Adaptive Insertion Policies for High Performance C.pdf}
}

@article{arlittEvaluatingContentManagement2000,
  langid = {english},
  title = {Evaluating Content Management Techniques for {{Web}} Proxy Caches},
  volume = {27},
  issn = {01635999},
  url = {http://portal.acm.org/citation.cfm?doid=346000.346003},
  doi = {10.1145/346000.346003},
  number = {4},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  urldate = {2019-11-28},
  date = {2000-03-01},
  pages = {3-11},
  author = {Arlitt, Martin and Cherkasova, Ludmila and Dilley, John and Friedrich, Rich and Jin, Tai},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\W34D6IUK\\Arlitt et al. - 2000 - Evaluating content management techniques for Web p.pdf}
}

@inproceedings{zhangWebCachingFramework1999,
  title = {Web Caching Framework: Analytical Models and Beyond},
  doi = {10.1109/WIAPP.1999.788030},
  shorttitle = {Web Caching Framework},
  abstract = {Many Web caching algorithms have been proposed in recent years. However the lack of analytical support and systematic evaluation environment significantly affect the applicability of these algorithms. We introduce a framework within which Web caching algorithms can be consistently analyzed and empirically examined. The framework consists of two complementary parts. The statistical model and the simulation environment. The analytical model covers both the Web trace characteristics and the caching algorithm behaviors. The simulation system, referred to as WebCASE (Web Caching Algorithm Simulation Environment), consists of an extensible simulation core and a front end graphical interface showing the running algorithm behaviors. By using this framework, we are able to better understand the performance discrepancies exhibited by different algorithms and develop more efficient new algorithms. These new algorithms take into consideration practical issues and make noticeable performance improvements over existing algorithms.},
  eventtitle = {Proceedings 1999 {{IEEE Workshop}} on {{Internet Applications}} ({{Cat}}. {{No}}.{{PR00197}})},
  booktitle = {Proceedings 1999 {{IEEE Workshop}} on {{Internet Applications}} ({{Cat}}. {{No}}.{{PR00197}})},
  date = {1999-07},
  pages = {132-141},
  keywords = {Algorithm design and analysis,analytical models,Analytical models,cache storage,caching algorithm behaviors,digital simulation,Electrical capacitance tomography,Explosives,extensible simulation core,file servers,front end graphical interface,graphical user interfaces,information resources,Internet,Laboratories,Measurement,National electric code,Performance analysis,performance discrepancies,performance improvements,running algorithm behaviors,search engines,simulation environment,simulation system,statistical model,Web Caching Algorithm Simulation Environment,Web caching algorithms,Web caching framework,Web server,Web sites,Web trace characteristics,WebCASE},
  author = {Zhang, J. and Izmailov, R. and Reininger, D. and Ott, M.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\KEIGEMGK\\Zhang et al. - 1999 - Web caching framework analytical models and beyon.pdf;C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\2NTFETQZ\\788030.html},
  issn = {null}
}

@article{kellyVariableQosShared1999,
  langid = {english},
  title = {Variable {{Qos}} from {{Shared Web Caches}}: {{User}}-{{Centered Design}} and {{Value}}-{{Sensitive Replacement}}},
  issn = {1556-5068},
  url = {http://www.ssrn.com/abstract=975737},
  doi = {10.2139/ssrn.975737},
  shorttitle = {Variable {{Qos}} from {{Shared Web Caches}}},
  abstract = {Due to differences in server capacity, external bandwidth, and client demand, some Web servers value cache hits more than others. Assuming that a shared cache knows the extent to which different servers value hits, it may employ a value-sensitive replacement policy in order to generate higher aggregate value for servers. We consider both the prediction and value aspects of this problem and introduce a novel value-sensitive LFU/LRU hybrid that biases the allocation of cache space toward documents whose origin servers value caching most highly. We compare our algorithm with others from the Web caching literature and discuss from an economic standpoint the problems associated with obtaining servers’ private valuation information.},
  journaltitle = {SSRN Electronic Journal},
  urldate = {2019-11-28},
  date = {1999},
  author = {Kelly, Terence and Jamin, Sugih and MacKie-Mason, Jeffrey K.},
  file = {C\:\\Users\\jr776\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\g2vl74k3.default\\zotero\\storage\\RQ8TT62R\\Kelly et al. - 1999 - Variable Qos from Shared Web Caches User-Centered.pdf}
}

@article{robinsonDataCacheManagement1990a,
  langid = {english},
  title = {Data Cache Management Using Frequency-Based Replacement},
  volume = {18},
  issn = {01635999},
  url = {http://portal.acm.org/citation.cfm?doid=98460.98523},
  doi = {10.1145/98460.98523},
  number = {1},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  urldate = {2019-11-28},
  date = {1990-04-01},
  pages = {134-142},
  author = {Robinson, John T. and Devarakonda, Murthy V.}
}


