
@article{abdelfattahLeastRecentlyFive2012,
  title = {Least {{Recently Plus Five Least Frequently Replacement Policy}} ({{LR}}+{{5LF}})},
  author = {AbdelFattah, Adwan and Samra, Aiman Abu},
  year = {2012},
  volume = {9},
  pages = {6},
  abstract = {In this paper, we present a new block replacement policy in which we proposed a new efficient algorithm for combining two important policies Least Recently Used (LRU) and Least Frequently Used (LFU). The implementation of the proposed policy is simple. It requires limited calculations to determine the victim block. We proposed our models to implement LRU and LFU policies. The new policy gives each block in cache two weighing values corresponding to LRU and LFU policies. Then a simple algorithm is used to get the overall value for each block. A comprehensive comparison is made between our Policy and LRU, First In First Out (FIFO), V-WAY, and Combined LRU and LFU (CRF) policies. Experimental results show that the LR+5LF replacement policy significantly reduces the number of cache misses. We modified simple scalar simulator version 3 under Linux Ubuntu 9.04 and we used speccpu2000 benchmark to simulate this policy. The results of simulations showed, that giving higher weighing to LFU policy gives this policy best performance characteristics over other policies. Substantial improvement on miss rate was achieved on instruction level 1 cache and at level 2 cache memory.},
  file = {/Users/jonathanrainer/Zotero/storage/8VMXNYT5/AbdelFattah and Samra - 2012 - Least Recently Plus Five Least Frequently Replacem.pdf},
  journal = {The International Arab Journal of Information Technology},
  language = {en},
  number = {1}
}

@inproceedings{abramsCachingProxiesLimitations1995,
  title = {Caching {{Proxies}}: {{Limitations}} and {{Potentials}}},
  booktitle = {Proceedings {{Fourth International World Wide Web Conference}}},
  author = {Abrams, Marc and Standridge, Charles R. and Abdulla, Ghaleb and Williams, Stephen and Fox, Edward A.},
  year = {1995},
  volume = {1},
  publisher = {{O'Reilly}},
  address = {{Boston, Mass.}},
  file = {/Users/jonathanrainer/Zotero/storage/BDZASLFE/155.html},
  isbn = {1-56592-169-0}
}

@article{acklandSinglechip6billion16b2000,
  title = {A Single-Chip, 1.6-Billion, 16-b {{MAC}}/s Multiprocessor {{DSP}}},
  author = {Ackland, B. and Anesko, A. and Brinthaupt, D. and Daubert, S.J. and Kalavade, A. and Knobloch, J. and Micca, E. and Moturi, M. and Nicol, C.J. and O'Neill, J.H. and Othmer, J. and Sackinger, E. and Singh, K.J. and Sweet, J. and Terman, C.J. and Williams, J.},
  year = {2000},
  month = mar,
  volume = {35},
  pages = {412--424},
  issn = {1558-173X},
  doi = {10.1109/4.826824},
  abstract = {An MIMD multiprocessor digital signal-processing (DSP) chip containing four 64-b processing elements (PE's) interconnected by a 128-b pipelined split transaction bus (STBus) is presented. Each PE contains a 32-b RISC core with DSP enhancements and a 64-b single-instruction, multiple-data vector coprocessor with four 16-b MAC/s and a vector reduction unit. PEs are connected to the STBus through reconfigurable dual-ported snooping L1 cache memories that support shared memory multiprocessing using a modified-MESI data coherency protocol. High-bandwidth data transfers between system memory and on-chip caches are managed in a pipelined memory controller that supports multiple outstanding transactions. An embedded RTOS dynamically schedules multiple tasks onto the PEs. Process synchronization is achieved using cached semaphores. The 200-mm/sup 2/, 0.25-/spl mu/m CMOS chip operates at 100 MHz and dissipates 4 W from a 3.3-V supply.},
  file = {/Users/jonathanrainer/Zotero/storage/PZKMNKUA/Ackland et al. - 2000 - A single-chip, 1.6-billion, 16-b MACs multiproces.pdf;/Users/jonathanrainer/Zotero/storage/RWREQ87P/826824.html},
  journal = {IEEE Journal of Solid-State Circuits},
  keywords = {0.25 mum,100 MHz,16-b MAC/s,3.3 V,32 bit,4 W,64 bit,64-b single-instruction,Cache memory,cache storage,cached semaphores,CMOS chip,CMOS digital integrated circuits,Control systems,Coprocessors,Digital signal processing,digital signal processing chips,Digital signal processing chips,digital signal-processing chip,DSP,DSP enhancement,dual-ported snooping L1 cache memories,embedded RTOS,high-bandwidth data transfer,Memory management,MIMD multiprocessor,modified-MESI data coherency protocol,multiple outstanding transactions,multiple-data vector coprocessor,multiprocessor interconnection networks,on-chip caches,parallel architectures,pipeline processing,pipelined memory controller,pipelined split transaction bus,Protocols,reconfigurable architectures,reduced instruction set computing,Reduced instruction set computing,RISC core,shared memory multiprocessing,Signal processing,synchronisation,synchronization,System-on-a-chip,vector processor systems,vector reduction},
  number = {3}
}

@inproceedings{agarwalColumnassociativeCachesTechnique1993,
  title = {Column-Associative {{Caches}}: {{A Technique For Reducing The Miss Rate Of Direct}}-Mapped {{Caches}}},
  shorttitle = {Column-Associative {{Caches}}},
  booktitle = {Proceedings of the 20th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Agarwal, A. and Pudar, S.D.},
  year = {1993},
  month = may,
  pages = {179--190},
  issn = {null},
  doi = {10.1109/ISCA.1993.698559},
  file = {/Users/jonathanrainer/Zotero/storage/WPGIZA5S/Agarwal and Pudar - 1993 - Column-associative Caches A Technique For Reducin.pdf;/Users/jonathanrainer/Zotero/storage/ZEAA6QJH/698559.html},
  keywords = {Analytical models,Computational modeling,Computer science,Computer simulation,Interference,Laboratories,Paper technology,Reduced instruction set computing,Very large scale integration,Workstations}
}

@inproceedings{agarwalExploringHighBandwidth2003,
  title = {Exploring High Bandwidth Pipelined Cache Architecture for Scaled Technology},
  booktitle = {Automation and {{Test}} in {{Europe Conference}} and {{Exhibition}} 2003 {{Design}}},
  author = {Agarwal, A. and Roy, K. and Vijaykumar, T.N.},
  year = {2003},
  month = mar,
  pages = {778--783},
  issn = {1530-1591},
  doi = {10.1109/DATE.2003.1253701},
  abstract = {In this paper, we propose a design technique to pipeline cache memories for high bandwidth applications. With the scaling of technology, cache access latencies are multiple clock cycles. The proposed pipelined cache architecture can be accessed every clock cycle and, thereby, enhances bandwidth and overall processor performance. The proposed architecture utilizes the idea of banking to reduce bit-line and word-line delay, making word-line to sense amplifier delay fit into a single clock cycle. Experimental results show that optimal banking allows the cache to be split into multiple stages whose delays are equal to the clock cycle time. The proposed design is fully scalable and can be applied to future technology generations. Power, delay and area estimates show that on average, the proposed pipelined cache improves MOPS (millions of operations per unit time per unit area per unit energy) by 40-50\% compared to current cache architectures.},
  file = {/Users/jonathanrainer/Zotero/storage/WUI95THT/Agarwal et al. - 2003 - Exploring high bandwidth pipelined cache architect.pdf;/Users/jonathanrainer/Zotero/storage/CHPRGHQT/1253701.html},
  keywords = {Bandwidth,banking,Banking,bit-line delay,cache access latency,cache access time reduction,cache storage,Clocks,CMOS technology,Computer architecture,Decoding,Delay effects,Delay estimation,high bandwidth architecture,integrated circuit design,logic design,logic simulation,MOPS improvement,pipeline processing,Pipeline processing,pipelined cache architecture,processor performance enhancement,scaled technology cache,single clock cycle access,Surface-mount technology,word-line/sense amplifier delay}
}

@article{aguilarCoherenceReplacementProtocolWeb2006,
  title = {A {{Coherence}}-{{Replacement Protocol For Web Proxy Cache Systems}}},
  author = {Aguilar, J. and Leiss, E. L.},
  year = {2006},
  month = jan,
  volume = {28},
  pages = {12--18},
  issn = {1206-212X},
  doi = {10.1080/1206212X.2006.11441783},
  abstract = {As World Wide Web usage has grown dramatically in recent years, so has the recognition that web caches (especially proxy caches) will have an important role in reducing server loads, client request latencies, and network traffic. In this paper, we propose an adaptive cache coherence-replacement scheme for web proxy cache systems that is based on several criteria about the system and applications, with the objective of optimizing the distributed cache system performance. Our coherence-replacement scheme assigns a replacement priority value to each cache block according to a set of criteria for deciding which block to remove. The goal is to provide an effective utilization of the distributed cache memory and a good application performance.},
  file = {/Users/jonathanrainer/Zotero/storage/PJVR3TZ5/Aguilar and Leiss - 2006 - A Coherence-Replacement Protocol For Web Proxy Cac.pdf;/Users/jonathanrainer/Zotero/storage/6LJTW773/1206212X.2006.html},
  journal = {International Journal of Computers and Applications},
  keywords = {coherency techniques,replacement techniques,Web caching},
  number = {1}
}

@article{aguilarGeneralAdaptiveCache2004,
  title = {A {{General Adaptive Cache Coherency}}-{{Replacement Protocol}} for {{Web Proxy Cache Systems}}},
  author = {Aguilar, Jose and Leiss, Ernst},
  year = {2004},
  volume = {8},
  pages = {1--14},
  abstract = {As World Wide Web usage has grown dramatically in recent years, so has grown the recognition that Web caches (especially proxy caches) will have an important role in reducing server loads, client request latencies, and network traffic. In this paper, we propose an adaptive cache coherence-replacement scheme for web proxy cache systems that is based on several criteria about the system and applications, with the objective of optimizing the distributed cache system performance. Our coherence-replacement scheme assigns a replacement priority value to each cache block according to a set of criteria to decide which block to remove. The goal is to provide an effective utilization of the distributed cache memory and a good application performance.},
  file = {/Users/jonathanrainer/Zotero/storage/ZPCHRQUQ/Aguilar and Leiss - 2001 - A General Adaptive Cache Coherency-Replacement Sch.pdf},
  journal = {Computaci{\'o}n y Sistemas},
  language = {en},
  number = {1}
}

@inproceedings{aimtongkhamNovelWebCaching2016,
  title = {A Novel Web Caching Scheme Using Hybrid Least Frequently Used and Support Vector Machine},
  booktitle = {2016 13th {{International Joint Conference}} on {{Computer Science}} and {{Software Engineering}} ({{JCSSE}})},
  author = {Aimtongkham, P. and {So-In}, C. and Sanguanpong, S.},
  year = {2016},
  month = jul,
  pages = {1--6},
  doi = {10.1109/JCSSE.2016.7748932},
  abstract = {The gargantuan uses of web access in various types of applications, such as text, image, audio, and video, across the globe have caused the limitation for service providers to optimally make use of Internet infrastructure. The advance of web proxy/caching has recently been in place to mitigate this phenomenon using the concept of locality and proximity. There exist some traditional caching schemes, such as FIFO, LFU, LRU, and Size, but with key limitations on the precision. On the other hands, soft computing has recently been investigated due to its advantage of high precision. Thus, this paper proposes a novel caching method by integrating these twos. SVM was first used for classification, to divide the caching probability - to be replaced or else. Then, LFU was applied for the actual replacement given new web objects (if cache full); and these are Hybrid LFU-SVM. Its performance is practically confirmed from our intensive evaluation against SVM-LRU and its traditional schemes like LFU and LRU in order of 14\% to 52.3\% and 18\% to 63.2\%, for hit and byte hit rate, respectively, using a standard NLANR dataset.},
  file = {/Users/jonathanrainer/Zotero/storage/R6UPCWHS/Aimtongkham et al. - 2016 - A novel web caching scheme using hybrid least freq.pdf;/Users/jonathanrainer/Zotero/storage/HKWFKRWW/7748932.html},
  keywords = {Artificial neural networks,cache storage,caching probability,FIFO,Fuzzy logic,Hybrid Caching,hybrid least frequently used,hybrid LFU-SVM,information retrieval,Internet,Internet infrastructure,Kernel,Least Frequency Used,locality concept,NLANR dataset,pattern classification,proximity concept,Proxy,Radio frequency,Replacement,service providers,Soft Computing,support vector machine,Support Vector Machine,support vector machines,Support vector machines,SVM-LRU,Training,Web access,Web Caching,Web caching scheme,Web objects,Web proxy,Web Proxy}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004,
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  booktitle = {Proceedings of the 42nd Annual {{Southeast}} Regional Conference on   - {{ACM}}-{{SE}} 42},
  author = {{Al-Zoubi}, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  year = {2004},
  pages = {267},
  publisher = {{ACM Press}},
  address = {{Huntsville, Alabama}},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  file = {/Users/jonathanrainer/Zotero/storage/9A3TBW67/Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf},
  isbn = {978-1-58113-870-2},
  language = {en}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004a,
  title = {Performance Evaluation of Cache Replacement Policies for the {{SPEC CPU2000}} Benchmark Suite},
  author = {{Al-Zoubi}, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  year = {2004},
  pages = {267},
  publisher = {{ACM Press}},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.},
  file = {/Users/jonathanrainer/Zotero/storage/VPWMJPZB/Al-Zoubi et al. - 2004 - Performance evaluation of cache replacement polici.pdf},
  isbn = {978-1-58113-870-2},
  language = {en}
}

@inproceedings{al-zoubiPerformanceEvaluationCache2004b,
  title = {Performance {{Evaluation}} of {{Cache Replacement Policies}} for the {{SPEC CPU2000 Benchmark Suite}}},
  booktitle = {Proceedings of the {{42Nd Annual Southeast Regional Conference}}},
  author = {{Al-Zoubi}, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
  year = {2004},
  pages = {267--272},
  publisher = {{ACM}},
  address = {{Huntsville, Alabama}},
  doi = {10.1145/986537.986601},
  abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions.Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50\%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.},
  file = {/Users/jonathanrainer/Zotero/storage/I66AQ9LF/Al-Zoubi et al. - 2004 - Performance Evaluation of Cache Replacement Polici.pdf},
  isbn = {978-1-58113-870-2},
  keywords = {cache memory,performance evaluation,replacement policy},
  series = {{{ACM}}-{{SE}} 42}
}

@article{albericioExploitingReuseLocality2013,
  title = {Exploiting {{Reuse Locality}} on {{Inclusive Shared Last}}-Level {{Caches}}},
  author = {Albericio, Jorge and Ib{\'a}{\~n}ez, Pablo and Vi{\~n}als, V{\'i}ctor and Llaber{\'i}a, Jose Mar{\'i}a},
  year = {2013},
  month = jan,
  volume = {9},
  pages = {38:1--38:19},
  issn = {1544-3566},
  doi = {10.1145/2400682.2400697},
  abstract = {Optimization of the replacement policy used for Shared Last-Level Cache (SLLC) management in a Chip-MultiProcessor (CMP) is critical for avoiding off-chip accesses. Temporal locality, while being exploited by first levels of private cache memories, is only slightly exhibited by the stream of references arriving at the SLLC. Thus, traditional replacement algorithms based on recency are bad choices for governing SLLC replacement. Recent proposals involve SLLC replacement policies that attempt to exploit reuse either by segmenting the replacement list or improving the rereference interval prediction. On the other hand, inclusive SLLCs are commonplace in the CMP market, but the interaction between replacement policy and the enforcement of inclusion has barely been discussed. After analyzing that interaction, this article introduces two simple replacement policies exploiting reuse locality and targeting inclusive SLLCs: Least Recently Reused (LRR) and Not Recently Reused (NRR). NRR has the same implementation cost as NRU, and LRR only adds one bit per line to the LRU cost. After considering reuse locality and its interaction with the invalidations induced by inclusion, the proposals are evaluated by simulating multiprogrammed workloads in an 8-core system with two private cache levels and an SLLC. LRR outperforms LRU by 4.5\% (performing better in 97 out of 100 mixes) and NRR outperforms NRU by 4.2\% (performing better in 99 out of 100 mixes). We also show that our mechanisms outperform rereference interval prediction, a recently proposed SLLC replacement policy and that similar conclusions can be drawn by varying the associativity or the SLLC size.},
  file = {/Users/jonathanrainer/Zotero/storage/G8LAASED/Albericio et al. - 2013 - Exploiting Reuse Locality on Inclusive Shared Last.pdf},
  journal = {ACM Trans. Archit. Code Optim.},
  keywords = {Replacement policy,shared resources management},
  number = {4}
}

@inproceedings{alexanderDistributedPrefetchbufferCache1996a,
  title = {Distributed Prefetch-Buffer/Cache Design for High Performance Memory Systems},
  booktitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Alexander, T. and Kedem, G.},
  year = {1996},
  month = feb,
  pages = {254--263},
  issn = {null},
  doi = {10.1109/HPCA.1996.501191},
  abstract = {Microprocessor execution speeds are improving at a rate of 50\%-80\% per year while DRAM access times are improving at a much lower rate of 5\%-10\% per year. Computer systems are rapidly approaching the point at which overall system performance is determined not by the speed of the CPU but by the memory system speed. We present a high performance memory system architecture that overcomes the growing speed disparity between high performance microprocessors and current generation DRAMs. A novel prediction and prefetching technique is combined with a distributed cache architecture to build a high performance memory system. We use a table based prediction scheme with a prediction cache to prefetch data from the on-chip DRAM array to an on-chip SRAM prefetch buffer. By prefetching data we are able to hide the large latency associated with DRAM access and cycle times. Our experiments show that with a small (32 KB) prediction cache we can get an effective main memory access time that is close to the access time of larger secondary caches.},
  file = {/Users/jonathanrainer/Zotero/storage/XQU7HUJ8/Alexander and Kedem - 1996 - Distributed prefetch-buffercache design for high .pdf;/Users/jonathanrainer/Zotero/storage/TR3EU34F/501191.html},
  keywords = {Bandwidth,cache design,cache storage,Clocks,Computer science,Delay,distributed cache architecture,distributed prefetch buffer design,DRAM access times,Gears,Hardware,high performance memory systems,memory architecture,memory system architecture,memory system speed,Microprocessors,Prefetching,prefetching technique,Random access memory,SRAM,System performance,table based prediction scheme}
}

@inproceedings{alghazoSFLRUCacheReplacement2004,
  title = {{{SF}}-{{LRU}} Cache Replacement Algorithm},
  booktitle = {Records of the 2004 {{International Workshop}} on {{Memory Technology}}, {{Design}} and {{Testing}}, 2004.},
  author = {Alghazo, J. and Akaaboune, A. and Botros, N.},
  year = {2004},
  month = aug,
  pages = {19--24},
  doi = {10.1109/MTDT.2004.1327979},
  abstract = {In this paper we propose a replacement algorithm, SF-LRU (second chance-frequency - least recently used) that combines the LRU (least recently used) and the LFU (least frequently used) using the second chance concept. A comprehensive comparison is made between our algorithm and both LRU and LFU algorithms. Experimental results show that the SF-LRU significantly reduces the number of cache misses compared the other two algorithms. Simulation results show that our algorithm can provide a maximum value of approximately 6.3\% improvement in the miss ratio over the LRU algorithm in data cache and approximately 9.3\% improvement in miss ratio in instruction cache. This performance improvement is attributed to the fact that our algorithm provides a second chance to the block that may be deleted according to LRU's rules. This is done by comparing the frequency of the block with the block next to it in the set.},
  file = {/Users/jonathanrainer/Zotero/storage/QAY5LIZV/Alghazo et al. - 2004 - SF-LRU cache replacement algorithm.pdf;/Users/jonathanrainer/Zotero/storage/HU2EYP6D/1327979.html},
  keywords = {Application software,Bridges,Cache memory,cache misses reduction,cache replacement algorithm,cache storage,Clocks,Costs,data cache,Delay,Energy consumption,Frequency,History,instruction cache,LFU,low power cache,low-power electronics,LRU rules,miss ratio improvement,performance improvement,second chance concept,second chance-frequency - least recently used,SF-LRU,System performance}
}

@inproceedings{altmanNovelMethodologyUsing1993,
  title = {A {{Novel Methodology}} Using {{Genetic Algorithms}} for the {{Design}} of {{Caches}} and {{Cache Replacement Policy}}},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Genetic Algorithms}}},
  author = {Altman, Erik R and Agarwal, Vinod K and Gao, Guang R},
  year = {1993},
  pages = {392--399},
  publisher = {{Morgan Kaufmann Publishers}},
  file = {/Users/jonathanrainer/Zotero/storage/339B4AMH/Altman et al. - A Novel Methodology using Genetic Algorithms for t.pdf},
  isbn = {1-55860-299-2},
  language = {en}
}

@article{altmeyerStaticProbabilisticTiming2015,
  title = {Static Probabilistic Timing Analysis for Real-Time Systems Using Random Replacement Caches},
  author = {Altmeyer, Sebastian and {Cucu-Grosjean}, Liliana and Davis, Robert I.},
  year = {2015},
  month = jan,
  volume = {51},
  pages = {77--123},
  issn = {1573-1383},
  doi = {10.1007/s11241-014-9218-4},
  abstract = {In this paper, we investigate static probabilistic timing analysis (SPTA) for single processor real-time systems that use a cache with an evict-on-miss random replacement policy. We show that previously published formulae for the probability of a cache hit can produce results that are optimistic and unsound when used to compute probabilistic worst-case execution time (pWCET) distributions. We investigate the correctness, optimality, and precision of different approaches to SPTA for random replacement caches. We prove that one of the previously published formulae for the probability of a cache hit is optimal with respect to the limited information (reuse distance and cache associativity) that it uses. We derive an alternative formulation that makes use of additional information in the form of the number of distinct memory blocks accessed (the stack distance). This provides a complementary lower bound that can be used together with previously published formula to obtain more accurate analysis. We improve upon this joint approach by using extra information about cache contention. To investigate the precision of various approaches to SPTA, we introduce a simple exhaustive method that computes a precise pWCET distribution, albeit at the cost of exponential complexity. We integrate this precise approach, applied to small numbers of frequently accessed memory blocks, with imprecise analysis of other memory blocks, to form a combined approach that improves precision, without significantly increasing complexity. The performance of the various approaches are compared on benchmark programs. We also make comparisons against deterministic analysis of the least recently used replacement policy.},
  file = {/Users/jonathanrainer/Zotero/storage/7B73XNLS/Altmeyer et al. - 2015 - Static probabilistic timing analysis for real-time.pdf},
  journal = {Real-Time Systems},
  keywords = {Random cache replacement,Static probabilistic timing analysis,Timing verification,WCET analysis},
  language = {en},
  number = {1}
}

@inproceedings{alyVariablewaySetAssociative2003,
  title = {Variable-Way Set Associative Cache Design for Embedded System Applications},
  booktitle = {2003 46th {{Midwest Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Aly, R.E. and Nallamilli, B.R. and Bayoumi, M.A.},
  year = {2003},
  month = dec,
  volume = {3},
  pages = {1435-1438 Vol. 3},
  issn = {1548-3746},
  doi = {10.1109/MWSCAS.2003.1562565},
  abstract = {Variable-way set associative cache is proposed as a new technique to maximize the cache performance especially for embedded applications or to reduce the power consumption with the same performance. Static profiling is used to determine the sets' behavior of the set-associative cache. Variable-way set-associative can be used in high-performance or low-power operation modes. Each set in the proposed design basically has different associativity to maximize the total performance for the cache size or reduce the power consumption. The proposed architecture is simulated on SimpleScalar simulator and tested on several Spec2000 Benchmarks. The results show on average 2\% reduction in the miss rate at the high-performance mode and up to 43\% reduction of the power consumption at low-power mode.},
  file = {/Users/jonathanrainer/Zotero/storage/25GNNL92/Aly et al. - 2003 - Variable-way set associative cache design for embe.pdf;/Users/jonathanrainer/Zotero/storage/GP34ZZ4T/1562565.html},
  keywords = {associativity,Benchmark testing,cache design,cache size,cache storage,Computational modeling,computer architecture,Computer architecture,content-addressable storage,Costs,Degradation,embedded system,Embedded system,embedded systems,Energy consumption,Hardware,low-power electronics,low-power operation modes,memory architecture,Memory architecture,miss rate reduction,power consumption,Power generation economics,set-associative cache,SimpleScalar simulator,Spec2000 Benchmarks,static profiling,variable-way set associative cache}
}

@inproceedings{anandkumarHybridCacheReplacement2014,
  title = {A Hybrid Cache Replacement Policy for Heterogeneous Multi-Cores},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  author = {AnandKumar, K. M. and S, A. and Ganesh, D. and Christy, M. S.},
  year = {2014},
  month = sep,
  pages = {594--599},
  doi = {10.1109/ICACCI.2014.6968209},
  abstract = {Future generation computer architectures are endeavoring to achieve high performance without compromise on energy efficiency. In a multiprocessor system, cache miss degrades the performance as the miss penalty scales by an exponential factor across a shared memory system when compared to general purpose processors. This instigates the need for an efficient cache replacement scheme to cater to the data needs of underlying functional units in case of a cache miss. Minimal cache miss improves resource utilization and reduces data movement across the core which in turn contributes to a high performance and lesser power dissipation. Existing replacement policies has several issues when implemented in a heterogeneous multi-core system. The commonly used LRU replacement policy does not offer optimal performance for applications with high dependencies. Motivated by the limitations of the existing algorithms, we propose a hybrid cache replacement policy which combines Least Recently Used (LRU) and Least Frequently Used (LFU) replacement policies. Each cache block has two weighing values corresponding to LRU and LFU policies and a cumulative weight is calculated using these two values. Conducting simulations over wide range of cache sizes and associativity, we show that our proposed approach has shown increased cache hit to miss ratio when compared with LRU and other conventional cache replacement policies.},
  file = {/Users/jonathanrainer/Zotero/storage/JQI5WCDH/AnandKumar et al. - 2014 - A hybrid cache replacement policy for heterogeneou.pdf;/Users/jonathanrainer/Zotero/storage/T8CJAVC8/6968209.html},
  keywords = {cache miss,Cache Replacement,cache storage,data movement,energy efficiency,future generation computer architecture,heterogeneous multicore system,heterogeneous multicores,hybrid cache replacement policy,least frequently used replacement policy,least recently used replacement policy,LFU policy,Libraries,LRU replacement policy,multi-core,multiprocessor system,performance evaluation,power dissipation,resource allocation,resource utilization,shared memory system,shared memory systems,Time-frequency analysis,weighing values}
}

@inproceedings{ariACMEAdaptiveCaching2002,
  title = {{{ACME}}: {{Adaptive Caching Using Multiple Experts}}},
  shorttitle = {{{ACME}}},
  booktitle = {Distributed {{Data}} \& {{Structures}} 4, {{Records}} of the 4th {{International Meeting}} ({{WDAS}} 2002), {{Paris}}, {{France}}, {{March}} 20-23, 2002},
  author = {Ari, Ismail and Amer, Ahmed and Gramacy, Robert B. and Miller, Ethan L. and Brandt, Scott A. and Long, Darrell D. E.},
  editor = {Litwin, Witold and L{\'e}vy, G{\'e}rard},
  year = {2002},
  volume = {14},
  pages = {143--158},
  publisher = {{Carleton Scientific}},
  file = {/Users/jonathanrainer/Zotero/storage/LYINEFII/Ari et al. - ACME Adaptive Caching Using Multiple Experts.pdf},
  series = {Proceedings in {{Informatics}}}
}

@article{arlittEvaluatingContentManagement2000,
  title = {Evaluating Content Management Techniques for {{Web}} Proxy Caches},
  author = {Arlitt, Martin and Cherkasova, Ludmila and Dilley, John and Friedrich, Rich and Jin, Tai},
  year = {2000},
  month = mar,
  volume = {27},
  pages = {3--11},
  issn = {01635999},
  doi = {10.1145/346000.346003},
  file = {/Users/jonathanrainer/Zotero/storage/W34D6IUK/Arlitt et al. - 2000 - Evaluating content management techniques for Web p.pdf},
  journal = {ACM SIGMETRICS Performance Evaluation Review},
  language = {en},
  number = {4}
}

@inproceedings{armejachTidyCacheImproving2015,
  title = {Tidy {{Cache}}: {{Improving Data Placement}} in {{Die}}-{{Stacked DRAM Caches}}},
  shorttitle = {Tidy {{Cache}}},
  booktitle = {2015 27th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  author = {Armejach, A. and Cristal, A. and Unsal, O. S.},
  year = {2015},
  month = oct,
  pages = {65--73},
  doi = {10.1109/SBAC-PAD.2015.23},
  abstract = {Die-stacked DRAM caches are likely to become available in mainstream chips in the near future. DRAM caches are typically used as a last level shared cache behind the traditional hierarchy of on-chip SRAM caches. However, its internal organization differs from traditional caches as it is based on DRAM technology that provides significantly diverse access latencies depending on the state of its internal structures. Accesses that hit in the row-buffer require only one DRAM command and are significantly faster than those that require closing the row-buffer to load a new row to read from. Prior work has focused on maximizing row-buffer locality while maintaining high cache hit ratios. However, past designs do not consider performance problems that may arise due to interleaved accesses from different applications that compete for the shared DRAM resources, nor the different access patterns and locality characteristics that each of these applications may have. In this paper, we first identify performance pathologies that are specific to DRAM caches which arise due to the interference caused by interleaved accesses from multiple cores. We then propose Tidy Cache, a novel DRAM cache design that is able to ameliorate these performance pathologies by dynamically adapting the replacement policy for demanded data. Our performance evaluation results show that our design outperforms the state-of-the-art by 9.2\% for multi-programmed SPEC workloads and by 16.7\% for a set of TPC-H queries, mainly due to significantly better cache miss ratios.},
  file = {/Users/jonathanrainer/Zotero/storage/UFPC3JNF/Armejach et al. - 2015 - Tidy Cache Improving Data Placement in Die-Stacke.pdf;/Users/jonathanrainer/Zotero/storage/5ZEB2AQ8/7379835.html},
  keywords = {3D stacking,access latencies,Bandwidth,cache,cache miss ratios,cache storage,data placement improvement,data replacement policy,die-stacked DRAM caches,DRAM,DRAM cache design,DRAM chips,DRAM command,Interference,interleaved access,internal structures,multiprogrammed SPEC workloads,on-chip SRAM caches,Organizations,Pathology,performance evaluation,Proposals,Random access memory,row-buffer,shared DRAM resources,SRAM chips,System-on-chip,Tidy Cache,TPC-H queries}
}

@inproceedings{aroraCompositeDataPrefetcher2014,
  title = {A Composite Data {{Prefetcher}} Framework for Multilevel Caches},
  booktitle = {2014 {{International Conference}} on {{Advances}} in {{Computing}}, {{Communications}} and {{Informatics}} ({{ICACCI}})},
  author = {Arora, H. and Banerjee, S. and Davina, V.},
  year = {2014},
  month = sep,
  pages = {1827--1833},
  doi = {10.1109/ICACCI.2014.6968442},
  abstract = {The increasing difference between the Processor speed and the DRAM performance have led to the assertive need to hide memory latency and reduce memory access time. It is noticed that the Processor remains stalled on memory references. Data Prefetching is a technique that fetches that next instruction's data parallel to the current instruction execution in a typical Processor-Cache-DRAM system. A Prefetcher anticipates a cache miss that might take place in the next instruction and fetches the data before the actual memory reference. The goal of prefetching is to reduce as many cache misses as possible. In this paper we present a detailed summary of the different prefetching techniques, and implement a composite prefetcher prototype that employs the techniques of Sequential, Stride and Distance Prefetching.},
  file = {/Users/jonathanrainer/Zotero/storage/HHVXUMNB/Arora et al. - 2014 - A composite data Prefetcher framework for multilev.pdf;/Users/jonathanrainer/Zotero/storage/AZQXL5GK/6968442.html},
  keywords = {Arbitrary Stride Prefetching,Average Memory Access Time,cache miss,cache storage,composite data prefetcher framework,data prefetching,distance prefetching,Distance Prefetching,DRAM chips,DRAM performance,dynamic random access memory,Dynamic Read Only Memory,Educational institutions,Global History Buffer,Hardware,History,Markov processes,memory access time,memory latency,multilevel cache,Prefetching,processor speed,processor-cache-DRAM system,Random access memory,sequential prefetching,storage management,stride prefetching}
}

@inproceedings{asaduzzamanEffectiveLockingfreeCaching2014,
  title = {An Effective Locking-Free Caching Technique for Power-Aware Multicore Computing Systems},
  booktitle = {2014 {{International Conference}} on {{Informatics}}, {{Electronics Vision}} ({{ICIEV}})},
  author = {Asaduzzaman, Abu and Allen, Mark P. and Jareen, Tania},
  year = {2014},
  month = may,
  pages = {1--6},
  issn = {null},
  doi = {10.1109/ICIEV.2014.6850861},
  abstract = {In multicore/manycore systems, multiple caches increase the total power consumption and intensify latency because it is nearly impossible to hide last-level latency. Studies suggest that there are opportunities to increase the performance to power ratio by locking selected memory blocks inside the caches during runtime. However, the cache locking technique reduces the effective cache size and may introduce additional configuration difficulties, especially for multicore architectures. Furthermore, there may be other restrictions (example: PowerPC 750GX processor does not allow cache locking at level-1). In this paper, we propose a Smart Victim Cache (SVC) assisted caching technique that eliminates traditional cache locking without compromising the performance to power ratio. In addition to functioning as a normal victim cache, the proposed SVC holds memory blocks that may cause higher cache misses and supports stream buffering to increase cache hits. We model a Quad-Core System that has Private First Level Caches (PFLCs), a Shared Last Level Cache (SLLC), and a shared SVC located between the PFLCs and SLLC. We run simulation programs using a diverse group of applications including MPEG-4 and H.264/AVC. Experimental results suggest that the proposed SVC added multicore cache memory subsystem helps decrease the total power consumption and average latency up to 21\% and 17\%, respectively, when compared with that of SLLC cache locking mechanism without SVC.},
  file = {/Users/jonathanrainer/Zotero/storage/XZ2RR7FN/Asaduzzaman et al. - 2014 - An effective locking-free caching technique for po.pdf;/Users/jonathanrainer/Zotero/storage/UXI8FRNT/6850861.html},
  keywords = {Cache locking,cache locking technique,cache storage,effective locking free caching technique,green technology,Informatics,intensify latency,low-power computing,Memory management,multicore architecture,multicore cache memory subsystem,Multicore processing,multicore-manycore systems,multiprocessing systems,PFLCs,power aware computing,power aware multicore computing systems,power consumption,Power demand,power ratio,PowerPC 750GX processor,private first level caches,quadcore system,selected memory blocks,shared last level cache,SLLC,smart victim cache,Static VAr compensators,SVC,Transform coding,victim cache,Video coding}
}

@inproceedings{azimiTwoLevelCache1992,
  title = {Two Level Cache Architectures},
  booktitle = {Digest of {{Papers COMPCON Spring}} 1992},
  author = {Azimi, M. and Prasad, B. and Bhat, K.},
  year = {1992},
  month = feb,
  pages = {344--349},
  issn = {null},
  doi = {10.1109/CMPCON.1992.186736},
  abstract = {The authors discuss the performance measures required in building two-level cache solutions in uniprocessor systems based on more aggressive processors than the Intel486 microprocessor for desktop applications. The performance of serial second level caches is shown to exceed that of parallel caches by 10\%-20\%. The effect of second-level cache parameters such as cache/line/associativity/sector sizes is examined. It is shown that, as long as one of the two caches in the cache hierarchy is operating in the write back mode, the performance will be close to the case of both functioning in the write back mode. The authors quantify the fact that second-level caches reduce memory latency sensitivities. The performance gain of a full speed interface between the two levels of the cache hierarchy versus a half speed interface is shown to be about 10\% for desktop applications.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/B5IV76D4/Azimi et al. - 1992 - Two level cache architectures.pdf;/Users/jonathanrainer/Zotero/storage/Q6UFIV6V/186736.html},
  keywords = {buffer storage,cache hierarchy,Clocks,Delay,Engines,Frequency,full speed interface,half speed interface,memory architecture,memory latency sensitivities,Microprocessors,Paper technology,parallel caches,Performance analysis,performance evaluation,Performance gain,performance measures,Random access memory,serial second level caches,Trademarks,two level cache architectures,uniprocessor systems,write back mode}
}

@inproceedings{baerEffectiveOnchipPreloading1991a,
  title = {An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty},
  booktitle = {Supercomputing '91:{{Proceedings}} of the 1991 {{ACM}}/{{IEEE Conference}} on {{Supercomputing}}},
  author = {Baer, Jean-Loup and Chen, Tien-Fu},
  year = {1991},
  month = nov,
  pages = {176--186},
  issn = {null},
  doi = {10.1145/125826.125932},
  abstract = {Conventional cache prefetching approaches can be either hardware-based, generally by using a one-block-lookahead technique, or compiler-directed, with insertions of non-blocking prefetch instructions. We introduce a new hardware scheme based on the prediction of the execution of the instruction stream and associated operand references. It consists of a reference prediction table and a look-ahead program counter and its associated logic. With this scheme, data with regular access patterns is preloaded, independently of the stride size, and preloading of data with irregular access patterns is prevented. We evaluate our design through trace driven simulation by comparing it with a pure data cache approach under three different memory access models. Our experiments show that this scheme is very effective for reducing the data access penalty for scientific programs and that is has moderate success for other applications.},
  file = {/Users/jonathanrainer/Zotero/storage/Q9PPSAAT/Baer and Chen - 1991 - An effective on-chip preloading scheme to reduce d.pdf;/Users/jonathanrainer/Zotero/storage/ZXAELSDR/5348911.html},
  keywords = {cache storage,Counting circuits,data access penalty reduction,Hardware,information retrieval,instruction stream,Logic,look-ahead program counter,multiprocessing systems,on-chip preloading scheme,Prefetching,reference prediction table,storage management}
}

@inproceedings{bansalCARClockAdaptive2004,
  title = {{{CAR}}: {{Clock}} with {{Adaptive Replacement}}},
  shorttitle = {{{CAR}}},
  booktitle = {Proceedings of the 3rd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  author = {Bansal, Sorav and Modha, Dharmendra S.},
  year = {2004},
  pages = {187--200},
  publisher = {{USENIX Association}},
  address = {{San Francisco, CA}},
  abstract = {CLOCK is a classical cache replacement policy dating back to 1968 that was proposed as a low-complexity approximation to LRU. On every cache hit, the policy LRU needs to move the accessed item to the most recently used position, at which point, to ensure consistency and correctness, it serializes cache hits behind a single global lock. CLOCK eliminates this lock contention, and, hence, can support high concurrency and high throughput environments such as virtual memory (for example, Multics, UNIX, BSD, AIX) and databases (for example, DB2). Unfortunately, CLOCK is still plagued by disadvantages of LRU such as disregard for "frequency", susceptibility to scans, and low performance.As our main contribution, we propose a simple and elegant new algorithm, namely, CLOCK with Adaptive Replacement (CAR), that has several advantages over CLOCK: (i) it is scan-resistant; (ii) it is self-tuning and it adaptively and dynamically captures the "recency" and "frequency" features of a workload; (iii) it uses essentially the same primitives as CLOCK, and, hence, is low-complexity and amenable to a high-concurrency implementation; and (iv) it outperforms CLOCK across a wide-range of cache sizes and workloads. The algorithm CAR is inspired by the Adaptive Replacement Cache (ARC) algorithm, and inherits virtually all advantages of ARC including its high performance, but does not serialize cache hits behind a single global lock. As our second contribution, we introduce another novel algorithm, namely, CAR with Temporal filtering (CART), that has all the advantages of CAR, but, in addition, uses a certain temporal filter to distill pages with long-term utility from those with only short-term utility.},
  file = {/Users/jonathanrainer/Zotero/storage/9UNXPD7I/Bansal and Modha - CAR Clock with Adaptive Replacement.pdf},
  series = {{{FAST}} '04}
}

@inproceedings{batsonReactiveassociativeCaches2001,
  title = {Reactive-Associative Caches},
  booktitle = {Proceedings 2001 {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  author = {Batson, B. and Vijaykumar, T.N.},
  year = {2001},
  month = sep,
  pages = {49--60},
  issn = {1089-796X},
  doi = {10.1109/PACT.2001.953287},
  abstract = {While set-associative caches typically incur fewer misses than direct-mapped caches, set-associative caches have slower hit tithes. We propose the reactive-associative cache (r-a cache), which provides flexible associativity by placing most blocks in direct-mapped positions and reactively displacing only conflicting blocks to set-associative positions. The r-a cache uses way-prediction (like the predictive associative cache, PSA) to access displaced blocks on the initial probe. Unlike PSA, however, the r-a cache employs a novel feedback mechanism to prevent unpredictable blocks from being displaced. Reactive displacement and feedback allow the r-a cache to use a novel PC-based way-prediction and achieve high accuracy; without impractical block swapping as in column associative and group associative, and without relying on timing-constrained XOR way prediction. A one-port, 4-way r-a cache achieves up to 9\% speedup over a direct-mapped cache and performs within 2\% of an idealized 2-way set-associative, 1-cycle cache. A 4-way r-a cache achieves up to 13\% speedup over a PSA cache, with both r-a and PSA rising the PC scheme. CACTI estimates that for sizes larger than 8KB, a 4-way r-a cache is within 1\% of direct-mapped hit times, and 24\% faster than a 2-way set-associative cache.},
  file = {/Users/jonathanrainer/Zotero/storage/ZK6CP9LL/Batson and Vijaykumar - 2001 - Reactive-associative caches.pdf;/Users/jonathanrainer/Zotero/storage/AWHKFLW8/953287.html},
  keywords = {2-way set-associative 1-cycle cache,Bandwidth,block swapping,cache storage,CACTI,conflicting blocks,content-addressable storage,Data engineering,Degradation,Delay,direct-mapped caches,direct-mapped positions,displaced blocks,Feedback,feedback mechanism,flexible associativity,hit tithes,instruction sets,one-port 4-way r-a cache,PC scheme,pipeline processing,predictive associative cache,Probes,PSA,PSA cache,reactive-associative caches,set-associative caches,set-associative positions,storage allocation,timing-constrained XOR way prediction,way-prediction}
}

@article{beladyAnomalySpacetimeCharacteristics1969,
  title = {An Anomaly in Space-Time Characteristics of Certain Programs Running in a Paging Machine},
  author = {Belady, L. A. and Nelson, R. A. and Shedler, G. S.},
  year = {1969},
  month = jun,
  volume = {12},
  pages = {349--353},
  issn = {00010782},
  doi = {10.1145/363011.363155},
  file = {/Users/jonathanrainer/Zotero/storage/3LYVC4U5/Belady et al. - 1969 - An anomaly in space-time characteristics of certai.pdf},
  journal = {Communications of the ACM},
  number = {6}
}

@article{beladyStudyReplacementAlgorithms1966,
  title = {A Study of Replacement Algorithms for a Virtual-Storage Computer},
  author = {Belady, L. A.},
  year = {1966},
  volume = {5},
  pages = {78--101},
  issn = {0018-8670},
  doi = {10.1147/sj.52.0078},
  abstract = {One of the basic limitations of a digital computer is the size of its available memory.1In most cases, it is neither feasible nor economical for a user to insist that every problem program fit into memory. The number of words of information in a program often exceeds the number of cells (i.e., word locations) in memory. The only way to solve this problem is to assign more than one program word to a cell. Since a cell can hold only one word at a time, extra words assigned to the cell must be held in external storage. Conventionally, overlay techniques are employed to exchange memory words and external-storage words whenever needed; this, of course, places an additional planning and coding burden on the programmer. For several reasons, it would be advantageous to rid the programmer of this function by providing him with a ``virtual'' memory larger than his program. An approach that permits him to use a sufficiently large address range can accomplish this objective, assuming that means are provided for automatic execution of the memory-overlay functions.},
  file = {/Users/jonathanrainer/Zotero/storage/YP53KECD/Belady - 1966 - A study of replacement algorithms for a virtual-st.pdf;/Users/jonathanrainer/Zotero/storage/EGMDU6VS/5388441.html},
  journal = {IBM Systems Journal},
  number = {2}
}

@article{belaynehDiscussionNonblockingLockupfree1996,
  title = {A Discussion on Non-Blocking/Lockup-Free Caches},
  author = {Belayneh, Samson and Kaeli, David R.},
  year = {1996},
  month = jun,
  volume = {24},
  pages = {18--25},
  issn = {01635964},
  doi = {10.1145/381718.381727},
  file = {/Users/jonathanrainer/Zotero/storage/ITV6N3ED/Belayneh and Kaeli - 1996 - A discussion on non-blockinglockup-free caches.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {3}
}

@inproceedings{bieniaPARSECBenchmarkSuite2008,
  title = {The {{PARSEC}} Benchmark Suite: Characterization and Architectural Implications},
  shorttitle = {The {{PARSEC}} Benchmark Suite},
  booktitle = {Proceedings of the 17th International Conference on {{Parallel}} Architectures and Compilation Techniques - {{PACT}} '08},
  author = {Bienia, Christian and Kumar, Sanjeev and Singh, Jaswinder Pal and Li, Kai},
  year = {2008},
  pages = {72},
  publisher = {{ACM Press}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/1454115.1454128},
  abstract = {This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on highperformance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic. The benchmark suite has been made available to the public.},
  file = {/Users/jonathanrainer/Zotero/storage/77XP977J/Bienia et al. - 2008 - The PARSEC benchmark suite characterization and a.pdf},
  isbn = {978-1-60558-282-5},
  language = {en}
}

@article{bodinSkewedAssociativityImproves1997,
  title = {Skewed Associativity Improves Program Performance and Enhances Predictability},
  author = {Bodin, F. and Seznec, A.},
  year = {1997},
  month = may,
  volume = {46},
  pages = {530--544},
  issn = {2326-3814},
  doi = {10.1109/12.589219},
  abstract = {Performance tuning becomes harder as computer technology advances. One of the factors is the increasing complexity of memory hierarchies. Most modern machines now use at least one level of cache memory. To reduce execution stalls, cache misses must be very low. Software techniques used to improve locality have been developed for numerical codes, such as loop blocking and copying. Unfortunately, the behavior of direct mapped and set associative caches is still erratic when large data arrays are accessed. Execution time can vary drastically for the same loop kernel depending on uncontrolled factors such as array leading size. The only software method available to improve execution time stability is the copying of frequently used data, which is costly in execution time. Users are not usually cache organization experts. They are not aware of such phenomena and have no control over it. In this paper, we show that the recently proposed four-way skewed associative cache yields very stable execution times and good average miss ratios on blocked algorithms. As a result, execution time is faster and much more predictable than with conventional caches. It is therefore possible to use larger block sizes in blocked algorithms, which will further reduce blocking overhead costs.},
  file = {/Users/jonathanrainer/Zotero/storage/NNTJXFDX/Bodin and Seznec - 1997 - Skewed associativity improves program performance .pdf;/Users/jonathanrainer/Zotero/storage/YWB9VUS9/589219.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Application software,average miss ratios,blocked algorithms,cache memory,Cache memory,cache storage,content-addressable storage,Costs,Degradation,four-way skewed associative cache,Kernel,loop blocking,performance evaluation,performance tuning,program performance,skewed associativity,Stability},
  number = {5}
}

@inproceedings{bournoutianMissReductionEmbedded2008,
  title = {Miss Reduction in Embedded Processors through Dynamic, Power-Friendly Cache Design},
  booktitle = {2008 45th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Bournoutian, Garo and Orailoglu, Alex},
  year = {2008},
  month = jun,
  pages = {304--309},
  issn = {0738-100X},
  doi = {10.1145/1391469.1391546},
  abstract = {Today, embedded processors are expected to be able to run complex, algorithm-heavy applications that were originally designed and coded for general-purpose processors. As a result, traditional methods for addressing performance and determinism become inadequate. This paper explores a new data cache design for use in modern high-performance embedded processors that will dynamically improve execution time, power efficiency, and determinism within the system. The simulation results show significant improvement in cache miss ratios and reduction in power consumption of approximately 30\% and 15\%, respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/IIZFFH64/Bournoutian and Orailoglu - 2008 - Miss reduction in embedded processors through dyna.pdf;/Users/jonathanrainer/Zotero/storage/ADXB94DH/4555828.html},
  keywords = {Algorithm design and analysis,cache miss ratios,cache storage,Cellular phones,Computational modeling,data cache,dynamic associativity,Embedded computing,embedded processors,Embedded system,embedded systems,Energy consumption,execution time,Hardware,high-performance embedded processors,memory architecture,microprocessor chips,multi-core,Permission,Pervasive computing,power consumption reduction,power efficiency,power-friendly data cache design,Video codecs}
}

@inproceedings{calderPredictiveSequentialAssociative1996,
  title = {Predictive Sequential Associative Cache},
  booktitle = {Proceedings. {{Second International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Calder, B. and Grunwald, D. and Emer, J.},
  year = {1996},
  month = feb,
  pages = {244--253},
  issn = {null},
  doi = {10.1109/HPCA.1996.501190},
  abstract = {In this paper we propose a cache design that provides the same miss rate as a two-way set associative cache, but with an access time closer to a direct-mapped cache. As with other designs, a traditional direct-mapped cache is conceptually partitioned into multiple banks, and the blocks in each set are probed, or examined, sequentially. Other designs either probe the set in a fixed order or add extra delay in the access path for all accesses. We use prediction sources to guide the cache examination, reducing the amount of searching and thus the average access latency. A variety of accurate prediction sources are considered, with some being available in early pipeline stages. We feel that our design offers the same or better performance and is easier to implement than previous designs.},
  file = {/Users/jonathanrainer/Zotero/storage/WGLXTEDR/Calder et al. - 1996 - Predictive sequential associative cache.pdf;/Users/jonathanrainer/Zotero/storage/QB9G3X3I/501190.html},
  keywords = {access latency,access time,Added delay,Computer science,content-addressable storage,Delay effects,direct-mapped cache,memory architecture,miss rate,Pipelines,prediction sources,predictive sequential associative cache,Probes,storage management,Timing}
}

@inproceedings{callahanSoftwarePrefetching1991,
  title = {Software Prefetching},
  author = {Callahan, David and Kennedy, Ken and Porterfield, Allan},
  year = {1991},
  pages = {40--52},
  publisher = {{ACM Press}},
  doi = {10.1145/106972.106979},
  file = {/Users/jonathanrainer/Zotero/storage/NVFGGAGB/Callahan et al. - 1991 - Software prefetching.pdf},
  isbn = {978-0-89791-380-5},
  language = {en}
}

@misc{caoStudyIntegratedPrefetching1995,
  title = {A Study of Integrated Prefetching and Caching Strategies},
  author = {Cao, Pei and Felten, Edward W. and Karlin, Anna R. and Li, Kai},
  year = {1995},
  month = may,
  publisher = {{Association for Computing Machinery}},
  abstract = {Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetching and caching must satisfy, and then presents and studies two such integrated strategies, called aggressive and conservative. We prove that the performance of the conservative approach is within a factor of two of optimal and that the performance of the aggressive strategy is a factor significantly less than twice that of the optimal case. We have evaluated these two approaches by trace-driven simulation with a collection of file access traces. Our results show that the two integrated prefetching and caching strategies are indeed close to optimal and that these strategies can reduce the running time of applications by up to 50\%.},
  file = {/Users/jonathanrainer/Zotero/storage/WWVRMBF4/Cao et al. - 1995 - A study of integrated prefetching and caching stra.pdf}
}

@inproceedings{carterImpulseBuildingSmarter1999,
  title = {Impulse: Building a Smarter Memory Controller},
  shorttitle = {Impulse},
  booktitle = {Proceedings {{Fifth International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Carter, J. and Hsieh, W. and Stoller, L. and Swanson, M. and {Lixin Zhang} and Brunvand, E. and Davis, A. and {Chen-Chi Kuo} and Kuramkote, R. and Parker, M. and Schaelicke, L. and Tateyama, T.},
  year = {1999},
  month = jan,
  pages = {70--79},
  doi = {10.1109/HPCA.1999.744334},
  abstract = {Impulse is a new memory system architecture that adds two important features to a traditional memory controller. First, Impulse supports application-specific optimizations through configurable physical address remapping. By remapping physical addresses, applications control how their data is accessed and cached, improving their cache and bus utilization. Second, Impulse supports prefetching at the memory controller, which can hide much of the latency of DRAM accesses. In this paper we describe the design of the Impulse architecture, and show how an Impulse memory system can be used to improve the performance of memory-bound programs. For the NAS conjugate gradient benchmark, Impulse improves performance by 67\%. Because it requires no modification to processor, cache, or bus designs, Impulse can be adopted in conventional systems. In addition to scientific applications, we expect that Impulse will benefit regularly strided memory-bound applications of commercial importance, such as database and multimedia programs.},
  file = {/Users/jonathanrainer/Zotero/storage/YBBYXM6M/Carter et al. - 1999 - Impulse building a smarter memory controller.pdf;/Users/jonathanrainer/Zotero/storage/8HLH9ZUF/744334.html},
  keywords = {application-specific optimization,Bandwidth,bus design,cache design,cache storage,Cities and towns,Computer science,configurable physical address remapping,conjugate gradient methods,data access,data caching,database management systems,database programs,Databases,Delay,DRAM access latency hiding,Electronic switching systems,Impulse memory system architecture,memory architecture,memory controller,memory-bound program performance,Microprocessors,multimedia computing,multimedia programs,NAS conjugate gradient benchmark,performance,prefetching,Prefetching,processor design,Random access memory,scientific applications,Sparse matrices}
}

@inproceedings{chamskiTracebasedRuntimeAnalysis2010a,
  title = {Trace-Based Runtime Analysis of Embedded Real-Time Systems},
  booktitle = {Proceedings of the 17th {{International Conference Mixed Design}} of {{Integrated Circuits}} and {{Systems}} - {{MIXDES}} 2010},
  author = {Chamski, Z. and Borz{\k{e}}cki, M. and {\'S}wiercz, B.},
  year = {2010},
  month = jun,
  pages = {117--120},
  abstract = {Execution tracing is one of the key techniques for analyzing and validating the operation of embedded products. After reviewing several approaches to the runtime behavior analysis of embedded systems, we present the experience gained in developing a range of high-bandwidth communications devices combining multiple wireless and wired link technologies. In particular, all cases studies are based on actual product development.},
  file = {/Users/jonathanrainer/Zotero/storage/9B7DX9RL/Chamski et al. - 2010 - Trace-based runtime analysis of embedded real-time.pdf;/Users/jonathanrainer/Zotero/storage/RBTRBMBM/5551285.html},
  keywords = {code optimization,embedded devices,embedded product,embedded real time system,embedded systems,execution tracing,Hardware,high bandwidth communication device,Kernel,Linux,monitoring,Monitoring,multiple wireless technology,Probes,product development,profiling,Program processors,real time,system monitoring,systems analysis,telecommunication links,trace based runtime analysis,tracing,wired link technology}
}

@inproceedings{changAdaptiveBufferCache2016,
  title = {An {{Adaptive Buffer Cache Management Scheme}}},
  booktitle = {2016 {{International Computer Symposium}} ({{ICS}})},
  author = {Chang, H. and Chiang, C. and Yu, Y.},
  year = {2016},
  month = dec,
  pages = {124--127},
  doi = {10.1109/ICS.2016.0033},
  abstract = {Previous cache replacement algorithms utilize the access history information to make replacement decisions. However, they fail to deliver utmost performance since the history information exploited is incomplete. Motivated by the limitations of existing algorithms, this paper proposes a novel replacement scheme, called the Pattern-assisted Adaptive Recency Caching (PARC). PARC simultaneously utilizes the history information of recency, frequency, and access patterns to estimate the locality strength and to select the victim block. Specifically, PARC exploits the reference regularities exhibited in past behaviors, including looping or sequential references, to actively and rapidly adapt the recency and frequency information of blocks so as to exactly distill blocks with long-term utility from those with only short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache replacement algorithm is similar, PARC always outperforms the least recently used (LRU) scheme and other existing cache replacement algorithms.},
  file = {/Users/jonathanrainer/Zotero/storage/4SBWTBUH/Chang et al. - 2016 - An Adaptive Buffer Cache Management Scheme.pdf;/Users/jonathanrainer/Zotero/storage/PL67HAC7/7858455.html},
  keywords = {adaptive buffer cache management scheme,buffer cache,cache replacement algorithm,cache replacement algorithms,cache storage,Classification algorithms,Disk drives,file systems,History,memory management,Optimized production technology,page cache,pattern-assisted adaptive recency caching,Prediction algorithms,Radiation detectors,random workloads,replacement algorithms,Robustness}
}

@techreport{changLRUWWWProxy1999,
  title = {The {{LRU}}*{{WWW}} Proxy Cache Document Replacement Algorithm},
  author = {Chang, Chung-yi and McGregor, Anthony James and Holmes, Geoffrey},
  year = {1999},
  abstract = {Obtaining good performance from WWW proxy caches is critically dependent on the document replacement policy used by the proxy. This paper validates the work of other authors by reproducing their studies of proxy cache document replacement algorithms. From this basis a cross-trace study is mounted. This demonstrates that the performance of most document replacement algorithms is dependent on the type of workload that they are presented with. Finally we propose a new algorithm, LRU*, that consistently performs well across all our traces.},
  file = {/Users/jonathanrainer/Zotero/storage/DZRX9ZZZ/uow-cs-wp-1999-09.pdf},
  keywords = {caching,document replacement algorithm,Machine learning,WWW},
  language = {en},
  note = {99/09},
  type = {Working {{Paper}}}
}

@article{changPARCNovelOS2018,
  title = {{{PARC}}: {{A}} Novel {{OS}} Cache Manager},
  shorttitle = {{{PARC}}},
  author = {Chang, Hsung-Pin and Chiang, Cheng-Pang},
  year = {2018},
  volume = {48},
  pages = {2193--2222},
  issn = {1097-024X},
  doi = {10.1002/spe.2633},
  abstract = {To boost input-output performance, operating systems employ a kernel-managed caching space called the buffer cache or page cache. Given the limited size of a buffer cache, an effective cache manager is required to decide which blocks should be evicted from the cache. Previous cache managers use historical information to make replacement decisions. However, existing approaches are unable to maximize performance since they rely on limited historical information. Motivated by the limitations of existing solutions, this paper proposes a novel manager called the Pattern-assisted Adaptive Recency Caching (PARC) manager. PARC simultaneously uses the historical information of recency, frequency, and access patterns to estimate the locality strengths of blocks and, upon a cache miss, evicts the block with the least strength. Specifically, PARC exploits the reference regularities exhibited in past input-output behaviors to actively and rapidly adapt the recency and frequency information of blocks so as to precisely distinguish blocks with long- and short-term utility. Through comprehensive simulations on a variety of traces of different access patterns, we show that PARC is robust since, except for random workloads where the performance of each cache manager is similar, PARC always outperforms existing solutions.},
  copyright = {\textcopyright{} 2018 John Wiley \& Sons, Ltd.},
  file = {/Users/jonathanrainer/Zotero/storage/6AILZJZF/Chang and Chiang - 2018 - PARC A novel OS cache manager.pdf;/Users/jonathanrainer/Zotero/storage/7ZJDPIJI/spe.html},
  journal = {Software: Practice and Experience},
  keywords = {buffer cache,cache manager,operating systems,page cache,replacement algorithms},
  language = {en},
  number = {12}
}

@inproceedings{changReevaluatingLatencyClaims2013,
  title = {Reevaluating the Latency Claims of {{3D}} Stacked Memories},
  booktitle = {2013 18th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP}}-{{DAC}})},
  author = {Chang, D. W. and Byun, G. and Kim, H. and Ahn, M. and Ryu, S. and Kim, N. S. and Schulte, M.},
  year = {2013},
  month = jan,
  pages = {657--662},
  doi = {10.1109/ASPDAC.2013.6509675},
  abstract = {In recent years, 3D technology has been a popular area of study that has allowed researchers to explore a number of novel computer architectures. One of the more popular topics is that of integrating 3D main memory dies below the computing die and connecting them with through-silicon vias (TSVs). This is assumed to reduce off-chip main memory access latencies by roughly 45\% to 60\%. Our detailed circuit-level models, however, demonstrate that this latency reduction from the TSVs is significantly less. In this paper, we present these models, compare 2D and 3D main memory latencies, and show that the reduction in latency from using 3D main memory to be no more than 2.4 ns. We also show that although the wider I/O bus width enabled by using TSVs increases performance, it may do so with an increase in power consumption. Although TSVs consume less power per bit transfer than off-chip metal interconnects (11.2 times less power per bit transfer), TSVs typically use considerably more bits and may result in a net increase in power due to the large number of bits in the memory I/O bus. Our analysis shows that although a 3D memory hierarchy exploiting a wider memory bus can increase performance, this performance increase may not justify the net increase in power consumption.},
  file = {/Users/jonathanrainer/Zotero/storage/2F9MCVTP/Chang et al. - 2013 - Reevaluating the latency claims of 3D stacked memo.pdf;/Users/jonathanrainer/Zotero/storage/3VDJGLZ8/6509675.html},
  keywords = {3D stacked memories,Benchmark testing,computer architectures,DRAM chips,Integrated circuit interconnections,latency claims,Memory management,power consumption,Random access memory,Solid modeling,Three-dimensional displays,three-dimensional integrated circuits,through-silicon vias,Through-silicon vias}
}

@inproceedings{chaudhuriPseudoLIFOFoundationNew2009,
  title = {Pseudo-{{LIFO}}: The Foundation of a New Family of Replacement Policies for Last-Level Caches},
  shorttitle = {Pseudo-{{LIFO}}},
  author = {Chaudhuri, Mainak},
  year = {2009},
  pages = {401},
  publisher = {{ACM Press}},
  doi = {10.1145/1669112.1669164},
  abstract = {Cache blocks often exhibit a small number of uses during their life time in the last-level cache. Past research has exploited this property in two different ways. First, replacement policies have been designed to evict dead blocks early and retain the potentially live blocks. Second, dynamic insertion policies attempt to victimize single-use blocks (dead on fill) as early as possible, thereby leaving most of the working set undisturbed in the cache. However, we observe that as the last-level cache grows in capacity and associativity, the traditional dead block prediction-based replacement policy loses effectiveness because often the LRU block itself is dead leading to an LRU replacement decision. The benefit of dynamic insertion policies is also small in a large class of applications that exhibit a significant number of cache blocks with small, yet more than one, uses.},
  file = {/Users/jonathanrainer/Zotero/storage/3TI5CXEM/Chaudhuri - 2009 - Pseudo-LIFO the foundation of a new family of rep.pdf},
  isbn = {978-1-60558-798-1},
  language = {en}
}

@inproceedings{chegeniDesignHighSpeed2007,
  title = {Design of a {{High Speed}}, {{Low Latency}} and {{Low Power Consumption DRAM Using}} Two-Transistor {{Cell Structure}}},
  booktitle = {2007 14th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}}},
  author = {Chegeni, A. and Hadidi, K. and Khoei, A.},
  year = {2007},
  month = dec,
  pages = {1167--1170},
  doi = {10.1109/ICECS.2007.4511203},
  abstract = {This paper presents a new structure of DRAM, using two-transistor cell. The most important advantages of this structure are: a) High speed read, write and refresh operation b) low data access latency c) low power consumption compared to other structures d) each write/refresh operation can be carried out just in one cycle and e) no need to special process and compatible with standard digital process.},
  file = {/Users/jonathanrainer/Zotero/storage/7EYIFN9L/Chegeni et al. - 2007 - Design of a High Speed, Low Latency and Low Power .pdf;/Users/jonathanrainer/Zotero/storage/DZNH26XH/4511203.html},
  keywords = {Capacitance,Circuits,data access latency,Delay,Distributed power generation,DRAM,DRAM chips,Energy consumption,Laboratories,Microelectronics,Microprocessors,power consumption,Random access memory,two-transistor cell structure,Voltage,write-refresh operation}
}

@article{chenEffectiveHardwarebasedData1995,
  title = {Effective Hardware-Based Data Prefetching for High-Performance Processors},
  author = {Chen, Tien-Fu and {Jean-Loup Baer}},
  year = {1995},
  month = may,
  volume = {44},
  pages = {609--623},
  issn = {2326-3814},
  doi = {10.1109/12.381947},
  abstract = {Memory latency and bandwidth are progressing at a much slower pace than processor performance. In this paper, we describe and evaluate the performance of three variations of a hardware function unit whose goal is to assist a data cache in prefetching data accesses so that memory latency is hidden as often as possible. The basic idea of the prefetching scheme is to keep track of data access patterns in a reference prediction table (RPT) organized as an instruction cache. The three designs differ mostly on the timing of the prefetching. In the simplest scheme (basic), prefetches can be generated one iteration ahead of actual use. The lookahead variation takes advantage of a lookahead program counter that ideally stays one memory latency time ahead of the real program counter and that is used as the control mechanism to generate the prefetches. Finally the correlated scheme uses a more sophisticated design to detect patterns across loop levels. These designs are evaluated by simulating the ten SPEC benchmarks on a cycle-by-cycle basis. The results show that 1) the three hardware prefetching schemes all yield significant reductions in the data access penalty when compared with regular caches, 2) the benefits are greater when the hardware assist augments small on-chip caches, and 3) the lookahead scheme is the preferred one cost-performance wise.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/LM4Q6ZZW/Tien-Fu Chen and Jean-Loup Baer - 1995 - Effective hardware-based data prefetching for high.pdf;/Users/jonathanrainer/Zotero/storage/77GV4N64/381947.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Bandwidth,Bridges,cache storage,Coherence,Computer science,Counting circuits,data access patterns,data cache,Delay,fault tolerant computing,Hardware,hardware function unit,hardware prefetching schemes,hardware-based data prefetching,high-performance processors,instruction cache,lookahead program counter,lookahead scheme,memory latency,performance evaluation,Predictive models,Prefetching,prefetching data accesses,reference prediction table,SPEC benchmarks,Timing},
  number = {5}
}

@inproceedings{chenMALRUMisspenaltyAware2017,
  title = {{{MALRU}}: {{Miss}}-Penalty Aware {{LRU}}-Based Cache Replacement for Hybrid Memory Systems},
  shorttitle = {{{MALRU}}},
  booktitle = {Design, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}}), 2017},
  author = {Chen, D. and Jin, H. and Liao, X. and Liu, H. and Guo, R. and Liu, D.},
  year = {2017},
  month = mar,
  pages = {1086--1091},
  doi = {10.23919/DATE.2017.7927151},
  abstract = {Current DRAM based memory systems face the scalability challenges in terms of storage density, power, and cost. Hybrid memory architecture composed of emerging Non-Volatile Memory (NVM) and DRAM is a promising approach to large-capacity and energy-efficient main memory. However, hybrid memory systems pose a new challenge to on-chip cache management due to the asymmetrical penalty of memory access to DRAM and NVM in case of cache misses. Cache hit rate is no longer an effective metric for evaluating memory access performance in hybrid memory systems. Current cache replacement policies that aim to improve cache hit rate are not efficient either. In this paper, we take into account the asymmetry of cache miss penalty on DRAM and NVM, and advocate a more general metric, Average Memory Access Time (AMAT), to evaluate the performance of hybrid memories. We propose a miss penalty-aware LRU-based (MALRU) cache replacement policy for hybrid memory systems. MALRU is aware of the source (DRAM or NVM) of missing blocks and prevents high-latency NVM blocks as well as low-latency DRAM blocks with good temporal locality from being evicted. Experimental results show that MALRU improves system performance against LRU and the state-of-the-art HAP policy by up to 20.4\% and 11.7\% (11.1\% and 5.7\% on average), respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/LBGSBKZV/Chen et al. - 2017 - MALRU Miss-penalty aware LRU-based cache replacem.pdf;/Users/jonathanrainer/Zotero/storage/VVAR3IGB/7927151.html},
  keywords = {AMAT metric,average memory access time metric,Benchmark testing,cache miss penalty asymmetry,cache storage,DRAM based memory systems,DRAM chips,hybrid memory architecture,hybrid memory systems,Hybrid power systems,large-capacity energy-efficient main-memory,Layout,MALRU cache replacement policy,Mathematical model,Measurement,memory architecture,miss-penalty aware LRU-based cache replacement,nonvolatile memory,Nonvolatile memory,NVM,on-chip cache management,performance evaluation,Random access memory,temporal locality}
}

@inproceedings{chenReducingMemoryLatency1992,
  title = {Reducing Memory Latency via Non-Blocking and Prefetching Caches},
  booktitle = {Proceedings of the Fifth International Conference on {{Architectural}} Support for Programming Languages and Operating Systems},
  author = {Chen, Tien-Fu and Baer, Jean-Loup},
  year = {1992},
  month = sep,
  pages = {51--61},
  publisher = {{Association for Computing Machinery}},
  address = {{Boston, Massachusetts, USA}},
  doi = {10.1145/143365.143486},
  file = {/Users/jonathanrainer/Zotero/storage/N38PNDNI/Chen and Baer - 1992 - Reducing memory latency via non-blocking and prefe.pdf},
  isbn = {978-0-89791-534-2},
  series = {{{ASPLOS V}}}
}

@inproceedings{chenSSARCShortSightedAdaptive2009,
  title = {{{SSARC}}: {{The Short}}-{{Sighted Adaptive Replacement Cache}}},
  shorttitle = {{{SSARC}}},
  booktitle = {2009 11th {{IEEE International Conference}} on {{High Performance Computing}} and {{Communications}}},
  author = {Chen, Z. and Xiao, N. and Liu, F. and Zhao, Y.},
  year = {2009},
  month = jun,
  pages = {551--556},
  doi = {10.1109/HPCC.2009.82},
  abstract = {As the performance gap between disks and processors continues to increase, dozens of cache replacement policies come up to handle the problem. Unfortunately, most of the policies are static. Nimrod Megiddo etc put forward a low overhead adaptive policy called ARC. It outperforms most of the static policies in most situations. But, ARC adapts itself to the workloads by the feedback of the missed pages. It hasn 't carried out the adaption before missed pages are discovered. We propose a high performance adaptive replacement policy. It adapts itself to the workloads by the feedback of the hit pages, so, it is more sensitive to the changes of the workloads than ARC. As the policy stares at the tails of the queues regardless of other pages, we name the policy as short-sighted adaptive replacement policy. The ARC usually regrets for the missed pages and wishes to rescue the neighborhood of them. However, SSARC endeavors to protect the would-be-reused pages from being replaced aggressively. So, it outperforms ARC in most situations. We compared SSARC with LRU, 2Q and ARC. The trace-driven experiments represent that SSARC gains higher performance.},
  file = {/Users/jonathanrainer/Zotero/storage/NVRT2KCP/Chen et al. - 2009 - SSARC The Short-Sighted Adaptive Replacement Cach.pdf;/Users/jonathanrainer/Zotero/storage/9ZX3IEXM/5167043.html},
  keywords = {adaptive,Cache,cache replacement policies,cache storage,Computer science,Delay,Feedback,High performance computing,low overhead adaptive policy,Performance gain,Protection,short-sighted adaptive replacement cache,short-sighted adaptive replacement policy,SSARC,storge,Tail}
}

@inproceedings{chenxizhangMulticolumnImplementationsCache1997,
  title = {Multi-Column Implementations for Cache Associativity},
  booktitle = {Proceedings {{International Conference}} on {{Computer Design VLSI}} in {{Computers}} and {{Processors}}},
  author = {Chenxi Zhang and Xiaodong Zhang and Yong Yan},
  year = {1997},
  month = oct,
  pages = {504--509},
  issn = {1063-6404},
  doi = {10.1109/ICCD.1997.628915},
  abstract = {We propose two schemes for implementing higher associativity: the sequential multi-column cache, which is an extension of the column associative cache, and the parallel multi-column cache. In order to achieve the same access cycle time as that of a direct-mapped cache, data memory in the cache is organized into one bank in both schemes. We use the multiple MRU block technique to increase the first hit ratio, thus reducing the average access time. While the parallel multi-column cache performs the tag checking in parallel, the sequential multi-column cache sequentially searches through places in a set, and uses index information to filter out unnecessary probes. In the case of an associativity of 4, they both achieve the low miss rate of a 4-way set-associative cache. Our simulation results using ATUM traces show that both schemes can effectively reduce the average access time.},
  file = {/Users/jonathanrainer/Zotero/storage/HSABFC3W/Chenxi Zhang et al. - 1997 - Multi-column implementations for cache associativi.pdf;/Users/jonathanrainer/Zotero/storage/ZCJRQ8PT/628915.html},
  keywords = {4-way set-associative cache,access cycle time,ATUM traces,average access time,cache associativity,cache storage,Clocks,column associative cache,Costs,data memory,DRAM access time,DRAM chips,Educational institutions,Filters,first hit ratio,Interference,Logic,memory architecture,multi-column implementations,multiple MRU block,parallel multi-column cache,performance evaluation,Random access memory,reduced instruction set computing,Reduced instruction set computing,RISC,sequential multi-column cache,simulation results,tag checking,Very large scale integration,VLSI technology,Whales}
}

@article{chenxizhangTwoFastHighassociativity1997,
  title = {Two Fast and High-Associativity Cache Schemes},
  author = {Chenxi Zhang and Xiaodong Zhang and Yong Yan},
  year = {1997},
  month = sep,
  volume = {17},
  pages = {40--49},
  issn = {1937-4143},
  doi = {10.1109/40.621212},
  abstract = {In the race to improve cache performance, many researchers have proposed schemes that increase a cache's associativity. The associativity of a cache is the number of places in the cache where a block may reside. In a direct-mapped cache, which has an associativity of 1, there is only one location to search for a match for each reference. In a cache with associativity n-an n-way set-associative cache-there are n locations. Increasing associativity reduces the miss rate by decreasing the number of conflict, or interference, references. The column-associative cache and the predictive sequential associative cache seem to have achieved near-optimal performance for an associativity of two. Increasing associativity beyond two, therefore, is one of the most important ways to further improve cache performance. We propose two schemes for implementing associativity greater than two: the sequential multicolumn cache, which is an extension of the column-associative cache, and the parallel multicolumn cache. For an associativity of four, they achieve the low miss rate of a four-way set-associative cache. Our simulation results show that both schemes can effectively reduce the average access time.},
  file = {/Users/jonathanrainer/Zotero/storage/M5VQP3DS/Chenxi Zhang et al. - 1997 - Two fast and high-associativity cache schemes.pdf;/Users/jonathanrainer/Zotero/storage/8GLX4A38/621212.html},
  journal = {IEEE Micro},
  keywords = {access time,associativity,cache performance,cache schemes,cache storage,column-associative cache,content-addressable storage,Educational institutions,high-associativity,low miss rate,memory architecture,Multiplexing,parallel multicolumn cache,performance evaluation,sequential multicolumn cache,Sliding mode control},
  number = {5}
}

@article{chhedaMemorySystemsOverview,
  title = {Memory {{Systems}}: {{Overview}} and {{Trends}}},
  author = {Chheda, Saurabh and Chittamuru, Jeevan Kumar and Moritz, Csaba Andras},
  pages = {12},
  abstract = {Computer pioneers have correctly predicted that programmers would want unlimited amounts of memory. An economical solution to this desire is the implementation of a Memory Hierarchical System, which takes advantage of locality and cost/performance of memory technologies. As time has gone by, the technology has progressed, bringing about various changes in the way memory systems are built. Memory systems must be flexible enough to accommodate various levels of memory hierarchies, and must be able to emulate an environment with unlimited amount of memory. For more than two decades the main emphasis of memory system designers has been achieving high performance. However, recent market trends and application requirements suggest that other design goals such as low-power, predictability, and flexibility/reconfigurability are becoming equally important to consider. This paper gives a comprehensive overview of memory systems with the objective to give any reader a broad overview. Emphasis is put on the various components of a typical memory system of present-day systems and emerging new memory system architecture trends. We focus on emerging memory technologies, system architectures, compiler technology, which are likely to shape the computer industry in the future.},
  file = {/Users/jonathanrainer/Zotero/storage/NISUVJUW/Chheda et al. - Memory Systems Overview and Trends.pdf},
  language = {en}
}

@inproceedings{chienLowLatencyMemory2007,
  title = {A {{Low Latency Memory Controller}} for {{Video Coding Systems}}},
  booktitle = {2007 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Chien, C. and Wang, C. and Lin, C. and Hsieh, T. and Chu, Y. and Guo, J.},
  year = {2007},
  month = jul,
  pages = {1211--1214},
  doi = {10.1109/ICME.2007.4284874},
  abstract = {The dynamic memory controller plays an important role in system-on-a-chip (SoC) designs to provide enough memory bandwidth through external memory for DSP and multimedia processing. However, the overhead cycles in accessing the data located in external memory have much influence on the SoC performance. In this paper, we propose a low latency memory controller with AHB interface to reduce the overhead cycles for the SDR memory access in the SoC designs. Through the pre-calculated addresses of impending transfers, two memory control schemes, i.e. Burst terminates Burst (BTB) and Anticipative Row Activation (ARA), are used to reduce the latency of SDR memory access. The experimental results show that the proposed memory controller reduces the memory bandwidth by 33\% in a typical MPEG-4 video decoding system.},
  file = {/Users/jonathanrainer/Zotero/storage/LFPHEKY6/Chien et al. - 2007 - A Low Latency Memory Controller for Video Coding S.pdf;/Users/jonathanrainer/Zotero/storage/3AHHJWJJ/4284874.html},
  keywords = {anticipative row activation,Bandwidth,burst terminates burst,Control systems,Costs,Decoding,Delay,DRAM chips,dynamic memory controller,low latency memory controller,MPEG 4 Standard,MPEG-4 video decoding system,Multimedia systems,Scheduling,SoC design,SRAM chips,System-on-a-chip,system-on-chip,video coding,Video coding,video coding systems}
}

@inproceedings{choiMultipleCloneRow2015,
  title = {Multiple {{Clone Row DRAM}}: {{A}} Low Latency and Area Optimized {{DRAM}}},
  shorttitle = {Multiple {{Clone Row DRAM}}},
  booktitle = {2015 {{ACM}}/{{IEEE}} 42nd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Choi, J. and Shin, W. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, L.},
  year = {2015},
  month = jun,
  pages = {223--234},
  doi = {10.1145/2749469.2750402},
  abstract = {Several previous works have changed DRAM bank structure to reduce memory access latency and have shown performance improvement. However, changes in the area-optimized DRAM bank can incur large area-overhead. To solve this problem, we propose Multiple Clone Row DRAM (MCR-DRAM), which uses existing DRAM bank structure without any modification.},
  file = {/Users/jonathanrainer/Zotero/storage/ATDZYEJF/Choi et al. - 2015 - Multiple Clone Row DRAM A low latency and area op.pdf;/Users/jonathanrainer/Zotero/storage/DEIKM2ER/7284068.html},
  keywords = {area optimized DRAM,bank structure,DRAM chips,low latency DRAM,MCR-DRAM,memory access latency,multiple clone row DRAM}
}

@article{chooDIGDegreeInterreference2006,
  title = {{{DIG}}: {{Degree}} of Inter-Reference Gap for a Dynamic Buffer Cache Management},
  shorttitle = {{{DIG}}},
  author = {Choo, Hyunseung and Lee, Young Jae and Yoo, Seong-Moo},
  year = {2006},
  month = apr,
  volume = {176},
  pages = {1032--1044},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2005.01.018},
  abstract = {The effectiveness of the buffer cache replacement is critical to the performance of I/O systems. In this paper, we propose a degree of inter-reference gap (DIG) based block replacement scheme. This scheme keeps the simplicity of the least recently used (LRU) scheme and does not depend on the detection of access regularities. The proposed scheme is based on the low inter-reference recency set (LIRS) scheme, which is currently known to be very effective. However, the proposed scheme employs several history information items whereas the LIRS scheme uses only one history information item. The overhead of the proposed scheme is almost negligible. To evaluate the performance of the proposed scheme, the comprehensive trace-driven computer simulation is used in general access patterns. Our simulation results show that the cache hit ratio (CHR) in the proposed scheme is improved as much as 65.3\% (with an average of 26.6\%) compared to the LRU for the same workloads, and up to 6\% compared to the LIRS in multi3 trace.},
  file = {/Users/jonathanrainer/Zotero/storage/TP762LZU/Choo et al. - 2006 - DIG Degree of inter-reference gap for a dynamic b.pdf;/Users/jonathanrainer/Zotero/storage/6VZZ28PM/S0020025505000551.html},
  journal = {Information Sciences},
  keywords = {Buffer cache management,Cache hit ratio,Degree of inter-reference gap,Least recently used,Low inter-reference recency set},
  number = {8}
}

@inproceedings{collinsDynamicSpeculativePrecomputation2001,
  title = {Dynamic Speculative Precomputation},
  booktitle = {Proceedings. 34th {{ACM}}/{{IEEE International Symposium}} on {{Microarchitecture}}. {{MICRO}}-34},
  author = {Collins, J.D. and Tullsen, D.M. and Hong Wang and Shen, J.P.},
  year = {2001},
  month = dec,
  pages = {306--317},
  issn = {1072-4451},
  doi = {10.1109/MICRO.2001.991128},
  abstract = {A large number of memory accesses in memory-bound applications are irregular, such as pointer dereferences, and can be effectively targeted by thread-based prefetching techniques like Speculative Precomputation. These techniques execute instructions, for example on an available SMT thread context, that have been extracted directly from the program they are trying to accelerate. Proposed techniques typically require manual user intervention to extract and optimize instruction sequences. This paper proposes Dynamic Speculative Precomputation, which performs all necessary instruction analysis, extraction, and optimization through the use of back-end instruction analysis hardware, located off the processor's critical path. For a set of memory limited benchmarks an average speedup of 14\% is achieved when constructing simple p-slices, and this gain grows to 33\% when making use of aggressive optimizations.},
  file = {/Users/jonathanrainer/Zotero/storage/GVXTNDWW/Collins et al. - 2001 - Dynamic speculative precomputation.pdf;/Users/jonathanrainer/Zotero/storage/M2EA5LQV/991128.html},
  keywords = {Acceleration,Application software,backend instruction analysis,Computer science,Delay,dynamic speculative precomputation,Hardware,instruction sequences,instruction sets,Manuals,memory accesses,memory limited benchmarks,Microprocessors,multi-threading,p-slices,pointer dereferences,Prefetching,SMT thread context,storage management,Surface-mount technology,thread-based prefetching,Yarn}
}

@inproceedings{collinsPointerCacheAssisted2002,
  title = {Pointer Cache Assisted Prefetching},
  author = {Collins, J. and Sair, S. and Calder, B. and Tullsen, D.M.},
  year = {2002},
  pages = {62--73},
  publisher = {{IEEE Comput. Soc}},
  doi = {10.1109/MICRO.2002.1176239},
  abstract = {Data prefetching effectively reduces the negative effects of long load latencies on the performance of modern processors. Hardware prefetchers employ hardware structures to predict future memory addresses based on previous patterns. Thread-based prefetchers use portions of the actual program code to determine future load addresses for prefetching.},
  file = {/Users/jonathanrainer/Zotero/storage/RPPLFBIR/Collins et al. - 2002 - Pointer cache assisted prefetching.pdf},
  isbn = {978-0-7695-1859-6},
  language = {en}
}

@article{cookseyStatelessContentdirectedData2002a,
  title = {A Stateless, Content-Directed Data Prefetching Mechanism},
  author = {Cooksey, Robert and Jourdan, Stephan and Grunwald, Dirk},
  year = {2002},
  month = oct,
  volume = {36},
  pages = {279--290},
  issn = {0163-5980},
  doi = {10.1145/635508.605427},
  abstract = {Although central processor speeds continues to improve, improvements in overall system performance are increasingly hampered by memory latency, especially for pointer-intensive applications. To counter this loss of performance, numerous data and instruction prefetch mechanisms have been proposed. Recently, several proposals have posited a memory-side prefetcher; typically, these prefetchers involve a distinct processor that executes a program slice that would effectively prefetch data needed by the primary program. Alternative designs embody large state tables that learn the miss reference behavior of the processor and attempt to prefetch likely misses.This paper proposes Content-Directed Data Prefetching, a data prefetching architecture that exploits the memory allocation used by operating systems and runtime systems to improve the performance of pointer-intensive applications constructed using modern language systems. This technique is modeled after conservative garbage collection, and prefetches "likely" virtual addresses observed in memory references. This prefetching mechanism uses the underlying data of the application, and provides an 11.3\% speedup using no additional processor state. By adding less than {$\frac{1}{2}\%$} space overhead to the second level cache, performance can be further increased to 12.6\% across a range of "real world" applications.},
  file = {/Users/jonathanrainer/Zotero/storage/UBPDUNLF/Cooksey et al. - 2002 - A stateless, content-directed data prefetching mec.pdf},
  journal = {ACM SIGOPS Operating Systems Review},
  number = {5}
}

@incollection{corbatoPagingExperimentMultics1969,
  title = {A {{Paging Experiment}} with the {{Multics System}}},
  booktitle = {In Honor of {{Philip M}}. {{Morse}}},
  author = {Corbat{\'o}, Fernando J.},
  editor = {Feshbach, Herman and Ingard, K. Uno and Morse, Philip M.},
  year = {1969},
  pages = {217--228},
  publisher = {{M.I.T. Press}},
  address = {{Cambridge}},
  file = {/Users/jonathanrainer/Zotero/storage/F7MW59JS/paging-experiment.pdf},
  isbn = {978-0-262-06028-8},
  keywords = {Morse; Philip M,Philip McCord,Physics},
  lccn = {QC71 .I43}
}

@misc{CoreSightBaseSystem18,
  title = {{{CoreSight Base System Architecture}}},
  year = {18},
  month = jul,
  publisher = {{ARM}},
  file = {/Users/jonathanrainer/Zotero/storage/JN5U2XCE/CoreSight Base System Architecture.pdf},
  language = {en}
}

@article{cuiNewHybridApproach2003,
  title = {A {{New Hybrid Approach}} to {{Exploit Localities}}: {{LRFU}} with {{Adaptive Prefetching}}},
  shorttitle = {A {{New Hybrid Approach}} to {{Exploit Localities}}},
  author = {Cui, Jike and Samadzadeh, Mansur. H.},
  year = {2003},
  month = dec,
  volume = {31},
  pages = {37--43},
  issn = {0163-5999},
  doi = {10.1145/974036.974041},
  abstract = {This paper reviewed a number of existing methods to exploit the spatial and temporal locality commonly existing in programs, and provided detailed analysis and testing of adaptive prefetching (a method designed to utilize spatial locality) and the least recently and frequently used (LRFU) method (a method designed to utilize temporal locality). The two methods were combined in this work in terms of their exploitation of locality. The comparative studies of the methods were done using real traces, and hit rate was used as an evaluation measure.Results showed that by using adaptive prefetching, the hit rate improved significantly by an average of 11.7\% over the hit rate of LRU in the traces and cache configurations used. It also showed that LRFU consistently gives higher hit rates than LRU, but not by much in the trace files and cache configurations tested. And the X value (a controllable parameter which determines the Weights given to recency and frequency) has to be in a certain range, which is usually narrow, in order to get the best performance for hit rate. Compared to adaptive prefetching and LRU, the hybrid approach of combining adaptive prefetching and LRFU gave a consistently higher hit rate also. But, affected by the performance of LRFU, the improvement in the hit rate by the combination was low.},
  file = {/Users/jonathanrainer/Zotero/storage/VVBD52LY/Cui and Samadzadeh - 2003 - A New Hybrid Approach to Exploit Localities LRFU .pdf},
  journal = {SIGMETRICS Perform. Eval. Rev.},
  number = {3}
}

@inproceedings{cuppuPerformanceComparisonContemporary1999,
  title = {A Performance Comparison of Contemporary {{DRAM}} Architectures},
  booktitle = {Proceedings of the 26th {{International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{99CB36367}})},
  author = {Cuppu, V. and Jacob, B. and Davis, B. and Mudge, T.},
  year = {1999},
  month = may,
  pages = {222--233},
  doi = {10.1109/ISCA.1999.765953},
  abstract = {In response to the growing gap between memory access time and processor speed, DRAM manufacturers have created several new DRAM architectures. This paper presents a simulation-based performance study of a representative group, each evaluated in a small system organization. These small-system organizations correspond to workstation-class computers and use on the order of IO DRAM chips. The study covers Fast Page Mode, Extended Data Out, Synchronous, Enhanced Synchronous, Synchronous Link, Rambus, and Direct Rambus designs. Our simulations reveal several things: (a) current advanced DRAM technologies are attacking the memory bandwidth problem but not the latency problem; (b) bus transmission speed will soon become a primary factor limiting memory-system performance; (c) the post-12 address stream still contains significant locality, though it varies from application to application; and (d) as we move to wider buses, row access time becomes more prominent, making it important to investigate techniques to exploit the available locality to decrease access time.},
  file = {/Users/jonathanrainer/Zotero/storage/96RK28Z9/Cuppu et al. - 1999 - A performance comparison of contemporary DRAM arch.pdf;/Users/jonathanrainer/Zotero/storage/NKYR8DCJ/765953.html},
  keywords = {Bandwidth,Capacitors,Computer architecture,contemporary DRAM architectures,Costs,Delay,digital simulation,DRAM chips,Jacobian matrices,memory access time,memory-system performance,Out of order,parallel architectures,performance comparison,performance evaluation,Pins,processor speed,Random access memory,simulation-based performance study,simulations,small-system organizations,Time measurement,workstation-class computers}
}

@inproceedings{dahlgrenFixedAdaptiveSequential1993,
  title = {Fixed and {{Adaptive Sequential Prefetching}} in {{Shared Memory Multiprocessors}}},
  booktitle = {1993 {{International Conference}} on {{Parallel Processing}} - {{ICPP}}'93},
  author = {Dahlgren, Fredrik and Dubois, Michel and Stenstrom, Per},
  year = {1993},
  month = aug,
  volume = {1},
  pages = {56--63},
  issn = {0190-3918},
  doi = {10.1109/ICPP.1993.92},
  abstract = {To offset the effect of read miss penalties on processor utilization in shared-memory multiprocessors, several software- and hardware-based data prefetching schemes have been proposed. A major advantage of hardware tech niques is that they need no support from the programmer or compiler. Sequential prefetching is a simple hardware-controlled prefetching technique which relies on the automatic prefetch of consecutive blocks following the block that misses in the cache. In its simplest form, the number of prefetched blocks on each miss is fixed throughout the exe cution. However, since the prefetching efficiency varies during the execution of a program, we propose to adapt the number of pref etched blocks according to a dynamic measure of prefetching effectiveness. Simulations of this adaptive scheme show significant reductions of the read penalty and of the overall execution time.},
  file = {/Users/jonathanrainer/Zotero/storage/J3BE4RFY/Dahlgren et al. - 1993 - Fixed and Adaptive Sequential Prefetching in Share.pdf;/Users/jonathanrainer/Zotero/storage/DDPB8TXD/4134114.html},
  keywords = {Concurrent computing,Costs,Delay,Etching,Hardware,Parallel processing,Prefetching,Program processors,Programming profession,Traffic control}
}

@phdthesis{damienStudyDifferentCache2007,
  title = {Study of {{Different Cache Line Replacement Algorithms}} in {{Embedded Systems}}},
  author = {Damien, Gille},
  year = {2007},
  address = {{Stockholm}},
  abstract = {The increasing speed gap between processors and memories underlines the criticalness of cache memories. The strong area and power consumption constraints of general purpose embedded systems limit the size of cache memories. With the development of embedded systems provided with an operative system, these constraints are even stronger. The selection of an efficient replacement policy thus appears as critical. The Least Recently Used (LRU) strategy performs well on most memory patterns but this performance is obtained at the expense of the hardware requirements and of the power consumption. Consequently, new algorithms have been developed and this work is devoted to the evaluation of their performance. The implementation of a cache simulator allowed us to carry out a detailed investigation of the behaviour of the policies, which among others demonstrated the occurrence of Belady's anomaly for a pseudo-LRU replacement algorithm, PLRUm. The replacement strategies that emerged from this study were then integrated in the ARM11 MPCore processor and their performance results were compared with the cache simulator ones. Our results show that the MRU-based pseudo-LRU replacement policy (PLRUm) approximates the LRU algorithm very closely and can even outperform it with low hardware and power consumption},
  file = {/Users/jonathanrainer/Zotero/storage/MVYMQV9B/Damien et al. - 2007 - Company Industrial supervisor.pdf;/Users/jonathanrainer/Zotero/storage/UTW98MYD/summary.html},
  school = {KTH Royal Institute of Technology},
  type = {Masters {{Thesis}}}
}

@inproceedings{dasArbitrationCacheReplacements2016,
  title = {An Arbitration on Cache Replacements Based on Frequency \textemdash{} {{Recency}} Product Values},
  booktitle = {2016 {{International Conference}} on {{VLSI Systems}}, {{Architectures}}, {{Technology}} and {{Applications}} ({{VLSI}}-{{SATA}})},
  author = {Das, S. and Banerjee, A.},
  year = {2016},
  month = jan,
  pages = {1--6},
  doi = {10.1109/VLSI-SATA.2016.7593031},
  abstract = {Evolving an efficient cache replacement policy has been a challenge since the last few decades. LRU (Least Recently Used) and LFU (Least Frequently Used) cache replacement techniques and a variety of their combinations were the most sought after. This paper proposes a new combination of the LRU and LFU in such a style that the time and complexity to replace moves below the current benchmarks. Here, a frequency-recency product value is computed which dictates the cache replacement arbitration. It out performs the existing methods by a significant reduction in computational overhead.},
  file = {/Users/jonathanrainer/Zotero/storage/62NDP6DC/Das and Banerjee - 2016 - An arbitration on cache replacements based on freq.pdf;/Users/jonathanrainer/Zotero/storage/H22QFACA/7593031.html},
  keywords = {Cache memory,cache storage,Complexity theory,frequency,Indexes,least frequently used cache replacement techniques,least recently used cache replacement techniques,LFRU,LFU,Loading,LRU,minima,Organizations,Radiation detectors,recency,replacement policy,set-associative mapping,Very large scale integration}
}

@inproceedings{dasDynamicAssociativityManagement2013,
  title = {Dynamic {{Associativity Management Using Fellow Sets}}},
  booktitle = {2013 {{International Symposium}} on {{Electronic System Design}}},
  author = {Das, Shirshendu and Kapoor, Hemangee K.},
  year = {2013},
  month = dec,
  pages = {133--137},
  issn = {null},
  doi = {10.1109/ISED.2013.33},
  abstract = {The memory accesses of to days applications are non-uniformly distributed across the cache sets and as a result some sets of the cache are heavily used while some other sets remain underutilized. This paper presents CMP-SVR, an approach to dynamically increase the associativity of heavily used sets without increasing the cache size. It divides the last level cache (LLC) into two sections: normal storage (NT) and reserve storage (RT). Some number of ways (25\% to 50\%) from each set are reserved for RT and the remaining ways belongs to NT. The sets are divided into some groups called fellow-groups and a set can use the reserve ways of its fellow sets to increase its associativity during execution. An additional tag-array (SA-TGS) for RT has been used to make the searching easier and less expensive. SA-TGS is almost like an N-way set associative cache where its associativity depends on the number of reserve ways per set and the number of sets in a fellow-group. CMP-SVR has less storage, area and energy overhead as compared to the other techniques proposed for dynamic associativity management. Full system simulation shows on average of 8\% improvement in cycles per instruction (CPI) and 28\% improvement in miss-rate.},
  file = {/Users/jonathanrainer/Zotero/storage/XHZPGI4X/Das and Kapoor - 2013 - Dynamic Associativity Management Using Fellow Sets.pdf;/Users/jonathanrainer/Zotero/storage/RUXQ7E2D/6808656.html},
  keywords = {Arrays,Benchmark testing,cache sets,cache storage,CMP,CMP-SVR,Computers,dynamic associativity management,Fellows,integrated memory circuits,last level cache,LLC,memory accesses,Microarchitecture,microprocessor chips,normal storage,NUCA,reserve storage,SA-TGS,Tiles,V-way,Victim Retention}
}

@inproceedings{dasLatencyAwareBlock2017,
  title = {Latency {{Aware Block Replacement}} for {{L1 Caches}} in {{Chip Multiprocessor}}},
  booktitle = {2017 {{IEEE Computer Society Annual Symposium}} on {{VLSI}} ({{ISVLSI}})},
  author = {Das, Shirshendu and Kapoor, Hemangee K.},
  year = {2017},
  month = jul,
  pages = {182--187},
  issn = {2159-3477},
  doi = {10.1109/ISVLSI.2017.40},
  abstract = {Performance of chip multiprocessors (CMPs) heavily depends on the efficiency of the caches. There is still a large gap between implemented replacement policies and the theoretical optimal policy. Among other factors, replacement policies play a major role in deciding the memory access time as they directly affect the miss-rate. In a CMP environment, the cost incurred by a miss in the higher level cache needs to take into account the communication latency over the on-chip network as well as power consumption.There are existing replacement policies which use this misscost rather than miss-count as an optimisation factor. These existing policies targeting the optimisation of miss-cost are mainly designed for the Last-Level Caches (LLC). Due to their significant hardware overheads, these policies are not directly applicable for the L1 caches. In this paper, we propose a new replacement policy, LA-LRU, for L1 caches which uses communication latency as a factor in deciding the victim. Experimental evaluation on full system simulation shows 7.4\% improvement in Average Memory Access Time (AMAT) which leads to 7\% improvement in Cycles Per Instruction (CPI). Reduction in on-chip communication also helps in improving the on-chip network power consumption by 14.8\%.},
  file = {/Users/jonathanrainer/Zotero/storage/G2KYKJ3X/Das and Kapoor - 2017 - Latency Aware Block Replacement for L1 Caches in C.pdf;/Users/jonathanrainer/Zotero/storage/IN5EVC53/7987516.html},
  keywords = {AMAT,average memory access time,Benchmark testing,Cache Architectures,cache efficiency,cache storage,chip multiprocessor,CMP,communication latency,CPI,cycles per instruction,Face,Hardware,L1 cache,LA-LRU replacement policy,last-level cache,latency aware block replacement,memory access time,multiprocessing systems,NUCA,Optimized production technology,Power demand,Replacement Policy,System-on-chip,Systems simulation}
}

@inproceedings{dasRandomLRUReplacementPolicy2013,
  title = {Random-{{LRU}}: {{A Replacement Policy}} for {{Chip Multiprocessors}}},
  shorttitle = {Random-{{LRU}}},
  booktitle = {{{VLSI Design}} and {{Test}}},
  author = {Das, Shirshendu and Polavarapu, Nagaraju and Halwe, Prateek D. and Kapoor, Hemangee K.},
  editor = {Gaur, Manoj Singh and Zwolinski, Mark and Laxmi, Vijay and Boolchandani, Dharmendra and Sing, Virendra and Sing, Adit D.},
  year = {2013},
  pages = {204--213},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-42024-5_25},
  abstract = {As the number of cores and associativity of the last level cache (LLC) on a Chip Multi-processor increases, the role of replacement policies becomes more vital. Though, pure least recently used (LRU) policy has some issues it has been generally believed that some versions of LRU policy performs better than the other policies. Therefore, a lot of work has been proposed to improve the performance of LRU-based policies. However, it has been shown that the true LRU imposes additional complexity and area overheads when implemented on high associative LLCs. Most of the LRU based works are more motivated towards the performance improvement than the reduction of area and hardware overhead of true LRU scheme. In this paper we proposed an LRU based cache replacement policy especially for the LLC to improve the performance of LRU as well as to reduce the area and hardware cost of pure LRU by more than a half. We use a combination of random and LRU replacement policy for each cache set. Instead of using LRU policy for the entire set we use it only for some number of ways within the set. Experiments conducted on a full-system simulator shows 36\% and 11\% improvements over miss rate and CPI respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/BVBS7NC3/Das et al. - 2013 - Random-LRU A Replacement Policy for Chip Multipro.pdf},
  isbn = {978-3-642-42024-5},
  keywords = {LRU,NUCA,Pseudo-LRU,Random-LRU,Tiled CMP},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@inproceedings{dasRandomLRUReplacementPolicy2013a,
  title = {Random-{{LRU}}: {{A Replacement Policy}} for {{Chip Multiprocessors}}},
  shorttitle = {Random-{{LRU}}},
  booktitle = {{{VLSI Design}} and {{Test}}},
  author = {Das, Shirshendu and Polavarapu, Nagaraju and Halwe, Prateek D. and Kapoor, Hemangee K.},
  editor = {Gaur, Manoj Singh and Zwolinski, Mark and Laxmi, Vijay and Boolchandani, Dharmendra and Sing, Virendra and Sing, Adit D.},
  year = {2013},
  pages = {204--213},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-42024-5_25},
  abstract = {As the number of cores and associativity of the last level cache (LLC) on a Chip Multi-processor increases, the role of replacement policies becomes more vital. Though, pure least recently used (LRU) policy has some issues it has been generally believed that some versions of LRU policy performs better than the other policies. Therefore, a lot of work has been proposed to improve the performance of LRU-based policies. However, it has been shown that the true LRU imposes additional complexity and area overheads when implemented on high associative LLCs. Most of the LRU based works are more motivated towards the performance improvement than the reduction of area and hardware overhead of true LRU scheme. In this paper we proposed an LRU based cache replacement policy especially for the LLC to improve the performance of LRU as well as to reduce the area and hardware cost of pure LRU by more than a half. We use a combination of random and LRU replacement policy for each cache set. Instead of using LRU policy for the entire set we use it only for some number of ways within the set. Experiments conducted on a full-system simulator shows 36\% and 11\% improvements over miss rate and CPI respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/ASDI8YX6/Das et al. - 2013 - Random-LRU A Replacement Policy for Chip Multipro.pdf},
  isbn = {978-3-642-42024-5},
  keywords = {LRU,NUCA,Pseudo-LRU,Random-LRU,Tiled CMP},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@inproceedings{dasRoleCacheReplacement2019,
  title = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}: {{A Survey}}},
  shorttitle = {Role of {{Cache Replacement Policies}} in {{High Performance Computing Systems}}},
  booktitle = {Communication, {{Networks}} and {{Computing}}},
  author = {Das, Purnendu},
  editor = {Verma, Shekhar and Tomar, Ranjeet Singh and Chaurasia, Brijesh Kumar and Singh, Vrijendra and Abawajy, Jemal},
  year = {2019},
  pages = {400--410},
  publisher = {{Springer Singapore}},
  abstract = {Cache replacement policies play important roles in efficiently processing the current big data applications. The performance of any high performance computing system is highly depending on the performance of its cache memory. A better replacement policy allows the important blocks to be placed nearer to the core. Hence reduces the overall execution latency and gives better computational efficiency. There are different replacement policies exits. The main difference among these policies is how to select the victim block from the cache such that it can be replaced with another newly fetched block. Non-optimal replacement policy may remove important blocks from the cache when some less important (dead) blocks also present in the cache. Proposing better replacement policy for cache memory is a major research area from last three decades. The most widely used replacement policies used for classical cache memories are Least Recently Used Policy (LRU), Random Replacement Policy or Pseudo-LRU. As the technology advances the technology of cache memory is also changing. For efficient processing of big data based applications today's computer having high performance computing ability requires larger cache memory. Such larger cache memory makes the task of replacement policies more challenging. In this paper we have done a survey about the innovations done in cache replacement policies to support the efficient processing of big data based applications.},
  file = {/Users/jonathanrainer/Zotero/storage/FD9N4ZE3/Das - 2019 - Role of Cache Replacement Policies in High Perform.pdf},
  isbn = {9789811323720},
  keywords = {Cache memory,Efficient data processing,Multicore-system,Replacement policies},
  language = {en},
  series = {Communications in {{Computer}} and {{Information Science}}}
}

@article{dasVictimRetentionReducing2014,
  title = {Victim Retention for Reducing Cache Misses in Tiled Chip Multiprocessors},
  author = {Das, Shirshendu and Kapoor, Hemangee K.},
  year = {2014},
  month = jun,
  volume = {38},
  pages = {263--275},
  issn = {01419331},
  doi = {10.1016/j.micpro.2013.11.005},
  abstract = {This paper presents CMP-VR (Chip-Multiprocessor with Victim Retention), an approach to improve cache performance by reducing the number of off-chip memory accesses. The objective of this approach is to retain the chosen victim cache blocks on the chip for the longest possible time. It may be possible that some sets of the CMPs last level cache (LLC) are heavily used, while certain others are not. In CMP-VR, some number of ways from every set are used as reserved storage. It allows a victim block from a heavily used set to be stored into the reserve space of another set. In this way the load of heavily used sets are distributed among the underused sets. This logically increases the associativity of the heavily used sets without increasing the actual associativity and size of the cache. Experimental evaluation using full-system simulation shows that CMP-VR has less off-chip miss-rate as compared to baseline Tiled CMP. Results are presented for different cache sizes and associativity for CMP-VR and baseline configuration. The best improvements obtained are 45.5\% and 14\% in terms of miss rate and cycles per instruction (CPI) respectively for a 4 MB, 4-way set associative LLC. Reduction in CPI and miss rate together guarantees performance improvement.},
  file = {/Users/jonathanrainer/Zotero/storage/X9KU888H/Das and Kapoor - 2014 - Victim retention for reducing cache misses in tile.pdf},
  journal = {Microprocessors and Microsystems},
  language = {en},
  number = {4}
}

@inproceedings{deckerOnlineAnalysisDebug2018,
  title = {Online Analysis of Debug Trace Data for Embedded Systems},
  booktitle = {2018 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Decker, N. and Dreyer, B. and Gottschling, P. and Hochberger, C. and Lange, A. and Leucker, M. and Scheffel, T. and Wegener, S. and Weiss, A.},
  year = {2018},
  month = mar,
  pages = {851--856},
  doi = {10.23919/DATE.2018.8342124},
  abstract = {Modern multi-core Systems-on-Chip (SoC) provide very high computational power. On the downside, they are hard to debug and it is often very difficult to understand what is going on in these chips because of the limited observability inside the SoC. Chip manufacturers try to compensate this difficulty by providing highly compressed trace data from the individual cores. In the past, the common way to deal with this data was storing it for later offline analysis, which severely limits the time span that can be observed. In this contribution, we present an FPGA-based solution that is able to process the trace data in real-time, enabling continuous observation of the state of a core. Moreover, we discuss applications enabled by this technology.},
  file = {/Users/jonathanrainer/Zotero/storage/WWBGHZM7/Decker et al. - 2018 - Online analysis of debug trace data for embedded s.pdf;/Users/jonathanrainer/Zotero/storage/W5G8AMFU/8342124.html},
  keywords = {Bandwidth,Chip manufacturers,continuous observation,debug trace data,embedded systems,field programmable gate arrays,Field programmable gate arrays,FPGA-based solution,Hardware,high computational power,highly compressed trace data,individual cores,Instruments,modern multicore Systems-on-Chip,Monitoring,multiprocessing systems,offline analysis,online analysis,program debugging,Runtime,SoC,Software,system-on-chip,time span}
}

@inproceedings{deepikaHybridwayCacheMobile2011,
  title = {Hybrid-Way {{Cache}} for {{Mobile Processors}}},
  booktitle = {2011 {{Eighth International Conference}} on {{Information Technology}}: {{New Generations}}},
  author = {Deepika, Bobbala Lakshmi and Lee, Byeong Kil},
  year = {2011},
  month = apr,
  pages = {707--712},
  issn = {null},
  doi = {10.1109/ITNG.2011.125},
  abstract = {As multi-core trends are becoming dominant, cache structures are being sophisticated and complicated. Also, the bigger shared level-2 (L2) caches are demanded for higher cache performance. However, the big cache size is directly related to the area and power consumption. Designing a cache memory, one of the easiest ways to increase the performance is doubling the cache size. In mobile processors, however, simple increase of the cache size may significantly affect its chip area and power. To address this issue, in this paper, we propose the hy-way cache (hybrid-way cache) which is a composite cache mechanism to maximize cache performance within a given cache size. This mechanism can improve cache performance without increasing cache size and set associativity by emphasizing the utilization of primary way(s) and pseudo-associativity. Based on our experiments with the sampled SPEC CPU2000 workload, the proposed cache mechanism shows the remarkable reduction in cache misses with the penalty of additional hardware cost and additional power consumption. The variation of performance improvement depends on cache size and set associativity, but the proposed scheme shows more sensitivity to cache size increase than set associativity increase.},
  file = {/Users/jonathanrainer/Zotero/storage/5SNDMAUM/Deepika and Lee - 2011 - Hybrid-way Cache for Mobile Processors.pdf;/Users/jonathanrainer/Zotero/storage/XTG3PUVU/5945323.html},
  keywords = {Arrays,Benchmark testing,cache design,cache memory,cache size,cache storage,Hardware,hybrid-way cache,hybrid-way cache structure,Mobile communication,mobile computing,mobile processor,mobile processors,multiprocessing systems,Performance evaluation,Power demand,Program processors,set associative cache,set associativity,shared level-2 caches}
}

@article{delshadtehraniNileProgrammableMonitoring2018,
  title = {Nile: {{A Programmable Monitoring Coprocessor}}},
  shorttitle = {Nile},
  author = {Delshadtehrani, L. and Eldridge, S. and Canakci, S. and Egele, M. and Joshi, A.},
  year = {2018},
  month = jan,
  volume = {17},
  pages = {92--95},
  issn = {1556-6056},
  doi = {10.1109/LCA.2017.2784416},
  abstract = {Researchers widely employ hardware performance counters (HPCs) as well as debugging and profiling tools in processors for monitoring different events such as cache hits, cache misses, and branch prediction statistics during the execution of programs. The collected information can be used for power, performance, and thermal management of the system as well as detecting anomalies or malicious behavior in the software. However, monitoring new or complex events using HPCs and existing tools is a challenging task because HPCs only provide a fixed pool of raw events to monitor. To address this challenge, we propose the implementation of a programmable hardware monitor in a complete system framework including the hardware monitor architecture and its interface with an in-order single-issue RISC-V processor as well as an operating system. As a proof of concept, we demonstrate how to programmatically implement a shadow stack using our hardware monitor and how the programmed shadow stack detects stack buffer overflow attacks. Our hardware monitor design incurs a 26 percent power overhead and a 15 percent area overhead over an unmodified RISC-V processor. Our programmed shadow stack has less than 3 percent performance overhead in the worst case.},
  file = {/Users/jonathanrainer/Zotero/storage/5CLZ8NEA/Delshadtehrani et al. - 2018 - Nile A Programmable Monitoring Coprocessor.pdf;/Users/jonathanrainer/Zotero/storage/634735H9/8219379.html},
  journal = {IEEE Computer Architecture Letters},
  keywords = {branch prediction statistics,cache hits,cache misses,cache storage,complete system framework,complex events,coprocessors,Coprocessors,debugging,fixed pool,Hardware,Hardware coprocessor,hardware monitor architecture,hardware monitor design,hardware performance counters,HPCs,Linux,malicious behavior,Monitoring,Nile,operating system,operating systems (computers),Pattern matching,performance evaluation,performance overhead,power overhead,profiling tools,Program processors,programmable hardware,programmable hardware monitor,programmable monitoring coprocessor,programmed shadow stack,raw events,reduced instruction set computing,Rockets,security,shadow stack,single-issue RISC-V processor,stack buffer overflow attack,stack buffer overflow attacks,thermal management,unmodified RISC-V processor},
  number = {1}
}

@inproceedings{denningThrashingItsCauses1968,
  title = {Thrashing: Its Causes and Prevention},
  shorttitle = {Thrashing},
  author = {Denning, Peter J.},
  year = {1968},
  pages = {915},
  publisher = {{ACM Press}},
  doi = {10.1145/1476589.1476705},
  file = {/Users/jonathanrainer/Zotero/storage/2BYK7LVK/Denning - 1968 - Thrashing its causes and prevention.pdf},
  language = {en}
}

@article{denningWorkingSetModel1968,
  title = {The Working Set Model for Program Behavior},
  author = {Denning, Peter J.},
  year = {1968},
  month = may,
  volume = {11},
  pages = {323--333},
  issn = {00010782},
  doi = {10.1145/363095.363141},
  file = {/Users/jonathanrainer/Zotero/storage/XANGYT6N/Denning - The Working Set Model for Program Behavior.pdf},
  journal = {Communications of the ACM},
  number = {5}
}

@inproceedings{desaiProcessVariationAware2012,
  title = {Process Variation Aware {{DRAM}} Design Using Block Based Adaptive Body Biasing Algorithm},
  booktitle = {Thirteenth {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  author = {Desai, S. and Roy, S. and Chakraborty, K.},
  year = {2012},
  month = mar,
  pages = {255--261},
  doi = {10.1109/ISQED.2012.6187503},
  abstract = {Large dense structures like DRAMs are particularly susceptible to process variation, which can lead to variable latencies in different memory arrays. However, very little work exists on variation studies in the DRAM. This is due to the fact that DRAMs were traditionally placed off-chip and their latency changes due to process variation did not impact the overall processor performance. However, emerging technology trends like three dimensional integration, use of sophisticated memory controllers and continued scaling of technology nodes, substantially reduces DRAM access latency. Hence future technology nodes will see widespread adoption of embedded DRAMs. This makes process variation a critical upcoming challenge in DRAMs that must be addressed in current and forthcoming technology generations. In this paper, we present techniques for modeling the effect of random as well as spatial variation in large DRAM array structures. We use sensitivity based gate level process variation models combined with statistical timing analysis to estimate the impact of process variation on the DRAM performance and leakage power. We also propose a simulated annealing based Vth assignment algorithm using adaptive body biasing to improve the yield of DRAM structures. Applying our algorithm on a 1GB DRAM array, we report an average of 10.3\% improvement in the DRAM yield. To the best of our knowledge, ours is the first technique to model the impact of process variation on large scale DRAM arrays.},
  file = {/Users/jonathanrainer/Zotero/storage/XLSSMPGY/Desai et al. - 2012 - Process variation aware DRAM design using block ba.pdf;/Users/jonathanrainer/Zotero/storage/3KL3ADMV/6187503.html},
  keywords = {Adaptive body bias,Algorithm design and analysis,block based adaptive body biasing algorithm,Correlation,Delay,DRAM,DRAM chips,Integrated circuit modeling,Logic gates,Mathematical model,memory arrays,memory controllers,Process Variation,process variation aware DRAM design,Random access memory,sensitivity based gate level process variation models,simulated annealing,simulated annealing based Vth assignment algorithm,statistical analysis,statistical timing analysis}
}

@article{devilleLowcostUsagebasedReplacement1990,
  title = {A Low-Cost Usage-Based Replacement Algorithm for Cache Memories},
  author = {Deville, Yannick},
  year = {1990},
  month = dec,
  volume = {18},
  pages = {52--58},
  issn = {01635964},
  doi = {10.1145/121973.121979},
  file = {/Users/jonathanrainer/Zotero/storage/RWRDHF8Z/Deville - 1990 - A low-cost usage-based replacement algorithm for c.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {4}
}

@inproceedings{dingMemoryBandwidthBottleneck2000,
  title = {The Memory of Bandwidth Bottleneck and Its Amelioration by a Compiler},
  booktitle = {Proceedings 14th {{International Parallel}} and {{Distributed Processing Symposium}}. {{IPDPS}} 2000},
  author = {Ding, Chen and Kennedy, K.},
  year = {2000},
  month = may,
  pages = {181--189},
  issn = {null},
  doi = {10.1109/IPDPS.2000.845980},
  abstract = {As the speed gap between CPU and memory widens, memory hierarchy has become the primary factor limiting program performance. Until now, the principal focus of hardware and software innovations has been overcoming latency. However, the advent of latency tolerance techniques such as non-blocking cache and software prefetching begins the process of trading bandwidth for latency by overlapping and pipelining memory transfers. Since actual latency is the inverse of the consumed bandwidth, memory latency cannot be fully tolerated without infinite bandwidth. This perspective has led us to two questions. Do current machines provide sufficient data bandwidth? If not, can a program be restructured to consume less bandwidth? This paper answers these questions in two parts. The first part defines a new bandwidth-based performance model and demonstrates the serious performance bottleneck due to the lack of memory bandwidth. The second part describes a new set of compiler optimizations for reducing bandwidth consumption of programs.},
  file = {/Users/jonathanrainer/Zotero/storage/L2R5HP7N/Chen Ding and Kennedy - 2000 - The memory of bandwidth bottleneck and its amelior.pdf;/Users/jonathanrainer/Zotero/storage/ZFA4GI77/845980.html},
  keywords = {Bandwidth,bandwidth bottleneck memory,bandwidth-based performance model,compiler optimizations,Delay,Electronic switching systems,Lapping,latency,memory hierarchy,optimising compilers,Optimizing compilers,performance bottleneck,Pipeline processing,program performance,Program processors,Read-write memory,software performance evaluation,software prefetching,Technological innovation,Tellurium}
}

@inproceedings{djordjalianMinimallyskewedassociativeCaches2002,
  title = {Minimally-Skewed-Associative Caches},
  booktitle = {14th {{Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}, 2002. {{Proceedings}}.},
  author = {Djordjalian, A.},
  year = {2002},
  month = oct,
  pages = {100--107},
  issn = {null},
  doi = {10.1109/CAHPC.2002.1180765},
  abstract = {Skewed-associativity is a technique that reduces the miss ratios of CPU caches by applying different indexing functions to each way of an associative cache. Even though it showed impressive hit/miss statistics, the scheme has not been welcomed by the industry, presumably because implementation of the original version is complex and might involve access-time penalties among other costs. This paper presents a simplified, easy to implement variant that we call "minimally-skewed-associativity" (MSkA). We show that MSkA caches, for many cases, should not have penalties in either access time or power consumption when compared to set-associative caches of the same associativity. Hit/miss statistics were obtained by means of trace-driven simulations. Miss ratios are not as good as those for full skewing, but they are still advantageous. Minimal-skewing is thus proposed as a way to improve the hit/miss performance of caches, often without producing access-time delays or increases in power consumption as other techniques do (for example, using higher associativities).},
  file = {/Users/jonathanrainer/Zotero/storage/U73XSP4L/Djordjalian - 2002 - Minimally-skewed-associative caches.pdf;/Users/jonathanrainer/Zotero/storage/KR5RQVRH/1180765.html},
  keywords = {access-time penalties,Bandwidth,Cache memory,cache storage,Computer architecture,content-addressable storage,Costs,CPU caches,Delay,Energy consumption,Energy efficiency,Indexing,minimally-skewed-associative caches,MSkA caches,skewed associativity,Statistics,trace-driven simulations,Visualization}
}

@inproceedings{dongheeleeImplementationPerformanceEvaluation1997,
  title = {Implementation and Performance Evaluation of the {{LRFU}} Replacement Policy},
  booktitle = {Proceedings 23rd {{Euromicro Conference New Frontiers}} of {{Information Technology}} - {{Short Contributions}} -},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Honggi Choe} and {Sam H. Noh} and {Sang Lyul Min} and {Yookun Cho}},
  year = {1997},
  month = sep,
  pages = {106--111},
  doi = {10.1109/EMSCNT.1997.658446},
  abstract = {Recently, a new block replacement policy called the LRFU (Least Recently/Frequently Used) policy was proposed that subsumes both the LRU and LFU policies, and provides a spectrum of replacement policies between them. We describe an implementation of the LRFU replacement policy in the FreeBSD 2.1.5 and present a performance evaluation of the implementation using the SPEC SDET benchmark. The results show that the new policy gives up to a 30\% performance improvement over the LRU block replacement policy.},
  file = {/Users/jonathanrainer/Zotero/storage/X45T4U8Y/Donghee Lee et al. - 1997 - Implementation and performance evaluation of the L.pdf;/Users/jonathanrainer/Zotero/storage/GEJN9C9I/658446.html},
  keywords = {block replacement policy,cache storage,Databases,FreeBSD,Frequency,History,Least Recently/Frequently Used policy,LFU policies,LRFU replacement policy,LRU block replacement policy,Operating systems,performance evaluation,performance improvement,Random access memory,SPEC SDET benchmark,Very large scale integration}
}

@article{dongheeleeLRFUSpectrumPolicies2001,
  title = {{{LRFU}}: A Spectrum of Policies That Subsumes the Least Recently Used and Least Frequently Used Policies},
  shorttitle = {{{LRFU}}},
  author = {{Donghee Lee} and {Jongmoo Choi} and {Jong-Hun Kim} and Noh, S. H. and {Sang Lyul Min} and {Yookun Cho} and {Chong Sang Kim}},
  year = {2001},
  month = dec,
  volume = {50},
  pages = {1352--1361},
  doi = {10.1109/TC.2001.970573},
  file = {/Users/jonathanrainer/Zotero/storage/VV9J789A/Donghee Lee et al. - 2001 - LRFU a spectrum of policies that subsumes the lea.pdf;/Users/jonathanrainer/Zotero/storage/K8TTHISS/970573.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Bridges,Databases,File systems,Frequency,Hard disks,History},
  number = {12}
}

@article{duongSCOREScoreBasedMemory2010,
  title = {{{SCORE}}: {{A Score}}-{{Based Memory Cache Replacement Policy}}},
  shorttitle = {{{SCORE}}},
  author = {Duong, Nam and Cammarota, Rosario and Zhao, Dali and Kim, Taesu and Veidenbaum, Alex},
  year = {2010},
  month = jun,
  abstract = {We propose SCORE, a novel adaptive cache replacement policy, which uses a score system to select a cache line to replace. Results show that SCORE o\textregistered{}ers low over-all miss rates on SPEC CPU2006 benchmarks, and provides an average IPC that is 4.9\% higher than LRU and 7.4\% higher than LIP.},
  file = {/Users/jonathanrainer/Zotero/storage/VBA6ZPP7/Duong et al. - 2010 - SCORE A Score-Based Memory Cache Replacement Poli.pdf;/Users/jonathanrainer/Zotero/storage/ZLYYHHS8/inria-00492956.html},
  language = {en}
}

@inproceedings{duttMemoryOrganizationExploration1997,
  title = {Memory {{Organization}} and {{Exploration}} for {{Embedded Systems}}-on-{{Silicon}}},
  booktitle = {Proc. 1997 {{International Conference}} on {{VLSI}} and {{CAD}} ({{ICVC}}'97},
  author = {Dutt, Nikil},
  year = {1997},
  abstract = {The growing gap between processor and memory speeds makes memory issues a major bottleneck in the design of systems-on-silicon. When the system is designed for a targeted application (as is the case with embedded systems-onsilicon) , several strategies can be employed to resolve this memory bandwidth bottleneck, including reorganization of data, exploiting locality of reference to tune the memory hierarchy, and intelligent partitioning of data amongst different levels of the memory hierarchy. From a system designer 's perspective, this requires an early memory exploration capability in order to evaluate its effects on system performance, cost and power, before the tasks of hardware and software co-design are performed. The research problems at this level require integration of the traditionally disjoint fields of computer architecture, compiler design, and digital CAD. This paper describes some strategies for memory organization and exploration that will aid system-level designers for ...},
  file = {/Users/jonathanrainer/Zotero/storage/LUX7LJVK/Dutt - 1997 - Memory Organization and Exploration for Embedded S.pdf;/Users/jonathanrainer/Zotero/storage/BIE5MJ49/summary.html}
}

@inproceedings{dybdahlLRUbasedReplacementAlgorithm2006,
  title = {An {{LRU}}-Based {{Replacement Algorithm Augmented}} with {{Frequency}} of {{Access}} in {{Shared Chip}}-Multiprocessor {{Caches}}},
  booktitle = {Proceedings of the 2006 {{Workshop}} on {{MEmory Performance}}: {{DEaling}} with {{Applications}}, {{Systems}} and {{Architectures}}},
  author = {Dybdahl, Haakon and Stenstr{\"o}m, Per and Natvig, Lasse},
  year = {2006},
  pages = {45--52},
  publisher = {{ACM}},
  address = {{Seattle, Washington, USA}},
  doi = {10.1145/1166133.1166139},
  abstract = {This paper proposes a new replacement algorithm to protect cache lines with potential future reuse from being evicted. In contrast to the recency based approaches used in the past (LRU for example), our algorithm also uses the notion of frequency of access. Instead of evicting the least recently used block, our algorithm identifies among a set of LRU blocks the one that is also least-frequently-used (according to a heuristic) and chooses that as a victim. We have implemented this replacement algorithm in a detailed simulation model of a chip multiprocessor system driven by SPEC2000 benchmarks. We have found that the new scheme improves performance for memory intensive applications. Moreover, as compared to other attempts, our replacement algorithm provides robust improvements across all benchmarks. We have also extended an earlier scheme proposed by Wong and Baer so it is switched off when performance is not improved. Our results show that this makes the scheme much more suitable for CMP configurations.},
  file = {/Users/jonathanrainer/Zotero/storage/69F9ZYYN/Dybdahl et al. - 2006 - An LRU-based Replacement Algorithm Augmented with .pdf},
  isbn = {978-1-59593-568-7},
  series = {{{MEDEA}} '06}
}

@article{eastonUseBitScanning1979,
  title = {Use {{Bit Scanning}} in {{Replacement Decisions}}},
  author = {Easton and Franaszek},
  year = {1979},
  month = feb,
  volume = {C-28},
  pages = {133--141},
  issn = {2326-3814},
  doi = {10.1109/TC.1979.1675302},
  abstract = {In paged storage systems, page replacement policies generally depend on a use bit for each page frame. The use bit is automatically turned on when the resident page is referenced. Typically, a page is considered eligible for replacement if its use bit has been scanned and found to be off on \textmu{} consecutive occasions, where \textmu{} is a parameter of the algorithm. This investigation focuses on the dependence of the number of bit-scanning operations on the value of \textmu{} and on properties of the string of page references. The number of such operations is a measure of the system overhead incurred while making replacement decisions. In particular, for several algorithms, the number of scans per reference is shown to be approximately proportional to \textmu{} However, empirical results from single-program traces show that the value of \textmu{} has little effect on the miss ratio. Although the miss ratios for the bit-scanning algorithms are close to those of least recently used (LRU), it is pointed out that increasing the value of \textmu{} need not bring the bit-scanning policies closer to LRU management.},
  file = {/Users/jonathanrainer/Zotero/storage/78TC4848/Easton and Franaszek - 1979 - Use Bit Scanning in Replacement Decisions.pdf;/Users/jonathanrainer/Zotero/storage/R6TVFL2Q/1675302.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Miss ratio,operating system overhead,page fault rate,page replacement algorithms,paged memories,paged storage},
  number = {2}
}

@inproceedings{ekanayakeAsynchronousDRAMDesign2003,
  title = {Asynchronous {{DRAM}} Design and Synthesis},
  booktitle = {Ninth {{International Symposium}} on {{Asynchronous Circuits}} and {{Systems}}, 2003. {{Proceedings}}.},
  author = {Ekanayake, V. N. and Manohar, R.},
  year = {2003},
  month = may,
  pages = {174--183},
  doi = {10.1109/ASYNC.2003.1199177},
  abstract = {We present the design of a high performance on-chip pipelined asynchronous DRAM suitable for use in a microprocessor cache. Although traditional DRAM structures suffer from long access latency and even longer cycle times, our design achieves a simulated core sub-nanosecond latency and a respectable cycle time of 4.8 ns in a standard 0.25 /spl mu/m logic process. We also show how the cycle time penalty can be overcome by using pipelined interleaved banks with quasi-delay insensitive asynchronous control circuits. We can thus approach the performance of SRAM, which is typically used for caches, while still benefiting from the smaller area footprint of DRAM.},
  file = {/Users/jonathanrainer/Zotero/storage/G6U57F85/Ekanayake and Manohar - 2003 - Asynchronous DRAM design and synthesis.pdf;/Users/jonathanrainer/Zotero/storage/FH333QQZ/1199177.html},
  keywords = {0.25 micron,4.8 ns,asynchronous circuits,asynchronous control circuits,asynchronous DRAM design,Banking,cache storage,Circuit simulation,cycle time penalty,Delay,Design engineering,High performance computing,integrated circuit design,Laboratories,logic design,Logic design,long access latency,microprocessor cache,microprocessor chips,Microprocessors,on-chip asynchronous DRAM,pipeline processing,pipelined asynchronous DRAM,pipelined interleaved banks,quasi-delay insensitive control circuits,Random access memory,random-access storage,System-on-a-chip}
}

@article{emmaExploringLimitsPrefetching2005,
  title = {Exploring the Limits of Prefetching},
  author = {Emma, P. G. and Hartstein, A. and Puzak, T. R. and Srinivasan, V.},
  year = {2005},
  month = jan,
  volume = {49},
  pages = {127--144},
  issn = {0018-8646},
  doi = {10.1147/rd.491.0127},
  abstract = {We formulate a new approach for evaluating a prefetching algorithm. We first carry out a profiling run of a program to identify all of the misses and corresponding locations in the program where prefetches for the misses can be initiated. We then systematically control the number of misses that are prefetched, the timeliness of these prefetches, and the number of unused prefetches. We validate the accuracy of our approach by comparing it to one based on a Markov prefetch algorithm. This allows us to measure the potential benefit that any application can receive from prefetching and to analyze application behavior under conditions that cannot be explored with any known prefetching algorithm. Next, we analyze a system parameter that is vital to prefetching performance, the line transfer interval, which is the number of processor cycles required to transfer a cache line. This interval is determined by technology and bandwidth. We show that under ideal conditions, prefetching can remove nearly all of the stalls associated with cache misses. Unfortunately, real processor implementations are less than ideal. In particular, the trend in processor frequency is outrunning on-chip and off-chip bandwidths. Today, it is not uncommon for processor frequency to be three or four times bus frequency. Under these conditions, we show that nearly all of the performance benefits derived from prefetching are eroded, and in many cases prefetching actually degrades performance. We carry out quantitative and qualitative analyses of these tradeoffs and show that there is a linear relationship between overall performance and three metrics: percentage of misses prefetched, percentage of unused prefetches, and bandwidth.},
  file = {/Users/jonathanrainer/Zotero/storage/2AABA5YD/Emma et al. - 2005 - Exploring the limits of prefetching.pdf;/Users/jonathanrainer/Zotero/storage/328ACD4Y/5388854.html},
  journal = {IBM Journal of Research and Development},
  number = {1}
}

@inproceedings{fanApplyingVictimCache2016,
  title = {Applying {{Victim Cache}} in {{High Performance GPGPU Computing}}},
  booktitle = {2016 15th {{International Symposium}} on {{Parallel}} and {{Distributed Computing}} ({{ISPDC}})},
  author = {Fan, Fengfeng and Wang, Jianfei and Jiang, Li and Liang, Xiaoyao and Jing, Naifeng},
  year = {2016},
  month = jul,
  pages = {24--29},
  issn = {null},
  doi = {10.1109/ISPDC.2016.12},
  abstract = {Modern GPGPUs employ thousands of threads for parallel execution. The massive threads often compete in the small sized first level data (L1D) cache, which leads to severe cache thrashing problem and hurts the GPGPU performance. In this paper, we apply victim cache design into GPGPUs to alleviate L1D cache thrashing problem for better data locality and system performance. Instead of a small fully associative victim cache design, we first change the victim cache structure to meet needs from the large number of concurrent threads commonly in GPGPU applications. Then, we propose to use the unallocated registers determined by compiler to further provide storage for victim cache data. The experiment results show that using our approach, the on chip data cache hit rate can be increased largely, which leads to a better performance of 32.7\% on average with only small changes to the GPGPU design.},
  file = {/Users/jonathanrainer/Zotero/storage/XZKLQRN8/Fan et al. - 2016 - Applying Victim Cache in High Performance GPGPU Co.pdf;/Users/jonathanrainer/Zotero/storage/FIP82N8C/7904265.html},
  keywords = {cache storage,cache thrashing,chip data cache hit rate,compilers,Computer architecture,data locality,first level data cache,general purpose computers,GPGPU applications,GPGPUs,graphics processing units,Hardware,high performance GPGPU computing,Instruction sets,L1D cache thrashing problem,massive threads,Message systems,optimising compilers,parallel execution,parallel processing,Radio frequency,Registers,RF under-utilization,small fully associative victim cache design,System-on-chip,unallocated registers,victim cache}
}

@inproceedings{faresPerformanceEvaluationTraditional2012,
  title = {Performance {{Evaluation}} of {{Traditional Caching Policies}} on a {{Large System}} with {{Petabytes}} of {{Data}}},
  booktitle = {2012 {{IEEE Seventh International Conference}} on {{Networking}}, {{Architecture}}, and {{Storage}}},
  author = {Fares, R. and Romoser, B. and Zong, Z. and Nijim, M. and Qin, X.},
  year = {2012},
  month = jun,
  pages = {227--234},
  doi = {10.1109/NAS.2012.32},
  abstract = {Caching is widely known to be an effective method for improving I/O performance by storing frequently used data on higher speed storage components. However, most existing studies that focus on caching performance evaluate fairly small files populating a relatively small cache. Few reports are available that detail the performance of traditional cache replacement policies on extremely large caches. Do such traditional caching policies still work effectively when applied to systems with petabytes of data? In this paper, we comprehensively evaluate the performance of several cache policies, which include First-In-First-Out (FIFO), Least Recently Used (LRU) and Least Frequently Used (LFU), on the global satellite imagery distribution application maintained by the U.S. Geological Survey (USGS) Earth Resources Observation and Science Center (EROS). Evidence is presented suggesting traditional caching policies are capable of providing performance gains when applied to large data sets as with smaller data sets. Our evaluation is based on approximately three million real-world satellite images download requests representing global user download behavior since October 2008.},
  file = {/Users/jonathanrainer/Zotero/storage/RDRSFF3U/Fares et al. - 2012 - Performance Evaluation of Traditional Caching Poli.pdf;/Users/jonathanrainer/Zotero/storage/RVYUGNMK/6310897.html},
  keywords = {cache replacement policy,cache storage,caching performance evaluation,Computer science,data petabytes,Earth,Earth Resources Observation and Science Center,EROS,FIFO policy,first-in-first-out policy,frequently used data,global satellite imagery distribution application,global user download behavior,higher speed storage components,I/O performance,least frequently used policy,least recently used policy,LFU policy,LRU policy,NASA,performance evaluation,Performance evaluation,performance gains,real-world satellite images download requests,Remote sensing,Satellites,Servers,traditional caching policy,U.S. Geological Survey,USGS}
}

@inproceedings{farkasHowUsefulAre1995,
  title = {How Useful Are Non-Blocking Loads, Stream Buffers and Speculative Execution in Multiple Issue Processors?},
  booktitle = {Proceedings of 1995 1st {{IEEE Symposium}} on {{High Performance Computer Architecture}}},
  author = {Farkas, K.I. and Jouppi, N.P. and Chow, P.},
  year = {1995},
  month = jan,
  pages = {78--89},
  issn = {null},
  doi = {10.1109/HPCA.1995.386553},
  abstract = {We investigate the relative performance impact of non-blocking loads, stream buffers, and speculative execution both used individually and in conjunction with each other. We have simulated the SPEC92 benchmarks on a statically scheduled quad-issue processor model, running code from the Multiflow compiler. Non-blocking loads and stream buffers both provide a significant performance advantage, and their combination performs significantly better than either alone. For example, with a 64-byte, 2-way set associative cache with 32 cycle fetch latency, non-blocking loads reduce the run-time by 21\% while stream-buffers reduce it by 26\%, and the combined use of the two yields a 47\% reduction. The addition of speculative execution further improves the performance of the systems that we have simulated, with or without non-blocking loads and stream buffers, by an additional 20\% to 4O\%. We expect that the use of all three of these techniques will be important in future generations of microprocessors.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/A96K8MK5/Farkas et al. - 1995 - How useful are non-blocking loads, stream buffers .pdf;/Users/jonathanrainer/Zotero/storage/2DW7ZYWJ/386553.html},
  keywords = {associative cache,cache storage,Computational modeling,Costs,Delay,Educational institutions,fetch latency,Hardware,Microprocessors,Multiflow compiler,multiple issue processors,nonblocking loads,parallel architectures,performance evaluation,Prefetching,Processor scheduling,reduced instruction set computing,Registers,Runtime,SPEC92 benchmarks,speculative execution,statically scheduled quad-issue processor model,stream buffers,virtual machines}
}

@article{fernandezEffectReplacementAlgorithms1978,
  title = {Effect of {{Replacement Algorithms}} on a {{Paged Buffer Database System}}},
  author = {Fernandez, E. B. and Lang, T. and Wood, C.},
  year = {1978},
  month = mar,
  volume = {22},
  pages = {185--196},
  issn = {0018-8646},
  doi = {10.1147/rd.222.0185},
  abstract = {In a database system a buffer may be used to hold recently referenced pages. If this buffer is in virtual memory, the database paging system and the memory paging system affect its performance. The study of the effect of main memory replacement algorithms on the number of main memory page faults is the basic objective of this paper. We assume that the buffer replacement algorithm is least recently used (LRU), and page fault rates for LRU, random (R), and generalized least recently used (GLRU) main memory replacement algorithms are calculated and compared. A set of experiments validates these fault rate expressions and establishes some conditions for the practical application of the results.},
  file = {/Users/jonathanrainer/Zotero/storage/29Y8EBKP/Fernandez et al. - 1978 - Effect of Replacement Algorithms on a Paged Buffer.pdf;/Users/jonathanrainer/Zotero/storage/UTVMC8IQ/5390934.html},
  journal = {IBM Journal of Research and Development},
  number = {2}
}

@inproceedings{frittsMultilevelMemoryPrefetching2002a,
  title = {Multi-Level Memory Prefetching for Media and Stream Processing},
  booktitle = {Proceedings. {{IEEE International Conference}} on {{Multimedia}} and {{Expo}}},
  author = {Fritts, J.},
  year = {2002},
  month = aug,
  volume = {2},
  pages = {101-104 vol.2},
  issn = {null},
  doi = {10.1109/ICME.2002.1035522},
  abstract = {This paper presents a multi-level memory prefetch hierarchy for media and stream processing applications. Two major bottlenecks in the performance of multimedia and network applications are long memory latencies and limited off-chip processor bandwidth. Aggressive prefetching can be used to mitigate the memory latency problem, but overly aggressive prefetching may overload the limited external processor bandwidth. To accommodate both problems, we propose multilevel memory prefetching. The multi-level organization enables conservative prefetching on-chip and more aggressive prefetching off-chip. The combination provides aggressive prefetching while minimally impacting off-chip bandwidth, enabling more efficient memory performance for media and stream processing. This paper presents preliminary results for multi-level memory prefetching, which show that combining prefetching at the L1 and DRAM memory levels provides the most effective prefetching with minimal extra bandwidth.},
  file = {/Users/jonathanrainer/Zotero/storage/CEKVCIFM/Fritts - 2002 - Multi-level memory prefetching for media and strea.pdf;/Users/jonathanrainer/Zotero/storage/XNB2DTVK/1035522.html},
  keywords = {Application software,bandwidth,Bandwidth,buffer storage,Computer networks,Computer science,Delay,DRAM memory,L1 memory,media processing,memory latencies,memory performance,multi-level memory prefetch hierarchy,multi-level memory prefetching,multi-level organization,multimedia applications,multimedia communication,multimedia computing,Multimedia computing,network applications,off-chip processor bandwidth,on-chip,Personal digital assistants,Prefetching,Random access memory,stream processing,Streaming media}
}

@inproceedings{fuDataPrefetchingMultiprocessor1991,
  title = {Data Prefetching in Multiprocessor Vector Cache Memories},
  booktitle = {Proceedings of the 18th Annual International Symposium on {{Computer}} Architecture},
  author = {Fu, John W. C. and Patel, Janak H.},
  year = {1991},
  month = apr,
  pages = {54--63},
  publisher = {{Association for Computing Machinery}},
  address = {{Toronto, Ontario, Canada}},
  doi = {10.1145/115952.115959},
  file = {/Users/jonathanrainer/Zotero/storage/HHCSWCA2/Fu and Patel - 1991 - Data prefetching in multiprocessor vector cache me.pdf;/Users/jonathanrainer/Zotero/storage/VX95LRT6/Fu and Patel - Data Prefetching in Multiprocessor Vector Cache Me.pdf},
  isbn = {978-0-89791-394-2},
  series = {{{ISCA}} '91}
}

@article{furiaModelingTimeComputing2010,
  title = {Modeling Time in Computing: {{A}} Taxonomy and a Comparative Survey},
  shorttitle = {Modeling Time in Computing},
  author = {Furia, Carlo A. and Mandrioli, Dino and Morzenti, Angelo and Rossi, Matteo},
  year = {2010},
  month = feb,
  volume = {42},
  pages = {1--59},
  issn = {03600300},
  doi = {10.1145/1667062.1667063},
  file = {/Users/jonathanrainer/Zotero/storage/UWY2P8D5/Furia et al. - 2010 - Modeling time in computing A taxonomy and a compa.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {2}
}

@article{fuStrideDirectedPrefetching1992,
  title = {Stride Directed Prefetching in Scalar Processors},
  author = {Fu, John W. C. and Patel, Janak H. and Janssens, Bob L.},
  year = {1992},
  month = dec,
  volume = {23},
  pages = {102--110},
  issn = {1050-916X},
  doi = {10.1145/144965.145006},
  file = {/Users/jonathanrainer/Zotero/storage/DVLVK67U/Fu et al. - 1992 - Stride directed prefetching in scalar processors.pdf},
  journal = {ACM SIGMICRO Newsletter},
  number = {1-2}
}

@inproceedings{gaoCollectiveLoopFusion1993,
  title = {Collective Loop Fusion for Array Contraction},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Gao, G. and Olsen, R. and Sarkar, V. and Thekkath, R.},
  editor = {Banerjee, Utpal and Gelernter, David and Nicolau, Alex and Padua, David},
  year = {1993},
  pages = {281--295},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {In this paper we propose a loop fusion algorithm specifically designed to increase opportunities for array contraction. Array contraction is an optimization that transforms array variables into scalar variables within a loop nest. In contrast to array elements, scalar variables have better cache behavior and can be allocated to registers. In past work we investigated loop interchange and loop reversal as optimizations that increase opportunities for array contraction [13]. This paper extends this work by including the loop fusion optimization. The fusion method discussed in this paper uses the maxflow-mincut algorithm to do loop clustering. Our collective loop fusion algorithm is efficient, and we demonstrate its usefulness for array contraction with a simple example.},
  file = {/Users/jonathanrainer/Zotero/storage/R7HINZNJ/Gao et al. - 1993 - Collective loop fusion for array contraction.pdf},
  isbn = {978-3-540-48201-7}
}

@phdthesis{garsideRealtimePrefetchingSharedmemory2015,
  title = {Real-Time {{Prefetching On Shared}}-Memory {{Multi}}-Core {{Systems}}},
  author = {Garside, Jamie},
  year = {2015},
  month = jul,
  address = {{York, UK}},
  abstract = {In recent years, there has been a growing trend towards using multi-core processors
in real-time systems to cope with the rising computation requirements of
real-time tasks. Coupled with this, the rising memory requirements of these tasks
pushes demand beyond what can be provided by small, private on-chip caches, requiring
the use of larger, slower off-chip memories such as DRAM. Due to the cost,
power requirements and complexity of these memories, they are typically shared
between all of the tasks within the system.
In order for the execution time of these tasks to be bounded, the response time
of the memory and the interference from other tasks also needs to be bounded.
While there is a great amount of current research on bounding this interference,
one popular method is to effectively partition the available memory bandwidth
between the processors in the system. Of course, as the number of processors
increases, so does the worst-case blocking, and worst-case blocking times quickly
increase with the number of processors.
It is difficult to further optimise the arbitration scheme; instead, this scaling problem
needs to be approached from another angle. Prefetching has previously been
shown to improve the execution time of tasks by speculatively issuing memory
accesses ahead of time for items which may be useful in the near future, although
these prefetchers are typically not used in real-time systems due to their unpredictable
nature. Instead, this work presents a framework by which a prefetcher
can be safely used alongside a composable memory arbiter, a predictable prefetching
scheme, and finally a method by which this predictable prefetcher can be used
to improve the worst-case execution time of a running task.},
  file = {/Users/jonathanrainer/Zotero/storage/WZNZW7PQ/thesis.pdf},
  school = {The University of York},
  type = {Doctoral}
}

@article{gautschiNearThresholdRISCVCore2017,
  title = {Near-{{Threshold RISC}}-{{V Core With DSP Extensions}} for {{Scalable IoT Endpoint Devices}}},
  author = {Gautschi, Michael and Schiavone, Pasquale Davide and Traber, Andreas and Loi, Igor and Pullini, Antonio and Rossi, Davide and Flamand, Eric and Gurkaynak, Frank K. and Benini, Luca},
  year = {2017},
  month = oct,
  volume = {25},
  pages = {2700--2713},
  issn = {1063-8210, 1557-9999},
  doi = {10.1109/TVLSI.2017.2654506},
  abstract = {Endpoint devices for Internet-of-Things not only need to work under extremely tight power envelope of a few milliwatts, but also need to be flexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold (NT) operation can achieve higher energy efficiency, and the performance scalability can be gained through parallelism. In this paper, we describe the design of an open-source RISC-V processor core specifically designed for NT operation in tightly coupled multicore clusters. We introduce instruction extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure toward the shared-memory hierarchy. For typical data-intensive sensor processing workloads, the proposed core is, on average, 3.5\texttimes{} faster and 3.2\texttimes{} more energy efficient, thanks to a smart L0 buffer to reduce cache access contentions and support for compressed instructions. Single Instruction Multiple Data extensions, such as dot products, and a built-in L0 storage further reduce the shared-memory accesses by 8\texttimes{} reducing contentions by 3.2\texttimes. With four NT-optimized cores, the cluster is operational from 0.6 to 1.2 V, achieving a peak efficiency of 67 MOPS/mW in a low-cost 65-nm bulk CMOS technology. In a low-power 28-nm FD-SOI process, a peak efficiency of 193 MOPS/mW (40 MHz and 1 mW) can be achieved.},
  file = {/Users/jonathanrainer/Zotero/storage/5TKEB9LZ/Gautschi et al. - 2017 - Near-Threshold RISC-V Core With DSP Extensions for.pdf},
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  language = {en},
  number = {10}
}

@inproceedings{ghasemzadehModifiedPseudoLRU2006,
  title = {Modified Pseudo {{LRU}} Replacement Algorithm},
  booktitle = {13th {{Annual IEEE International Symposium}} and {{Workshop}} on {{Engineering}} of {{Computer}}-{{Based Systems}} ({{ECBS}}'06)},
  author = {Ghasemzadeh, H. and Mazrouee, S. and Kakoee, M. R.},
  year = {2006},
  month = mar,
  pages = {6 pp.-376},
  doi = {10.1109/ECBS.2006.52},
  abstract = {Although the LRU replacement algorithm has been widely used in cache memory management, it is well-known for its inability to be easily implemented in hardware. Most of primary caches employ a simple block replacement algorithm like pseudo LRU to avoid the disadvantages of a complex hardware design. In this paper, we propose a novel block replacement scheme, MPLRU (modified pseudo LRU), by exploiting second chance concept in pseudo LRU algorithm. A comprehensive comparison is made between our algorithm and both true LRU and other conventional schemes such as FIFO, random and pseudo LRU. Experimental results show that MPLRU significantly reduces the number of cache misses compared to the other algorithms. Simulation results reveal that in average our algorithm can provide a value of 8.52\% improvement on the miss ratio compared to the pseudo LRU algorithm. Moreover, it provides 7.93\% and 11.57\%performance improvement compared to FIFO and random replacement policies respectively},
  file = {/Users/jonathanrainer/Zotero/storage/UKBMBFKC/Ghasemzadeh et al. - 2006 - Modified pseudo LRU replacement algorithm.pdf;/Users/jonathanrainer/Zotero/storage/ENN8RYAP/1607387.html},
  keywords = {Algorithm design and analysis,block replacement scheme,Cache memory,cache memory management,cache storage,Computational modeling,Costs,Delay,Engineering management,FIFO,first in first out algorithm,Hardware,High performance computing,least recently used algorithm,Memory management,modified pseudo LRU replacement algorithm,random LRU,Random number generation}
}

@inproceedings{ghasemzadehPseudoFIFOArchitectureLRU2005,
  title = {Pseudo-{{FIFO Architecture}} of {{LRU Replacement Algorithm}}},
  booktitle = {2005 {{Pakistan Section Multitopic Conference}}},
  author = {Ghasemzadeh, H. and Fatemi, S. O.},
  year = {2005},
  month = dec,
  pages = {1--7},
  doi = {10.1109/INMIC.2005.334496},
  abstract = {Cache replacement algorithms have been widely used in modern computer systems to reduce the number of cache misses. The LRU algorithm has been shown to be an efficient replacement policy in terms of miss rates. However, most of the processors employ a block replacement algorithm which is very simple to implement in hardware or that is an approximation to the true LRU. In this paper, we propose a new implementation of block replacement algorithms in CPU caches by designing the circuitry required to implement an LRU replacement policy in set associative caches. We propose a simple and efficient architecture, Pseudo-FIFO, such that the true LRU replacement algorithm can be implemented without the disadvantages of the traditional implementations. Experimental results show that the Pseudo-FIFO significantly reduces the number of memory cells needed for hardware implementation. Simulation results reveal that our proposed architecture can provide an average value of 26\% improvement in the chip area compared to "reference matrix" and "basic architecture" circuits. Furthermore, it operates about 2.4 times faster than other architectures},
  file = {/Users/jonathanrainer/Zotero/storage/HFKKRY28/Ghasemzadeh and Fatemi - 2005 - Pseudo-FIFO Architecture of LRU Replacement Algori.pdf;/Users/jonathanrainer/Zotero/storage/7QVD44TI/4133511.html},
  keywords = {Algorithm design and analysis,Approximation algorithms,basic architecture circuits,block replacement algorithm,Cache memory,cache misses,cache replacement algorithms,cache storage,Central Processing Unit,chip area,Circuit simulation,Computer architecture,first in first out,Hardware,High performance computing,least recently used",LRU replacement algorithm,memory cells,Modems,Optical computing,pseudo FIFO architecture,reference matrix,replacement policy,set associative caches}
}

@inproceedings{glassAdaptivePageReplacement1997,
  title = {Adaptive {{Page Replacement Based}} on {{Memory Reference Behavior}}},
  booktitle = {Proceedings of the 1997 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Glass, Gideon and Cao, Pei},
  year = {1997},
  pages = {115--126},
  publisher = {{ACM}},
  address = {{Seattle, Washington, USA}},
  doi = {10.1145/258612.258681},
  abstract = {As disk performance continues to lag behind that of memory systems and processors, virtual memory management becomes increasingly important for overall system performance. In this paper we study the page reference behavior of a collection of memory-intensive applications, and propose a new virtual memory page replacement algorithm, SEQ. SEQ detects long sequences of page faults and applies most-recently-used replacement to those sequences. Simulations show that for a large class of applications, SEQ performs close to the optimal replacement algorithm, and significantly better than Least-Recently-Used (LRU). In addition, SEQ performs similarly to LRU for applications that do not exhibit sequential faulting.},
  file = {/Users/jonathanrainer/Zotero/storage/HVCBHFN3/Glass and Cao - 1997 - Adaptive Page Replacement Based on Memory Referenc.pdf},
  isbn = {978-0-89791-909-8},
  series = {{{SIGMETRICS}} '97}
}

@inproceedings{gomezOptimizingMemoryBandwidth2004,
  title = {Optimizing the Memory Bandwidth with Loop Morphing},
  booktitle = {Proceedings. 15th {{IEEE International Conference}} on {{Application}}-{{Specific Systems}}, {{Architectures}} and {{Processors}}, 2004.},
  author = {Gomez, J.I. and Marchal, P. and Verdoorlaege, S. and Pinuel, L. and Catthoor, L.},
  year = {2004},
  month = sep,
  pages = {213--223},
  issn = {2160-0511},
  doi = {10.1109/ASAP.2004.1342472},
  abstract = {The memory bandwidth largely determines the performance of embedded systems. However, very often compilers ignore the actual behavior of the memory architecture, causing large performance loss. To better utilize the memory bandwidth, several researchers have introduced instruction scheduling/data assignment techniques. Because they only optimize the bandwidth inside each basic block, they often fail to use all available bandwidth. Loop fusion is an interesting alternative to more globally optimize the memory access schedule. By fusing loops we increase the number of independent memory operations inside each basic block. The compiler can then better exploit the available bandwidth and increase the system's performance. However, existing fusion techniques can only combine loops with a conformable header. To overcome this limitation we present loop morphing; we combine fusion with strip mining and loop splitting. We also introduce a technique to steer loop morphing such that we find a compact memory access schedule. Experimental results show that with our approach we can decrease the execution time up to 88\%.},
  file = {/Users/jonathanrainer/Zotero/storage/BYWGTP86/Gomez et al. - 2004 - Optimizing the memory bandwidth with loop morphing.pdf;/Users/jonathanrainer/Zotero/storage/LB42VFU5/1342472.html},
  keywords = {Bandwidth,bandwidth allocation,Costs,data assignment,Delay,Embedded system,embedded systems,instruction scheduling,loop fusion,loop morphing,loop splitting,memory access scheduling,memory architecture,Memory architecture,memory bandwidth optimization,optimisation,Optimizing compilers,Performance loss,Processor scheduling,program compilers,program control structures,Runtime,scheduling,strip mining,Strips}
}

@inproceedings{gornishIntegratedHardwareSoftware1994,
  title = {An {{Integrated Hardware}}/{{Software Data Prefetching Scheme}} for {{Shared}}-{{Memory Multiprocessors}}},
  booktitle = {1994 {{Internatonal Conference}} on {{Parallel Processing Vol}}. 2},
  author = {Gornish, E.H. and Veidenbaum, A.},
  year = {1994},
  month = aug,
  volume = {2},
  pages = {281--284},
  issn = {null},
  doi = {10.1109/ICPP.1994.57},
  abstract = {Both hardware and software prefetching have been shown to be effective in tolerating the large memory latencies inherent in in in shared-memory multiprocessors; however, both types of prefetching have their shortcomings. In this paper, we propose an integrated hardware/software prefetching method that uses simple hardware that can handle most data accesses and software prefetching for the few remaining accesses. This yields an effective scheme that minimizes both CPU overhead and hardware costs. Execution-driven simulations show our method to be very effective.},
  file = {/Users/jonathanrainer/Zotero/storage/ZGVD7Y55/Gornish and Veidenbaum - 1994 - An Integrated HardwareSoftware Data Prefetching S.pdf;/Users/jonathanrainer/Zotero/storage/38JY4SQR/5727800.html}
}

@incollection{gramacyAdaptiveCachingRefetching2003,
  title = {Adaptive {{Caching}} by {{Refetching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  author = {Gramacy, Robert B and Warmuth, Manfred K. K and Brandt, Scott A. and Ari, Ismail},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2003},
  pages = {1489--1496},
  publisher = {{MIT Press}},
  file = {/Users/jonathanrainer/Zotero/storage/RQLA4MCZ/Gramacy et al. - 2003 - Adaptive Caching by Refetching.pdf;/Users/jonathanrainer/Zotero/storage/KBZ9XU4J/2296-adaptive-caching-by-refetching.html}
}

@inproceedings{guptaPreeminentPairReplacement2009,
  title = {Preeminent Pair of Replacement Algorithms for {{L1}} and {{L2}} Cache for Proxy Server},
  booktitle = {2009 {{First Asian Himalayas International Conference}} on {{Internet}}},
  author = {Gupta, R. and Tokekar, S.},
  year = {2009},
  month = nov,
  pages = {1--5},
  doi = {10.1109/AHICI.2009.5340334},
  abstract = {Access to the internet and WWW is growing extensively, which results in heavy network traffic. To reduce the network traffic proxy server is used. Proxy server reduces the load of server. If the cache replacement algorithm of proxy server's cache is efficient then proxy server will be helpful to reduce the network traffic in more efficient manner. In this paper we are considering proxy server cache to be Level 1 (L1) cache and storage cache of proxy server to be Level 2 (L2) cache. For collecting the real trace CC proxy server is used in an organization. Log of proxy server gives the information of various URLs accessed by various clients with time. For performing experiments various URLs were given a numeric identity. This paper proposes an efficient replacement algorithm on L1 for proxy server. The replacement algorithms taken into consideration are Least Recently Used (LRU), Least Frequently Used (LFU), First In First Out (FIFO). Access Pattern of L1 and L2 are different as if the desired page is not on L1 then it accesses to L2. Thus L1 is having better temporal locality than L2. Thus the replacement algorithm which is giving efficient results for L1 may not be suitable for L2. This paper also proposes a preeminent pair of replacement algorithms for L1 and L2 cache for proxy server.},
  file = {/Users/jonathanrainer/Zotero/storage/XFCP27DN/Gupta and Tokekar - 2009 - Preeminent pair of replacement algorithms for L1 a.pdf;/Users/jonathanrainer/Zotero/storage/KF4B3I2S/5340334.html},
  keywords = {cache replacement algorithm,cache storage,Cache storage,client,Computer science,Costs,Delay,heavy network traffic,Internet,IP networks,least frequently used algorithms,least recently used algorithms,Level 1 cache,Level 1 Cache (L1),Level 2 cache,Level 2 Cache (L2),network servers,Network servers,network traffic proxy server,Proxy server,proxy server cache,Replacement Algorithm,storage cache,Telecommunication traffic,Uniform resource locators,web access pattern,Web server,World Wide Web}
}

@inproceedings{gustafssonMalardalenWCETBenchmarks2010a,
  title = {The {{M{\"a}lardalen WCET Benchmarks}} -- {{Past}}, {{Present}} and {{Future}}},
  booktitle = {Proceedings 10th {{International Workshop}} on {{Worst}}-{{Case Execution Time Analysis}} ({{WCET}}'2010)},
  author = {Gustafsson, Jan and Betts, Adam and Ermedahl, Andreas and Lisper, Bj{\"o}rn},
  editor = {Lisper, Bj{\"o}rn},
  year = {2010},
  month = jul,
  pages = {137--147},
  publisher = {{OCG}},
  address = {{Brussels, Belgium}}
}

@inproceedings{guTheoryPotentialLRUMRU2011,
  title = {On the {{Theory}} and {{Potential}} of {{LRU}}-{{MRU Collaborative Cache Management}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Management}}},
  author = {Gu, Xiaoming and Ding, Chen},
  year = {2011},
  pages = {43--54},
  publisher = {{ACM}},
  address = {{San Jose, California, USA}},
  doi = {10.1145/1993478.1993485},
  abstract = {The goal of cache management is to maximize data reuse. Collaborative caching provides an interface for software to communicate access information to hardware. In theory, it can obtain optimal cache performance. In this paper, we study a collaborative caching system that allows a program to choose different caching methods for its data. As an interface, it may be used in arbitrary ways, sometimes optimal but probably suboptimal most times and even counter productive. We develop a theoretical foundation for collaborative caches to show the inclusion principle and the existence of a distance metric we call LRU-MRU stack distance. The new stack distance is important for program analysis and transformation to target a hierarchical collaborative cache system rather than a single cache configuration. We use 10 benchmark programs to show that optimal caching may reduce the average miss ratio by 24\%, and a simple feedback-driven compilation technique can utilize collaborative cache to realize 50\% of the optimal improvement.},
  file = {/Users/jonathanrainer/Zotero/storage/SXHBKDQ4/Gu and Ding - 2011 - On the Theory and Potential of LRU-MRU Collaborati.pdf},
  isbn = {978-1-4503-0263-0},
  keywords = {bipartite cache,cache replacement algorithm,collaborative caching,lru,mru,opt},
  series = {{{ISMM}} '11}
}

@inproceedings{hadkeDesignEvaluationOptical2008,
  title = {Design and Evaluation of an Optical {{CPU}}-{{DRAM}} Interconnect},
  booktitle = {2008 {{IEEE International Conference}} on {{Computer Design}}},
  author = {Hadke, A. and Benavides, T. and Amirtharajah, R. and Farrens, M. and Akella, V.},
  year = {2008},
  month = oct,
  pages = {492--497},
  doi = {10.1109/ICCD.2008.4751906},
  abstract = {We present OCDIMM (Optically Connected DIMM), a CPU-DRAM interface that uses multiwavelength optical interconnects. We show that OCDIMM is more scalable and offers higher bandwidth and lower latency than FBDIMM (Fully-Buffered DIMM), a state-of-the-art electrical alternative. Though OCDIMM is more power efficient than FBDIMM, we show that ultimately the total power consumption in the memory subsystem is a key impediment to scalability and thus to achieving truly balanced computing systems in the terascale era.},
  file = {/Users/jonathanrainer/Zotero/storage/DGPSV7V7/Hadke et al. - 2008 - Design and evaluation of an optical CPU-DRAM inter.pdf;/Users/jonathanrainer/Zotero/storage/SBIIRJH9/4751906.html},
  keywords = {balanced computing,Bandwidth,Clocks,CPU-DRAM interface,Delay,DRAM chips,Energy consumption,Frequency,fully-buffered DIMM,memory subsystem,Multicore processing,Optical computing,optical CPU-DRAM interconnect,Optical design,optical interconnections,Optical interconnections,Scalability,total power consumption}
}

@inproceedings{hallnorFullyAssociativeSoftwaremanaged2000,
  title = {A Fully Associative Software-Managed Cache Design},
  booktitle = {Proceedings of 27th {{International Symposium}} on {{Computer Architecture}} ({{IEEE Cat}}. {{No}}.{{RS00201}})},
  author = {Hallnor, E.G. and Reinhardt, S.K.},
  year = {2000},
  month = jun,
  pages = {107--116},
  issn = {1063-6897},
  doi = {10.1145/339647.339660},
  abstract = {As DRAM access latencies approach a thousand instruction-execution times and on-chip caches grow to multiple megabytes, it is not clear that conventional cache structures continue to be appropriate. Two key features-full associativity and software management-have been used successfully in the virtual-memory domain to cope with disk access latencies. Future systems will need to employ similar techniques to deal with DRAM latencies. This paper presents a practical, fully associative, software-managed secondary cache system that provides performance competitive with or superior to traditional caches without OS or application involvement. We see this structure as the first step toward OS- and application-aware management of large on-chip caches. This paper has two primary contributions a practical design for a fully associative memory structure, the indirect index cache (IIC), and a novel replacement algorithm, generational replacement, that is specifically designed to work with the IIC. We analyze the behavior of an IIC with generational replacement as a drop-in, transparent substitute for a conventional secondary cache. We achieve miss rate reductions from 8\% to 85\% relative to a 4-way associative LRU organization, matching or beating a (practically infeasible) fully associative true LRU cache. Incorporating these miss rates into a rudimentary timing model indicates that the IIC/generational replacement cache could be competitive with a conventional cache at today's DRAM latencies, and will outperform a conventional cache as these CPU-relative latencies grow.},
  file = {/Users/jonathanrainer/Zotero/storage/8GADT5LA/Hallnor and Reinhardt - 2000 - A fully associative software-managed cache design.pdf;/Users/jonathanrainer/Zotero/storage/2RIKFP5N/854382.html},
  keywords = {Aging,Algorithm design and analysis,Application software,associative memory,cache storage,Computer architecture,Computer science,content-addressable storage,Delay,Laboratories,Microprocessors,Prefetching,Random access memory,secondary cache system,software management,software-managed cache}
}

@inproceedings{hameedAdaptiveCacheManagement2013a,
  title = {Adaptive Cache Management for a Combined {{SRAM}} and {{DRAM}} Cache Hierarchy for Multi-Cores},
  booktitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Hameed, Fazal and Bauer, Lars and Henkel, J{\"o}rg},
  year = {2013},
  month = mar,
  pages = {77--82},
  issn = {1530-1591},
  doi = {10.7873/DATE.2013.030},
  abstract = {On-chip DRAM caches may alleviate the memory bandwidth problem in future multi-core architectures through reducing off-chip accesses via increased cache capacity. For memory intensive applications, recent research has demonstrated the benefits of introducing high capacity on-chip L4-DRAM as Last-Level-Cache between L3-SRAM and off-chip memory. These multi-core cache hierarchies attempt to exploit the latency benefits of L3-SRAM and capacity benefits of L4-DRAM caches. However, not taking into consideration the cache access patterns of complex applications can cause inter-core DRAM interference and inter-core cache contention. In this paper, we contest to re-architect existing cache hierarchies by proposing a hybrid cache architecture, where the Last-Level-Cache is a combination of SRAM and DRAM caches. We propose an adaptive DRAM placement policy in response to the diverse requirements of complex applications with different cache access behaviors. It reduces inter-core DRAM interference and inter-core cache contention in SRAM/DRAM-based hybrid cache architectures: increasing the harmonic mean instruction-per-cycle throughput by 23.3\% (max. 56\%) and 13.3\% (max. 35.1\%) compared to state-of-the-art.},
  file = {/Users/jonathanrainer/Zotero/storage/FF82WQMF/Hameed et al. - 2013 - Adaptive cache management for a combined SRAM and .pdf;/Users/jonathanrainer/Zotero/storage/EZGBU67P/6513476.html},
  keywords = {Arrays,Interference,Magnetic cores,Phase change random access memory,Radiation detectors,System-on-chip}
}

@inproceedings{hameedReducingLatencySRAM2014,
  title = {Reducing Latency in an {{SRAM}}/{{DRAM}} Cache Hierarchy via a Novel {{Tag}}-{{Cache}} Architecture},
  booktitle = {2014 51st {{ACM}}/{{EDAC}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Hameed, F. and Bauer, L. and Henkel, J.},
  year = {2014},
  month = jun,
  pages = {1--6},
  doi = {10.1145/2593069.2593197},
  abstract = {Memory speed has become a major performance bottleneck as more and more cores are integrated on a multi-core chip. The widening latency gap between high speed cores and memory has led to the evolution of multi-level SRAM/DRAM cache hierarchies that exploit the latency benefits of smaller caches (e.g. private L1 and L2 SRAM caches) and the capacity benefits of larger caches (e.g. shared L3 SRAM and shared L4 DRAM cache). The main problem of employing large L3/L4 caches is their high tag lookup latency. To solve this problem, we introduce the novel concept of small and low latency SRAM/DRAM Tag-Cache structures that can quickly determine whether an access to the large L3/L4 caches will be a hit or a miss. The performance of the proposed Tag-Cache architecture depends upon the Tag-Cache hit rate and to improve it we propose a novel Tag-Cache insertion policy and a DRAM row buffer mapping policy that reduce the latency of memory requests. For a 16-core system, this improves the average harmonic mean instruction per cycle throughput of latency sensitive applications by 13.3\% compared to state-of-the-art.},
  file = {/Users/jonathanrainer/Zotero/storage/9RFWJJ2X/Hameed et al. - 2014 - Reducing latency in an SRAMDRAM cache hierarchy v.pdf;/Users/jonathanrainer/Zotero/storage/H4LE6X34/6881364.html},
  keywords = {Arrays,average harmonic mean instruction,cache storage,DRAM chips,DRAM row buffer mapping policy,high speed cores,high tag lookup latency,Monitoring,multi-core chip,Multicore processing,Organizations,private L1 SRAM caches,private L2 SRAM caches,Program processors,Radiation detectors,Random access memory,shared L3 SRAM cache,shared L4 DRAM cache,SRAM chips,SRAM/DRAM cache hierarchy,tag-cache architecture,tag-cache insertion policy,widening latency gap}
}

@inproceedings{hashemiAcceleratingDependentCache2016,
  title = {Accelerating {{Dependent Cache Misses}} with an {{Enhanced Memory Controller}}},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Hashemi, M. and {Khubaib} and Ebrahimi, E. and Mutlu, O. and Patt, Y. N.},
  year = {2016},
  month = jun,
  pages = {444--455},
  doi = {10.1109/ISCA.2016.46},
  abstract = {On-chip contention increases memory access latency for multi-core processors. We identify that this additional latency has a substantial effect on performance for an important class of latency-critical memory operations: those that result in a cache miss and are dependent on data from a prior cache miss. We observe that the number of instructions between the first cache miss and its dependent cache miss is usually small. To minimize dependent cache miss latency, we propose adding just enough functionality to dynamically identify these instructions at the core and migrate them to the memory controller for execution as soon as source data arrives from DRAM. This migration allows memory requests issued by our new Enhanced Memory Controller (EMC) to experience a 20\% lower latency than if issued by the core. On a set of memory intensive quad-core workloads, the EMC results in a 13\% improvement in system performance and a 5\% reduction in energy consumption over a system with a Global History Buffer prefetcher, the highest performing prefetcher in our evaluation.},
  file = {/Users/jonathanrainer/Zotero/storage/AKW5FWD2/Hashemi et al. - 2016 - Accelerating Dependent Cache Misses with an Enhanc.pdf;/Users/jonathanrainer/Zotero/storage/J5AFZFAB/7551413.html},
  keywords = {Benchmark testing,cache miss latency,cache storage,Correlation,Delays,dependent cache misses,DRAM,DRAM chips,Electromagnetic compatibility,EMC,energy consumption,enhanced memory controller,global history buffer prefetcher,latency-critical memory operations,memory access latency,memory intensive quad-core workloads,memory requests,multicore processors,multiprocessing systems,on-chip contention,Prefetching,Random access memory,system performance,System-on-chip}
}

@inproceedings{hassanChargeCacheReducingDRAM2016,
  title = {{{ChargeCache}}: {{Reducing DRAM}} Latency by Exploiting Row Access Locality},
  shorttitle = {{{ChargeCache}}},
  booktitle = {2016 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Hassan, H. and Pekhimenko, G. and Vijaykumar, N. and Seshadri, V. and Lee, D. and Ergin, O. and Mutlu, O.},
  year = {2016},
  month = mar,
  pages = {581--593},
  doi = {10.1109/HPCA.2016.7446096},
  abstract = {DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called ChargeCache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.},
  file = {/Users/jonathanrainer/Zotero/storage/CZS9BHC3/Hassan et al. - 2016 - ChargeCache Reducing DRAM latency by exploiting r.pdf;/Users/jonathanrainer/Zotero/storage/UMFE5G4A/7446096.html},
  keywords = {cache storage,Capacitors,ChargeCache,Computer architecture,DRAM chips,DRAM latency reduction,DRAM request,Hardware,memory controller,multicore system,Parallel processing,row access locality,row address,single-core system,storage allocation,system performance bottleneck,Timing,timing parameters}
}

@inproceedings{hassanOffChipMemoryLatency2018,
  title = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}: {{Is DDR DRAM Really}} the {{Best Option}}?},
  shorttitle = {On the {{Off}}-{{Chip Memory Latency}} of {{Real}}-{{Time Systems}}},
  booktitle = {2018 {{IEEE Real}}-{{Time Systems Symposium}} ({{RTSS}})},
  author = {Hassan, M.},
  year = {2018},
  month = dec,
  pages = {495--505},
  doi = {10.1109/RTSS.2018.00062},
  abstract = {Predictable execution time upon accessing shared memories in multi-core real-time systems is a stringent requirement. A plethora of existing works focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR DRAMs), or redesigning its memory to provide predictable memory behavior. In this paper, we show that DDR DRAMs by construction suffer inherent limitations associated with achieving such predictability. These limitations lead to 1) highly variable access latencies that fluctuate based on various factors such as access patterns and memory state from previous accesses, and 2) overly pessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some real-time systems that mandate a strict predictable performance with tight timing constraints. Targeting these systems, we promote an alternative off-chip memory solution that is based on the emerging Reduced Latency DRAM (RLDRAM) protocol, and propose a predictable memory controller (RLDC) managing accesses to this memory. Comparing with the state-of-the-art predictable DDR controllers, the proposed solution provides up to 11\texttimes{} less timing variability and 6.4\texttimes{} reduction in the worst case memory latency.},
  file = {/Users/jonathanrainer/Zotero/storage/NAAUU4UL/Hassan - 2018 - On the Off-Chip Memory Latency of Real-Time System.pdf;/Users/jonathanrainer/Zotero/storage/7SPVFUU6/8603238.html},
  keywords = {alternative off-chip memory solution,Data transfer,DDR DRAM,Delays,double data rate dynamic random access memories,DRAM,DRAM chips,Latency Analysis,Memory,multicore real-time systems,multiprocessing systems,off-chip memory latency,pessimistic latency bounds,Predictability,predictable execution time,predictable memory controller,Protocols,Random access memory,real-time systems,Real-time systems,Real-Time Systems,reduced latency DRAM protocol,RLDRAM protocol,state-of-the-art predictable DDR controllers,Task analysis,tight timing constraints,variable access latencies}
}

@book{hennessyComputerArchitectureQuantitative2019,
  title = {Computer {{Architecture}}: A {{Quantitative Approach}}},
  shorttitle = {Computer Architecture},
  author = {Hennessy, John L. and Patterson, David A.},
  year = {2019},
  edition = {Sixth},
  publisher = {{Morgan Kaufmann Publishers}},
  address = {{Cambridge, MA}},
  file = {/Users/jonathanrainer/Zotero/storage/JV6VPGZ7/Hennessy - Computer Architecture A Quantitative Approach.pdf},
  isbn = {978-0-12-811905-1},
  keywords = {Computer architecture},
  lccn = {QA76.9.A73 P377 2019}
}

@inproceedings{hongAVICAAccesstimeVariation2013,
  title = {{{AVICA}}: {{An}} Access-Time Variation Insensitive {{L1}} Cache Architecture},
  shorttitle = {{{AVICA}}},
  booktitle = {2013 {{Design}}, {{Automation Test}} in {{Europe Conference Exhibition}} ({{DATE}})},
  author = {Hong, Seokin and Kim, Soontae},
  year = {2013},
  month = mar,
  pages = {65--70},
  issn = {1530-1591},
  doi = {10.7873/DATE.2013.028},
  abstract = {Ever scaling process technology increases variations in transistors. The process variations cause large fluctuations in the access times of SRAM cells. Caches made of those SRAM cells cannot be accessed within the target clock cycle time, which reduces yield of processors. To combat these access time failures in caches, many schemes have been proposed, which are, however, limited in their coverage and do not scale well at high failure rates. We propose a new L1 cache architecture (AVICA) employing asymmetric pipelining and pseudo multi-banking. Asymmetric pipelining eliminates all access time failures in L1 caches. Pseudo multi-banking minimizes the performance impact of asymmetric pipelining. For further performance improvement, architectural techniques are proposed. Our experimental results show that our proposed L1 cache architecture incurs less than 1\% performance hit compared to the conventional cache architecture with no access time failure. Our proposed architecture is not sensitive to access time failure rates and has low overheads compared to the previously proposed competitive schemes.},
  file = {/Users/jonathanrainer/Zotero/storage/F9IEP5K8/Hong and Kim - 2013 - AVICA An access-time variation insensitive L1 cac.pdf;/Users/jonathanrainer/Zotero/storage/9ZZRAYBU/6513474.html},
  keywords = {Bandwidth,Benchmark testing,Computer architecture,Microprocessors,Pipeline processing,Program processors,Transistors}
}

@inproceedings{hormdeeAsynchronousVictimCache2002,
  title = {An Asynchronous Victim Cache},
  booktitle = {Proceedings {{Euromicro Symposium}} on {{Digital System Design}}. {{Architectures}}, {{Methods}} and {{Tools}}},
  author = {Hormdee, D. and Garside, J.D. and Furber, S.B.},
  year = {2002},
  month = sep,
  pages = {4--11},
  issn = {null},
  doi = {10.1109/DSD.2002.1115345},
  abstract = {Memory bandwidth is a limiting factor with many modem microprocessors and it is usual to include a cache to reduce the amount of memory traffic. Of the two commonly used cache write-policies, the copy-back approach is better than the write-through approach in this respect. The performance of both approaches can be further aided by the inclusion of a small buffer in the path of outgoing writes to the main memory, especially if this buffer is capable of forwarding its contents back into the main cache if they are needed again before they are emptied from the buffer This is what is known as a victim cache. For an asynchronous microprocessor it is logical that the cache system should be asynchronous as well; since a large degree of the flexibility of an asynchronous microprocessor would be lost if it were to use a standard synchronous memory interface. However implementing a forwarding mechanism in an asynchronous system is more difficult because the data to be forwarded is flowing in a manner unsynchronised to the process which requires it. This paper presents an architecture for a victim cache to resolve forwarding in a totally asynchronous environment. The resultant structure forms a key part of an asynchronous copy-back cache system for the Amulet3, a third generation asynchronous implementation of the ARM processor.},
  file = {/Users/jonathanrainer/Zotero/storage/BC3FCYEW/Hormdee et al. - 2002 - An asynchronous victim cache.pdf;/Users/jonathanrainer/Zotero/storage/WAEWLRW2/1115345.html},
  keywords = {Algorithm design and analysis,Amulet3,ARM processor,asynchronous copy-back cache system,asynchronous victim cache,Bandwidth,cache storage,cache write-policies,Computer science,copy-back approach,Costs,Delay,Digital systems,forwarding mechanism,Hardware,limiting factor,memory architecture,memory bandwidth,microprocessor chips,microprocessors,Microprocessors,third generation asynchronous implementation,Writing}
}

@inproceedings{huangATCacheReducingDRAM2014,
  title = {{{ATCache}}: {{Reducing DRAM}} Cache Latency via a Small {{SRAM}} Tag Cache},
  shorttitle = {{{ATCache}}},
  booktitle = {2014 23rd {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Huang, C. and Nagarajan, V.},
  year = {2014},
  month = aug,
  pages = {51--60},
  doi = {10.1145/2628071.2628089},
  abstract = {3D-stacking technology has enabled the option of embedding a large DRAM onto the processor. Prior works have proposed to use this as a DRAM cache. Because of its large size (a DRAM cache can be in the order of hundreds of megabytes), the total size of the tags associated with it can also be quite large (in the order of tens of megabytes). The large size of the tags has created a problem. Should we maintain the tags in the DRAM and pay the cost of a costly tag access in the critical path? Or should we maintain the tags in the faster SRAM by paying the area cost of a large SRAM for this purpose? Prior works have primarily chosen the former and proposed a variety of techniques for reducing the cost of a DRAM tag access. In this paper, we first establish (with the help of a study) that maintaining the tags in SRAM, because of its smaller access latency, leads to overall better performance. Motivated by this study, we ask if it is possible to maintain tags in SRAM without incurring high area overhead. Our key idea is simple. We propose to cache the tags in a small SRAM tag cache - we show that there is enough spatial and temporal locality amongst tag accesses to merit this idea. We propose the ATCache which is a small SRAM tag cache. Similar to a conventional cache, the ATCache caches recently accessed tags to exploit temporal locality; it exploits spatial locality by prefetching tags from nearby cache sets. In order to avoid the high miss latency and cache pollution caused by excessive prefetching, we use a simple technique to throttle the number of sets prefetched. Our proposed ATCache (which consumes 0.4\% of overall tag size) can satisfy over 60\% of DRAM cache tag accesses on average.},
  file = {/Users/jonathanrainer/Zotero/storage/5JV8LE9V/Huang and Nagarajan - 2014 - ATCache Reducing DRAM cache latency via a small S.pdf;/Users/jonathanrainer/Zotero/storage/SIJII4GP/7855888.html},
  keywords = {3D-stacking technology,ATCache,cache pollution,cache storage,Compounds,Design,DRAM cache,DRAM cache latency,DRAM chips,Media,Performance,Pollution,Prefetching,prefetching tags,Random access memory,Servers,small SRAM tag cache,SRAM chips,storage management,Systems architecture}
}

@inproceedings{huApplicationsOnchipTrace2007,
  title = {Applications of {{On}}-Chip {{Trace}} on {{Debugging Embedded Processor}}},
  booktitle = {Eighth {{ACIS International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}}, and {{Parallel}}/{{Distributed Computing}} ({{SNPD}} 2007)},
  author = {Hu, X. and Chen, S.},
  year = {2007},
  month = jul,
  volume = {1},
  pages = {140--145},
  doi = {10.1109/SNPD.2007.227},
  abstract = {On-chip trace system records run-time information of the embedded processor with special hardware to overcome the obstacle of non-intrusive debug and optimization of traditional techniques. This paper explores the mechanism, characteristics and applications of on-chip trace technique with TraceDo, an on-chip trace system of a multi-core SoC. The functions and structures of TraceDo are introduced, the working process of path trace is explained and the trace applications with two cases of debug and optimization are also discussed.},
  file = {/Users/jonathanrainer/Zotero/storage/QQYXBLJF/Hu and Chen - 2007 - Applications of On-chip Trace on Debugging Embedde.pdf;/Users/jonathanrainer/Zotero/storage/9PGPS2UP/4287490.html},
  keywords = {Application software,Concurrent computing,debugging embedded processor,Distributed computing,embedded system,Embedded system,embedded systems,Hardware,microprocessor chips,multicore SoC,Network-on-a-chip,on-chip trace system,performance evaluation,Real time systems,Software debugging,software reliability,Software tools,System-on-a-chip,TraceDo}
}

@inproceedings{jainBackFutureLeveraging2016,
  title = {Back to the {{Future}}: {{Leveraging Belady}}'s {{Algorithm}} for {{Improved Cache Replacement}}},
  shorttitle = {Back to the {{Future}}},
  booktitle = {2016 {{ACM}}/{{IEEE}} 43rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Jain, A. and Lin, C.},
  year = {2016},
  month = jun,
  pages = {78--89},
  doi = {10.1109/ISCA.2016.17},
  abstract = {Belady's algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady's algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efficient, as we introduce a new method of efficiently simulating Belady's behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4\%, as opposed to 6.2\% for the previous state-of-the-art. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0\%, compared to 12.0\% for the previous state-of-the-art.},
  file = {/Users/jonathanrainer/Zotero/storage/KG9345TE/Jain and Lin - 2016 - Back to the Future Leveraging Belady's Algorithm .pdf;/Users/jonathanrainer/Zotero/storage/XIN4JXM5/7551384.html},
  keywords = {4-core system,Art,Belady algorithm,Belady behavior,Belady's Algorithm,Benchmark testing,cache accesses,cache replacement,Cache replacement,cache storage,Hardware,History,LLC,Marine vehicles,memory-intensive subset,Optimized production technology,Prediction algorithms,replacement state,sampling techniques,tag array}
}

@inproceedings{jainSoftwareassistedCacheReplacement2001,
  title = {Software-Assisted Cache Replacement Mechanisms for Embedded Systems},
  author = {Jain, P. and Devadas, S. and Engels, D. and Rudolph, L.},
  year = {2001},
  pages = {119--126},
  publisher = {{IEEE}},
  doi = {10.1109/ICCAD.2001.968607},
  abstract = {We address the problem of improving cache predictability and performance in embedded systems through the use of softwareassisted replacement mechanisms. These mechanisms require additional software controlled state information that affects the cache replacement decision. Software instructions allow a program to kill a particular cache element, i.e., effectively make the element the least recently used element, or keep that cache element, i.e., the element will never be evicted.},
  file = {/Users/jonathanrainer/Zotero/storage/VSPKXH5W/Jain et al. - 2001 - Software-assisted cache replacement mechanisms for.pdf},
  isbn = {978-0-7803-7247-4},
  language = {en}
}

@inproceedings{jaleelHighPerformanceCache2010,
  title = {High Performance Cache Replacement Using Re-Reference Interval Prediction ({{RRIP}})},
  booktitle = {Proceedings of the 37th Annual International Symposium on {{Computer}} Architecture - {{ISCA}} '10},
  author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
  year = {2010},
  pages = {60},
  publisher = {{ACM Press}},
  address = {{Saint-Malo, France}},
  doi = {10.1145/1815961.1815971},
  file = {/Users/jonathanrainer/Zotero/storage/R2TMJIAW/Jaleel et al. - 2010 - High performance cache replacement using re-refere.pdf},
  isbn = {978-1-4503-0053-7},
  language = {en}
}

@inproceedings{jeongCostsensitiveCacheReplacement2003,
  title = {Cost-Sensitive Cache Replacement Algorithms},
  author = {Jeong, J. and Dubois, M.},
  year = {2003},
  pages = {327--337},
  publisher = {{IEEE Comput. Soc}},
  doi = {10.1109/HPCA.2003.1183550},
  abstract = {Cache replacement algorithms originally developed in the context of simple uniprocessor systems aim to reduce the miss count. However, in modern systems, cache misses have different costs. The cost may be latency, penalty, power consumption, bandwidth consumption, or any other ad-hoc numerical property attached to a miss. In many practical situations, it is desirable to inject the cost of a miss into the replacement policy.},
  file = {/Users/jonathanrainer/Zotero/storage/NADWS3CC/Jeong and Dubois - 2003 - Cost-sensitive cache replacement algorithms.pdf},
  isbn = {978-0-7695-1871-8},
  language = {en}
}

@inproceedings{jeongOptimalReplacementsCaches1999,
  title = {Optimal {{Replacements}} in {{Caches}} with {{Two Miss Costs}}},
  booktitle = {Proceedings of the {{Eleventh Annual ACM Symposium}} on {{Parallel Algorithms}} and {{Architectures}}},
  author = {Jeong, Jaeheon and Dubois, Michel},
  year = {1999},
  pages = {155--164},
  publisher = {{ACM}},
  address = {{Saint Malo, France}},
  doi = {10.1145/305619.305636},
  file = {/Users/jonathanrainer/Zotero/storage/XZBS3D8B/Jeong and Dubois - 1999 - Optimal Replacements in Caches with Two Miss Costs.pdf},
  isbn = {978-1-58113-124-6},
  series = {{{SPAA}} '99}
}

@inproceedings{jevdjicUnisonCacheScalable2014,
  title = {Unison {{Cache}}: {{A Scalable}} and {{Effective Die}}-{{Stacked DRAM Cache}}},
  shorttitle = {Unison {{Cache}}},
  booktitle = {2014 47th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Jevdjic, D. and Loh, G. H. and Kaynak, C. and Falsafi, B.},
  year = {2014},
  month = dec,
  pages = {25--37},
  doi = {10.1109/MICRO.2014.51},
  abstract = {Recent research advocates large die-stacked DRAM caches in many core servers to break the memory latency and bandwidth wall. To realize their full potential, die-stacked DRAM caches necessitate low lookup latencies, high hit rates and the efficient use of off-chip bandwidth. Today's stacked DRAM cache designs fall into two categories based on the granularity at which they manage data: block-based and page-based. The state-of-the-art block-based design, called Alloy Cache, collocates a tag with each data block (e.g., 64B) in the stacked DRAM to provide fast access to data in a single DRAM access. However, such a design suffers from low hit rates due to poor temporal locality in the DRAM cache. In contrast, the state-of-the-art page-based design, called Footprint Cache, organizes the DRAM cache at page granularity (e.g., 4KB), but fetches only the blocks that will likely be touched within a page. In doing so, the Footprint Cache achieves high hit rates with moderate on-chip tag storage and reasonable lookup latency. However, multi-gigabyte stacked DRAM caches will soon be practical and needed by server applications, thereby mandating tens of MBs of tag storage even for page-based DRAM caches. We introduce a novel stacked-DRAM cache design, Unison Cache. Similar to Alloy Cache's approach, Unison Cache incorporates the tag metadata directly into the stacked DRAM to enable scalability to arbitrary stacked-DRAM capacities. Then, leveraging the insights from the Footprint Cache design, Unison Cache employs large, page-sized cache allocation units to achieve high hit rates and reduction in tag overheads, while predicting and fetching only the useful blocks within each page to minimize the off-chip traffic. Our evaluation using server workloads and caches of up to 8GB reveals that Unison cache improves performance by 14\% compared to Alloy Cache due to its high hit rate, while outperforming the state-of-the art page-based designs that require impractical SRAM-based tags of around 50MB.},
  file = {/Users/jonathanrainer/Zotero/storage/XUD7AKME/Jevdjic et al. - 2014 - Unison Cache A Scalable and Effective Die-Stacked.pdf;/Users/jonathanrainer/Zotero/storage/YZW8T95W/7011375.html},
  keywords = {3D die stacking,Alloy Cache approach,Bandwidth,bandwidth wall,block-based data management,cache storage,caches,die-stacked DRAM cache,DRAM,DRAM access,DRAM chips,footprint cache design,lookup latencies,manycore servers,memory,memory latency,Metals,multigigabyte stacked DRAM caches,off-chip bandwidth,off-chip traffic,on-chip tag storage,Organizations,page granularity,page-based data management,page-based DRAM caches,page-sized cache allocation units,paged storage,Random access memory,Resource management,server workloads,servers,Servers,SRAM chips,SRAM-based tags,stacked-DRAM capacities,System-on-chip,Unison cache}
}

@inproceedings{jiangLIRSEfficientLow2002,
  title = {{{LIRS}}: An Efficient Low Inter-Reference Recency Set Replacement Policy to Improve Buffer Cache Performance},
  shorttitle = {{{LIRS}}},
  booktitle = {Proceedings of the 2002 {{ACM SIGMETRICS}} International Conference on {{Measurement}} and Modeling of Computer Systems  - {{SIGMETRICS}} '02},
  author = {Jiang, Song and Zhang, Xiaodong},
  year = {2002},
  pages = {31},
  publisher = {{ACM Press}},
  address = {{Marina Del Rey, California}},
  doi = {10.1145/511334.511340},
  file = {/Users/jonathanrainer/Zotero/storage/EIQV69FC/Jiang and Zhang - LIRS An Efﬁcient Low Inter-reference Recency Set .pdf},
  isbn = {978-1-58113-531-2},
  language = {en}
}

@inproceedings{johnDesignPerformanceEvaluation1997,
  title = {Design and Performance Evaluation of a Cache Assist to Implement Selective Caching},
  booktitle = {Proceedings {{International Conference}} on {{Computer Design VLSI}} in {{Computers}} and {{Processors}}},
  author = {John, L.K. and Subramanian, A.},
  year = {1997},
  month = oct,
  pages = {510--518},
  issn = {1063-6404},
  doi = {10.1109/ICCD.1997.628916},
  abstract = {Conventional cache architectures exploit locality, but do so rather blindly. By forcing all references through a single structure, the cache's effectiveness on many references is reduced. This paper presents a cache assist namely the annex cache which implements a selective caching scheme. Except for filling a main cache at cold start, all entries come to the cache via the annex cache. Items referenced only rarely will be excluded from the main cache, eliminating several conflict misses. The basic premise is that an item deserves to be in the main cache only if it can prove its right to exist in the main cache by demonstrating locality. The annex cache combines the features of Jouppi's (1990) victim caches and McFarling's (1992) cache exclusion schemes. Extensive simulation studies for annex and victim caches using a variety of SPEC programs are presented in the paper. Annex caches were observed to be significantly better than conventional caches, better than victim caches in certain cases, and comparable to victim caches in other cases.},
  file = {/Users/jonathanrainer/Zotero/storage/6F86HIH8/John and Subramanian - 1997 - Design and performance evaluation of a cache assis.pdf;/Users/jonathanrainer/Zotero/storage/UDZYZTBC/628916.html},
  keywords = {annex cache,cache architectures,cache exclusion schemes,cache storage,cache storage design,Computer science,conflict misses,Costs,Delay,Filling,locality,memory architecture,performance evaluation,Pollution control,Read-write memory,references,selective caching,selective caching scheme,simulation,SPEC programs,storage management,victim cache}
}

@inproceedings{johnson2QLowOverhead1994,
  title = {{{2Q}}: {{A Low Overhead High Performance Buffer Management Replacement Algorithm}}},
  shorttitle = {{{2Q}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Very Large Data Bases}}},
  author = {Johnson, Theodore and Shasha, Dennis},
  year = {1994},
  month = sep,
  pages = {439--450},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  file = {/Users/jonathanrainer/Zotero/storage/AFJKYPES/Johnson et al. - Q A Low Overhead High Performance Buffer Manageme.pdf},
  isbn = {978-1-55860-153-6},
  series = {{{VLDB}} '94}
}

@article{jongmoochoiDesignImplementationPerformance2002,
  title = {Design, Implementation, and Performance Evaluation of a Detection-Based Adaptive Block Replacement Scheme},
  author = {{Jongmoo Choi} and Noh, S.H. and {Sang Lyul Min} and {Eun-Yong Ha} and {Yookun Cho}},
  year = {2002},
  month = jul,
  volume = {51},
  pages = {793--800},
  issn = {0018-9340},
  doi = {10.1109/TC.2002.1017699},
  abstract = {{\DH}A new buffer replacement scheme, called DEAR (DEtection-based Adaptive Replacement), is presented for effective caching of disk blocks in the operating system. The proposed DEAR scheme automatically detects block reference patterns of applications and applies different replacement policies to different applications depending on the detected reference pattern. The detection is made by a periodic process and is based on the relationship between block attribute values, such as backward distance and frequency gathered in a period, and the forward distance observed in the next period. This paper also describes an implementation and performance measurement of the DEAR scheme in FreeBSD. The results from performance measurements of several real applications show that, compared with the LRU scheme, the proposed scheme reduces the number of disk I/Os by up to 51 percent (with an average of 23 percent) and the response time by up to 35 percent (with an average of 12 percent) in the case of single application executions. For multiple application executions, the results show that the proposed scheme reduces the number of disk I/Os by up to 20 percent (with an average of 12 percent) and the overall response time by up to 18 percent (with an average of 8 percent).},
  file = {/Users/jonathanrainer/Zotero/storage/LWSRAZEX/Jongmoo Choi et al. - 2002 - Design, implementation, and performance evaluation.pdf},
  journal = {IEEE Transactions on Computers},
  language = {en},
  number = {7}
}

@inproceedings{jouppiImprovingDirectmappedCache1990,
  title = {Improving {{Direct}}-Mapped {{Cache Performance}} by the {{Addition}} of a {{Small Fully}}-Associative {{Cache}} and {{Prefetch Buffers}}},
  booktitle = {Proceedings of the 17th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Jouppi, Norman P.},
  year = {1990},
  pages = {364--373},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/325164.325162},
  abstract = {Projections of computer technology forecast processors with peak performance of 1,000 MIPS in the relatively near future. These processors could easily lose half or more of their performance in the memory hierarchy if the hierarchy design is based on conventional caching techniques. This paper presents hardware techniques to improve the performance of caches.
Miss caching places a small fully-associative cache between a cache and its refill path. Misses in the cache that hit in the miss cache have only a one cycle miss penalty, as opposed to a many cycle miss penalty without the miss cache. Small miss caches of 2 to 5 entries are shown to be very effective in removing mapping conflict misses in first-level direct-mapped caches.
Victim caching is an improvement to miss caching that loads the small fully-associative cache with the victim of a miss and not the requested line. Small victim caches of 1 to 5 entries are even more effective at removing conflict misses than miss caching.
Stream buffers prefetch cache lines starting at a cache miss address. The prefetched data is placed in the buffer and not in the cache. Stream buffers are useful in removing capacity and compulsory cache misses, as well as some instruction cache conflict misses. Stream buffers are more effective than previously investigated prefetch techniques at using the next slower level in the memory hierarchy when it is pipelined. An extension to the basic stream buffer, called multi-way stream buffers, is introduced. Multi-way stream buffers are useful for prefetching along multiple intertwined data reference streams.
Together, victim caches and stream buffers reduce the miss rate of the first level in the cache hierarchy by a factor of two to three on a set of six large benchmarks.},
  file = {/Users/jonathanrainer/Zotero/storage/GK96BXW6/Jouppi - 1990 - Improving Direct-mapped Cache Performance by the A.pdf},
  isbn = {978-0-89791-366-9},
  series = {{{ISCA}} '90}
}

@inproceedings{jouppiTradeoffsTwolevelOnchip1994,
  title = {Tradeoffs in Two-Level on-Chip Caching},
  booktitle = {Proceedings of 21 {{International Symposium}} on {{Computer Architecture}}},
  author = {Jouppi, N.P. and Wilton, S.J.E.},
  year = {1994},
  month = apr,
  pages = {34--45},
  issn = {null},
  doi = {10.1109/ISCA.1994.288163},
  abstract = {The performance of two-level on-chip caching is investigated for a range of technology and architecture assumptions. The area and access time of each level of cache is modeled in detail. The results indicate that for most workloads, two-level cache configurations (with a set-associative second level) perform marginally better than single-level cache configurations that require the same chip area once the first-level cache sizes are 64 KB or larger. Two-level configurations become even more important in systems with no off-chip cache and in systems in which the memory cells in the first-level caches are multiported and hence larger than those in the second-level cache. Finally, a new replacement policy called two-level exclusive caching is introduced. Two-level exclusive caching improves the performance of two-level caching organizations by increasing the effective associativity and capacity.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/BHU63L5P/Jouppi and Wilton - 1994 - Tradeoffs in two-level on-chip caching.pdf;/Users/jonathanrainer/Zotero/storage/88ZLLWUC/288163.html},
  keywords = {associativity,Bandwidth,Bonding,buffer storage,capacity,Delay,Educational institutions,memory architecture,Microprocessors,Modems,on-chip caching,Process design,replacement policy,set-associative second level,storage management,two-level cache configurations,two-level exclusive caching,two-level on-chip caching}
}

@inproceedings{ju-hotangPerformanceDesignChoices1994,
  title = {Performance and Design Choices of Level-Two Caches},
  booktitle = {1994 {{Proceedings}} of the {{Twenty}}-{{Seventh Hawaii International Conference}} on {{System Sciences}}},
  author = {{Ju-Ho Tang} and Kimming So},
  year = {1994},
  month = jan,
  volume = {1},
  pages = {422--430},
  issn = {null},
  doi = {10.1109/HICSS.1994.323143},
  abstract = {The increasing disparity of speed between processor and its main memory makes ways for multi-level cache hierarchies in almost any of today's computer systems; specifically, the second-level (L2) caches with larger capacity but longer access time than the first-level (L1) caches have been adopted to reduce this memory gap. In this study an enhanced one-pass trace-driven simulation technique is used to evaluate the design choices of three L2 cache parameters: associativity, line size, and cache size. The traces are from large workloads of both commercial and scientific applications.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/JNVSUSVP/Ju-Ho Tang and Kimming So - 1994 - Performance and design choices of level-two caches.pdf;/Users/jonathanrainer/Zotero/storage/AMKX3ULA/323143.html},
  keywords = {associativity,buffer storage,cache size,Computational modeling,content-addressable storage,Delay,Graphics,level-two caches,line size,memory architecture,Microprocessors,multi-level cache hierarchies,one-pass trace-driven simulation,Operating systems,performance,performance evaluation,Prefetching,Silicon,Workstations}
}

@inproceedings{juanImprovedMulticoreShared2012,
  title = {An {{Improved Multi}}-Core {{Shared Cache Replacement Algorithm}}},
  booktitle = {2012 11th {{International Symposium}} on {{Distributed Computing}} and {{Applications}} to {{Business}}, {{Engineering Science}}},
  author = {Juan, F. and Chengyan, L.},
  year = {2012},
  month = oct,
  pages = {13--17},
  doi = {10.1109/DCABES.2012.39},
  abstract = {Many multi-core processors employ a large last-level cache (LLC) shared among the multiple cores. Past research has demonstrated that traditional LRU and its approximation can lead to poor performance and unfairness when the multiple cores compete for the limited LLC capacity, and is susceptible to thrashing for memory-intensive workloads that have a working set greater than the available cache size. As the LLC grows in capacity, associativity, the performance gap between the LRU and the theoretical optimal replacement algorithms has widened. In this paper, we propose FLRU (Frequency based LRU) replacement algorithm, which is applied to multi-core shared L2 cache, and it takes the recent access information, partition and the frequency information into consideration. FLRU manages to filter the less reused blocks through dynamic insertion/promotion policy and victim selection strategy to ensure that some fraction of the working set is retained in the cache so that at least that fraction of the working set can contribute to cache hits and to avoid trashing, meanwhile we augment traditional cache partition with victim selection, insertion and promotion policies to manage shared L2 caches.},
  file = {/Users/jonathanrainer/Zotero/storage/88TYZ4JD/Juan and Chengyan - 2012 - An Improved Multi-core Shared Cache Replacement Al.pdf;/Users/jonathanrainer/Zotero/storage/GPBN84E6/6385229.html},
  keywords = {access information,cache storage,Computers,dynamic insertion-promotion policy,Educational institutions,FLRU,frequency based LRU replacement algorithm,frequency information,last-level cache,LLC,memory-intensive workloads,multi-core,Multicore processing,multicore processors,multicore shared cache replacement algorithm,multicore shared L2 cache,multiprocessing systems,partition information,Partitioning algorithms,Radiation detectors,replacement,Runtime,shared cache,USA Councils,victim selection strategy}
}

@inproceedings{kampeSelfCorrectingLRUReplacement2004,
  title = {Self-{{Correcting LRU Replacement Policies}}},
  booktitle = {In {{Proceedings}} of the 1st {{Conference}} on {{Computing Frontiers}}},
  author = {Kampe, Martin and Stenstrom, Per and Dubois, Michel},
  year = {2004},
  pages = {181--191},
  abstract = {With wider associativity the replacement algorithm becomes critical. Although LRU makes many good replacement decisions, the wide performance gap between OPT, the optimum off-line algorithm, and LRU suggests that LRU still makes too many mistakes. Self-correcting},
  file = {/Users/jonathanrainer/Zotero/storage/7TKWKSHY/Kampe et al. - 2004 - Self-Correcting LRU Replacement Policies.pdf;/Users/jonathanrainer/Zotero/storage/GKRS54DW/summary.html}
}

@article{karedlaCachingStrategiesImprove1994,
  title = {Caching Strategies to Improve Disk System Performance},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  year = {1994},
  month = mar,
  volume = {27},
  pages = {38--46},
  issn = {0018-9162},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/KLR6PJKM/Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;/Users/jonathanrainer/Zotero/storage/ETYTQD2H/268884.html},
  journal = {Computer},
  keywords = {Algorithm design and analysis,buffer storage,cache replacement algorithms,caching strategies,computer system hierarchy,Control systems,Costs,data throughput,Delay,disk rotation speeds,disk system performance,Drives,frequency-based variation,Hardware,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,latency,least recently used algorithm,magnetic disc storage,Operating systems,performance evaluation,random replacement,segmented LRU,solid-state disks,System performance,system response time,Terminology,Throughput},
  number = {3}
}

@article{karedlaCachingStrategiesImprove1994a,
  title = {Caching Strategies to Improve Disk System Performance},
  author = {Karedla, R. and Love, J. S. and Wherry, B. G.},
  year = {1994},
  month = mar,
  volume = {27},
  pages = {38--46},
  doi = {10.1109/2.268884},
  abstract = {I/O subsystem manufacturers attempt to reduce latency by increasing disk rotation speeds, incorporating more intelligent disk scheduling algorithms, increasing I/O bus speed, using solid-state disks, and implementing caches at various places in the I/O stream. In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size. After explaining some basic caching issues, we examine some popular caching strategies and cache replacement algorithms, as well as the advantages and disadvantages of caching at different levels of the computer system hierarchy. Finally, we investigate the performance of three cache replacement algorithms: random replacement (RR), least recently used (LRU), and a frequency-based variation of LRU known as segmented LRU (SLRU).{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/MWSHVCFE/Karedla et al. - 1994 - Caching strategies to improve disk system performa.pdf;/Users/jonathanrainer/Zotero/storage/FBWVQRK9/268884.html},
  journal = {Computer},
  keywords = {Algorithm design and analysis,buffer storage,cache replacement algorithms,caching strategies,computer system hierarchy,Control systems,Costs,data throughput,Delay,disk rotation speeds,disk system performance,Drives,frequency-based variation,Hardware,I/O bus speed,I/O subsystem,intelligent disk scheduling algorithms,latency,least recently used algorithm,magnetic disc storage,Operating systems,performance evaluation,random replacement,segmented LRU,solid-state disks,System performance,system response time,Terminology,Throughput},
  number = {3}
}

@techreport{kegleyPredictiveCacheModeling2011,
  title = {Predictive {{Cache Modeling}} and {{Analysis}}},
  author = {Kegley, Russell and Preston, Jonathan and Dougherty, Brian and White, Jules and Gokhale, Anirudda},
  year = {2011},
  month = nov,
  address = {{Fort Worth, TX}},
  institution = {{Lockheed Martin Aeronautics Corporation}},
  abstract = {This work applied particle swarm heuristic optimization techniques to the problem of finding a near-optimal order in which to schedule tasks in a real-time embedded system in order to minimize cache miss rates experienced by the software. Reducing the number of cache misses is an important component of runtime execution efficiency. We demonstrated runtime reductions of 3-5\% in execution time, significant for embedded systems attempting to add new capability without upgrading hardware. The expectation is that these gains can be improved further by the use of hardware with pseudo-LRU cache behavior.},
  file = {/Users/jonathanrainer/Zotero/storage/M5W66TST/Kegley et al. - 2011 - Predictive Cache Modeling and Analysis.pdf;/Users/jonathanrainer/Zotero/storage/W6K7EAKC/ADA552968.html},
  language = {en},
  number = {AFRL-RI-RS-TR-2011-266},
  type = {Technical {{Report}}}
}

@article{kellyVariableQosShared1999,
  title = {Variable {{Qos}} from {{Shared Web Caches}}: {{User}}-{{Centered Design}} and {{Value}}-{{Sensitive Replacement}}},
  shorttitle = {Variable {{Qos}} from {{Shared Web Caches}}},
  author = {Kelly, Terence and Jamin, Sugih and {MacKie-Mason}, Jeffrey K.},
  year = {1999},
  issn = {1556-5068},
  doi = {10.2139/ssrn.975737},
  abstract = {Due to differences in server capacity, external bandwidth, and client demand, some Web servers value cache hits more than others. Assuming that a shared cache knows the extent to which different servers value hits, it may employ a value-sensitive replacement policy in order to generate higher aggregate value for servers. We consider both the prediction and value aspects of this problem and introduce a novel value-sensitive LFU/LRU hybrid that biases the allocation of cache space toward documents whose origin servers value caching most highly. We compare our algorithm with others from the Web caching literature and discuss from an economic standpoint the problems associated with obtaining servers' private valuation information.},
  file = {/Users/jonathanrainer/Zotero/storage/RQ8TT62R/Kelly et al. - 1999 - Variable Qos from Shared Web Caches User-Centered.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@inproceedings{kelwadeReputationBasedCache2017,
  title = {Reputation Based Cache Management Policy for Performance Improvement},
  booktitle = {2017 {{International Conference}} on {{Intelligent Sustainable Systems}} ({{ICISS}})},
  author = {Kelwade, K. and Sahu, S. and Kawade, G. and Korde, N. and Upadhye, S. and Motghare, M.},
  year = {2017},
  month = dec,
  pages = {582--587},
  doi = {10.1109/ISS1.2017.8389236},
  abstract = {As the performance gap between CPU and main memory speed increases, memory subsystem design more critical. Caches are important part of modern memory hierarchies. Cache performance is important in computer system. As the cache size is limited it should be properly utilized. After the cache block is used by an application it should be moved from cache to main memory so that another block required by other application can use its space [2]. Now a day's multi-level cache are used everywhere to provide better cache performance. As the block which is moved from cache memory to main memory may be required in future, so it is kept in lower level cache in the cache hierarchy. Therefore the cost of accessing the block from main memory is minimized. Many approaches like LRU-K, LFU, LRFU, PROMOTE, DEMOTE are used to increase cache performance by promoting or demoting a memory block into the cache depending upon its latest history [1] [21]. Some are used for single level cache and some for hierarchical cache. The drawback of existing multi-level cache management techniques is that it uses hints of level L1 only. It does not provide sufficient information about the history of the cache block at all previous cache levels. Therefore, a hybrid cache management policy is proposed in this paper which takes its replacement decision based on the multiple level cache level. In this paper, a hybrid cache management policy is proposed which constitute the feature of two existing algorithms PEOMOTE and DEMTE that gives better hit ratio than other existing policies.},
  file = {/Users/jonathanrainer/Zotero/storage/XXVPEPZ4/Kelwade et al. - 2017 - Reputation based cache management policy for perfo.pdf;/Users/jonathanrainer/Zotero/storage/4CQIK3WJ/8389236.html},
  keywords = {cache memory,Cache performance,cache storage,Conferences,demote,DEMTE algorithms,hierarchical cache,hierarchical systems,hints,hybrid cache management policy,memory block,memory subsystem design,modern memory hierarchies,Multi-level cache,multilevel cache management,multiple level cache level,PEOMOTE algorithms,promote,recency,reputation based cache management policy,single level cache,storage management}
}

@inproceedings{keramidasCacheReplacementBased2007,
  title = {Cache Replacement Based on Reuse-Distance Prediction},
  author = {Keramidas, Georgios and Petoumenos, Pavlos and Kaxiras, Stefanos},
  year = {2007},
  month = oct,
  pages = {245--250},
  publisher = {{IEEE}},
  doi = {10.1109/ICCD.2007.4601909},
  abstract = {Several cache management techniques have been proposed that indirectly try to base their decisions on cacheline reuse-distance, like Cache Decay which is a postdiction of reuse-distances: if a cacheline has not been accessed for some "decay interval" we know that its reuse-distance is at least as large as this decay interval. In this work, we propose to directly predict reuse-distances via instruction-based (PC) prediction and use this information for cache level optimizations. In this paper, we choose as our target for optimization the replacement policy of the L2 cache, because the gap between the LRU and the theoretical optimal replacement algorithm is comparatively large for L2 caches. This indicates that, in many situations, there is ample room for improvement. We evaluate our reusedistance based replacement policy using a subset of the most memory intensive SPEC2000 and our results show significant benefits across the board.},
  file = {/Users/jonathanrainer/Zotero/storage/9XVX79PW/Keramidas et al. - 2007 - Cache replacement based on reuse-distance predicti.pdf},
  isbn = {978-1-4244-1257-0},
  language = {en}
}

@inproceedings{kesslerInexpensiveImplementationsSetAssociativity1989,
  title = {Inexpensive {{Implementations Of Set}}-{{Associativity}}},
  booktitle = {The 16th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Kessler, R.E. and Jooss, R. and Lebeck, A. and Hill, M.D.},
  year = {1989},
  month = may,
  pages = {131--139},
  issn = {1063-6897},
  doi = {10.1109/ISCA.1989.714547},
  abstract = {The traditional approach to implementing wide set- associativity is expensive, requiring a wide tag memory (directory) and many comparators. Here we examine alternative implementations of associativity that use hardware similar to that used to implement a direct-mapped cache. One approach scans tags serially from most-recently used to least-recently used. Another uses a partial compare of a few bits from each tag to reduce the number of tags that must be examined serially. The drawback of both approaches is that they increase cache access time by a factor of two or more over the traditional implementation of set- associativity, making them inappropriate for cache designs in which a fast access time is crucial (e.g. level one caches, caches directly servicing processor requests). These schemes are useful, however, if (1) the low miss ratio of wide set-associative caches is desired, (2) the low cost of a direct-mapped implementation is preferred, and (3) the slower access time of these approaches can be tolerated. We expect these conditions to be true for caches in multiprocessors designed to reduce memory interconnection traffic, caches implemented with large, narrow memory chips, and level two (or higher) caches in a cache hierarchy.},
  file = {/Users/jonathanrainer/Zotero/storage/9BSPD8NP/Kessler et al. - 1989 - Inexpensive Implementations Of Set-Associativity.pdf;/Users/jonathanrainer/Zotero/storage/WNG87YS7/714547.html},
  keywords = {Costs,Delay effects,Distributed computing,Educational institutions,Hardware,Information retrieval,Machinery,Permission,Traffic control}
}

@inproceedings{khalidPerformanceEvaluationNew1996,
  title = {Performance Evaluation of a New Cache Replacement Scheme Using {{SPEC}}},
  booktitle = {Conference {{Proceedings}} of the 1996 {{IEEE Fifteenth Annual International Phoenix Conference}} on {{Computers}} and {{Communications}}},
  author = {Khalid, H. and Obaidat, M. S.},
  year = {1996},
  month = mar,
  pages = {144--150},
  doi = {10.1109/PCCC.1996.493626},
  abstract = {Presents a new neural network-based algorithm called KORA (Khalid-Obaidat Replacement Algorithm), that uses a backpropagation neural network (BPNN) for the purpose of guiding the line/block replacement decisions in a cache. The KORA algorithm attempts to approximate the replacement decisions made by the optimal scheme (OPT). The key to our algorithm is to identify and subsequently discard the dead lines in cache memories. This allows our algorithm to provide better cache performance as compared to the conventional LRU (least recently used), MRU (most recently used) and FIFO (first-in, first-out) replacement policies. Extensive trace-driven simulations were performed for 30 different cache configurations using different SPEC (Standard Performance Evaluation Corp.) programs. Simulation results have shown that KORA can provide a substantial improvement in the miss ratio over the conventional algorithms. Our work opens up new dimensions for research in the development of new and improved page replacement schemes for virtual memory systems and disk caches.},
  file = {/Users/jonathanrainer/Zotero/storage/PKS4BSLC/493626.html},
  keywords = {backpropagation,Backpropagation algorithms,backpropagation neural network,block replacement decisions,cache configurations,Cache memory,cache miss ratio,cache replacement scheme,cache storage,Cities and towns,Costs,dead line discarding,discrete event simulation,disk caches,Educational institutions,feedforward neural nets,High performance computing,History,Khalid-Obaidat Replacement Algorithm,KORA algorithm,line replacement decisions,Neural networks,optimal scheme,Optimized production technology,page replacement schemes,paged storage,performance evaluation,Performance evaluation,SPEC benchmark programs,Standard Performance Evaluation Corp.,trace-driven simulations,virtual memory systems}
}

@inproceedings{khanDecoupledDynamicCache2012,
  title = {Decoupled Dynamic Cache Segmentation},
  booktitle = {{{IEEE International Symposium}} on {{High}}-{{Performance Comp Architecture}}},
  author = {Khan, S. M. and Wang, Z. and Jim{\'e}nez, D. A.},
  year = {2012},
  month = feb,
  pages = {1--12},
  doi = {10.1109/HPCA.2012.6169030},
  abstract = {The least recently used (LRU) replacement policy performs poorly in the last-level cache (LLC) because temporal locality of memory accesses is filtered by first and second level caches. We propose a cache segmentation technique that dynamically adapts to cache access patterns by predicting the best number of not-yet-referenced and already-referenced blocks in the cache. This technique is independent from the LRU policy so it can work with less expensive replacement policies. It can automatically detect when to bypass blocks to the CPU with no extra overhead. In a 2MB LLC single-core processor with a memory intensive subset of SPEC CPU 2006 benchmarks, it outperforms LRU replacement on average by 5.2\% with not-recently-used (NRU) replacement and on average by 2.2\% with random replacement. The technique also complements existing shared cache partitioning techniques. Our evaluation with 10 multi-programmed workloads shows that this technique improves performance of an 8MB LLC four-core system on average by 12\%, with a random replacement policy requiring only half the space of the LRU policy.},
  file = {/Users/jonathanrainer/Zotero/storage/B7WXJSBQ/Khan et al. - 2012 - Decoupled dynamic cache segmentation.pdf;/Users/jonathanrainer/Zotero/storage/92KZJS84/6169030.html},
  keywords = {2MB LLC single-core processor,8MB LLC four-core system,already-referenced block prediction,Benchmark testing,cache access patterns,cache storage,Decision trees,decoupled dynamic cache segmentation,Electronics packaging,last-level cache,least recently used replacement policy,LRU replacement,memory access temporal locality,memory intensive subset,microprocessor chips,not-recently-used replacement,not-yet-referenced block prediction,off-chip memory latency mitigation,on-chip caches,Proposals,Resistance,Runtime,shared cache partitioning techniques,SPEC CPU 2006 benchmarks,System-on-a-chip}
}

@inproceedings{khanUsingDeadBlocks2010,
  title = {Using Dead Blocks as a Virtual Victim Cache},
  booktitle = {2010 19th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Khan, Samira and Burger, Doug and Jim{\'e}nez, Daniel A. and Falsafi, Babak},
  year = {2010},
  month = sep,
  pages = {489--500},
  issn = {null},
  abstract = {Caches mitigate the long memory latency that limits the performance of modern processors. However, caches can be quite inefficient. On average, a cache block in a 2MB L2 cache is dead 59\% of the time, i.e., it will not be referenced again before it is evicted. Increasing cache efficiency can improve performance by reducing miss rate, or alternately, improve power and energy by allowing a smaller cache with the same miss rate.},
  file = {/Users/jonathanrainer/Zotero/storage/939TZX62/Khan et al. - 2010 - Using dead blocks as a virtual victim cache.pdf;/Users/jonathanrainer/Zotero/storage/F36VC7VX/7851546.html},
  keywords = {2MB L2 cache,Benchmark testing,cache block,cache management,cache storage,Coherence,dead blocks,long memory latency,Memory management,microarchitecture,miss rate,modern processors,performance evaluation,prediction,Prefetching,Radiation detectors,storage management chips,Throughput,virtual victim cache}
}

@inproceedings{kharbutliCounterbasedCacheReplacement2005,
  title = {Counter-Based Cache Replacement Algorithms},
  booktitle = {In {{Proceedings}} of {{ICCD}} 2005},
  author = {Kharbutli, Mazen and Solihin, Yan},
  year = {2005},
  abstract = {Recent studies have shown that in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, suggesting that alternative replacement algorithms can improve the performance of the cache. One main reason for this performance gap is that in the LRU replacement algorithm, a line is only evicted after it becomes the LRU line, long after its last access/touch, while unnecessarily occupying the cache space for a long time. This paper proposes a new approach to deal with the problem: counter-based L2 cache replacement. In this approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest, such as a cache access to the same set, occurs. When the counter exceeds a threshold, the line ``expires'', and becomes evictable. When expired lines are evicted early from the cache, they make extra space for lines that may be more useful, reducing the number of capacity and conflict misses. Each line's threshold is unique and is dynamically learned and stored in a small 40-Kbyte counter prediction table. We propose two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 (out of 21) SPEC2000 benchmarks by up to 40 \% and 11 \% on average. 1.},
  file = {/Users/jonathanrainer/Zotero/storage/5PF4PHWP/Kharbutli and Solihin - 2005 - Counter-based cache replacement algorithms.pdf;/Users/jonathanrainer/Zotero/storage/QZDW5SLD/summary.html}
}

@inproceedings{kharbutliCounterBasedCacheReplacement2005,
  title = {Counter-{{Based Cache Replacement Algorithms}}},
  booktitle = {Proceedings of the 2005 {{International Conference}} on {{Computer Design}}},
  author = {Kharbutli, Mazen and Solihin, Yan},
  year = {2005},
  month = oct,
  pages = {61--68},
  publisher = {{IEEE Computer Society}},
  address = {{USA}},
  doi = {10.1109/ICCD.2005.41},
  abstract = {Recent studies have shown that in highly associative caches, the performance gap between the Least Recently Used (LRU) and the theoretical optimal replacement algorithms is large, suggesting that alternative replacement algorithms can improve the performance of the cache. One main reason for this performance gap is that in the LRU replacement algorithm, a line is only evicted after it becomes the LRU line, long after its last access/touch, while unnecessarily occupying the cache space for a long time. This paper proposes a new approach to deal with the problem: counterbased L2 cache replacement . In this approach, each line in the L2 cache is augmented with an event counter that is incremented when an event of interest, such as a cache access to the same set, occurs. When the counter exceeds a threshold, the line "expires", and becomes evictable. When expired lines are evicted early from the cache, they make extra space for lines that may be more useful, reducing the number of capacity and conflict misses. Each line s threshold is unique and is dynamically learned and stored in a small 40-Kbyte counter prediction table. We propose two new replacement algorithms: Access Interval Predictor (AIP) and Live-time Predictor (LvP). AIP and LvP speed up 10 (out of 21) SPEC2000 benchmarks by up to 40\% and 11\% on average.},
  file = {/Users/jonathanrainer/Zotero/storage/VIA7DTVD/Kharbutli and Solihin - 2005 - Counter-Based Cache Replacement Algorithms.pdf},
  isbn = {978-0-7695-2451-1},
  series = {{{ICCD}} '05}
}

@article{kharbutliLACSLocalityAwareCostSensitive2014,
  title = {{{LACS}}: {{A Locality}}-{{Aware Cost}}-{{Sensitive Cache Replacement Algorithm}}},
  shorttitle = {{{LACS}}},
  author = {Kharbutli, Mazen and Sheikh, Rami},
  year = {2014},
  month = aug,
  volume = {63},
  pages = {1975--1987},
  issn = {2326-3814},
  doi = {10.1109/TC.2013.61},
  abstract = {The design of an effective last-level cache (LLC) in general-and an effective cache replacement/partitioning algorithm in particular-is critical to the overall system performance. The processor's ability to hide the LLC miss penalty differs widely from one miss to another. The more instructions the processor manages to issue during the miss, the better it is capable of hiding the miss penalty and the lower the cost of that miss. This nonuniformity in the processor's ability to hide LLC miss latencies, and the resultant nonuniformity in the performance impact of LLC misses, opens up an opportunity for a new cost-sensitive cache replacement algorithm. This paper makes two key contributions. First, It proposes a framework for estimating the costs of cache blocks at run-time based on the processor's ability to (partially) hide their miss latencies. Second, It proposes a simple, low-hardware overhead, yet effective, cache replacement algorithm that is locality-aware and cost-sensitive (LACS). LACS is thoroughly evaluated using a detailed simulation environment. LACS speeds up 12 LLC-performance-constrained SPEC CPU2006 benchmarks by up to 51\% and 11\% on average. When evaluated using a dual/quad-core CMP with a shared LLC, LACS significantly outperforms LRU in terms of performance and fairness, achieving improvements up to 54\%.},
  file = {/Users/jonathanrainer/Zotero/storage/NP56TFAU/Kharbutli and Sheikh - 2014 - LACS A Locality-Aware Cost-Sensitive Cache Replac.pdf;/Users/jonathanrainer/Zotero/storage/C5FAPT5T/6484053.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Aggregates,Algorithm design and analysis,Benchmark testing,cache block cost estimation,Cache memories,cache replacement algorithms,cache storage,caches,cost-sensitive cache replacement,costing,effective cache replacement-partitioning algorithm,Estimation,LACS,last-level cache,LLC miss penalty,locality-aware cost-sensitive cache replacement algorithm,low-hardware overhead,Optimization,Partitioning algorithms,Prediction algorithms,processor ability,shared caches},
  number = {8}
}

@inproceedings{kimAdaptiveNonuniformCache2002,
  title = {An Adaptive, Non-Uniform Cache Structure for Wire-Delay Dominated on-Chip Caches},
  booktitle = {Proceedings of the 10th International Conference on {{Architectural}} Support for Programming Languages and Operating Systems},
  author = {Kim, Changkyu and Burger, Doug and Keckler, Stephen W.},
  year = {2002},
  month = oct,
  pages = {211--222},
  publisher = {{Association for Computing Machinery}},
  address = {{San Jose, California}},
  doi = {10.1145/605397.605420},
  abstract = {Growing wire delays will force substantive changes in the designs of large caches. Traditional cache architectures assume that each level in the cache hierarchy has a single, uniform access time. Increases in on-chip communication delays will make the hit time of large on-chip caches a function of a line's physical location within the cache. Consequently, cache access times will become a continuum of latencies rather than a single discrete latency. This non-uniformity can be exploited to provide faster access to cache lines in the portions of the cache that reside closer to the processor. In this paper, we evaluate a series of cache designs that provides fast hits to multi-megabyte cache memories. We first propose physical designs for these Non-Uniform Cache Architectures (NUCAs). We extend these physical designs with logical policies that allow important data to migrate toward the processor within the same level of the cache. We show that, for multi-megabyte level-two caches, an adaptive, dynamic NUCA design achieves 1.5 times the IPC of a Uniform Cache Architecture of any size, outperforms the best static NUCA scheme by 11\%, outperforms the best three-level hierarchy--while using less silicon area--by 13\%, and comes within 13\% of an ideal minimal hit latency solution.},
  file = {/Users/jonathanrainer/Zotero/storage/B5WJKJAK/Kim et al. - 2002 - An adaptive, non-uniform cache structure for wire-.pdf},
  isbn = {978-1-58113-574-9},
  series = {{{ASPLOS X}}}
}

@inproceedings{kimLowoverheadHighperformanceUnified2000,
  title = {A {{Low}}-Overhead {{High}}-Performance {{Unified Buffer Management Scheme That Exploits Sequential}} and {{Looping References}}},
  booktitle = {Proceedings of the 4th {{Conference}} on {{Symposium}} on {{Operating System Design}} \& {{Implementation}} - {{Volume}} 4},
  author = {Kim, Jong Min and Choi, Jongmoo and Kim, Jesung and Noh, Sam H. and Min, Sang Lyul and Cho, Yookun and Kim, Chong Sang},
  year = {2000},
  publisher = {{USENIX Association}},
  address = {{San Diego, California}},
  abstract = {In traditional file system implementations, the Least Recently Used (LRU) block replacement scheme is widely used to manage the buffer cache due to its simplicity and adaptability. However, the LRU scheme exhibits performance degradations because it does not make use of reference regularities such as sequential and looping references. In this paper, we present a Unified Buffer Management (UBM) scheme that exploits these regularities and yet, is simple to deploy. The UBM scheme automatically detects sequential and looping references and stores the detected blocks in separate partitions of the buffer cache. These partitions are managed by appropriate replacement schemes based on their detected patterns. The allocation problem among the divided partitions is also tackled with the use of the notion of marginal gains. In both trace-driven simulation experiments and experimental studies using an actual implementation in the FreeBSD operating system, the performance gains obtained through the use of this scheme are substantial. The results show that the hit ratios improve by as much as 57.7\% (with an average of 29.2\%) and the elapsed times are reduced by as much as 67.2\% (with an average of 28.7\%) compared to the LRU scheme for the workloads we used.},
  file = {/Users/jonathanrainer/Zotero/storage/4HJFLF7N/Kim et al. - 2000 - A Low-overhead High-performance Unified Buffer Man.pdf},
  series = {{{OSDI}}'00}
}

@inproceedings{kimSolarDRAMReducingDRAM2018,
  title = {Solar-{{DRAM}}: {{Reducing DRAM Access Latency}} by {{Exploiting}} the {{Variation}} in {{Local Bitlines}}},
  shorttitle = {Solar-{{DRAM}}},
  booktitle = {2018 {{IEEE}} 36th {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Kim, J. and Patel, M. and Hassan, H. and Mutlu, O.},
  year = {2018},
  month = oct,
  pages = {282--291},
  doi = {10.1109/ICCD.2018.00051},
  abstract = {DRAM latency is a major bottleneck for many applications in modern computing systems. In this work, we rigorously characterize the effects of reducing DRAM access latency on 282 state-of-the-art LPDDR4 DRAM modules. As found in prior work on older DRAM generations (DDR3), we show that regions of LPDDR4 DRAM modules can be accessed with latencies that are significantly lower than manufacturer-specified values without causing failures. We present novel data that 1) further supports the viability of such latency reduction mechanisms and 2) exposes a variety of new cases in which access latencies can be effectively reduced. Using our observations, we propose a new low-cost mechanism, Solar-DRAM, that 1) identifies failure-prone regions of DRAM at reduced latency and 2) robustly reduces average DRAM access latency while maintaining data correctness, by issuing DRAM requests with reduced access latencies to non-failure-prone DRAM regions. We evaluate Solar-DRAM on a wide variety of multi-core workloads and show that for 4-core homogeneous workloads, Solar-DRAM provides an average (maximum) system performance improvement of 4.31\% (10.87\%) compared to using the default fixed DRAM access latency.},
  file = {/Users/jonathanrainer/Zotero/storage/FY5PMU3Y/Kim et al. - 2018 - Solar-DRAM Reducing DRAM Access Latency by Exploi.pdf;/Users/jonathanrainer/Zotero/storage/3PI38KHX/8615701.html},
  keywords = {average DRAM access,Capacitors,Decoding,DRAM access latency,DRAM chips,DRAM generations,DRAM requests,DRAM-Characterization,DRAM-Latency,failure analysis,latency reduction mechanisms,local bitlines,LPDDR4,LPDDR4 DRAM modules,Memory,Memory-Controllers,nonfailure-prone DRAM regions,Organizations,Process-Variation,Random access memory,reduced access latencies,Reliability,Solar-DRAM,Timing,Transistors}
}

@article{kiniwaLookaheadSchedulingRequests1998,
  title = {Lookahead {{Scheduling Requests}} for {{Efficient Paging}}},
  author = {Kiniwa, Jun and Kameda, Tiko},
  year = {1998},
  month = apr,
  volume = {1041},
  pages = {27--34},
  file = {/Users/jonathanrainer/Zotero/storage/PCLEMN47/Kiniwa and Kameda - Lookahead Scheduling Requests for Eﬃcient Paging.pdf},
  journal = {RIMS K{\^o}ky{\^u}roku},
  language = {en},
  series = {Algorithms and {{Theory}} of {{Computing}}}
}

@inproceedings{krishnapillaiRankSwitchingOpenRowDRAM2014,
  title = {A {{Rank}}-{{Switching}}, {{Open}}-{{Row DRAM Controller}} for {{Time}}-{{Predictable Systems}}},
  booktitle = {2014 26th {{Euromicro Conference}} on {{Real}}-{{Time Systems}}},
  author = {Krishnapillai, Y. and Wu, Z. P. and Pellizzoni, R.},
  year = {2014},
  month = jul,
  pages = {27--38},
  doi = {10.1109/ECRTS.2014.37},
  abstract = {We introduce ROC, a Rank-switching, Open-row Controller for Double Data Rate Dynamic RAM (DDR DRAM). ROC is optimized for mixed-criticality multicore systems using modern DDR devices: compared to existing real-time memory controllers, it provides significantly lower worst case latency bounds for hard real-time tasks and supports throughput-oriented optimizations for soft real-time applications. The key to improved performance is an innovative rank-switching mechanism which hides the latency of write-read transitions in DRAM devices without requiring unpredictable request reordering. We further employ open row policy to take advantage of the data caching mechanism (row buffering) in each device. ROC provides complete timing isolation between hard and soft tasks and allows for compositional timing analysis over the number of cores and memory ranks in the system. We implemented and synthesized the ROC back end in Verilog RTL, and evaluated its performance on both synthetic tasks and a set of representative benchmarks.},
  file = {/Users/jonathanrainer/Zotero/storage/K5UD57JB/Krishnapillai et al. - 2014 - A Rank-Switching, Open-Row DRAM Controller for Tim.pdf;/Users/jonathanrainer/Zotero/storage/GRGK4BQ6/6932587.html},
  keywords = {cache storage,Clocks,compositional timing analysis,data caching mechanism,DDR DRAM,Delays,double data rate dynamic RAM,DRAM chips,Memory management,microcontrollers,mixed-criticality multicore systems,multiprocessing systems,optimisation,Random access memory,rank-switching open-row DRAM controller,Real-time systems,ROC,row buffering,Switches,throughput-oriented optimizations,time-predictable systems,Verilog RTL,worst case latency bounds,write-read transition latency}
}

@inproceedings{kroftLockupfreeInstructionFetch1981,
  title = {Lockup-Free Instruction Fetch/Prefetch Cache Organization},
  booktitle = {Proceedings of the 8th Annual Symposium on {{Computer Architecture}}},
  author = {Kroft, David},
  year = {1981},
  month = may,
  pages = {81--87},
  publisher = {{IEEE Computer Society Press}},
  address = {{Minneapolis, Minnesota, USA}},
  abstract = {In the past decade, there has been much literature describing various cache organizations that exploit general programming idiosyncrasies to obtain maximum hit rate (the probability that a requested datum is now resident in the cache). Little, if any, has been presented to exploit: (1) the inherent dual input nature of the cache and (2) the many-datum reference type central processor instructions. No matter how high the cache hit rate is, a cache miss may impose a penalty on subsequent cache references. This penalty is the necessity of waiting until the missed requested datum is received from central memory and, possibly, for cache update. For the two cases above, the cache references following a miss do not require the information of the datum not resident in the cache, and are therefore penalized in this fashion. In this paper, a cache organization is presented that essentially eliminates this penalty. This cache organizational feature has been incorporated in a cache/memory interface subsystem design, and the design has been implemented and prototyped. An existing simple instruction set machine has verified the advantage of this feature; future, more extensive and sophisticated instruction set machines may obviously take more advantage. Prior to prototyping, simulations verified the advantage.},
  file = {/Users/jonathanrainer/Zotero/storage/PY6PR67U/Kroft - 1981 - Lockup-free instruction fetchprefetch cache organ.pdf},
  series = {{{ISCA}} '81}
}

@article{kumarOverviewHardwareBased2015,
  title = {An {{Overview}} of {{Hardware Based Cache Optimization Techniques}}},
  author = {Kumar, Swadhesh and Singh, Dr P K},
  year = {Sepetember 2015},
  volume = {4},
  pages = {159--166},
  issn = {2319-8354},
  abstract = {Cache Memory is a high speed semiconductor memory acts as a buffer between CPU and Main Memory. In current generation processors, the processor- memory bandwidth is the main bottleneck, because a number of processor cores sharing it through the same processor memory interface or bus. The on chip memory hierarchy is an important resource that should be managed efficiently against the raising performance gap between processor and memory. This Paper yields a comprehensive survey to improve the cache performance on the basis of miss rate, hit rate, latency, efficiency and cost.},
  file = {/Users/jonathanrainer/Zotero/storage/9EZ9GYBX/Kumar and Singh - AN OVERVIEW OF HARDWARE BASED CACHE OPTIMIZATION T.pdf},
  journal = {International Journal of Advance Research in Science and Engineering},
  language = {en},
  number = {Special Issue 1}
}

@inproceedings{kumarOverviewModernCache2016,
  title = {An Overview of Modern Cache Memory and Performance Analysis of Replacement Policies},
  booktitle = {2016 {{IEEE International Conference}} on {{Engineering}} and {{Technology}} ({{ICETECH}})},
  author = {Kumar, S. and Singh, P. K.},
  year = {2016},
  month = mar,
  pages = {210--214},
  doi = {10.1109/ICETECH.2016.7569243},
  abstract = {Memory hierarchy in current generation computers is formed by keeping registers inside, cache on or outside the processor and virtual memory on Hard disk. The principle of locality of reference is used to make memory hierarchy work efficiently. In recent years various advances have been made to improve the cache memory performance on the basis of hit rate, latency, speed, replacement policies and energy consumption. Cache replacement policy is one of the important design parameter which affects the overall processor performance and also become more important with recent technological moves towards highly associative cache. This paper yields a survey of current generation processors on the basis of various factors effecting cache memory performance and throughput. The main focus of this paper is the study and performance analysis of the cache replacement policies on the basis of simulation on several benchmarks.},
  file = {/Users/jonathanrainer/Zotero/storage/4LDS84RN/Kumar and Singh - 2016 - An overview of modern cache memory and performance.pdf;/Users/jonathanrainer/Zotero/storage/PEAXWT9X/7569243.html},
  keywords = {associative cache,benchmark testing,cache memory,Cache memory,Cache Performance,cache replacement policy,cache storage,Clocks,Conferences,current generation processors,design parameter,energy consumption,FIFO,hard disk,Hardware,hit rate,Hit rate,LFU,LRU,memory architecture,memory hierarchy,microprocessor chips,Miss rate,Multicore processing,Organizations,performance analysis,performance evaluation,performance improvement,processor performance,RANDOM,registers,Registers,Replacement Policy,virtual memory,virtual storage}
}

@inproceedings{laiDeadblockPredictionDeadblock2001,
  title = {Dead-Block Prediction Dead-Block Correlating Prefetchers},
  booktitle = {Proceedings 28th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Lai, An-Chow and Fide, C. and Falsafi, B.},
  year = {2001},
  month = jun,
  pages = {144--154},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2001.937443},
  abstract = {Effective data prefetching requires accurate mechanisms to predict both "which" cache blocks to prefetch and "when" to prefetch them. This paper proposes the Dead-Block Predictors (DBPs), trace-based predictors that accurately identify "when" an Ll data cache block becomes evictable or "dead". Predicting a dead block significantly enhances prefetching lookahead and opportunity, and enables placing data directly into Ll, obviating the need for auxiliary prefetch buffers. This paper also proposes Dead-Block Correlating Prefetchers (DBCPs), that use address correlation to predict "which" subsequent block to prefetch when a block becomes evictable. A DBCP enables effective data prefetching in a wide spectrum of pointer-intensive, integer, and floating-point applications. We use cycle-accurate simulation of an out-of-order superscalar processor and memory-intensive benchmarks to show that: (1) dead-block prediction enhances prefetching lookahead at least by an order of magnitude as compared to previous techniques, (2) a DBP can predict dead blocks on average with a coverage of 90\% only mispredicting 4\% of the time, (3) a DBCP offers an address prediction coverage of 86\% only mispredicting 3\% of the time, and (4) DBCPs improve performance by 62\% on average and 282\% at best in the benchmarks we studied.},
  file = {/Users/jonathanrainer/Zotero/storage/X4KKEDCV/An-Chow Lai et al. - 2001 - Dead-block prediction dead-block correlating prefe.pdf;/Users/jonathanrainer/Zotero/storage/58454CRN/937443.html},
  keywords = {address correlation,auxiliary prefetch buffers,benchmarks,cycle-accurate simulation,Data engineering,data prefetching,data structures,Data structures,dead-block correlating prefetchers,dead-block prediction,Delay,Engines,floating-point applications,Hardware,History,memory-intensive benchmarks,Out of order,out-of-order superscalar processor,parallel processing,performance evaluation,Prefetching,prefetching lookahead,Proposals,storage management,Sun}
}

@inproceedings{leeTieredlatencyDRAMLow2013,
  title = {Tiered-Latency {{DRAM}}: {{A}} Low Latency and Low Cost {{DRAM}} Architecture},
  shorttitle = {Tiered-Latency {{DRAM}}},
  booktitle = {2013 {{IEEE}} 19th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Lee, D. and Kim, Y. and Seshadri, V. and Liu, J. and Subramanian, L. and Mutlu, O.},
  year = {2013},
  month = feb,
  pages = {615--626},
  doi = {10.1109/HPCA.2013.6522354},
  abstract = {The capacity and cost-per-bit of DRAM have historically scaled to satisfy the needs of increasingly large and complex computer systems. However, DRAM latency has remained almost constant, making memory latency the performance bottleneck in today's systems. We observe that the high access latency is not intrinsic to DRAM, but a trade-off made to decrease cost-per-bit. To mitigate the high area overhead of DRAM sensing structures, commodity DRAMs connect many DRAM cells to each sense-amplifier through a wire called a bitline. These bitlines have a high parasitic capacitance due to their long length, and this bitline capacitance is the dominant source of DRAM latency. Specialized low-latency DRAMs use shorter bitlines with fewer cells, but have a higher cost-per-bit due to greater sense-amplifier area overhead. In this work, we introduce Tiered-Latency DRAM (TL-DRAM), which achieves both low latency and low cost-per-bit. In TL-DRAM, each long bitline is split into two shorter segments by an isolation transistor, allowing one segment to be accessed with the latency of a short-bitline DRAM without incurring high cost-per-bit. We propose mechanisms that use the low-latency segment as a hardware-managed or software-managed cache. Evaluations show that our proposed mechanisms improve both performance and energy-efficiency for both single-core and multi-programmed workloads.},
  file = {/Users/jonathanrainer/Zotero/storage/85DI4VUK/Lee et al. - 2013 - Tiered-latency DRAM A low latency and low cost DR.pdf;/Users/jonathanrainer/Zotero/storage/RWMTV3ES/6522354.html},
  keywords = {bitline,cache storage,Capacitance,Capacitors,complex computer systems,Computer architecture,DRAM capacity,DRAM chips,DRAM cost-per-bit,DRAM sensing structures,energy conservation,energy-efficiency,hardware-managed cache,high parasitic capacitance,isolation transistor,low-latency low-cost DRAM architecture,memory architecture,memory latency,multiprogrammed workloads,multiprogramming,performance evaluation,power aware computing,single-core workloads,software-managed cache,tiered-latency DRAM,Timing,TL-DRAM,transistors,Transistors}
}

@article{leeWhenPrefetchingWorks2012,
  title = {When {{Prefetching Works}}, {{When It Doesn}}'t, and {{Why}}},
  author = {Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  year = {2012},
  month = mar,
  volume = {9},
  pages = {2:1--2:29},
  issn = {1544-3566},
  doi = {10.1145/2133382.2133384},
  abstract = {In emerging and future high-end processor systems, tolerating increasing cache miss latency and properly managing memory bandwidth will be critical to achieving high performance. Prefetching, in both hardware and software, is among our most important available techniques for doing so; yet, we claim that prefetching is perhaps also the least well-understood. Thus, the goal of this study is to develop a novel, foundational understanding of both the benefits and limitations of hardware and software prefetching. Our study includes: source code-level analysis, to help in understanding the practical strengths and weaknesses of compiler- and software-based prefetching; a study of the synergistic and antagonistic effects between software and hardware prefetching; and an evaluation of hardware prefetching training policies in the presence of software prefetching requests. We use both simulation and measurement on real systems. We find, for instance, that although there are many opportunities for compilers to prefetch much more aggressively than they currently do, there is also a tangible risk of interference with training existing hardware prefetching mechanisms. Taken together, our observations suggest new research directions for cooperative hardware/software prefetching.},
  file = {/Users/jonathanrainer/Zotero/storage/KPNHA95N/Lee et al. - 2012 - When Prefetching Works, When It Doesn’t, and Why.pdf},
  journal = {ACM Transactions on Architecture and Code Optimization},
  keywords = {cache,Prefetching},
  number = {1}
}

@inproceedings{liCRFPNovelAdaptive2008,
  title = {{{CRFP}}: {{A Novel Adaptive Replacement Policy Combined}} the {{LRU}} and {{LFU Policies}}},
  shorttitle = {{{CRFP}}},
  booktitle = {2008 {{IEEE}} 8th {{International Conference}} on {{Computer}} and {{Information Technology Workshops}}},
  author = {Li, Z. and Liu, D. and Bi, H.},
  year = {2008},
  month = jul,
  pages = {72--79},
  doi = {10.1109/CIT.2008.Workshops.22},
  abstract = {A variety of cache replacement algorithms have been proposed and applied in different situations, in which the LRU (least recently used) and LFU (least frequently used) replacement policies are two of the most popular policies. However, most real systems donpsilat consider obtaining a maximized throughput by switching between the two policies in response to the access pattern. In this paper, we propose a novel adaptive replacement policy that combined the LRU and LFU Policies (CRFP); CRFP is self-tuning and can switch between different cache replacement policies adaptively and dynamically in response to the access pattern changes.Conducting simulations with a variety of file access patterns and a wide range of buffer size, we show that the CRFP outperforms other algorithms in many cases and performs the best in most of these cases.},
  file = {/Users/jonathanrainer/Zotero/storage/26J4WQQQ/Li et al. - 2008 - CRFP A Novel Adaptive Replacement Policy Combined.pdf;/Users/jonathanrainer/Zotero/storage/TIWG2ELW/4568482.html},
  keywords = {adaptive replacement policy,Bismuth,cache replacement algorithms,cache storage,Caching,Computers,Conferences,CRFP,Data engineering,Database systems,file access patterns,Frequency,Information technology,Laboratories,least frequently used replacement policy,least recently used replacement policy,LFU,LFU policy,LRU,LRU policy,Replacement Policy,Switches,Throughput}
}

@inproceedings{limayeWorkloadCharacterizationSPEC2018,
  title = {A {{Workload Characterization}} of the {{SPEC CPU2017 Benchmark Suite}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Limaye, Ankur and Adegbija, Tosiron},
  year = {2018},
  month = apr,
  pages = {149--158},
  publisher = {{IEEE}},
  address = {{Belfast}},
  doi = {10.1109/ISPASS.2018.00028},
  file = {/Users/jonathanrainer/Zotero/storage/TJR2M8LS/Limaye and Adegbija - 2018 - A Workload Characterization of the SPEC CPU2017 Be.pdf},
  isbn = {978-1-5386-5010-3}
}

@inproceedings{linDRAMLevelPrefetchingFullyBuffered2007a,
  title = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}: {{Design}}, {{Performance}} and {{Power Saving}}},
  shorttitle = {{{DRAM}}-{{Level Prefetching}} for {{Fully}}-{{Buffered DIMM}}},
  booktitle = {2007 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems Software}}},
  author = {Lin, Jiang and Zheng, Hongzhong and Zhu, Zhichun and Zhang, Zhao and David, Howard},
  year = {2007},
  month = apr,
  pages = {94--104},
  issn = {null},
  doi = {10.1109/ISPASS.2007.363740},
  abstract = {We have studied DRAM-level prefetching for the fully buffered DIMM (FB-DIMM) designed for multi-core processors. FB-DIMM has a unique two-level interconnect structure, with FB-DIMM channels at the first-level connecting the memory controller and advanced memory buffers (AMBs); and DDR2 buses at the second-level connecting the AMBs with DRAM chips. We propose an AMB prefetching method that prefetches memory blocks from DRAM chips to AMBs. It utilizes the redundant bandwidth between the DRAM chips and AMBs but does not consume the crucial channel bandwidth. The proposed method fetches K memory blocks of L2 cache block sizes around the demanded block, where K is a small value ranging from two to eight. The method may also reduce the DRAM power consumption by merging some DRAM precharges and activations. Our cycle-accurate simulation shows that the average performance improvement is 16\% for single-core and multi-core workloads constructed from memory-intensive SPEC2000 programs with software cache prefetching enabled; and no workload has negative speedup. We have found that the performance gain comes from the reduction of idle memory latency and the improvement of channel bandwidth utilization. We have also found that there is only a small overlap between the performance gains from the AMB prefetching and the software cache prefetching. The average of estimated power saving is 15\%},
  file = {/Users/jonathanrainer/Zotero/storage/5JA6ZN9G/Lin et al. - 2007 - DRAM-Level Prefetching for Fully-Buffered DIMM De.pdf;/Users/jonathanrainer/Zotero/storage/AIESMVKT/4211026.html},
  keywords = {Bandwidth,channel bandwidth utilization,DRAM chip,DRAM chips,DRAM power consumption,DRAM-level prefetching,dual in-line memory module,dynamic random access memory,Energy consumption,fully-buffered DIMM,idle memory latency,interconnect structure,Joining processes,L2 cache block,memory block,memory controller,Merging,Multicore processing,multicore processor,Performance gain,power saving,Prefetching,Process design,Random access memory,redundant bandwidth,software cache prefetching,Software performance,SPEC2000 program,storage management,storage management chips}
}

@techreport{linPredictingLastTouchReferences2002,
  title = {Predicting {{Last}}-{{Touch References}} under {{Optimal Replacement}}},
  author = {Lin, Wei-Fen and Reinhardt, Steven K},
  year = {2002},
  pages = {17},
  institution = {{University of Michigan}},
  abstract = {Effective cache replacement is becoming an increasingly important issue in cache hierarchy design as large set-associative caches are widely used in high-performance systems. This paper proposes a novel approach to approximate the decisions made by an optimal replacement algorithm (OPT) using last-touch prediction. The central idea is to identify, via prediction, the final reference to a cache block before the block would be evicted under OPT\textemdash{}the ``OPT last touch''. Given perfect prediction, replacing the referenced block immediately after each OPT last touch would give optimal replacement behavior.},
  file = {/Users/jonathanrainer/Zotero/storage/6LFUIS36/Lin and Reinhardt - Predicting Last-Touch References under Optimal Rep.pdf},
  language = {en},
  number = {CSE-TR-447-02},
  type = {Technical {{Report}}}
}

@inproceedings{linReducingDRAMLatencies2001,
  title = {Reducing {{DRAM}} Latencies with an Integrated Memory Hierarchy Design},
  booktitle = {Proceedings {{HPCA Seventh International Symposium}} on {{High}}-{{Performance Computer Architecture}}},
  author = {Lin, Wei-Fen and Reinhardt, S.K. and Burger, D.},
  year = {2001},
  month = jan,
  pages = {301--312},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2001.903272},
  abstract = {In this paper we address the severe performance gap caused by high processor clock rates and slow DRAM accesses. We show that even with an aggressive, next-generation memory system using four Direct Rambus channels and an integrated one-megabyte level-two cache, a processor still spends over half of its time stalling for L2 misses. Large cache blocks can improve performance, but only when coupled with wide memory channels. DRAM address mappings also affect performance significantly. We evaluate an aggressive prefetch unit integrated with the L2 cache and memory, controllers. By issuing prefetches only when the Rambus channels are idle, prioritizing them to maximize DRAM row buffer hits, and giving them low replacement priority, we achieve a 43\% speedup across 10 of the 26 SPEC2000 benchmarks, without degrading performance an the others. With eight Rambus channels, these ten benchmarks improve to within 10\% of the performance of a perfect L2 cache.},
  file = {/Users/jonathanrainer/Zotero/storage/2585M6ZA/Wei-Fen Lin et al. - 2001 - Reducing DRAM latencies with an integrated memory .pdf;/Users/jonathanrainer/Zotero/storage/E5QI2WYI/903272.html},
  keywords = {Banking,benchmarks,cache blocks,cache storage,Clocks,Computer science,Degradation,Delay,DRAM accesses,Dynamic scheduling,Frequency,High performance computing,integrated memory hierarchy,memory architecture,next-generation memory system,performance,performance evaluation,performance gap,Prefetching,Rambus channels,Random access memory}
}

@inproceedings{lipastiSPAIDSoftwarePrefetching1995,
  title = {{{SPAID}}: Software Prefetching in Pointer- and Call-Intensive Environments},
  shorttitle = {{{SPAID}}},
  booktitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  author = {Lipasti, M.H. and Schmidt, W.J. and Kunkel, S.R. and Roediger, R.R.},
  year = {1995},
  month = nov,
  pages = {231--236},
  issn = {1072-4451},
  doi = {10.1109/MICRO.1995.476830},
  abstract = {Software prefetching, typically in the context of numeric- or loop-intensive benchmarks, has been proposed as one remedy for the performance bottleneck imposed on computer systems by the cost of servicing cache misses. This paper proposes a new heuristic-SPAID-for utilizing prefetch instructions in pointer- and call-intensive environments. We use trace-driven cache simulation of a number of pointer- and call-intensive benchmarks to evaluate the benefits and implementation trade-offs of SPAID. Our results indicate that a significant proportion of the cost of data cache misses can be eliminated or reduced with SPAID without unduly increasing memory traffic.},
  file = {/Users/jonathanrainer/Zotero/storage/D675DDSE/Lipasti et al. - 1995 - SPAID software prefetching in pointer- and call-i.pdf;/Users/jonathanrainer/Zotero/storage/NGI2TPU4/476830.html},
  keywords = {benchmarks,cache misses,cache simulation,cache storage,call-intensive,Clocks,Computational modeling,Computer industry,Costs,Delay,discrete event simulation,Hardware,High performance computing,performance bottleneck,pointer-intensive,Prefetching,program compilers,Software performance,software performance evaluation,software prefetching,SPAID,trace-driven,Traffic control}
}

@inproceedings{liTracebasedAnalysisMethodology2016,
  title = {Trace-Based {{Analysis Methodology}} of {{Program Flash Contention}} in {{Embedded Multicore Systems}}},
  booktitle = {Proceedings of the 2016 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Li, Lin and Mayer, Albrecht},
  year = {2016},
  pages = {199--204},
  publisher = {{Research Publishing Services}},
  doi = {10.3850/9783981537079_0442},
  abstract = {Contention for shared resources is a major performance issue in multicore systems. In embedded multicore microcontrollers, contentions of program flash accesses have a significant performance impact, because the flash access has a large latency compared to a core clock cycle. Therefore, the detection and analysis of program flash contentions are necessary to remedy this situation. With a lack of existing tools being able to fulfill this task, a novel post-processing analysis methodology is proposed in this paper to acquire the information of program flash contentions in detail based on the non-intrusive trace. This information can be utilized to improve the overall performance and particularly to enhance the real-time performance of specific threads or functions for hard real-time multicore systems.},
  file = {/Users/jonathanrainer/Zotero/storage/6QLMYXHQ/Li and Mayer - 2016 - Trace-based Analysis Methodology of Program Flash .pdf},
  isbn = {978-3-9815370-7-9},
  language = {en}
}

@article{liuBridgingProcessormemoryPerformance2005,
  title = {Bridging the Processor-Memory Performance Gap with {{3D IC}} Technology},
  author = {Liu, C. C. and Ganusov, I. and Burtscher, M. and {Sandip Tiwari}},
  year = {2005},
  month = nov,
  volume = {22},
  pages = {556--564},
  issn = {0740-7475},
  doi = {10.1109/MDT.2005.134},
  abstract = {Microprocessor performance has been improving at roughly 60\% per year. Memory access times, however, have improved by less than 10\% per year. The resulting gap between logic and memory performance has forced microprocessor designs toward complex and power-hungry architectures that support out-of-order and speculative execution. Moreover, processors have been designed with increasingly large cache hierarchies to hide main memory latency. This article examines how 3D IC technology can improve interactions between the processor and memory. Our work examines the performance of a single-core, single-threaded processor under representative work loads. We have shown that reducing memory latency by bringing main memory on chip gives us near-perfect performance. Three-dimensional IC technology can provide the much needed bandwidth without the cost, design complexity, and power issues associated with a large number of off-chip pins. The principal challenge remains the demonstration of a highly manufacturable 3D IC technology with high yield and low cost.},
  file = {/Users/jonathanrainer/Zotero/storage/PLDS6YDF/Liu et al. - 2005 - Bridging the processor-memory performance gap with.pdf;/Users/jonathanrainer/Zotero/storage/7AKAYTWA/1541918.html},
  journal = {IEEE Design Test of Computers},
  keywords = {3D IC technology,Bandwidth,cache storage,Costs,Delay,DRAM chips,integrated circuit technology,Logic design,main memory on chip,Manufacturing,memory access,memory architecture,microprocessor chips,microprocessor design,microprocessor performance,Microprocessors,Out of order,Pins,Process design,processor-memory performance,single-threaded processor,Three-dimensional integrated circuits,three-dimensional integration 3-D ICs microprocessor cache design stream prefetching embedded DRAM},
  number = {6}
}

@inproceedings{liuCacheBurstsNew2008,
  title = {Cache Bursts: {{A}} New Approach for Eliminating Dead Blocks and Increasing Cache Efficiency},
  shorttitle = {Cache Bursts},
  booktitle = {2008 41st {{IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Liu, Haiming and Ferdman, Michael and Huh, Jaehyuk and Burger, Doug},
  year = {2008},
  month = nov,
  pages = {222--233},
  issn = {2379-3155},
  doi = {10.1109/MICRO.2008.4771793},
  abstract = {Data caches in general-purpose microprocessors often contain mostly dead blocks and are thus used inefficiently. To improve cache efficiency, dead blocks should be identified and evicted early. Prior schemes predict the death of a block immediately after it is accessed; however, these schemes yield lower prediction accuracy and coverage. Instead, we find that predicting the death of a block when it just moves out of the MRU position gives the best tradeoff between timeliness and prediction accuracy/coverage. Furthermore, the individual reference history of a block in the L1 cache can be irregular because of data/control dependence. This paper proposes a new class of dead-block predictors that predict dead blocks based on bursts of accesses to a cache block. A cache burst begins when a block becomes MRU and ends when it becomes non-MRU. Cache bursts are more predictable than individual references because they hide the irregularity of individual references. When used at the L1 cache, the best burst-based predictor can identify 96\% of the dead blocks with a 96\% accuracy. With the improved dead-block predictors, we evaluate three ways to increase cache efficiency by eliminating dead blocks early: replacement optimization, bypassing, and prefetching. The most effective approach, prefetching into dead blocks, increases the average L1 efficiency from 8\% to 17\% and the L2 efficiency from 17\% to 27\%. This increased cache efficiency translates into higher overall performance: prefetching into dead blocks outperforms the same prefetch scheme without dead-block prediction by 12\% at the L1 and by 13\% at the L2.},
  file = {/Users/jonathanrainer/Zotero/storage/SBJUL4NZ/Liu et al. - 2008 - Cache bursts A new approach for eliminating dead .pdf;/Users/jonathanrainer/Zotero/storage/74XCECAG/4771793.html},
  keywords = {Accuracy,cache efficiency,cache storage,data cache burst,dead block elimination,dead-block prediction,general-purpose microprocessor,Hardware,History,LI cache,microprocessor chips,Microprocessors,Prefetching,System performance}
}

@article{liuControllerArchitectureLowPower2017,
  title = {Controller {{Architecture}} for {{Low}}-{{Power}}, {{Low}}-{{Latency DRAM With Built}}-in {{Cache}}},
  author = {Liu, Z. and Shih, H. and Lin, B. and Wu, C.},
  year = {2017},
  month = apr,
  volume = {34},
  pages = {69--78},
  issn = {2168-2356},
  doi = {10.1109/MDAT.2016.2524445},
  abstract = {Memory wall is a critical issue for many today's electronic systems. Tiered latency DRAM with asymmetric bit lines was proposed to optimize the power and latency. This paper proposes a controller architecture for the tiered latency DRAM in which the small array is operated like a cache.},
  file = {/Users/jonathanrainer/Zotero/storage/DG9BL29C/Liu et al. - 2017 - Controller Architecture for Low-Power, Low-Latency.pdf;/Users/jonathanrainer/Zotero/storage/L272EZ4H/7397924.html},
  journal = {IEEE Design Test},
  keywords = {3D DRAM,asymmetric bit lines,built-in cache,built-in cache DRAM (BC-DRAM),cache storage,controller architecture,controllers,Delays,DRAM,DRAM chips,DRAM controller,electronic systems,low-power electronics,low-power low-latency DRAM,memory hierarchy,memory wall,Microprocessors,Random access memory,tiered latency DRAM,tiered latency DRAM (TL-DRAM),Transistors},
  number = {2}
}

@inproceedings{liuMultibankMemoryAccess2010,
  title = {Multi-Bank Memory Access Scheduler and Scalability},
  booktitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  author = {Liu, D. and Pan, G. and Xie, L.},
  year = {2010},
  month = apr,
  volume = {2},
  pages = {V2-723-V2-727},
  doi = {10.1109/ICCET.2010.5485729},
  abstract = {With the progress of semiconductor manufacture techniques and the development of processor architecture, the gap between processor and DRAM speed is becoming larger and larger, memory bandwidth is now the primary bottleneck of improving computer system performance. Modern DRAM provide several independent memory banks, according to this character, we present a virtual channel based memory access scheduler, and least wait time and read-fist schedule approach. This approach significantly reduce observed main memory access latency and improve the effective memory bandwidth.},
  file = {/Users/jonathanrainer/Zotero/storage/FFS6FDZQ/Liu et al. - 2010 - Multi-bank memory access scheduler and scalability.pdf;/Users/jonathanrainer/Zotero/storage/5FFBJM2U/5485729.html},
  keywords = {Bandwidth,Computer aided manufacturing,Computer architecture,computer system performance,DRAM chips,DRAM memory system,DRAM speed,Job shop scheduling,Manufacturing processes,memory access latency,memory access scheduling,memory bandwidth,multibank memory access scheduler,processor scheduling,Processor scheduling,Random access memory,Scalability,Semiconductor device manufacture,semiconductor manufacture technique,storage management,System performance,virtual channel,virtual channel based memory access scheduler}
}

@inproceedings{lohEfficientlyEnablingConventional2011,
  title = {Efficiently Enabling Conventional Block Sizes for Very Large Die-Stacked {{DRAM}} Caches},
  booktitle = {2011 44th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Loh, Gabriel H. and Hill, Mark D.},
  year = {2011},
  month = dec,
  pages = {454--464},
  issn = {null},
  abstract = {Die-stacking technology enables multiple layers of DRAM to be integrated with multicore processors. A promising use of stacked DRAM is as a cache, since its capacity is insufficient to be all of main memory (for all but some embedded systems). However, a 1GB DRAM cache with 64-byte blocks requires 96MB of tag storage. Placing these tags on-chip is impractical (larger than on-chip L3s) while putting them in DRAM is slow (two full DRAM accesses for tag and data). Larger blocks and sub-blocking are possible, but less robust due to fragmentation. This work efficiently enables conventional block sizes for very large die-stacked DRAM caches with two innovations. First, we make hits faster than just storing tags in stacked DRAM by scheduling the tag and data accesses as a compound access so the data access is always a row buffer hit. Second, we make misses faster with a MissMap that eschews stacked-DRAM access on all misses. Like extreme sub-blocking, our implementation of the MissMap stores a vector of block-valid bits for each ``page'' in the DRAM cache. Unlike conventional sub-blocking, the MissMap (a) points to many more pages than can be stored in the DRAM cache (making the effects of fragmentation rare) and (b) does not point to the ``way'' that holds a block (but defers to the off-chip tags). For the evaluated large-footprint commercial workloads, the proposed cache organization delivers 92.9\% of the performance benefit of an ideal 1GB DRAM cache with an impractical 96MB on-chip SRAM tag array.},
  file = {/Users/jonathanrainer/Zotero/storage/A82DQ9UL/Loh and Hill - 2011 - Efficiently enabling conventional block sizes for .pdf;/Users/jonathanrainer/Zotero/storage/ET2MIPYM/7851494.html},
  keywords = {Arrays,Bandwidth,block sizes,block-valid bits,cache organization,cache storage,Compounds,data access,Design,die-stacked DRAM caches,die-stacking technology,DRAM chips,MissMap,Multicore processing,multicore processors,page storage,paged storage,Performance,Program processors,Random access memory,row buffer hit,System-on-chip,tag scheduling,tag storage}
}

@inproceedings{luImprovingDRAMLatency2015,
  title = {Improving {{DRAM}} Latency with Dynamic Asymmetric Subarray},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Lu, S. and Lin, Y. and Yang, C.},
  year = {2015},
  month = dec,
  pages = {255--266},
  doi = {10.1145/2830772.2830827},
  abstract = {The evolution of DRAM technology has been driven by capacity and bandwidth during the last decade. In contrast, DRAM access latency stays relatively constant and is trending to increase. Much efforts have been devoted to tolerate memory access latency but these techniques have reached the point of diminishing returns. Having shorter bitline and wordline length in a DRAM device will reduce the access latency. However by doing so it will impact the array efficiency. In the mainstream market, manufacturers are not willing to trade capacity for latency. Prior works had proposed hybrid-bitline DRAM design to overcome this problem. However, those methods are either intrusive to the circuit and layout of the DRAM design, or there is no direct way to migrate data between the fast and slow levels. In this paper, we proposed a novel asymmetric DRAM with capability to perform low cost data migration between subarrays. Having this design we determined a simple management mechanism and explored many management related policies. We showed that with this new design and our simple management technique we could achieve 7.25\% and 11.77\% performance improvement in single- and multi-programming workloads, respectively, over a system with traditional homogeneous DRAM. This gain is above 80\% of the potential performance gain of a system based on a hypothetical DRAM which is made out of short bitlines entirely.},
  file = {/Users/jonathanrainer/Zotero/storage/S5PNHIBT/Lu et al. - 2015 - Improving DRAM latency with dynamic asymmetric sub.pdf;/Users/jonathanrainer/Zotero/storage/GIW27Z6W/7856603.html},
  keywords = {asymmetric DRAM,Capacitors,Computer architecture,Delays,DRAM access latency,DRAM chips,DRAM device,DRAM technology,dynamic asymmetric subarray,hybrid-bitline DRAM design,low cost data migration,memory access latency,Microprocessors,multiprogramming workloads,performance evaluation,performance improvement,single-programming workloads,Transistors,wordline length}
}

@inproceedings{lukCompilerbasedPrefetchingRecursive1996,
  title = {Compiler-Based Prefetching for Recursive Data Structures},
  booktitle = {Proceedings of the Seventh International Conference on {{Architectural}} Support for Programming Languages and Operating Systems},
  author = {Luk, Chi-Keung and Mowry, Todd C.},
  year = {1996},
  month = sep,
  pages = {222--233},
  publisher = {{Association for Computing Machinery}},
  address = {{Cambridge, Massachusetts, USA}},
  doi = {10.1145/237090.237190},
  abstract = {Software-controlled data prefetching offers the potential for bridging the ever-increasing speed gap between the memory subsystem and today's high-performance processors. While prefetching has enjoyed considerable success in array-based numeric codes, its potential in pointer-based applications has remained largely unexplored. This paper investigates compiler-based prefetching for pointer-based applications---in particular, those containing recursive data structures. We identify the fundamental problem in prefetching pointer-based data structures and propose a guideline for devising successful prefetching schemes. Based on this guideline, we design three prefetching schemes, we automate the most widely applicable scheme (greedy prefetching) in an optimizing research compiler, and we evaluate the performance of all three schemes on a modern superscalar processor similar to the MIPS R10000. Our results demonstrate that compiler-inserted prefetching can significantly improve the execution speed of pointer-based codes---as much as 45\% for the applications we study. In addition, the more sophisticated algorithms (which we currently perform by hand, but which might be implemented in future compilers) can improve performance by as much as twofold. Compared with the only other compiler-based pointer prefetching scheme in the literature, our algorithms offer substantially better performance by avoiding unnecessary overhead and hiding more latency.},
  file = {/Users/jonathanrainer/Zotero/storage/ILUQWRXC/Luk and Mowry - 1996 - Compiler-based prefetching for recursive data stru.pdf},
  isbn = {978-0-89791-767-4},
  series = {{{ASPLOS VII}}}
}

@inproceedings{luoDesignRealizationOptimized2010,
  title = {Design and {{Realization}} of an {{Optimized Memory Access Scheduler}}},
  booktitle = {2010 {{Third International Joint Conference}} on {{Computational Science}} and {{Optimization}}},
  author = {Luo, L. and He, H. and Liao, C. and Dou, Q. and Xu, W.},
  year = {2010},
  month = may,
  volume = {2},
  pages = {288--292},
  doi = {10.1109/CSO.2010.81},
  abstract = {Memory Wall is a bottleneck of enhancing the performance of computer system, and appearance of multiprocessors (CMPs) makes it more. How to reduce Memory Access Latency is a critical issue we have to deal with. Memory controller is difficult to optimize, the controller needs to obey all DRAM timing constraints to provide correct functionality. State-of -the-art DDR2 SDRAM chips often have a large number of timing constraints that must be obeyed when scheduling commands, for instance, over 50 timing constrains. We have made deep research on optimized memory access scheduling. In order to efficiently utilize the bandwidth and reduce the latency, Memory Access Scheduling optimization adapts the characters of DRAM to reschedule the memory access. By studying effective data bar which is generated by Genetic Algorithm, we mine four rules. So we just use these four rules to schedule in Memory Access Controller. The results of experiment show that compared with FR-FCFS (first-ready first-come first-serve) scheduling strategy, the rule based algorithm improves the performance of scheduling and the ideal speedup is near 1.5 times. The best speedup of the test of spec2000 is 1.467, and the worst speedup is 1.078.},
  file = {/Users/jonathanrainer/Zotero/storage/R8GX48R9/Luo et al. - 2010 - Design and Realization of an Optimized Memory Acce.pdf;/Users/jonathanrainer/Zotero/storage/44V4AJRA/5533016.html},
  keywords = {Bandwidth,Constraint optimization,data mining,DDR2,DDR2 SDRAM chips,Delay,Design optimization,DRAM chips,first-ready first-come first-serve scheduling,FR-FCFS scheduling strategy,genetic algorithm,genetic algorithm (GA),genetic algorithms,Genetic algorithms,memory access controller,memory access latency,memory access scheduling,memory controller,multiprocessing systems,multiprocessors,optimized memory access scheduler,processor scheduling,Processor scheduling,Random access memory,rule based algorithm,Scheduling algorithm,scheduling commands,SDRAM,timing,Timing}
}

@article{malamy1994methods,
  title = {Methods and Apparatus for Implementing a Pseudo-Lru Cache Memory Replacement Scheme with a Locking Feature},
  author = {Malamy, Adam and Patel, Rajiv N and Hayes, Norman M},
  year = {1994},
  month = apr,
  publisher = {{Google Patents}},
  file = {/Users/jonathanrainer/Zotero/storage/XP468KC5/Malamy et al. - 1994 - Methods and apparatus for implementing a pseudo-lr.pdf}
}

@inproceedings{maLowlatencySDRAMController2010,
  title = {Low-Latency {{SDRAM}} Controller for Shared Memory in {{MPSoC}}},
  booktitle = {2010 10th {{IEEE International Conference}} on {{Solid}}-{{State}} and {{Integrated Circuit Technology}}},
  author = {Ma, P. and Zhao, J. and Li, K. and Zhu, L. and Shi, J.},
  year = {2010},
  month = nov,
  pages = {321--323},
  doi = {10.1109/ICSICT.2010.5667736},
  abstract = {In a memory structure shared by multiple processors based on Multiprocessor Systems on Chip (MPSoC), the efficiency of memory bus access becomes the bottleneck of the overall system efficiency. This paper presents a low-latency SDARM controller structure integrated in MPSoC, which controls the off-chip SDRAM memory. Consecutive same row optimization and odd-even bank optimization are used to eliminate precharge time and active to read/write execution in memory access. Burst mode supported by data transmit block improves the efficiency of the memory bus. Simulation results show that memory performance improves maximally by 56\% compared to pre-optimized, making it meet the high throughput requirements of shared-memory controller in MPSoC.},
  file = {/Users/jonathanrainer/Zotero/storage/TRLP7B57/Ma et al. - 2010 - Low-latency SDRAM controller for shared memory in .pdf;/Users/jonathanrainer/Zotero/storage/FBGVYY2P/5667736.html},
  keywords = {burst mode,Classification algorithms,Conferences,data transmit block,DRAM chips,low-latency SDRAM controller structure,Mathematical model,memory bus access,Memory management,memory structure,multiprocessor systems on chip,odd-even bank optimization,off-chip SDRAM memory,Optimization,same row optimization,SDRAM,shared memory systems,shared-memory controller,system-on-chip,Timing}
}

@inproceedings{manikantanNUcacheEfficientMulticore2011,
  title = {{{NUcache}}: {{An}} Efficient Multicore Cache Organization Based on {{Next}}-{{Use}} Distance},
  shorttitle = {{{NUcache}}},
  booktitle = {2011 {{IEEE}} 17th {{International Symposium}} on {{High Performance Computer Architecture}}},
  author = {Manikantan, R and Rajan, Kaushik and Govindarajan, R},
  year = {2011},
  month = feb,
  pages = {243--253},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2011.5749733},
  abstract = {The effectiveness of the last-level shared cache is crucial to the performance of a multi-core system. In this paper, we observe and make use of the DelinquentPC - Next-Use characteristic to improve shared cache performance. We propose a new PC-centric cache organization, NUcache, for the shared last level cache of multi-cores. NUcache logically partitions the associative ways of a cache set into MainWays and DeliWays. While all lines have access to the MainWays, only lines brought in by a subset of delinquent PCs, selected by a PC selection mechanism, are allowed to enter the DeliWays. The PC selection mechanism is an intelligent cost-benefit analysis based algorithm that utilizes Next-Use information to select the set of PCs that can maximize the hits experienced in DeliWays. Performance evaluation reveals that NUcache improves the performance over a baseline design by 9.6\%, 30\% and 33\% respectively for dual, quad and eight core workloads comprised of SPEC benchmarks. We also show that NUcache is more effective than other well-known cache-partitioning algorithms.},
  file = {/Users/jonathanrainer/Zotero/storage/JHQ9M72A/Manikantan et al. - 2011 - NUcache An efficient multicore cache organization.pdf;/Users/jonathanrainer/Zotero/storage/EDN89G7I/5749733.html},
  keywords = {Algorithm design and analysis,Benchmark testing,cache storage,cache-partitioning algorithms,Correlation,DelinquentPC,DeliWays,Histograms,intelligent cost-benefit analysis,last-level shared cache,MainWays,Measurement,multicore cache organization,Multicore processing,multicore system,multiprocessing systems,next-use distance,NUcache,Organizations,PC selection mechanism,PC-centric cache organization,performance evaluation,shared cache performance improvement}
}

@inproceedings{marchalOptimizingMemoryBandwidth2004,
  title = {Optimizing the Memory Bandwidth with Loop Fusion},
  booktitle = {International {{Conference}} on {{Hardware}}/{{Software Codesign}} and {{System Synthesis}}, 2004. {{CODES}} + {{ISSS}} 2004.},
  author = {Marchal, P. and Catthoor, F. and Gomez, J.I.},
  year = {2004},
  month = sep,
  pages = {188--193},
  issn = {null},
  doi = {10.1109/CODESS.2004.241216},
  abstract = {The memory bandwidth largely determines the performance and energy cost of embedded systems. At the compiler level, several techniques improve the memory bandwidth at the scope of a basic block, but often fail to exploit all. We propose a technique to optimize the memory bandwidth across the boundaries of a basic block. Our technique incrementally fuses loops to better use the available bandwidth. The resulting performance depends on how the data is assigned to the memories of the memory layer. At the same time, the assignment also strongly influences the energy cost. Therefore, we combine in our approach the fusion and assignment decisions. Designers can use our output to trade-off the energy cost with the system's performance.},
  file = {/Users/jonathanrainer/Zotero/storage/5I27RWK5/Marchal et al. - 2004 - Optimizing the memory bandwidth with loop fusion.pdf;/Users/jonathanrainer/Zotero/storage/AGMDL6BM/1360503.html},
  keywords = {Algorithm design and analysis,assignment decisions,Bandwidth,compiler memory bandwidth,Costs,Data structures,Delay,Embedded system,embedded systems,energy cost,Fuses,loop fusion,low power design,memory bandwidth optimization,optimisation,optimising compilers,Optimizing compilers,Permission,Processor scheduling,program control structures,storage management}
}

@inproceedings{martinDesignAsynchronousMIPS1997,
  title = {The Design of an Asynchronous {{MIPS R3000}} Microprocessor},
  booktitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  author = {Martin, A.J. and Lines, A. and Manohar, R. and Nystrom, M. and Penzes, P. and Southworth, R. and Cummings, U. and Tak Kwan Lee},
  year = {1997},
  month = sep,
  pages = {164--181},
  issn = {null},
  doi = {10.1109/ARVLSI.1997.634853},
  abstract = {The design of an asynchronous clone of a MIPS R3000 microprocessor is presented. In 0.6 /spl mu/m CMOS, we expect performance close to 280 MIPS, for a power consumption of 7 W. The paper describes the structure of a high-performance asynchronous pipeline, in particular precise exceptions, pipelined caches, arithmetic, and registers, and the circuit techniques developed to achieve high throughput.},
  file = {/Users/jonathanrainer/Zotero/storage/RNYWVVFA/Martin et al. - 1997 - The design of an asynchronous MIPS R3000 microproc.pdf;/Users/jonathanrainer/Zotero/storage/HSNQUHMM/634853.html},
  keywords = {0.6 micron,280 MIPS,7 W,arithmetic,asynchronous circuits,Asynchronous circuits,asynchronous microprocessor,circuit techniques,CMOS design,CMOS digital integrated circuits,CMOS technology,computer architecture,Computer science,Delay,Energy consumption,high throughput,high-performance asynchronous pipeline,Low voltage,microprocessor chips,Microprocessors,MIPS R3000,pipeline processing,pipelined caches,Pipelines,precise exceptions,registers,Robustness,Throughput,VLSI}
}

@inproceedings{megiddoARCSelfTuningLow2003,
  title = {{{ARC}}: {{A Self}}-{{Tuning}}, {{Low Overhead Replacement Cache}}},
  booktitle = {Proceedings of {{FAST}} '03: 2nd {{USENIX Conference}} on {{File}} and {{Storage Technologies}}},
  author = {Megiddo, Nimrod and Dharmendra S., Modha},
  year = {2003},
  month = apr,
  volume = {3},
  pages = {115--130},
  publisher = {{USENIX Association}},
  address = {{San Francisco}},
  abstract = {We consider the problem of cache management in a demand paging scenario with uniform page sizes. We propose a new cache management policy, namely, Adaptive Replacement Cache (ARC), that has several advantages.},
  file = {/Users/jonathanrainer/Zotero/storage/6WSSJ92Q/Francisco - Proceedings of FAST ’03 2nd USENIX Conference on .pdf},
  language = {en}
}

@inproceedings{mekhielMultiLevelCacheMost29,
  title = {Multi-{{Level Cache With Most Frequently Used Policy}}: {{A New Concept In Cache Design}}},
  booktitle = {Proceedings of the {{ISCA}} 8th {{International Conference}}},
  author = {Mekhiel, Nagi N},
  year = {29},
  month = nov,
  pages = {5},
  address = {{Honolulu, Hawaii}},
  abstract = {The number of unique references in any program represents a small part of the total number of requested references. The unique references (small part of the total requested references) consist of two types: unique references that are used several times (most frequently used references) and unique references that are used once (least frequently used references).},
  file = {/Users/jonathanrainer/Zotero/storage/I4ZD4QTM/Mekhiel - Multi-Level Cache With Most Frequently Used Policy.pdf},
  isbn = {1-880843-14-5},
  language = {en}
}

@incollection{menaudImprovingEffectivenessWeb2000,
  title = {Improving the {{Effectiveness}} of {{Web Caching}}},
  booktitle = {Advances in {{Distributed Systems}}},
  author = {Menaud, Jean-Marc and Issarny, Val{\'e}rie and Ban{\^a}tre, Michel},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Krakowiak, Sacha and Shrivastava, Santosh},
  year = {2000},
  volume = {1752},
  pages = {375--401},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-46475-1_16},
  file = {/Users/jonathanrainer/Zotero/storage/S9XUKFYZ/Menaud et al. - 2000 - Improving the Effectiveness of Web Caching.pdf},
  isbn = {978-3-540-67196-1 978-3-540-46475-4}
}

@inproceedings{mertzPracticalFeasibilitySoftware2019,
  title = {On the {{Practical Feasibility}} of {{Software Monitoring}}: A {{Framework}} for {{Low}}-{{Impact Execution Tracing}}},
  shorttitle = {On the {{Practical Feasibility}} of {{Software Monitoring}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 14th {{International Symposium}} on {{Software Engineering}} for {{Adaptive}} and {{Self}}-{{Managing Systems}} ({{SEAMS}})},
  author = {Mertz, Jhonny and Nunes, Ingrid},
  year = {2019},
  month = may,
  pages = {169--180},
  issn = {2157-2305},
  doi = {10.1109/SEAMS.2019.00030},
  abstract = {In order for a software system to self-adapt, it often needs to be aware of its behavior. A typical way of achieving this is by means of the runtime collection of execution traces, which requires the interception of the execution of, e.g. methods, and record information about them. Although this is simple in theory, in practice, it can be very costly because it might have a significant impact on the application performance or require huge amounts of memory or storage. This becomes a significant issue mainly in real-time applications, which are time-sensitive and must often meet deadlines in resource-constrained environments. We thus in this paper propose a two-phase tracing framework to cope with the monitoring overhead. In its first phase, the application is monitored in a lightweight fashion providing information for the second phase, which identifies an adaptive configuration and samples traces according to the current configuration. The adaptive configuration is determined by a set of criteria, specified through a proposed domain-specific language. We empirically evaluate our framework by instantiating it as a reduced-overhead monitoring solution, integrated into an existing automated application-level caching approach. We demonstrate that our solution reduces the overhead caused by monitoring, without compromising the performance improvements provided by the caching approach.},
  file = {/Users/jonathanrainer/Zotero/storage/J7MFF9YB/Mertz and Nunes - 2019 - On the Practical Feasibility of Software Monitorin.pdf;/Users/jonathanrainer/Zotero/storage/XAGCZTFR/8787058.html},
  keywords = {adaptive configuration,application performance,automated application-level caching approach,cache storage,caching,Computer bugs,execution trace,execution traces,Instruments,lightweight fashion,logging,low-impact execution tracing,Measurement,monitoring,Monitoring,monitoring overhead,performance,real-time applications,record information,reduced-overhead monitoring solution,resource-constrained environments,Runtime,runtime collection,sampling,Software,software monitoring,software system,specification languages,Synthetic aperture sonar,system monitoring,two-phase tracing framework}
}

@article{midorikawaAdaptiveReplacementBased2008,
  title = {On {{Adaptive Replacement Based}} on {{LRU}} with {{Working Area Restriction Algorithm}}},
  author = {Midorikawa, Edson T. and Piantola, Ricardo L. and Cassettari, Hugo H.},
  year = {2008},
  month = oct,
  volume = {42},
  pages = {81--92},
  issn = {0163-5980},
  doi = {10.1145/1453775.1453790},
  abstract = {Adaptive algorithms are capable of modifying their own behavior through time, depending on the execution characteristics. Recently, we have proposed LRU-WAR, an adaptive replacement algorithm whose objective is to minimize failures detected in LRU policy, preserving its simplicity and low overhead. In this paper, we present our contribution to the study of adaptive replacement algorithms describing their behavior under a number of workloads. Simulations include an analysis of the performance sensibility with the variation of the control parameters and its application in a multiprogrammed environment. In order to address LRU-WAR weakness as a global policy, we also introduce LRU-WARlock. The simulation results show that substantial performance improvements can be obtained.},
  file = {/Users/jonathanrainer/Zotero/storage/7PLKVN5X/Midorikawa et al. - 2008 - On Adaptive Replacement Based on LRU with Working .pdf},
  journal = {SIGOPS Oper. Syst. Rev.},
  keywords = {adaptive replacement,demand paging,LRU,virtual memory},
  number = {6}
}

@inproceedings{milenkovicPerformanceEvaluationMemory2003,
  title = {A Performance Evaluation of Memory Hierarchy in Embedded Systems},
  booktitle = {Proceedings of the 35th {{Southeastern Symposium}} on {{System Theory}}, 2003.},
  author = {Milenkovic, A. and Milenkovic, M. and Barnes, N.},
  year = {2003},
  month = mar,
  pages = {427--431},
  doi = {10.1109/SSST.2003.1194606},
  abstract = {The increasing speed gap between processors and memory makes the design of memory hierarchy one of the critical issues in general purpose embedded systems. As memory requirements for embedded applications grow, especially in emerging area of handheld multimedia devices, cache memories become crucial for providing high performance and reducing power. This paper describes a performance evaluation of typical cache design issues such as cache size and organization, block size, and replacement policy. The evaluation is done using simulation tools for architectural exploration based on ARM instruction set and MiBench benchmark suite. Our performance evaluation includes monitoring of dynamic cache behavior, since embedded systems designers are interested not only in the total number of cache misses, but also in the number of cache misses throughout application execution.},
  file = {/Users/jonathanrainer/Zotero/storage/WI5X4Q2N/Milenkovic et al. - 2003 - A performance evaluation of memory hierarchy in em.pdf;/Users/jonathanrainer/Zotero/storage/KF8Q7DSM/1194606.html},
  keywords = {ARM instruction set,block size,cache memories,Cache memory,cache misses,cache organization,cache size,cache storage,Clocks,Computer architecture,Computer industry,Costs,Design engineering,dynamic behavior,Embedded computing,Embedded system,embedded systems,handheld multimedia devices,memory architecture,memory hierarchy,MiBench benchmark suite,Monitoring,performance evaluation,Performance gain,replacement policy,simulation tools}
}

@inproceedings{miuraMemoryControllerThat2005,
  title = {A Memory Controller That Reduces Latency of Cached {{SDRAM}}},
  booktitle = {2005 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Miura, S. and Akiyama, S.},
  year = {2005},
  month = may,
  pages = {5250-5253 Vol. 5},
  doi = {10.1109/ISCAS.2005.1465819},
  abstract = {The proposed controller has two main control schemes, address-alignment control and dummy-cache control. These two schemes cooperatively control cached SDRAM to reduce its latency. Testing of the controller using benchmark programs demonstrated that latency was reduced 25\% and execution time was reduced 13\% compared to those of a sense-amplifier cache controller for standard SDRAM. The proposed controller requires 9.2 Kgates at a supply voltage of 1.8 V and an operating frequency of 133 MHz.},
  file = {/Users/jonathanrainer/Zotero/storage/QFVCVC2L/Miura and Akiyama - 2005 - A memory controller that reduces latency of cached.pdf;/Users/jonathanrainer/Zotero/storage/NEDWVNM9/1465819.html},
  keywords = {1.8 V,133 MHz,address-alignment control,benchmark programs,Benchmark testing,Cache memory,cache storage,cached SDRAM latency reduction,Centralized control,Delay,DRAM chips,dummy-cache control,Frequency,integrated circuit design,Laboratories,memory controller,Operational amplifiers,Random access memory,SDRAM,sense-amplifier cache controller,standard SDRAM,synchronous DRAM,Voltage control}
}

@inproceedings{modgilImprovingPerformanceChip2018,
  title = {Improving the {{Performance}} of {{Chip Multiprocessor}} by {{Delayed Write Drain}} and {{Prefetcher}} Based {{Memory Scheduler}}},
  booktitle = {2018 {{Second International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Modgil, A. and Sehgal, V. K.},
  year = {2018},
  month = mar,
  pages = {1864--1869},
  doi = {10.1109/ICECA.2018.8474846},
  abstract = {To improve the performance and energy consumption of chip multiprocessor (CMP) system, memory request serving latencies should be minimized. These latencies can be minimized by scheduling appropriate memory command at appropriate time. This paper proposes a scheduler that reduces latency related to serving memory read requests by delaying switching into write drain mode when memory traffic is not heavy and write queue is not full. Memory reads are more important to handle than memory writes for system's performance. Further precharge and activate operations are performed using constant stride prefetcher. In idle memory cycles the scheduler issues row precharge commands using cache prefetching technique based on Global History Buffer. Authors in [1] have used stride detector and Global History Buffer based speculative precharges and activates, but they treat memory reads and memory writes equally. Whereas, proposed scheduler in this paper prioritizes reads over writes for better system performance. Our evaluations show that proposed scheduling policy significantly outperforms previous schedulers [1], [2] in varied multicore environments in terms of performance as well as energy consumption. Across a wide range of workloads based on PARSEC benchmark suite, proposed policy improves systems performance by 2.51\%, on 2-core, 0.012\% on 4-core environment in comparison to scheduler proposed in [1].},
  file = {/Users/jonathanrainer/Zotero/storage/YNRWI8GF/Modgil and Sehgal - 2018 - Improving the Performance of Chip Multiprocessor b.pdf;/Users/jonathanrainer/Zotero/storage/2AHCGKDF/8474846.html},
  keywords = {Aerospace electronics,cache prefetching technique,cache storage,chip multiprocessor system,CMP,Conferences,constant stride prefetcher,delayed write drain,delaying switching,DRAM,DRAM chips,energy consumption,Energy consumption,Global History Buffer,History,idle memory cycles,Memory Access Scheduler,memory command,Memory management,memory request,memory traffic,multiprocessing systems,Performance,precharge commands,Prefetcher,prefetcher based memory scheduler,Prefetching,Random access memory,scheduling,scheduling policy,systems performance,write drain mode}
}

@inproceedings{mowryDesignEvaluationCompiler1992,
  title = {Design and Evaluation of a Compiler Algorithm for Prefetching},
  booktitle = {Proceedings of the Fifth International Conference on {{Architectural}} Support for Programming Languages and Operating Systems},
  author = {Mowry, Todd C. and Lam, Monica S. and Gupta, Anoop},
  year = {1992},
  month = sep,
  pages = {62--73},
  publisher = {{Association for Computing Machinery}},
  address = {{Boston, Massachusetts, USA}},
  doi = {10.1145/143365.143488},
  file = {/Users/jonathanrainer/Zotero/storage/7XQHCPJ2/Mowry et al. - 1992 - Design and evaluation of a compiler algorithm for .pdf},
  isbn = {978-0-89791-534-2},
  series = {{{ASPLOS V}}}
}

@inproceedings{mutluRunaheadExecutionAlternative2003,
  title = {Runahead Execution: An Alternative to Very Large Instruction Windows for out-of-Order Processors},
  shorttitle = {Runahead Execution},
  booktitle = {The {{Ninth International Symposium}} on {{High}}-{{Performance Computer Architecture}}, 2003. {{HPCA}}-9 2003. {{Proceedings}}.},
  author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y.N.},
  year = {2003},
  month = feb,
  pages = {129--140},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2003.1183532},
  abstract = {Today's high performance processors tolerate long latency operations by means of out-of-order execution. However, as latencies increase, the size of the instruction window must increase even faster if we are to continue to tolerate these latencies. We have already reached the point where the size of an instruction window that can handle these latencies is prohibitively large in terms of both design complexity and power consumption. And, the problem is getting worse. This paper proposes runahead execution as an effective way to increase memory latency tolerance in an out-of-order processor without requiring an unreasonably large instruction window. Runahead execution unblocks the instruction window blocked by long latency operations allowing the processor to execute far ahead in the program path. This results in data being prefetched into caches long before it is needed. On a machine model based on the Intel/spl reg/ Pentium/spl reg/ processor, having a 128-entry instruction window, adding runahead execution improves the IPC (instructions per cycle) by 22\% across a wide range of memory intensive applications. Also, for the same machine model, runahead execution combined with a 128-entry window performs within 1\% of a machine with no runahead execution and a 384-entry instruction window.},
  file = {/Users/jonathanrainer/Zotero/storage/CUGMVD6U/Mutlu et al. - 2003 - Runahead execution an alternative to very large i.pdf;/Users/jonathanrainer/Zotero/storage/XYQED8PI/1183532.html},
  keywords = {cache storage,caches,Delay,delays,Energy consumption,Engines,Hardware,high performance processors,instructions per cycle,memory latency tolerance,Microprocessors,Out of order,out-of-order processors,parallel architectures,performance evaluation,Prefetching,runahead execution,Software,Trademarks,Windows}
}

@article{mutluRunaheadExecutionEffective2003a,
  title = {Runahead Execution: {{An}} Effective Alternative to Large Instruction Windows},
  shorttitle = {Runahead Execution},
  author = {Mutlu, O. and Stark, J. and Wilkerson, C. and Patt, Y.N.},
  year = {2003},
  month = nov,
  volume = {23},
  pages = {20--25},
  issn = {1937-4143},
  doi = {10.1109/MM.2003.1261383},
  abstract = {An instruction window that can tolerate latencies to DRAM memory is prohibitively complex and power hungry. To avoid having to build such large windows, runahead execution uses otherwise-idle clock cycles to achieve an average 22 percent performance improvement for processors with instruction windows of contemporary sizes. This technique incurs only a small hardware cost and does not significantly increase the processor's complexity.},
  file = {/Users/jonathanrainer/Zotero/storage/HVDMXSFS/Mutlu et al. - 2003 - Runahead execution An effective alternative to la.pdf;/Users/jonathanrainer/Zotero/storage/52VB72BN/1261383.html},
  journal = {IEEE Micro},
  keywords = {Clocks,computer architecture,Costs,Delay,DRAM memory,Energy consumption,Hardware,instruction sets,instruction windows,Microarchitecture,Out of order,Random access memory,Registers,Retirement,runahead execution,storage management},
  number = {6}
}

@inproceedings{navarroAdaptiveVictimCache2014,
  title = {An Adaptive Victim Cache Scheme},
  booktitle = {2014 {{International Conference}} on {{ReConFigurable Computing}} and {{FPGAs}} ({{ReConFig14}})},
  author = {Navarro, Osvaldo and H{\"u}bner, Michael},
  year = {2014},
  month = dec,
  pages = {1--4},
  issn = {2325-6532},
  doi = {10.1109/ReConFig.2014.7032496},
  abstract = {A victim cache is a small cache block usually located between two main cache levels, which main objective is to recover conflict cache misses. In the usual case, the victim cache is designed as an always enabled cache block with fixed size. However, different applications may have very different memory access requirements. In this article, we present an analysis of the relations between a victim cache, the cache's logical organization and the application's behaviour, and based on preliminary results, an victim cache scheme which is reconfigured at runtime to adapt to the current conditions of the system is proposed.},
  file = {/Users/jonathanrainer/Zotero/storage/DYF378QS/Navarro and Hübner - 2014 - An adaptive victim cache scheme.pdf;/Users/jonathanrainer/Zotero/storage/WKCL7JHS/7032496.html},
  keywords = {adaptive victim cache scheme,Benchmark testing,cache application behaviour,cache logical organization,cache storage,Computer architecture,Energy consumption,Equations,Mathematical model,memory access,Organizations,Runtime,small cache block}
}

@inproceedings{neefsTechniqueHighBandwidth2000,
  title = {A Technique for High Bandwidth and Deterministic Low Latency Load/Store Accesses to Multiple Cache Banks},
  booktitle = {Proceedings {{Sixth International Symposium}} on {{High}}-{{Performance Computer Architecture}}. {{HPCA}}-6 ({{Cat}}. {{No}}.{{PR00550}})},
  author = {Neefs, H. and Vandierendonck, H. and De Bosschere, K.},
  year = {2000},
  month = jan,
  pages = {313--324},
  issn = {null},
  doi = {10.1109/HPCA.2000.824360},
  abstract = {One of the problems in future processors will be the resource conflicts caused by several load/store units competing to access the same cache bank. The traditional approach for handling this case is by introducing buffers combined with a cross-bar. This approach suffers from (i) the non-deterministic latency of a load/store and (ii) the extra latency caused by the cross-bar and the buffer management. A deterministic latency is of the utmost importance for the forwarding mechanism of out-of-order processors because it enables back-to-back operation of instructions. We propose a technique by which we eliminate the buffers and cross-bars from the critical path of the load/store execution. This results in both, a low and a deterministic latency. Our solution consists of predicting which bank is to be accessed. Only in the case of a wrong prediction a penalty results.},
  file = {/Users/jonathanrainer/Zotero/storage/XHQBTGVH/Neefs et al. - 2000 - A technique for high bandwidth and deterministic l.pdf;/Users/jonathanrainer/Zotero/storage/HLU49NVY/824360.html},
  keywords = {Bandwidth,buffer management,buffer storage,Clocks,computer architecture,Data mining,Delay,deterministic latency,deterministic low latency load/store accesses,high bandwidth,Information systems,instruction sets,load/store execution,multiple cache banks,Multiprocessor interconnection networks,Out of order,out-of-order processors,performance evaluation,Prediction algorithms,Read only memory,resource conflicts}
}

@inproceedings{nesbitACDCAdaptive2004,
  title = {{{AC}}/{{DC}}: An Adaptive Data Cache Prefetcher},
  shorttitle = {{{AC}}/{{DC}}},
  booktitle = {Proceedings. 13th {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}}, 2004. {{PACT}} 2004.},
  author = {Nesbit, K.J. and Dhodapkar, A.S. and Smith, J.E.},
  year = {2004},
  month = oct,
  pages = {135--145},
  issn = {1089-795X},
  doi = {10.1109/PACT.2004.1342548},
  abstract = {AC/DC is an adaptive method for prefetching data from main memory. The basic prefetch method divides the memory address space into equal-sized concentration zones (CZones), and uses a global history buffer to track and detect patterns in miss address "deltas" (differences between consecutive addresses) within each CZone. When simulated with a realistic desktop memory system, CZone prefetching with delta correlations (C/DC) outperforms four other previously proposed prefetching methods. C/DC yields an average performance improvement of 23 percent when compared with no prefetching. Adaptivity is then added to the basic method. A tuning algorithm dynamically configures the CZone size and prefetch degree (i.e. the amount of data pre-fetched) on a per program-phase basis. Adaptive reconfiguration provides additional performance improvements of 4\% over C/DC. Overall, the adaptive CZone / delta correlation (AC/DC) method outperforms other methods studied by 10\%.},
  file = {/Users/jonathanrainer/Zotero/storage/C29AFJ58/Nesbit et al. - 2004 - ACDC an adaptive data cache prefetcher.pdf;/Users/jonathanrainer/Zotero/storage/4L4N5NCB/1342548.html},
  keywords = {adaptive data cache prefetcher,cache storage,computer architecture,concentration zones,delta correlations,desktop memory system,global history buffer,instruction sets,Parallel architectures,performance evaluation,Prefetching,tuning algorithm}
}

@inproceedings{nesbitDataCachePrefetching2004,
  title = {Data {{Cache Prefetching Using}} a {{Global History Buffer}}},
  booktitle = {10th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}}'04)},
  author = {Nesbit, K.J. and Smith, J.E.},
  year = {2004},
  month = feb,
  pages = {96--96},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2004.10030},
  abstract = {A new structure for implementing data cache prefetching is proposed and analyzed via simulation. The structure is based on a Global History Buffer that holds the most recent miss addresses in FIFO order. Linked lists within this global history buffer connect addresses that have some common property, e.g. they were all generated by the same load instruction. The Global History Buffer can be used for implementing a number of previously proposed prefetch methods, as well as new ones. Prefetching with the Global History Buffer has two significant advantages over conventional table prefetching methods. First, the use of a FIFO history buffer can improve the accuracy of correlation prefetching by eliminating stale data from the table. Second, the Global History Buffer contains a more complete (and intact) picture of cache miss history, creating opportunities to design more effective prefetching methods. Global History Buffer prefetching can increase correlation prefetching performance by 20\% and cut its memory traffic by 90\%. Furthermore, the Global History Buffer can make correlations within a load's address stream, which can increase stride prefetching performance by 6\%. Collectively, the Global History Buffer prefetching methods perform as well or better than the conventional prefetching methods studied on 14 of 15 benchmarks.},
  file = {/Users/jonathanrainer/Zotero/storage/ATJG7J9P/Nesbit and Smith - 2004 - Data Cache Prefetching Using a Global History Buff.pdf;/Users/jonathanrainer/Zotero/storage/DGQGKT49/1410068.html},
  keywords = {Analytical models,Cache memory,Clocks,Computational modeling,Computer simulation,Delay,History,Microarchitecture,Microprocessors,Prefetching}
}

@misc{NiosIIProcessor2019,
  title = {Nios {{II Processor Reference Guide}}},
  year = {2019},
  month = jul,
  publisher = {{Intel}},
  file = {/Users/jonathanrainer/Zotero/storage/G9PF6HA3/Nios II Processor Reference Guide.pdf}
}

@inproceedings{nowatzykMissingMemoryWall1996,
  title = {Missing the {{Memory Wall}}: {{The Case}} for {{Processor}}/{{Memory Integration}}},
  shorttitle = {Missing the {{Memory Wall}}},
  booktitle = {23rd {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}}'96)},
  author = {Nowatzyk, A. and {Fong Pong} and Saulsbury, A.},
  year = {1996},
  month = may,
  pages = {90--90},
  doi = {10.1109/ISCA.1996.10008},
  abstract = {Current high performance computer systems use complex, large superscalar CPUs that interface to the main memory through a hierarchy of caches and interconnect systems. These CPU-centric designs invest a lot of power and chip area to bridge the widening gap between CPU and main memory speeds. Yet, many large applications do not operate well on these systems and are limited by the memory subsystem performance.This paper argues for an integrated system approach that uses less-powerful CPUs that are tightly integrated with advanced memory technologies to build competitive systems with greatly reduced cost and complexity. Based on a design study using the next generation 0.25\textmu{}m, 256Mbit dynamic random-access memory (DRAM) process and on the analysis of existing machines, we show that processor memory integration can be used to build competitive, scalable and cost-effective MP systems.We present results from execution driven uni- and multi-processor simulations showing that the benefits of lower latency and higher bandwidth can compensate for the restrictions on the size and complexity of the integrated processor. In this system, small direct mapped instruction caches with long lines are very effective, as are column buffer data caches augmented with a victim cache.},
  file = {/Users/jonathanrainer/Zotero/storage/39SLDQLP/Nowatzyk et al. - 1996 - Missing the Memory Wall The Case for ProcessorMe.pdf;/Users/jonathanrainer/Zotero/storage/TB95MCA6/1563038.html},
  keywords = {Analytical models,Application software,backward error recovery,Bridges,coherence protocol,Computer interfaces,Cost benefit analysis,Delay,fault-tolerance,High performance computing,Paper technology,Power system interconnection,Random access memory,Scalable Shared Memory Multiprocessors}
}

@inproceedings{olanrewajuStudyPerformanceEvaluation2016,
  title = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms: {{A}} Review},
  shorttitle = {A Study on Performance Evaluation of Conventional Cache Replacement Algorithms},
  booktitle = {2016 {{Fourth International Conference}} on {{Parallel}}, {{Distributed}} and {{Grid Computing}} ({{PDGC}})},
  author = {Olanrewaju, R. F. and Baba, A. and Khan, B. U. I. and Yaacob, M. and Azman, A. W. and Mir, M. S.},
  year = {2016},
  month = dec,
  pages = {550--556},
  doi = {10.1109/PDGC.2016.7913185},
  abstract = {Cache Replacement Policies play a significant and contributory role in the context of determining the effectiveness of cache memory cells. It has also become one of the major key features for efficient memory management from the technological aspect. Hence, owing to the existing critical computing systems, it has become too essential to attain faster processing of executable instructions under any adverse situations. In the current scenario, the state of art processors such as Intel multi-core processors for application specific integrated circuits, usually employ various cache replacement policies such as Least Recently Used (LRU) and Pseudo LRU (pLRU), Round Robin, etc. However, fewer amounts of existing research works are found till date to utter about explicit performance issues associated with the conventional cache replacement algorithms. Therefore, the proposed study intended to carry out a performance evaluation to explore the design space of conventional cache replacement policies under SPEC CPU2000 benchmark suite. It initiates and configures the experimental Simple Scalar toolbox prototype on a wide range of cache sizes. The experimental outcomes obtained from the benchmark suite show that PLRU outperforms the conventional LRU concerning computational complexity and a variety of cache blocks organization.},
  file = {/Users/jonathanrainer/Zotero/storage/XDIQGC4I/Olanrewaju et al. - 2016 - A study on performance evaluation of conventional .pdf;/Users/jonathanrainer/Zotero/storage/6QTGL6JS/7913185.html},
  keywords = {Benchmark testing,cache blocks organization,cache memory,Cache memory,cache memory cells,cache replacement algorithms,cache replacement policies,cache storage,Context,Decision support systems,Heuristic algorithms,memory management,performance evaluation,Performance evaluation,replacement algorithms,Simple Scalar toolbox prototype,Space exploration,SPEC CPU2000 benchmark suite,storage management}
}

@article{olukotunMultilevelOptimizationPipelined1997,
  title = {Multilevel Optimization of Pipelined Caches},
  author = {Olukotun, K. and Mudge, T. N. and Brown, R. B.},
  year = {1997},
  month = oct,
  volume = {46},
  pages = {1093--1102},
  doi = {10.1109/12.628394},
  abstract = {This paper formulates and shows how to solve the problem of selecting the cache size and depth of cache pipelining that maximizes the performance of a given instruction-set architecture. The solution combines trace-driven architectural simulations and the timing analysis of the physical implementation of the cache. Increasing cache size tends to improve performance but this improvement is limited because cache access time increases with its size. This trade-off results in an optimization problem we referred to as multilevel optimization, because it requires the simultaneous consideration of two levels of machine abstraction: the architectural level and the physical implementation level. The introduction of pipelining permits the use of larger caches without increasing their apparent access time, however, the bubbles caused by load and branch delays limit this technique. In this paper we also show how multilevel optimization can be applied to pipelined systems if software- and hardware-based strategies are considered for hiding the branch and load delays. The multilevel optimization technique is illustrated with the design of a pipelined cache for a high clock rate MIPS-based architecture. The results of this design exercise show that, because processors with pipelined caches can have shorter CPU cycle times and larger caches, a significant performance advantage is gained by using two or three pipeline stages to fetch data from the cache. Of course, the results are only optimal for the implementation technologies chosen for the design exercise; other choices could result in quite different optimal designs. The exercise is primarily to illustrate the steps in the design of pipelined caches using multilevel optimization; however, it does exemplify the importance of pipelined caches if high clock rate processors are to achieve high performance.},
  file = {/Users/jonathanrainer/Zotero/storage/5FS3XDXT/Olukotun et al. - 1997 - Multilevel optimization of pipelined caches.pdf;/Users/jonathanrainer/Zotero/storage/YVAARZIE/628394.html},
  journal = {IEEE Transactions on Computers},
  keywords = {cache pipelining,cache storage,clock rate processors,Clocks,Computer architecture,CPU cycle times,Delay effects,Design optimization,discrete event simulation,Gallium arsenide,instruction sets,instruction-set architecture,Logic design,memory architecture,Multichip modules,multilevel optimization,optimization problem,Packaging,performance,Pipeline processing,pipelined caches,pipelining,timing,Timing,timing analysis,trace-driven architectural simulations},
  number = {10}
}

@article{olukotunPerformanceOptimizationPipelined1992,
  title = {Performance Optimization of Pipelined Primary Cache},
  author = {Olukotun, Kunle and Mudge, Trevor and Brown, Richard},
  year = {1992},
  month = jun,
  volume = {20},
  pages = {181--190},
  issn = {01635964},
  doi = {10.1145/146628.139726},
  file = {/Users/jonathanrainer/Zotero/storage/V83DPL7A/Olukotun et al. - 1992 - Performance optimization of pipelined primary cach.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {2}
}

@inproceedings{oneilLRUKPageReplacement1993,
  title = {The {{LRU}}-{{K}} Page Replacement Algorithm for Database Disk Buffering},
  author = {O'Neil, Elizabeth J. and O'Neil, Patrick E. and Weikum, Gerhard},
  year = {1993},
  pages = {297--306},
  publisher = {{ACM Press}},
  doi = {10.1145/170035.170081},
  file = {/Users/jonathanrainer/Zotero/storage/E7TV8E25/O’Neil and O’Neill - The LRU-K Page Replacement Algorithm For Database .pdf},
  isbn = {978-0-89791-592-2},
  language = {en}
}

@misc{OpenSourceHardware,
  title = {Open {{Source Hardware Association}} - {{Homepage}}},
  howpublished = {https://www.oshwa.org/},
  journal = {Open Source Hardware Association}
}

@inproceedings{oRowbufferDecouplingCase2014,
  title = {Row-Buffer Decoupling: {{A}} Case for Low-Latency {{DRAM}} Microarchitecture},
  shorttitle = {Row-Buffer Decoupling},
  booktitle = {2014 {{ACM}}/{{IEEE}} 41st {{International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {O, S. and Son, Y. H. and Kim, N. S. and Ahn, J. H.},
  year = {2014},
  month = jun,
  pages = {337--348},
  doi = {10.1109/ISCA.2014.6853230},
  abstract = {Modern DRAM devices for the main memory are structured to have multiple banks to satisfy ever-increasing throughput, energy-efficiency, and capacity demands. Due to tight cost constraints, only one row can be buffered (opened) per bank and actively service requests at a time, while the row must be deactivated (closed) before a new row is stored into the row buffers. Hasty deactivation unnecessarily re-opens rows for otherwise row-buffer hits while hindsight accompanies the deactivation process on the critical path of accessing data for row-buffer misses. The time to (de)activate a row is comparable to the time to read an open row while applications are often sensitive to DRAM latency. Hence, it is critical to make the right decision on when to close a row. However, the increasing number of banks per DRAM device over generations reduces the number of requests per bank. This forces a memory controller to frequently predict when to close a row due to a lack of information on future requests, while the dynamic nature of memory access patterns limits the prediction accuracy. In this paper, we propose a novel DRAM microarchitecture that can eliminate the need for any prediction. First, we identify that precharging the bitlines dominates the deactivate time, while sense amplifiers that work as a row buffer are physically coupled with the bitlines such that a single command precharges both bitlines and sense amplifiers simultaneously. By decoupling the bitlines from the row buffers using isolation transistors, the bitlines can be precharged right after a row becomes activated. Therefore, only the sense amplifiers need to be precharged for a miss in most cases, taking an order of magnitude shorter time than the conventional deactivation process. Second, we show that this row-buffer decoupling enables internal DRAM {$\mu$}-operations to be separated and recombined, which can be exploited by memory controllers to make the main memory system more energy efficient. Our experiments demonstrate that row-buffer decoupling improves the geometric mean of the instructions per cycle and MIPS2/W by 14\% and 29\%, respectively, for memory-intensive SPEC CPU2006 applications.},
  file = {/Users/jonathanrainer/Zotero/storage/F2TEIUI5/O et al. - 2014 - Row-buffer decoupling A case for low-latency DRAM.pdf;/Users/jonathanrainer/Zotero/storage/GPVT642Y/6853230.html},
  keywords = {buffer storage,Buffer storage,Capacitance,deactivation process,DRAM chips,DRAM latency,energy efficiency,isolation transistors,low-latency DRAM microarchitecture,memory access patterns,memory controller,Memory management,memory system,memory-intensive SPEC CPU2006 applications,Microarchitecture,modern DRAM devices,Random access memory,row-buffer decoupling,row-buffer misses,Transistors}
}

@incollection{osawaGenerationalReplacementSchemes1997,
  title = {Generational Replacement Schemes for a {{WWW}} Caching Proxy Server},
  booktitle = {High-{{Performance Computing}} and {{Networking}}},
  author = {Osawa, Noritaka and Yuba, Toshitsugu and Hakozaki, Katsuya},
  editor = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Hertzberger, Bob and Sloot, Peter},
  year = {1997},
  volume = {1225},
  pages = {940--949},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0031665},
  abstract = {This paper proposes and evaluates generational replacement schemes that suit access patterns to the World Wide Web (WWW) proxy server cache. The schemes partition a cache into generations and put frequently accessed data into older generations where entries are less likely to be replaced. Using our schemes, the hit rate per page is improved by about 5.2 percentage points over the Least Recently Used (LRU) algorithm on the basis of logs of more than 8 million accesses. This improvement reduces the number of cache misses by about 10.8 percent with respect to LRU. Our improvement is roughly twice as good as the improvement of LRU over the First-In First-Out (FIFO) algorithm.},
  file = {/Users/jonathanrainer/Zotero/storage/D6QXG4EE/Osawa et al. - 1997 - Generational replacement schemes for a WWW caching.pdf},
  isbn = {978-3-540-62898-9 978-3-540-69041-2},
  language = {en}
}

@inproceedings{ozawaCacheMissHeuristics1995,
  title = {Cache Miss Heuristics and Preloading Techniques for General-Purpose Programs},
  booktitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  author = {Ozawa, T. and Kimura, Y. and Nishizaki, S.},
  year = {1995},
  month = nov,
  pages = {243--248},
  doi = {10.1109/MICRO.1995.476832},
  abstract = {Previous research on hiding memory latencies has tended to focus on regular numerical programs. This paper presents a latency-hiding compiler technique that is applicable to general-purpose C programs. By assuming a lock-up free cache and instruction score-boarding, our technique 'preloads' the data that are likely to cause a cache-miss before they are used, and thereby hiding the cache miss latency. We have developed simple compiler heuristics to identify load instructions that are likely to cause a cache-miss. Experimentation with a set of SPEC92 benchmarks shows that our heuristics are successful in identifying 85\% of cache misses. We have also developed an algorithm that flexibly schedules the selected load instruction and instructions that use the loaded data to hide memory latency. Our simulation suggests that our technique is successful in hiding memory latency and improves the overall performance.},
  file = {/Users/jonathanrainer/Zotero/storage/LMZYV6S3/Ozawa et al. - 1995 - Cache miss heuristics and preloading techniques fo.pdf;/Users/jonathanrainer/Zotero/storage/LKPBV58X/476832.html},
  keywords = {C language,C programs,cache miss,cache storage,compiler,compiler heuristics,Computational modeling,Computer simulation,Delay,general-purpose programs,instruction score-boarding,Laboratories,latency-hiding,Load flow analysis,load instructions,lock-up free cache,performance,preloading,Processor scheduling,program compilers,Program processors,Scheduling algorithm,SPEC92 benchmarks,storage management,Testing}
}

@incollection{pachecoParallelHardwareParallel2011,
  title = {Parallel {{Hardware}} and {{Parallel Software}}},
  booktitle = {An {{Introduction}} to {{Parallel Programming}}},
  author = {Pacheco, Peter S.},
  year = {2011},
  pages = {15--81},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-374260-5.00002-6},
  isbn = {978-0-12-374260-5},
  language = {en}
}

@article{palacharlaEvaluatingStreamBuffers1994,
  title = {Evaluating Stream Buffers as a Secondary Cache Replacement},
  author = {Palacharla, S. and Kessler, R. E.},
  year = {1994},
  month = apr,
  volume = {22},
  pages = {24--33},
  issn = {0163-5964},
  doi = {10.1145/192007.192014},
  abstract = {Today's commodity microprocessors require a low latency memory system to achieve high sustained performance. The conventional high-performance memory system provides fast data access via a large secondary cache. But large secondary caches can be expensive, particularly in large-scale parallel systems with many processors (and thus many caches).We evaluate a memory system design that can be both cost-effective as well as provide better performance, particularly for scientific workloads: a single level of (on-chip) cache backed up only by Jouppi's stream buffers [10] and a main memory. This memory system requires very little hardware compared to a large secondary cache and doesn't require modifications to commodity processors. We use trace-driven simulation of fifteen scientific applications from the NAS and PERFECT suites in our evaluation. We present two techniques to enhance the effectiveness of Jouppi's original stream buffers: filtering schemes to reduce their memory bandwidth requirement and a scheme that enables stream buffers to prefetch data being accessed in large strides. Our results show that, for the majority of our benchmarks, stream buffers can attain hit rates that are comparable to typical hit rates of secondary caches. Also, we find that as the data-set size of the scientific workload increases the performance of streams typically improves relative to secondary cache performance, showing that streams are more scalable to large data-set sizes.},
  file = {/Users/jonathanrainer/Zotero/storage/UBMDURPM/Palacharla and Kessler - 1994 - Evaluating stream buffers as a secondary cache rep.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  number = {2}
}

@article{pandaExpertPrefetchPrediction2016,
  title = {Expert {{Prefetch Prediction}}: {{An Expert Predicting}} the {{Usefulness}} of {{Hardware Prefetchers}}},
  shorttitle = {Expert {{Prefetch Prediction}}},
  author = {Panda, B. and Balachandran, S.},
  year = {2016},
  month = jan,
  volume = {15},
  pages = {13--16},
  issn = {1556-6056},
  doi = {10.1109/LCA.2015.2428703},
  abstract = {Hardware prefetching improves system performance by hiding and tolerating the latencies of lower levels of cache and off-chip DRAM. An accurate prefetcher improves system performance whereas an inaccurate prefetcher can cause cache pollution and consume additional bandwidth. Prefetch address filtering techniques improve prefetch accuracy by predicting the usefulness of a prefetch address and based on the outcome of the prediction, the prefetcher decides whether or not to issue a prefetch request. Existing techniques use only one signature to predict the usefulness of a prefetcher but no single predictor works well across all the applications. In this work, we propose weighted-majority filter, an expert way of predicting the usefulness of prefetch addresses. The proposed filter is adaptive in nature and uses the prediction of the best predictor(s) from a pool of predictors. Our filter is orthogonal to the underlying prefetching algorithm. We evaluate the effectiveness of our technique on 22 SPEC-2000/2006 applications. On an average, when employed with three state-of-the-art prefetchers such as AMPM, SMS, and GHB-PC/DC, our filter provides performance improvement of 8.1, 9.3, and 11 percent respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/G7L6PJ8W/Panda and Balachandran - 2016 - Expert Prefetch Prediction An Expert Predicting t.pdf;/Users/jonathanrainer/Zotero/storage/5E528GPU/7110318.html},
  journal = {IEEE Computer Architecture Letters},
  keywords = {Accuracy,AMPM,cache,Cache,cache storage,filtering theory,GHB-PC/DC,Hardware,hardware prefetchers,Hardware prefetching,Hardware Prefetching,memory systems,Memory systems,Pollution,Prediction algorithms,prefetch addresses,Prefetching,prefetching algorithm,Radiation detectors,Random access memory,SMS,weighted-majority filter},
  number = {1}
}

@article{pandaMemoryDataOrganization1997,
  title = {Memory Data Organization for Improved Cache Performance in Embedded Processor Applications},
  author = {Panda, Preeti Ranjan and Dutt, Nikil D. and Nicolau, Alexandru},
  year = {1997},
  month = oct,
  volume = {2},
  pages = {384--409},
  issn = {1084-4309, 1557-7309},
  doi = {10.1145/268424.268464},
  file = {/Users/jonathanrainer/Zotero/storage/N2FKNTRE/Panda et al. - 1997 - Memory data organization for improved cache perfor.pdf},
  journal = {ACM Transactions on Design Automation of Electronic Systems (TODAES)},
  language = {en},
  number = {4}
}

@inproceedings{pandaSurveyReplacementStrategies2016,
  title = {A Survey on Replacement Strategies in Cache Memory for Embedded Systems},
  booktitle = {2016 {{IEEE Distributed Computing}}, {{VLSI}}, {{Electrical Circuits}} and {{Robotics}} ({{DISCOVER}})},
  author = {Panda, Parag and Patil, Geeta and Raveendran, Biju},
  year = {2016},
  month = aug,
  pages = {12--17},
  publisher = {{IEEE}},
  address = {{Mangalore, India}},
  doi = {10.1109/DISCOVER.2016.7806218},
  abstract = {Cache is one of the most power-consuming components in computer architecture. Power reduction in cache can be achieved by reducing miss rate, miss penalty, latency per access, and power consumption per access. The power reduction can also be achieved by shutting down unused part of the cache, by allowing not so recently used cache banks to sleep, reconfiguring the cache for specific application and various combinations of one or more of these. The cache hit depends on the cache size, associativity and the cache line size. Replacement strategies in associative mapping schemes play an important role in cache hit rate performance. This survey paper proposes a classification of these strategies with detailed discussion on their advantages and disadvantages.},
  file = {/Users/jonathanrainer/Zotero/storage/LLZFK7ZJ/Panda et al. - 2016 - A survey on replacement strategies in cache memory.pdf},
  isbn = {978-1-5090-1623-5},
  language = {en}
}

@inproceedings{panImprovingVLIWProcessor2009,
  title = {Improving {{VLIW Processor Performance Using Three}}-{{Dimensional}} ({{3D}}) {{DRAM Stacking}}},
  booktitle = {2009 20th {{IEEE International Conference}} on {{Application}}-Specific {{Systems}}, {{Architectures}} and {{Processors}}},
  author = {Pan, Y. and Zhang, T.},
  year = {2009},
  month = jul,
  pages = {38--45},
  doi = {10.1109/ASAP.2009.11},
  abstract = {This work studies the potential of using emerging 3D integration to improve embedded VLIW computing system. We focus on the 3D integration of one VLIW processor die with multiple high-capacity DRAM dies. Our proposed memory architecture employs 3D stacking technology to bond one die containing several processing clusters to multiple DRAM dies for a primary memory. The 3D technology also enables wide low-latency buses between clusters and memory and enable the latency of 3D DRAM L2 cache comparable to 2D SRAM L2 cache. These enable it to replace the 2D SRAM L2 cache with 3D DRAM L2 cache. The die area for 2D SRAM L2 cache can be re-allocated to additional clusters that can improve the performance of the system. From the simulation results, we find 3D stacking DRAM main memory can improve the system performance by 10\% 80\% than 2D off-chip DRAM main memory depending on different benchmarks. Also, for a similar logic die area, a four clusters system with 3D DRAM L2 cache and 3D DRAM main memory outperforms a two clusters system with 2D SRAM L2 cache and 3D DRAM main memory by about 10\%.},
  file = {/Users/jonathanrainer/Zotero/storage/BUZIPI6T/Pan and Zhang - 2009 - Improving VLIW Processor Performance Using Three-D.pdf;/Users/jonathanrainer/Zotero/storage/48AKA72V/5200008.html},
  keywords = {2D SRAM L2 cache,3D DRAM,3D DRAM L2 cache,3D DRAM stacking,cache storage,Computer architecture,Delay,Digital signal processing,DRAM chips,DSP,Embedded computing,embedded VLIW computing system,instruction sets,low-latency buses,memory architecture,multiprocessing systems,parallel architectures,parallel machines,Parallel processing,Random access memory,SRAM chips,Stacking,System performance,USA Councils,VLIW}
}

@article{pattersonCaseIntelligentRAM1997,
  title = {A Case for Intelligent {{RAM}}},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  year = {1997},
  volume = {17},
  pages = {34--44},
  issn = {02721732},
  doi = {10.1109/40.592312},
  file = {/Users/jonathanrainer/Zotero/storage/WMAV5QJG/Patterson et al. - 1997 - A case for intelligent RAM.pdf},
  journal = {IEEE Micro},
  number = {2}
}

@book{pattersonComputerOrganizationDesign2018,
  title = {Computer Organization and Design: The Hardware/Software Interface},
  shorttitle = {Computer Organization and Design},
  author = {Patterson, David A. and Hennessy, John L.},
  year = {2018},
  edition = {RISC-V edition},
  publisher = {{Morgan Kaufmann Publishers, an imprint of Elsevier}},
  address = {{Cambridge, Massachusetts}},
  abstract = {The new RISC-V Edition of Computer Organization and Design features the RISC-V open source instruction set architecture, the first open source architecture designed to be used in modern computing environments such as cloud computing, mobile devices, and other embedded systems. With the post-PC era now upon us, Computer Organization and Design moves forward to explore this generational change with examples, exercises, and material highlighting the emergence of mobile computing and the Cloud. Updated content featuring tablet computers, Cloud infrastructure, and the x86 (cloud computing) and ARM (mobile computing devices) architectures is included},
  file = {/Users/jonathanrainer/Zotero/storage/4Q6FYX39/Computer-Organization-and-Design-The-Hardware-Software-Interface-RISC-V-Edition-.pdf},
  isbn = {978-0-12-812275-4},
  keywords = {Computer engineering,Computer interfaces,Computer organization},
  lccn = {QA76.9.C643 P37 2018},
  note = {OCLC: ocn993666159}
}

@inproceedings{pattersonIntelligentRAMIRAM1997,
  title = {Intelligent {{RAM}} ({{IRAM}}): Chips That Remember and Compute},
  shorttitle = {Intelligent {{RAM}} ({{IRAM}})},
  booktitle = {1997 {{IEEE International Solids}}-{{State Circuits Conference}}. {{Digest}} of {{Technical Papers}}},
  author = {Patterson, D. and Anderson, T. and Cardwell, N. and Fromm, R. and Keeton, K. and Kozyrakis, C. and Thomas, R. and Yelick, K.},
  year = {1997},
  month = feb,
  pages = {224--225},
  doi = {10.1109/ISSCC.1997.585348},
  abstract = {It is time to reconsider unifying logic and memory. Since most of the transistors on this merged chip will be devoted to memory, it is called 'intelligent RAM'. IRAM is attractive because the gigabit DRAM chip has enough transistors for both a powerful processor and a memory big enough to contain whole programs and data sets. It contains 1024 memory blocks each 1kb wide. It needs more metal layers to accelerate the long lines of 600mm/sup 2/ chips. It may require faster transistors for the high-speed interface of synchronous DRAM. Potential advantages of IRAM include lower memory latency, higher memory bandwidth, lower system power, adjustable memory width and size, and less board space. Challenges for IRAM include high chip yield given processors have not been repairable via redundancy, high memory retention rates given processors usually need higher power than DRAMs, and a fast processor given logic is slower in a DRAM process.},
  file = {/Users/jonathanrainer/Zotero/storage/7W5BM63I/Patterson et al. - 1997 - Intelligent RAM (IRAM) chips that remember and co.pdf;/Users/jonathanrainer/Zotero/storage/KI8AD45T/585348.html},
  keywords = {adjustable memory width,Bandwidth,board space,chip yield,Computer science,Delay,Electronics industry,high-speed interface,integrated circuit yield,integrated memory circuits,intelligent RAM,IRAM,Logic,memory architecture,memory bandwidth,memory latency,memory retention rates,merged chip,metal layers,microprocessor chips,Microprocessors,Random access memory,random-access storage,Read-write memory,Switches,system power,Vector processors}
}

@inproceedings{pendseInvestigationImpactVictim1998,
  title = {Investigation of Impact of Victim Cache and Victim Tracer on a Fully Associative Disk Cache},
  booktitle = {1998 {{Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{Cat}}. {{No}}. {{98CB36268}})},
  author = {Pendse, R. and Kushanagar, N. and Walterscheidt, U.},
  year = {1998},
  month = aug,
  pages = {78--81},
  issn = {null},
  doi = {10.1109/MWSCAS.1998.759439},
  abstract = {In this paper, we present results of investigation to study the impact of victim cache and victim tracer on the miss rates due to the LRU block replacement algorithm. A total of three different algorithms were implemented. They include the basic LRU, LRU with victim cache, and LRU with victim tracer. All the algorithms were implemented as a part of a disk cache. For victim cache and victim tracer, a unique priority scheme was used. Extensive simulations were performed on all the algorithms. The results indicate that adding a victim cache or victim tracer does improve the performance.},
  file = {/Users/jonathanrainer/Zotero/storage/7RUDQLBC/Pendse et al. - 1998 - Investigation of impact of victim cache and victim.pdf;/Users/jonathanrainer/Zotero/storage/9JQ6VNY6/759439.html},
  keywords = {Cache memory,cache storage,content-addressable storage,Degradation,Delay,Disk drives,Floods,fully associative disk cache,hard discs,Hard disks,LRU block replacement algorithm,Microprocessors,miss rates,priority scheme,Random access memory,Throughput,victim cache,victim tracer}
}

@inproceedings{pitkowSimpleRobustCaching1994,
  title = {A {{Simple Yet Robust Caching Algorithm Based}} on {{Dynamic Access Patterns}}},
  booktitle = {Proceedings of the {{Second International WWW Conference}}},
  author = {Pitkow, James E and Recker, Margaret M},
  year = {1994},
  month = oct,
  pages = {8},
  abstract = {The World-Wide Web continues its remarkable and seemingly unregulated growth. This growth has seen a corresponding increase in network loads and user response times. One common approach for improving the retrieval rate of large, distributed documents is via caching. In this paper, we present a caching algorithm that flexibly adapt its parameters to the hit rates and access patterns of users requesting documents. The algorithm is derived from an analysis of user accesses in a WWW database. In particular, the analysis is based upon a model from psychological research on human memory, which has long studied retrieval of memory items based on frequency and recency rates of past item occurrences. Results show that the model predicts document access with a high degree of accuracy. Furthermore, the model indicates that a caching algorithm based upon the recency rates of prior document access will reliably handle future document requests. The algorithm presented is simple, robust, and easily implementable.},
  file = {/Users/jonathanrainer/Zotero/storage/55HXWSK9/Pitkow and Recker - A Simple Yet Robust Caching Algorithm Based on Dyn.pdf},
  language = {en}
}

@article{podlipnigSurveyWebCache2003,
  title = {A {{Survey}} of {{Web Cache Replacement Strategies}}},
  author = {Podlipnig, Stefan and B{\"o}sz{\"o}rmenyi, Laszlo},
  year = {2003},
  month = dec,
  volume = {35},
  pages = {374--398},
  issn = {0360-0300},
  doi = {10.1145/954339.954341},
  abstract = {Web caching is an important technique to scale the Internet. One important performance factor of Web caches is the replacement strategy. Due to specific characteristics of the World Wide Web, there exist a huge number of proposals for cache replacement. This article proposes a classification for these proposals that subsumes prior classifications. Using this classification, different proposals and their advantages and disadvantages are described. Furthermore, the article discusses the importance of cache replacement strategies in modern proxy caches and outlines potential future research topics.},
  file = {/Users/jonathanrainer/Zotero/storage/U8RJRSPT/Podlipnig and Böszörmenyi - 2003 - A Survey of Web Cache Replacement Strategies.pdf},
  journal = {ACM Comput. Surv.},
  keywords = {replacement strategies,Web caching},
  number = {4}
}

@article{ponugotiEnablingOntheFlyHardware2019,
  title = {Enabling {{On}}-the-{{Fly Hardware Tracing}} of {{Data Reads}} in {{Multicores}}},
  author = {Ponugoti, Mounika and Milenkovic, Aleksandar},
  year = {2019},
  month = jun,
  volume = {18},
  pages = {1--27},
  issn = {15399087},
  doi = {10.1145/3322642},
  file = {/Users/jonathanrainer/Zotero/storage/4E6A5JFS/Ponugoti and Milenkovic - 2019 - Enabling On-the-Fly Hardware Tracing of Data Reads.pdf},
  journal = {ACM Transactions on Embedded Computing Systems},
  language = {en},
  number = {4}
}

@book{przybylskiCacheMemoryHierarchy1990,
  title = {Cache and Memory Hierarchy Design: A Performance-Directed Approach},
  shorttitle = {Cache and Memory Hierarchy Design},
  author = {Przybylski, Steven A.},
  year = {1990},
  publisher = {{Morgan Kaufmann Publishers}},
  address = {{San Mateo, Calif}},
  file = {/Users/jonathanrainer/Zotero/storage/GJ4LFRQ6/Przybylski - 1990 - Cache and memory hierarchy design a performance-d.pdf},
  isbn = {978-1-55860-136-9},
  keywords = {Cache memory,Memory hierarchy (Computer science)},
  language = {en},
  lccn = {TK7895.M4 P79 1990}
}

@inproceedings{przybylskiCharacteristicsPerformanceOptimalMultilevel1989,
  title = {Characteristics {{Of Performance}}-{{Optimal Multi}}-Level {{Cache Hierarchies}}},
  booktitle = {The 16th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Przybylski, S. and Horowitz, M. and Hennessy, J.},
  year = {1989},
  month = may,
  pages = {114--121},
  issn = {1063-6897},
  doi = {10.1109/ISCA.1989.714545},
  abstract = {The increasing speed of new generation processors will exacerbate the already large difference between CPU cycle times and main memory access times. As this difference. grows, it will be increasingly difficult to build single-level caches that are both fast enough to match these fast cycle times and large enough to effectively hide the slow main memory access times. One solution to this problem is to use a multi-level cache hierarchy. This paper examines the relationship between cache organization and program execution time for multi-level caches. We show that a first-level cache dramatically reduces the number of references seen by a second-level cache, without having a large effect on the number of second-level cache misses. This reduction in the number of second-level cache hits changes the optimal design point by decreasing the importance of the cycle-time of the second-level cache relative to its size. The lower the first-level cache miss rate, the less important the second- level cycle time becomes. This change in relative importance of cycle time and miss rate makes associativity more attractive and increases the optimal cache size for second-level caches over what they would be for an equivalent single-level cache system.},
  file = {/Users/jonathanrainer/Zotero/storage/NCQGRQF7/Przybylski et al. - 1989 - Characteristics Of Performance-Optimal Multi-level.pdf;/Users/jonathanrainer/Zotero/storage/3SX4922U/714545.html},
  keywords = {Distributed computing,Laboratories,Performance analysis,Permission,System performance,Upper bound}
}

@inproceedings{przybylskiPerformanceTradeoffsCache1988,
  title = {Performance Tradeoffs in Cache Design},
  booktitle = {[1988] {{The}} 15th {{Annual International Symposium}} on {{Computer Architecture}}. {{Conference Proceedings}}},
  author = {Przybylski, S. and Horowitz, M. and Hennessy, J.},
  year = {1988},
  month = may,
  pages = {290--298},
  issn = {null},
  doi = {10.1109/ISCA.1988.5239},
  abstract = {A series of simulations that explore the interactions between various organizational decisions and program execution time are presented. The tradeoffs between cache size and CPU/cache cycle-time, set associativity and cycle time, and block size and main-memory speed, are investigated. The results indicate that neither cycle time nor cache size dominates the other across the entire design space. For common implementation technologies, performance is maximized when the size is increased to the size is increased to the 32-kB to 128-kB range with modest penalties to the cycle time. If set associativity impacts the cycle time by more than a few nanoseconds, it increases overall execution time. Since the block size and memory-transfer rate combine to affect the cache miss penalty, the optimum block size is substantially smaller than that which minimizes the miss rate. The interdependence between optimal cache configuration and the main memory speed necessitates multilevel cache hierarchies for high-performance uniprocessors.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/7LC23WSB/Przybylski et al. - 1988 - Performance tradeoffs in cache design.pdf;/Users/jonathanrainer/Zotero/storage/JAEDYA4S/5239.html},
  keywords = {32 to 128 kB,32-kB to 128-kB,block size,buffer storage,cache miss penalty,cache performance,cache size,CPU/cache cycle-time,high-performance uniprocessors,main memory speed,main-memory speed,memory-transfer rate,multilevel cache hierarchies,nanoseconds,optimal cache configuration,optimum block size,organizational decisions,performance evaluation,program execution time,set associativity,Space technology,storage management}
}

@inproceedings{qaziOptimizationAccessLatency2016,
  title = {Optimization of Access Latency in {{DRAM}}},
  booktitle = {2016 {{International Conference}} on {{Computing}}, {{Electronic}} and {{Electrical Engineering}} ({{ICE Cube}})},
  author = {Qazi, A. and Ullah, Z. and Rehman, K. and Khan, M. H. and Bilal, M.},
  year = {2016},
  month = apr,
  pages = {163--168},
  doi = {10.1109/ICECUBE.2016.7495216},
  abstract = {Modern digital systems, which involve high data computations, suffer from high memory access latency; thus, latency becomes a core issue in the performance enhancement of these advance digital machines. Different factors are behind the high latency of advance digital systems. Approaches like array binding and allocation, code rewriting, and others are adopted to reduce the overall latency of these systems. In this paper, we explore new dimensions to achieve maximum latency optimization in applications that involve extensive memory access. The proposed algorithm of idle/slack time management utilizes empty slots in memory access of different memory modules by appropriately activating upcoming commands in advance. The optimization of latency is further increased by incorporating the multi-way conflict resolution algorithm in second stage and followed by the use of advance dynamic buffers in third stage. Our successive three stages approach of adopting slack time management, multi-way partitioning using min-cut algorithm, and the use of advance dynamic buffers yields better results. Comparison of the experimental results with different benchmarks shows that our proposed technique optimizes the existing page-mode technique by 9\%. Hence, adopting this proposed optimization strategy significantly reduces overall latency of modern digital systems.},
  file = {/Users/jonathanrainer/Zotero/storage/9TU9TPY5/Qazi et al. - 2016 - Optimization of access latency in DRAM.pdf;/Users/jonathanrainer/Zotero/storage/TTIFJIH4/7495216.html},
  keywords = {access latency optimization,allocation,array binding,Arrays,Bandwidth,buffer storage,Clocks,code rewriting,data computation,Delays,digital machines,digital system,Digital systems,DRAM,DRAM chips,dynamic buffer,extensive memory access,idle time management,memory access latency,memory modules,min-cut algorithm,multiway conflict resolution algorithm,multiway partitioning,Optimization,page-mode technique,performance enhancement,Resource management,slack time management}
}

@inproceedings{qiwangReducingDRAMCache2016,
  title = {Reducing {{DRAM Cache Access}} in Cache Miss via an Effective Predictor},
  booktitle = {2016 7th {{IEEE International Conference}} on {{Software Engineering}} and {{Service Science}} ({{ICSESS}})},
  author = {{Qi Wang} and {Yanzhen Xing} and {Donghui Wang}},
  year = {2016},
  month = aug,
  pages = {501--504},
  doi = {10.1109/ICSESS.2016.7883118},
  abstract = {As more and more cores are integrated on a single chip, memory speed has become a major performance bottleneck. The widening latency gap between high speed cores and main memory has led to the evolution of multi-level caches and using DRAM as the Last-Level-Cache (LLC). The main problem of employing DRAM cache is their high tag lookup latency. If DRAM cache misses, the latency of memory access will be increased comparing with the system without DRAM cache. To solve this problem, we propose an effective predictor to Reduce DRAM Cache Access (RCA) in cache miss. The predictor composes of a saturating counter and a Partial MissMap (P\_Map). If the saturating counter indicates a hit, then the request will be send to the P\_Map to further lookup whether it is a hit or not. The evaluation results show that RCA can improve system performance by 8.2\% and 3.4\% on average, compared to MissMap and MAP\_G, respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/BGTM47V4/Qi Wang et al. - 2016 - Reducing DRAM Cache Access in cache miss via an ef.pdf;/Users/jonathanrainer/Zotero/storage/U9QHFR6I/7883118.html},
  keywords = {cache miss,Cache miss,cache storage,DRAM cache,DRAM cache access,DRAM chips,effective predictor,last-level-cache,LLC,multilevel cache,P_Map,partial missmap,predictor,Random access memory,RCA,saturating counter,Two dimensional displays}
}

@inproceedings{quinonesUsingRandomizedCaches2009,
  title = {Using {{Randomized Caches}} in {{Probabilistic Real}}-{{Time Systems}}},
  author = {Qui{\~n}ones, Eduardo and Berger, Emery D. and Bernat, Guillem and Cazorla, Francisco J.},
  year = {2009},
  month = jul,
  pages = {129--138},
  publisher = {{IEEE}},
  doi = {10.1109/ECRTS.2009.30},
  abstract = {While hardware caches are generally effective at improving application performance, they greatly complicate performance prediction. Slight changes in memory layout or data access patterns can lead to large and systematic increases in cache misses, degrading performance. In the worst case, these misses can effectively render the cache useless. These pathological cases, or ``cache risk patterns'', are difficult to predict, test or debug, and their presence limits the usefulness of caches in safety critical real-time systems, especially in hard real-time environments.},
  file = {/Users/jonathanrainer/Zotero/storage/JQHCGV28/Quiñones et al. - 2009 - Using Randomized Caches in Probabilistic Real-Time.pdf},
  isbn = {978-0-7695-3724-5},
  language = {en}
}

@article{qureshiAdaptiveInsertionPolicies2007,
  title = {Adaptive Insertion Policies for High Performance Caching},
  author = {Qureshi, Moinuddin K. and Jaleel, Aamer and Patt, Yale N. and Steely, Simon C. and Emer, Joel},
  year = {2007},
  month = jun,
  volume = {35},
  pages = {381},
  issn = {01635964},
  doi = {10.1145/1273440.1250709},
  file = {/Users/jonathanrainer/Zotero/storage/BHHPMZSD/Qureshi et al. - Adaptive Insertion Policies for High Performance C.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {2}
}

@article{qureshiCaseMLPAwareCache2006,
  title = {A {{Case}} for {{MLP}}-{{Aware Cache Replacement}}},
  author = {Qureshi, Moinuddin K. and Lynch, Daniel N. and Mutlu, Onur and Patt, Yale N.},
  year = {2006},
  month = may,
  volume = {34},
  pages = {167--178},
  issn = {0163-5964},
  doi = {10.1145/1150019.1136501},
  abstract = {Performance loss due to long-latency memory accesses can be reduced by servicing multiple memory accesses concurrently. The notion of generating and servicing long-latency cache misses in parallel is called Memory Level Parallelism (MLP). MLP is not uniform across cache misses - some misses occur in isolation while some occur in parallel with other misses. Isolated misses are more costly on performance than parallel misses. However, traditional cache replacement is not aware of the MLP-dependent cost differential between different misses. Cache replacement, if made MLP-aware, can improve performance by reducing the number of performance-critical isolated misses. This paper makes two key contributions. First, it proposes a framework for MLP-aware cache replacement by using a runtime technique to compute the MLP-based cost for each cache miss. It then describes a simple cache replacement mechanism that takes both MLP-based cost and recency into account. Second, it proposes a novel, low-hardware overhead mechanism called Sampling Based Adaptive Replacement (SBAR), to dynamically choose between an MLP-aware and a traditional replacement policy, depending on which one is more effective at reducing the number of memory related stalls. Evaluations with the SPEC CPU2000 benchmarks show that MLP-aware cache replacement can improve performance by as much as 23\%.},
  file = {/Users/jonathanrainer/Zotero/storage/TTY6FRKA/Qureshi et al. - 2006 - A Case for MLP-Aware Cache Replacement.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  number = {2}
}

@article{qureshiSetDuelingControlledAdaptiveInsertion2008,
  title = {Set-{{Dueling}}-{{Controlled Adaptive Insertion}} for {{High}}-{{Performance Caching}}},
  author = {Qureshi, M. K. and Jaleel, A. and Patt, Y. N. and Steely, S. C. and Emer, J.},
  year = {2008},
  month = jan,
  volume = {28},
  pages = {91--98},
  doi = {10.1109/MM.2008.14},
  abstract = {The commonly used LRU replacement policy causes thrashing for memory- intensive workloads. A simple mechanism that dynamically changes the insertion policy used by LRU replacement reduces cache misses by 21 percent and requires a total storage overhead of less than 2 bytes.},
  file = {/Users/jonathanrainer/Zotero/storage/NTP2QE89/Qureshi et al. - 2008 - Set-Dueling-Controlled Adaptive Insertion for High.pdf;/Users/jonathanrainer/Zotero/storage/DYNNZ8PJ/4460516.html},
  journal = {IEEE Micro},
  keywords = {Art,Bridges,cache,cache storage,Cache storage,Data structures,Filters,Hardware,high-performance caching,insertion,LRU replacement policy,memory-intensive workloads,Proposals,replacement,set dueling,set sampling,set-dueling-controlled adaptive insertion,storage management,System performance,thrashing},
  number = {1}
}

@inproceedings{qureshiVWayCacheDemandbased2005,
  title = {The {{V}}-{{Way}} Cache: Demand-Based Associativity via Global Replacement},
  shorttitle = {The {{V}}-{{Way}} Cache},
  booktitle = {32nd {{International Symposium}} on {{Computer Architecture}} ({{ISCA}}'05)},
  author = {Qureshi, M.K. and Thompson, D. and Patt, Y.N.},
  year = {2005},
  month = jun,
  pages = {544--555},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2005.52},
  abstract = {As processor speeds increase and memory latency becomes more critical, intelligent design and management of secondary caches becomes increasingly important. The efficiency of current set-associative caches is reduced because programs exhibit a non-uniform distribution of memory accesses across different cache sets. We propose a technique to vary the associativity of a cache on a per-set basis in response to the demands of the program. By increasing the number of tag-store entries relative to the number of data lines, we achieve the performance benefit of global replacement while maintaining the constant hit latency of a set-associative cache. The proposed variable-way, or V-Way, set-associative cache achieves an average miss rate reduction of 13\% on sixteen benchmarks from the SPEC CPU2000 suite. This translates into an average IPC improvement of 8\%.},
  file = {/Users/jonathanrainer/Zotero/storage/HPAX4X92/Qureshi et al. - 2005 - The V-Way cache demand-based associativity via gl.pdf;/Users/jonathanrainer/Zotero/storage/BDSNJQCZ/1431585.html},
  keywords = {cache storage,constant hit latency,Costs,Delay,demand-based associativity,digital storage,Energy consumption,Engineering management,global replacement,Hardware,History,intelligent design,IPC improvement,memory access,memory architecture,memory latency,Memory management,Microprocessors,miss rate reduction,Optimized production technology,processor speed,secondary cache management,set-associative cache,SPEC CPU2000 suite,tag-store entry,Upper bound,V-Way cache,variable-way}
}

@inproceedings{rajanEmulatingOptimalReplacement2007,
  title = {Emulating {{Optimal Replacement}} with a {{Shepherd Cache}}},
  booktitle = {40th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}} 2007)},
  author = {Rajan, K. and Ramaswamy, G.},
  year = {2007},
  month = dec,
  pages = {445--454},
  doi = {10.1109/MICRO.2007.25},
  abstract = {The inherent temporal locality in memory accesses is filtered out by the L1 cache. As a consequence, an L2 cache with LRU replacement incurs significantly higher misses than the optimal replacement policy (OPT). We propose to narrow this gap through a novel replacement strategy that mimics the replacement decisions of OPT. The L2 cache is logically divided into two components, a Shepherd Cache (SC) with a simple FIFO replacement and a Main Cache (MC) with an emulation of optimal replacement. The SC plays the dual role of caching lines and guiding the replacement decisions in MC. Our proposed organization can cover 40\% of the gap between OPT and LRU for a 2MB cache resulting in 7\% overall speedup. Comparison with the dynamic insertion policy, a victim buffer, a V-Way cache and an LRU based fully associative cache demonstrates that our scheme performs better than all these strategies.},
  file = {/Users/jonathanrainer/Zotero/storage/WKTJIP29/4408275.html},
  keywords = {associative cache,Automation,cache storage,Computer science,Computer science education,content-addressable storage,dynamic insertion policy,Emulation,FIFO replacement,Frequency,History,L1 cache,L2 cache,main cache,memory access,Microarchitecture,optimal replacement policy,Optimized production technology,Proposals,Shepherd cache,Supercomputers,temporal locality}
}

@inproceedings{ramirezTraceCacheRedundancy2000,
  title = {Trace Cache Redundancy: Red and Blue Traces},
  shorttitle = {Trace Cache Redundancy},
  booktitle = {Proceedings {{Sixth International Symposium}} on {{High}}-{{Performance Computer Architecture}}. {{HPCA}}-6 ({{Cat}}. {{No}}.{{PR00550}})},
  author = {Ramirez, A. and {Larriba-Pey}, J.Ll. and Valero, M.},
  year = {2000},
  month = jan,
  pages = {325--333},
  issn = {null},
  doi = {10.1109/HPCA.2000.824361},
  abstract = {The objective of this paper is to improve the use of the hardware resources of the trace cache mechanism, reducing the implementation cost with no performance degradation. We achieve that by eliminating the replication of traces between the instruction cache and the trace cache. As we show, the trace cache mechanism is generating a high degree of redundancy between the traces stored in the trace cache and those built by the compiler, already present in the instruction cache. Furthermore, code reordering techniques like the software trace cache arrange the basic blocks in a program so that the fall-through path is the most common, effectively increasing this trace redundancy. We propose selective trace storage to avoid trace redundancy between the trace cache and the instruction cache. A simple modification of the fill unit allows the trace cache to store only those traces containing taken branches, which can nor be obtained in a single cycle from the instruction cache. Our results show that selective trace storage and the software trace cache used on a 32 entry trace cache (2 KB) perform as well as a 2048 entry trace cache (128 KB) without the enhancements. This shows that the cooperation between hardware and software is crucial to improve the performance and reduce the requirements of hardware mechanisms in the fetch engine.},
  file = {/Users/jonathanrainer/Zotero/storage/RQJL7RWI/Ramirez et al. - 2000 - Trace cache redundancy red and blue traces.pdf;/Users/jonathanrainer/Zotero/storage/VZQUJV8N/824361.html},
  keywords = {cache storage,Cache storage,code reordering,Collaboration,Contracts,Costs,Degradation,Engines,Hardware,hardware mechanisms,hardware resources,implementation cost,parallel processing,performance evaluation,Read only memory,redundancy,Runtime,software trace cache,trace cache,trace cache mechanism,trace cache redundancy}
}

@inproceedings{raveendranLLRULateLRU2007,
  title = {{{LLRU}}: {{Late LRU Replacement Strategy}} for {{Power Efficient Embedded Cache}}},
  shorttitle = {{{LLRU}}},
  booktitle = {15th {{International Conference}} on {{Advanced Computing}} and {{Communications}} ({{ADCOM}} 2007)},
  author = {Raveendran, Biju K. and Sudarshan, T.S.B. and Kumar, P. Dilip and Tangudu, Priyanka and Gurunarayanan, S.},
  year = {2007},
  month = dec,
  pages = {339--344},
  issn = {null},
  doi = {10.1109/ADCOM.2007.86},
  abstract = {This paper proposes a new cache replacement scheme, late least recently used (LLRU). LLRU takes care of shared pages improves its accessibility and offers improved cache performance. LLRU modifies the existing least recently used (LRU) algorithm. This scheme, improves cache performance for applications, which has shared pages. We also propose square matrix and counter based hardware design for LLRU. We show that the proposed scheme will achieve considerable improvement in hit rate. The experimental results are obtained using Simplescalar2.0 cache simulator benchmark. The hardware performance of LLRU counter and square matrix implementation is measured by using Modelsim and Leonardo spectrum.},
  file = {/Users/jonathanrainer/Zotero/storage/6CM2GNBV/Raveendran et al. - 2007 - LLRU Late LRU Replacement Strategy for Power Effi.pdf;/Users/jonathanrainer/Zotero/storage/I427MUNQ/4425994.html},
  keywords = {Cache memory,cache storage,Circuit simulation,Complexity theory,counter based hardware design,Counting circuits,Dynamic scheduling,Embedded computing,Energy consumption,Hardware,integrated circuit design,late least recently used cache replacement scheme,Leonardo spectrum,low-power electronics,Modelsim,power efficient embedded cache,Power measurement,Round robin,shared pages,Simplescalar2.0 cache simulator benchmark,square matrix}
}

@inproceedings{reddyIntelligentWebCaching1998,
  title = {Intelligent Web Caching Using Document Life Histories: {{A}} Comparison with Existing Cache Management Techniques},
  shorttitle = {Intelligent Web Caching Using Document Life Histories},
  author = {Reddy, Mike and Fletcher, Graham P.},
  year = {1998},
  abstract = {Hierarchical storage of web pages in proxy server and client browser caches introduce coherence problems, which require cache management techniques which are both accurate and computationally efficient. We suggest that current approaches, such as the most common Least Recently Used (LRU) technique, are inadequate for future network loads as they do not incorporate the dynamics of document selection and modification. We propose the use of an intelligent, adaptive cache management technique to overcome the coherence problem by using document life h stories to optimise cache performance. This work addresses the use of damped exponential smoothing to model accurately the frequency of file requests and modifications, in order to predict the future value of cached files. Finally, we make a mathematical analysis of LRU in comparison with our technique, showing how and why the use of document lif histories is a more effective cache management technique without imposing major computational overheads.},
  file = {/Users/jonathanrainer/Zotero/storage/6KZMUCTW/Reddy and Fletcher - 1998 - Intelligent web caching using document life histor.pdf},
  keywords = {Algorithmic efficiency,leukemia inhibitory factor,Proxy server,Server (computing),Smoothing (statistical technique),Web cache,Web page}
}

@article{reinekeRandomizedCachesConsidered2014,
  title = {Randomized {{Caches Considered Harmful}} in {{Hard Real}}-{{Time Systems}}},
  author = {Reineke, Jan},
  year = {2014},
  month = jun,
  volume = {Vol 1},
  pages = {No 1 (2014)-},
  doi = {10.4230/lites-v001-i001-a003},
  abstract = {We investigate the suitability of caches with randomized placement and replacement in the context of hard real-time systems. Such caches have been claimed to drastically reduce the amount of information required by static worst-case execution time (WCET) analysis, and to be an enabler for measurement-based probabilistic timing analysis. We refute these claims and conclude that with prevailing static and measurement-based analysis techniques caches with deterministic placement and least-recently-used replacement are preferable over randomized ones.},
  file = {/Users/jonathanrainer/Zotero/storage/SNUCVCYD/Reineke - 2014 - Randomized Caches Considered Harmful in Hard Real-.pdf},
  journal = {Leibniz Transactions on Embedded Systems},
  language = {en}
}

@incollection{riaz-ud-dinAcmeDBAdaptiveCaching2006,
  title = {Acme-{{DB}}: {{An Adaptive Caching Mechanism Using Multiple Experts}} for {{Database Buffers}}},
  shorttitle = {Acme-{{DB}}},
  booktitle = {Enterprise {{Information Systems VI}}},
  author = {{Riaz-ud-Din}, Faizal and Kirchberg, Markus},
  editor = {Seruca, Isabel and Cordeiro, Jos{\'e} and Hammoudi, Slimane and Filipe, Joaquim},
  year = {2006},
  pages = {72--81},
  publisher = {{Springer-Verlag}},
  address = {{Berlin/Heidelberg}},
  doi = {10.1007/1-4020-3675-2_9},
  file = {/Users/jonathanrainer/Zotero/storage/7UR5DVQZ/Riaz-ud-Din and Kirchberg - 2006 - Acme-DB An Adaptive Caching Mechanism Using Multi.pdf},
  isbn = {978-1-4020-3674-3},
  language = {en}
}

@inproceedings{riversHighbandwidthDataCache1997,
  title = {On High-Bandwidth Data Cache Design for Multi-Issue Processors},
  booktitle = {Proceedings of 30th {{Annual International Symposium}} on {{Microarchitecture}}},
  author = {Rivers, J.A. and Tyson, G.S. and Davidson, E.S. and Austin, T.M.},
  year = {1997},
  month = dec,
  pages = {46--56},
  issn = {1072-4451},
  doi = {10.1109/MICRO.1997.645796},
  abstract = {Highly aggressive multi-issue processor designs of the past few years and projections for the next decade require that we redesign the operation of the cache memory system. The number of instructions that must be processed (including correctly predicted ones) will approach 16 or more per cycle. Since memory operations account for about a third of all instructions executed these systems will have to support multiple data references per cycle. We explore reference stream characteristics to determine how best to meet the need for ever increasing access rates. We identify limitations of existing multi-ported cache designs and propose a new structure, the locality-based interleaved cache (LBIC), to exploit the characteristics of the data reference stream while approaching the economy of traditional multi-bank cache design. Experimental results show that the LBIC structure is capable of outperforming current multi-ported approaches.},
  file = {/Users/jonathanrainer/Zotero/storage/W8R55Z9X/Rivers et al. - 1997 - On high-bandwidth data cache design for multi-issu.pdf;/Users/jonathanrainer/Zotero/storage/K34WT3N2/645796.html},
  keywords = {access rates,Bandwidth,cache memory operation,cache storage,Clocks,Computer architecture,Costs,data reference stream,high-bandwidth data cache design,instruction sets,instructions,Laboratories,locality-based interleaved cache,memory architecture,Microcomputers,microcomputers5795109,Microprocessors,multi-bank cache design,multi-issue processors,multi-ported cache design,multiple data references,parallel architectures,parallel programming,performance,performance evaluation,Process design,reference stream characteristics,Rivers,Time division multiplexing}
}

@inproceedings{rixnerMemoryAccessScheduling2000,
  title = {Memory Access Scheduling},
  booktitle = {Proceedings of the 27th Annual International Symposium on {{Computer}} Architecture},
  author = {Rixner, Scott and Dally, William J. and Kapasi, Ujval J. and Mattson, Peter and Owens, John D.},
  year = {2000},
  month = may,
  pages = {128--138},
  publisher = {{Association for Computing Machinery}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.1145/339647.339668},
  abstract = {The bandwidth and latency of a memory system are strongly dependent on the manner in which accesses interact with the ``3-D'' structure of banks, rows, and columns characteristic of contemporary DRAM chips. There is nearly an order of magnitude difference in bandwidth between successive references to different columns within a row and different rows within a bank. This paper introduces memory access scheduling, a technique that improves the performance of a memory system by reordering memory references to exploit locality within the 3-D memory structure. Conservative reordering, in which the first ready reference in a sequence is performed, improves bandwidth by 40\% for traces from five media benchmarks. Aggressive reordering, in which operations are scheduled to optimize memory bandwidth, improves bandwidth by 93\% for the same set of applications. Memory access scheduling is particularly important for media processors where it enables the processor to make the most efficient use of scarce memory bandwidth.},
  file = {/Users/jonathanrainer/Zotero/storage/U22XYCJG/Rixner et al. - 2000 - Memory access scheduling.pdf},
  isbn = {978-1-58113-232-8},
  series = {{{ISCA}} '00}
}

@article{rizzoReplacementPoliciesProxy2000,
  title = {Replacement Policies for a Proxy Cache},
  author = {Rizzo, Luigi and Vicisano, Lorenzo},
  year = {2000},
  month = apr,
  volume = {8},
  pages = {158--170},
  issn = {1063-6692},
  doi = {10.1109/90.842139},
  file = {/Users/jonathanrainer/Zotero/storage/UAL3HQYV/Rizzo and Vicisano - 2000 - Replacement policies for a proxy cache.pdf},
  journal = {IEEE/ACM Transactions on Networking},
  keywords = {caching,communication networks,policies,replacement,Web},
  number = {2}
}

@inproceedings{robinsonDataCacheManagement1990,
  title = {Data {{Cache Management Using Frequency}}-Based {{Replacement}}},
  booktitle = {Proceedings of the 1990 {{ACM SIGMETRICS Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Robinson, John T. and Devarakonda, Murthy V.},
  year = {1990},
  pages = {134--142},
  publisher = {{ACM}},
  address = {{Univ. of Colorado, Boulder, Colorado, USA}},
  doi = {10.1145/98457.98523},
  abstract = {We propose a new frequency-based replacement algorithm for managing caches used for disk blocks by a file system, database management system, or disk control unit, which we refer to here as data caches. Previously, LRU replacement has usually been used for such caches. We describe a replacement algorithm based on the concept of maintaining reference counts in which locality has been ``factored out''. In this algorithm replacement choices are made using a combination of reference frequency and block age. Simulation results based on traces of file system and I/O activity from actual systems show that this algorithm can offer up to 34\% performance improvement over LRU replacement, where the improvement is expressed as the fraction of the performance gain achieved between LRU replacement and the theoretically optimal policy in which the reference string must be known in advance. Furthermore, the implementation complexity and efficiency of this algorithm is comparable to one using LRU replacement.},
  file = {/Users/jonathanrainer/Zotero/storage/2LPDE7P8/Robinson and Devarakonda - 1990 - Data Cache Management Using Frequency-based Replac.pdf},
  isbn = {978-0-89791-359-1},
  series = {{{SIGMETRICS}} '90}
}

@article{robinsonDataCacheManagement1990a,
  title = {Data Cache Management Using Frequency-Based Replacement},
  author = {Robinson, John T. and Devarakonda, Murthy V.},
  year = {1990},
  month = apr,
  volume = {18},
  pages = {134--142},
  issn = {01635999},
  doi = {10.1145/98460.98523},
  file = {/Users/jonathanrainer/Zotero/storage/KISL4NL5/Robinson and Devarakonda - Data Cache Management Using Frequency-Based Replac.pdf},
  journal = {ACM SIGMETRICS Performance Evaluation Review},
  language = {en},
  number = {1}
}

@article{rotenbergTraceCacheMicroarchitecture1999,
  title = {A Trace Cache Microarchitecture and Evaluation},
  author = {Rotenberg, E. and Bennett, S. and Smith, J.E.},
  year = {1999},
  month = feb,
  volume = {48},
  pages = {111--120},
  issn = {2326-3814},
  doi = {10.1109/12.752652},
  abstract = {As the instruction issue width of superscalar processors increases, instruction fetch bandwidth requirements will also increase. It will eventually become necessary to fetch multiple basic blocks per clock cycle. Conventional instruction caches hinder this effort because long instruction sequences are not always in contiguous cache locations. Trace caches overcome this limitation by caching traces of the dynamic instruction stream, so instructions that are otherwise noncontiguous appear contiguous. In this paper, we present and evaluate a microarchitecture incorporating a trace cache. The microarchitecture provides high instruction fetch bandwidth with low latency by explicitly sequencing through the program at the higher level of traces, both in terms of (1) control flow prediction acid (2) instruction supply. For the SPEC95 integer benchmarks, trace-level sequencing improves performance from 15 percent to 35 percent over an otherwise equally sophisticated, but contiguous, multiple-block fetch mechanism. Most of this performance improvement is due to the trace cache. However, for one benchmark whose performance is limited by branch mispredictions, the performance gain is almost entirely due to improved prediction accuracy.},
  file = {/Users/jonathanrainer/Zotero/storage/Y9XF684F/Rotenberg et al. - 1999 - A trace cache microarchitecture and evaluation.pdf;/Users/jonathanrainer/Zotero/storage/WENERUPM/752652.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Accuracy,Bandwidth,cache storage,Clocks,Decoding,Delay,dynamic instruction stream,Engines,evaluation,Frequency,high instruction fetch bandwidth,low latency,memory architecture,microarchitecture,Microarchitecture,performance evaluation,performance gain,Performance gain,SPEC95 integer benchmarks,trace cache,trace-level sequencing,Upper bound},
  number = {2}
}

@article{rothDependenceBasedPrefetching1998,
  title = {Dependence Based Prefetching for Linked Data Structures},
  author = {Roth, Amir and Moshovos, Andreas and Sohi, Gurindar S.},
  year = {1998},
  month = oct,
  volume = {33},
  pages = {115--126},
  issn = {0362-1340},
  doi = {10.1145/291006.291034},
  abstract = {We introduce a dynamic scheme that captures the accesspat-terns of linked data structures and can be used to predict future accesses with high accuracy. Our technique exploits the dependence relationships that exist between loads that produce addresses and loads that consume these addresses. By identzj+ing producer-consumer pairs, we construct a compact internal representation for the associated structure and its traversal. To achieve a prefetching eflect, a small prefetch engine speculatively traverses this representation ahead of the executing program. Dependence-based prefetching achieves speedups of up to 25\% on a suite of pointer-intensive programs.},
  file = {/Users/jonathanrainer/Zotero/storage/PXWF6883/Roth et al. - 1998 - Dependence based prefetching for linked data struc.pdf},
  journal = {ACM SIGPLAN Notices},
  number = {11}
}

@inproceedings{samieeWRPWeightingReplacement2008,
  title = {{{WRP}}: {{Weighting Replacement Policy}} to {{Improve Cache Performance}}},
  shorttitle = {{{WRP}}},
  booktitle = {International {{Symposium}} on {{Computer Science}} and Its {{Applications}}},
  author = {Samiee, K. and Rad, G. R.},
  year = {2008},
  month = oct,
  pages = {38--41},
  doi = {10.1109/CSA.2008.61},
  abstract = {As the performance gap between memory systems and processors has increased, virtual memory management plays an important role in system performance. Different caching policies have different effects on the system performance. This paper studies an adaptive replacement policy which has low overhead on system and is easy to implement. Simulations show that our algorithm performs better than least-recently-used (LRU) and least-frequently-used (LFU). In addition, it performs similarly to LRU in worst cases.},
  file = {/Users/jonathanrainer/Zotero/storage/WXHTTMWG/Samiee and Rad - 2008 - WRP Weighting Replacement Policy to Improve Cache.pdf;/Users/jonathanrainer/Zotero/storage/GUB5F8K4/4654057.html},
  keywords = {Adaptive systems,Algorithm design and analysis,cache performance improvement,cache storage,Computational modeling,Computers,least-frequently-used algorithm,least-recently-used algorithm,Memory management,paged storage,processor system,Radiation detectors,System performance,virtual memory management,weighting adaptive page replacement policy}
}

@inproceedings{sanchezZCacheDecouplingWays2010,
  title = {The {{ZCache}}: {{Decoupling Ways}} and {{Associativity}}},
  shorttitle = {The {{ZCache}}},
  booktitle = {2010 43rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Sanchez, Daniel and Kozyrakis, Christos},
  year = {2010},
  month = dec,
  pages = {187--198},
  issn = {1072-4451},
  doi = {10.1109/MICRO.2010.20},
  abstract = {The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale CMP with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.},
  file = {/Users/jonathanrainer/Zotero/storage/M68IJEBI/Sanchez and Kozyrakis - 2010 - The ZCache Decoupling Ways and Associativity.pdf;/Users/jonathanrainer/Zotero/storage/XTWFL2TU/5695536.html},
  keywords = {Arrays,associativity,Bandwidth,cache,cache storage,chip multiprocessor,content-addressable storage,cuckoo hashing,decoupling way,Delay,energy efficiency,Indexes,main memory latency,multi-core,multi-threading,multiprocessing systems,multiprogramming,multithreading,performance,probability,probability distribution,Process control,Program processors,Radiation detectors,set associative cache,skew associative cache,ZCache}
}

@article{sawantMemoryHierarchiesBasicDesign,
  title = {Memory {{Hierarchies}}-{{Basic Design}} and {{Optimization Techniques}}},
  author = {Sawant, Rahul and Ramaprasad, Bharath H and Govindwar, Sushrut and Mothe, Neelima},
  pages = {19},
  abstract = {In this paper we provide a comprehensive survey of the past and current work of Memory hierarchies and optimizations with a focus on cache optimizations. Firstly we discuss various types of memory hierarchies and basic optimizations possible. Then we shift our focus on cache optimizations and discuss the motivation for doing this survey on the same. Then we discuss different types of cache memories and their mapping policies. Further to avoid various categories of cache misses and achieve high performance and low energy consumption we discuss different basic and advance cache optimizations. Moving further we'll be discussing about other basic and advance cache optimizations techniques like Trace caches [12] and other optimization techniques. We then compare between the various cache optimizations discussed above and also discuss a few novel memory hierarchies and cache optimizations deviating from the conventional hierarchies like the Dynamic Memory Hierarchy Performance Optimization [6] and Way- Predicting Set-Associative cache for High Performance and Low energy consumption [7]. Lastly we discuss few open and challenging issues faced in various cache optimization techniques.},
  file = {/Users/jonathanrainer/Zotero/storage/AJMIXRYE/Sawant et al. - Memory Hierarchies-Basic Design and Optimization T.pdf},
  language = {en}
}

@inproceedings{scheipelSystemAwarePerformanceMonitoring2017,
  title = {System-{{Aware Performance Monitoring Unit}} for {{RISC}}-{{V Architectures}}},
  booktitle = {2017 {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  author = {Scheipel, T. and Mauroner, F. and Baunach, M.},
  year = {2017},
  month = aug,
  pages = {86--93},
  doi = {10.1109/DSD.2017.28},
  abstract = {Due to increasing complexity of software in embedded systems, performance aspects become much more important this days. This should happen early in the development process. Often execution times and events are not easily countable or measurable due to a lack of functionality in these systems. Execution time monitoring is also relevant in terms of reacting to internal and external events dynamically.Especially for systems using multiple tasks with internal or external resource dependencies, this is a major discipline. Another problem is that measurements during the development process are often done by interfering the system as a whole. This method leads to biases in the measurement results, because the finalized system gets deployed without these interfering functionalities and can therefore work more efficiently than the development system.The scope of the present work is to develop a module in a hardware description language (HDL) which is able to measure execution times and events task-aware and unaware without interfering the system. The measurements of this module must be handed to the programmer through an easy accessible interface. The main focuses of the project are the scalability, platform independency concerning processor and operating system (OS), as well as easy extendibility. Also, reusability of counters during runtime is included in this work.},
  file = {/Users/jonathanrainer/Zotero/storage/HP438XX3/Scheipel et al. - 2017 - System-Aware Performance Monitoring Unit for RISC-.pdf;/Users/jonathanrainer/Zotero/storage/KD3RHSUU/8049771.html},
  keywords = {embedded systems,field programmable gate array,Hardware,hardware/software codesign,Monitoring,performance monitoring unit,Phasor measurement units,Pipelines,Radiation detectors,Registers}
}

@inproceedings{seong-ilparkHistorybasedMemoryMode2003,
  title = {History-Based Memory Mode Prediction for Improving Memory Performance},
  booktitle = {Proceedings of the 2003 {{International Symposium}} on {{Circuits}} and {{Systems}}, 2003. {{ISCAS}} '03.},
  author = {{Seong-Il Park} and {In-Cheol Park}},
  year = {2003},
  month = may,
  volume = {5},
  pages = {V-V},
  doi = {10.1109/ISCAS.2003.1206226},
  abstract = {To increase the bandwidth of synchronous memories that are widely adopted for high performance memory systems, a predictive mode control scheme is proposed in this paper. Memory latency can be reduced by effectively managing the states of banks. The local access history of each bank is considered to predict the memory mode. Experimental results show that the proposed scheme, at the cost of negligible area overhead, reduces the memory latency by 19.0\% over the conventional scheme that always keeps the memory in idle state.},
  file = {/Users/jonathanrainer/Zotero/storage/2VUA6ZWA/Seong-Il Park and In-Cheol Park - 2003 - History-based memory mode prediction for improving.pdf;/Users/jonathanrainer/Zotero/storage/HYY2MUEJ/1206226.html},
  keywords = {area overhead,bandwidth,Bandwidth,Control systems,Costs,Decoding,Delay,DRAM chips,History,history-based memory mode prediction,idle state,integrated circuit design,latency,local access history,Memory management,memory mode,memory performance,predictive mode control scheme,Random access memory,SDRAM,synchronous memories,System-on-a-chip}
}

@inproceedings{seshadriGatherScatterDRAMInDRAM2015,
  title = {Gather-{{Scatter DRAM}}: {{In}}-{{DRAM}} Address Translation to Improve the Spatial Locality of Non-Unit Strided Accesses},
  shorttitle = {Gather-{{Scatter DRAM}}},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Seshadri, V. and Mullins, T. and Boroumand, A. and Mutlu, O. and Gibbons, P. B. and Kozuch, M. A. and Mowry, T. C.},
  year = {2015},
  month = dec,
  pages = {267--280},
  doi = {10.1145/2830772.2830820},
  abstract = {Many data structures (e.g., matrices) are typically accessed with multiple access patterns. Depending on the layout of the data structure in physical address space, some access patterns result in non-unit strides. In existing systems, which are optimized to store and access cache lines, non-unit strided accesses exhibit low spatial locality. Therefore, they incur high latency, and waste memory bandwidth and cache space. We propose the Gather-Scatter DRAM (GS-DRAM) to address this problem. We observe that a commodity DRAM module contains many chips. Each chip stores a part of every cache line mapped to the module. Our idea is to enable the memory controller to access multiple values that belong to a strided pattern from different chips using a single read/write command. To realize this idea, GS-DRAM first maps the data of each cache line to different chips such that multiple values of a strided access pattern are mapped to different chips. Second, instead of sending a separate address to each chip, GS-DRAM maps each strided pattern to a small pattern ID that is communicated to the module. Based on the pattern ID, each chip independently computes the address of the value to be accessed. The cache line returned by the module contains different values of the strided pattern gathered from different chips. We show that this approach enables GS-DRAM to achieve near-ideal memory bandwidth and cache utilization for many common access patterns. We design an end-to-end system to exploit GS-DRAM. Our evaluations show that 1) for in-memory databases, GS-DRAM obtains the best of the row store and the column store layouts, in terms of both performance and energy, and 2) for matrix-matrix multiplication, GS-DRAM seamlessly enables SIMD optimizations and outperforms the best tiled layout. Our framework is general, and can benefit many modern data-intensive applications.},
  file = {/Users/jonathanrainer/Zotero/storage/KICKZTBD/Seshadri et al. - 2015 - Gather-Scatter DRAM In-DRAM address translation t.pdf;/Users/jonathanrainer/Zotero/storage/YGS6DMYZ/7856604.html},
  keywords = {cache line,cache utilization,Caches,commodity module,data structures,data-intensive applications,DRAM,DRAM chips,Energy,explixit near-ideal memory bandwidth,gather-scatter DRAM,GS-DRAM,in-memory databases,In-memory databases,matrix multiplication,matrix-matrix multiplication,Memory bandwidth,memory controller,multiple access patterns,nonunit strided accesses,parallel processing,pattern ID,Performance,physical address space,Servers,SIMD,SIMD optimizations,single read command,single write command,spatial locality improvement,Strided accesses}
}

@inproceedings{seznecCaseTwowaySkewedassociative1993,
  title = {A Case for Two-Way Skewed-Associative Caches},
  booktitle = {Proceedings of the 20th Annual International Symposium on Computer Architecture},
  author = {Seznec, Andr{\'e}},
  year = {1993},
  month = may,
  pages = {169--178},
  publisher = {{Association for Computing Machinery}},
  address = {{San Diego, California, USA}},
  doi = {10.1145/165123.165152},
  file = {/Users/jonathanrainer/Zotero/storage/IARUG2Z8/Seznec - 1993 - A case for two-way skewed-associative caches.pdf},
  isbn = {978-0-8186-3810-7},
  series = {{{ISCA}} '93}
}

@inproceedings{seznecSkewedassociativeCaches1993,
  title = {Skewed-Associative Caches},
  booktitle = {{{PARLE}} '93 {{Parallel Architectures}} and {{Languages Europe}}},
  author = {Seznec, Andr{\'e} and Bodin, Francois},
  editor = {Bode, Arndt and Reeve, Mike and Wolf, Gottfried},
  year = {1993},
  pages = {305--316},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-56891-3_24},
  abstract = {During the past decade, microprocessor peak performance has increased at a tremendous rate using RISC concept, higher and higher clock frequencies and parallel/pipelined instruction issuing. As the gap between the main memory access time and the potential average instruction time is always increasing, it has become very important to improve the behavior of the caches, particularly when no secondary cache is used (i.e on all low cost microprocessor systems). In order to improve cache hit ratios, set-associative caches are used in some of the new superscalar microprocessors.In this paper, we present a new organization for a multi-bank cache: the skewed-associative cache. Skewed-associative caches have a better behavior than set-associative caches: typically a two-way skewed-associative cache has the hardware complexity of a two-way set-associative cache, yet simulations show that it exhibits approximatively the same hit ratio as a four-way set associative cache of the same size.},
  file = {/Users/jonathanrainer/Zotero/storage/NEM7WFGE/Seznec and Bodin - 1993 - Skewed-associative caches.pdf},
  isbn = {978-3-540-47779-2},
  keywords = {cache,microprocessors,set-associative cache,skewed-associative cache},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{shaoBurstSchedulingAccess2007a,
  title = {A {{Burst Scheduling Access Reordering Mechanism}}},
  booktitle = {2007 {{IEEE}} 13th {{International Symposium}} on {{High Performance Computer Architecture}}},
  author = {Shao, Jun and Davis, Brian T.},
  year = {2007},
  pages = {285--294},
  publisher = {{IEEE}},
  address = {{Scottsdale, AZ, USA}},
  doi = {10.1109/HPCA.2007.346206},
  abstract = {Utilizing the nonuniform latencies of SDRAM devices, access reordering mechanisms alter the sequence of main memory access streams to reduce the observed access latency. Using a revised M5 simulator with an accurate SDRAM module, the burst scheduling access reordering mechanism is proposed and compared to conventional in order memory scheduling as well as existing academic and industrial access reordering mechanisms. With burst scheduling, memory accesses to the same rows of the same banks are clustered into bursts to maximize bus utilization of the SDRAM device. Subject to a static threshold, memory reads are allowed to preempt ongoing writes for reduced read latency, while qualified writes are piggybacked at the end of bursts to exploit row locality in writes and prevent write queue saturation. Performance improvements contributed by read preemption and write piggybacking are identified. Simulation results show that burst scheduling reduces the average execution time of selected SPEC CPU2000 benchmarks by 21 \% over conventional bank in order memory scheduling. Burst scheduling also outperforms Intel's patented out of order memory scheduling and the row hit access reordering mechanism by 11\% and 6\% respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/WNWRTS5V/Shao and Davis - 2007 - A Burst Scheduling Access Reordering Mechanism.pdf},
  isbn = {978-1-4244-0804-7},
  language = {en}
}

@article{shinDRAMLatencyOptimizationInspired2016,
  title = {{{DRAM}}-{{Latency Optimization Inspired}} by {{Relationship}} between {{Row}}-{{Access Time}} and {{Refresh Timing}}},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Moon, Y. and Kwon, Y. and Kim, L.},
  year = {2016},
  month = oct,
  volume = {65},
  pages = {3027--3040},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2512863},
  abstract = {It is widely known that relatively long DRAM latency forms a bottleneck in computing systems. However, DRAM vendors are strongly reluctant to decrease DRAM latency due to the additional manufacturing cost. Therefore, we set our goal to reduce DRAM latency without any modification in the existing DRAM structure. To accomplish our goal, we focus on an intrinsic phenomenon in DRAM: electric charge variation in DRAM cell capacitors. Then, we draw two key insights: i) DRAM row-access latency of a row is a function of the elapsed time from when the row was last refreshed, and ii) DRAM row-access latency of a row is also a function of the remaining time until the row is next refreshed. Based on these two insights, we propose two mechanisms to reduce DRAM latency: NUAT-1 and NUAT-2. NUAT-1 exploits the first key insight and NUAT-2 exploits the second key insight. For evaluation, circuit- and system-level simulations are performed, which show the performance improvement for various environments.},
  file = {/Users/jonathanrainer/Zotero/storage/9AWRMECC/Shin et al. - 2016 - DRAM-Latency Optimization Inspired by Relationship.pdf;/Users/jonathanrainer/Zotero/storage/G7DNNSVB/7366754.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Capacitors,circuit simulation,circuit-level simulations,computing systems,Decoding,DRAM,DRAM cell capacitors,DRAM chips,DRAM latency optimization,DRAM row-access latency,DRAM structure,DRAM vendors,DRAM-latency,electric charge,electric charge variation,memory controller,Memory management,non-uniform access time (NUAT),nonuniform access time,NUAT-1,NUAT-2,Random access memory,refresh,refresh timing,row-access time,system-level simulations,timing,Transistors},
  number = {10}
}

@article{shinMcDRAMLowLatency2018,
  title = {{{McDRAM}}: {{Low Latency}} and {{Energy}}-{{Efficient Matrix Computations}} in {{DRAM}}},
  shorttitle = {{{McDRAM}}},
  author = {Shin, H. and Kim, D. and Park, E. and Park, S. and Park, Y. and Yoo, S.},
  year = {2018},
  month = nov,
  volume = {37},
  pages = {2613--2622},
  issn = {0278-0070},
  doi = {10.1109/TCAD.2018.2857044},
  abstract = {We propose a novel memory architecture for in-memory computation called McDRAM, where DRAM dies are equipped with a large number of multiply accumulate (MAC) units to perform matrix computation for neural networks. By exploiting high internal memory bandwidth and reducing offchip memory accesses, McDRAM realizes both low latency and energy efficient computation. In our experiments, we obtained the chip layout based on the state-of-the-art memory, LPDDR4 where McDRAM is equipped with 2048 MACs in a single chip package with a small area overhead (4.7\%). Compared with the state-ofthe-art accelerator, TPU and the power-efficient GPU, Nvidia P4, McDRAM offers 9.5\texttimes{} and 14.4\texttimes{} speedup, respectively, in the case that the large-scale MLPs and RNNs adopt the batch size of 1. McDRAM also gives 2.1\texttimes{} and 3.7\texttimes{} better computational efficiency in TOPS/W than TPU and P4, respectively, for the large batches.},
  file = {/Users/jonathanrainer/Zotero/storage/7QBFSJIT/Shin et al. - 2018 - McDRAM Low Latency and Energy-Efficient Matrix Co.pdf;/Users/jonathanrainer/Zotero/storage/JFGECBAF/8493536.html},
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  keywords = {Bandwidth,computational efficiency,Decoding,DRAM chips,Dynamic memory,energy efficient computation,energy-efficient matrix computations,high internal memory bandwidth,in-memory computation,matrix computation,McDRAM,memory architecture,Memory management,multiply accumulate units,neural nets,Neural networks,neural networks (NNs),Performance evaluation,power-efficient GPU,processing in memory,Random access memory},
  number = {11}
}

@inproceedings{shinNUATNonuniformAccess2014,
  title = {{{NUAT}}: {{A}} Non-Uniform Access Time Memory Controller},
  shorttitle = {{{NUAT}}},
  booktitle = {2014 {{IEEE}} 20th {{International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Shin, W. and Yang, J. and Choi, J. and Kim, L.},
  year = {2014},
  month = feb,
  pages = {464--475},
  doi = {10.1109/HPCA.2014.6835956},
  abstract = {With rapid development of micro-processors, off-chip memory access becomes a system bottleneck. DRAM, a main memory in most computers, has concentrated only on capacity and bandwidth for decades to achieve high performance computing. However, DRAM access latency should also be considered to keep the development trend in multi-core era. Therefore, we propose NUAT which is a new memory controller focusing on reducing memory access latency without any modification of the existing DRAM structure. We only exploit DRAM's intrinsic phenomenon: electric charge variation in DRAM cell capacitors. Given the cost-sensitive DRAM market, it is a big advantage in terms of actual implementation. NUAT gives a score to every memory access request and the request with the highest score obtains a priority. For scoring, we introduce two new concepts: Partitioned Bank Rotation (PBR) and PBR Page Mode (PPM). First, PBR is a mechanism that draws information of access speed from refresh timing and position; the request which has faster access speed gains higher score. Second, PPM selects a better page mode between open- and close-page modes based on the information from PBR. Evaluations show that NUAT decreases memory access latency significantly for various environments.},
  file = {/Users/jonathanrainer/Zotero/storage/C25T4M26/Shin et al. - 2014 - NUAT A non-uniform access time memory controller.pdf;/Users/jonathanrainer/Zotero/storage/8RTQX2Q5/6835956.html},
  keywords = {Abstracts,capacitors,Capacitors,cost-sensitive DRAM market,DRAM access latency,DRAM cell capacitors,DRAM chips,electric charge variation,Lead,memory access latency reduction,memory access request,microcontrollers,microprocessor development,nonuniform access time memory controller,NUAT,off-chip memory access,partitioned bank rotation,PBR page mode,PPM,Random access memory,Sensors,Standards,system bottleneck,Timing}
}

@article{shinQDRAMQuickAccessDRAM2016,
  title = {Q-{{DRAM}}: {{Quick}}-{{Access DRAM}} with {{Decoupled Restoring}} from {{Row}}-{{Activation}}},
  shorttitle = {Q-{{DRAM}}},
  author = {Shin, W. and Choi, J. and Jang, J. and Suh, J. and Kwon, Y. and Moon, Y. and Kim, H. and Kim, L.},
  year = {2016},
  month = jul,
  volume = {65},
  pages = {2213--2227},
  issn = {0018-9340},
  doi = {10.1109/TC.2015.2479587},
  abstract = {The relatively high latency of DRAM is mostly caused by the long row-activation time which in fact consists of sensing and restoring time. Memory controllers cannot distinguish between them since they are performed consecutively by a single row-activation command. If these two steps are separated, the restoring can be delayed until DRAM access is uncongested. Hence, we propose Quick-Access DRAM (Q-DRAM) which discriminates between sensing and restoring. Our approach is to allow destructive access (i.e., only sensing is performed without restoring by a row-activation command) using per-bank multiple row-buffers. We call the destructive access and per-bank multiple row-buffers quick-access and quick-buffers (q-buffers) respectively. In addition, we propose Quick-access Trigger (Q-TRIGGER) and RESTORER to utilize Q-DRAM. Q-TRIGGER makes a decision whether quick-access is required or not, and RESTORER decides when to restore the data at the destructed cell. Specifically, RESTORER detects the proper timing to hide restoring time by predicting data bus occupation and by exploiting bank-level locality. Evaluations show that Q-DRAM significantly improved performance for both single- and multi-core systems.},
  file = {/Users/jonathanrainer/Zotero/storage/9RRH87RD/Shin et al. - 2016 - Q-DRAM Quick-Access DRAM with Decoupled Restoring.pdf;/Users/jonathanrainer/Zotero/storage/WDB8AWTC/7271043.html},
  journal = {IEEE Transactions on Computers},
  keywords = {bank-level locality,Computer architecture,data bus occupation prediction,decoupled restoring,decoupling,Decoupling,destructive access,Destructive access,DRAM,DRAM access,DRAM chips,dynamic random access memory,memory controllers,Microprocessors,multi-core systems,Parallel processing,per-bank multiple row-buffers,Per-bank multiple row-buffers,Q-DRAM,Q-TRIGGER,quick-access,Quick-access,quick-access DRAM,quick-access trigger,Radiation detectors,Random access memory,RESTORER,restoring time,row-activation,Row-activation,row-activation time,sensing time,single-core systems,Timing},
  number = {7}
}

@inproceedings{shoukryProactiveSchedulingContent2014,
  title = {Proactive Scheduling for Content Pre-Fetching in Mobile Networks},
  booktitle = {2014 {{IEEE International Conference}} on {{Communications}} ({{ICC}})},
  author = {Shoukry, O. and ElMohsen, M. A. and Tadrous, J. and Gamal, H. E. and ElBatt, T. and Wanas, N. and Elnakieb, Y. and Khairy, M.},
  year = {2014},
  month = jun,
  pages = {2848--2854},
  doi = {10.1109/ICC.2014.6883756},
  abstract = {The global adoption of smart phones has raised major concerns about a potential surge in the wireless traffic due to the excessive demand on multimedia services. This ever increasing demand is projected to cause significant congestions and degrade the quality of service for network users. In this paper, we develop a proactive caching framework that utilizes the predictability of the mobile user behavior to offload predictable traffic through the WiFi networks ahead of time. First, we formulate the proactive scheduling problem with the objective of maximizing the user-content hit ratio subject to constrains stemming from the user behavioral models. Second, we propose a quadratic-complexity (in the number of slots per day) greedy, yet, high performance heuristic algorithm that pinpoints the best download slot for each content item to attain maximal hit ratio. We confirm the merits of the proposed scheme based on the traces of a real dataset leveraging a large number of smart phone users who consistently utilized our framework for two months.},
  file = {/Users/jonathanrainer/Zotero/storage/9KVCQZEL/Shoukry et al. - 2014 - Proactive scheduling for content pre-fetching in m.pdf;/Users/jonathanrainer/Zotero/storage/4WIAE3B5/6883756.html},
  keywords = {Batteries,behavioral models,content pre-fetching,Content pre-fetching,Data models,greedy algorithms,high performance heuristic algorithm,IEEE 802.11 Standards,maximal hit ratio,Mobile communication,Mobile computing,mobile networks,mobile radio,mobile user behavior predictability,multimedia services,offload predictable traffic,proactive caching framework,proactive scheduling problem,quadratic-complexity,quality of service,scheduling,smart phone user traces,smart phones,Smart phones,storage management,telecommunication traffic,traffic offloading,WiFi networks,Wireless communication,wireless LAN,wireless traffic}
}

@inproceedings{shrawankarBlockPatternBased2013,
  title = {Block Pattern Based Buffer Cache Management},
  booktitle = {2013 8th {{International Conference}} on {{Computer Science Education}}},
  author = {Shrawankar, Urmila and Gupta, Reetu},
  year = {2013},
  month = apr,
  pages = {963--968},
  issn = {null},
  doi = {10.1109/ICCSE.2013.6554052},
  abstract = {Efficient caching of the data block in the buffer cache can overcome, the costly delays, associated with accesses made to secondary storage devices. Pattern based buffer cache management methodology is being proposed for enhancing the system performance. The block to be replaced is determined by identifying access patterns such as sequential, looping exhibited in the I/O request issued to the block, through the use of program counter. The blocks are stored in separate, pattern based partitions of the cache, which supports multiple replacement policies to best utilize the cache under that reference pattern. Marginal gain functions are used for managing the partitions. As the suggested methodology aims at improving the buffer cache hit ratio it is capable of enhancing the performance of multimedia application and heterogeneous storage environment. Also it can used for power saving in database server as increased cache hit ratio reduces the memory traffic and thus saves the energy.},
  file = {/Users/jonathanrainer/Zotero/storage/6CV38YDX/Shrawankar and Gupta - 2013 - Block pattern based buffer cache management.pdf;/Users/jonathanrainer/Zotero/storage/H7FQ76ML/6554052.html},
  keywords = {access patterns program counter \&cache partition,Bismuth,block pattern based buffer cache management,buffer cache,buffer cache hit ratio,cache storage,Complexity theory,database server,heterogeneous storage environment,History,Linux,looping access pattern,marginal gain function,memory traffic,multimedia application,partition management,pattern based partition,power saving,program counter,replacement policy,replacemt policies,secondary storage device,sequential access pattern}
}

@article{singhResourceThroughputAware2016,
  title = {Resource and {{Throughput Aware Execution Trace Analysis}} for {{Efficient Run}}-{{Time Mapping}} on {{MPSoCs}}},
  author = {Singh, Amit Kumar and Shafique, Muhammad and Kumar, Akash and Henkel, Jorg},
  year = {2016},
  month = jan,
  volume = {35},
  pages = {72--85},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2015.2446938},
  abstract = {There have been several efforts on run-time mapping of applications on multiprocessor-systems-on-chip. These traditional efforts perform either on-the-fly processing or use design-time analyzed results. However, on-the-fly processing often leads to low-quality mappings, and design-time analysis becomes computationally costly for large-size problems and require huge storage for large number of applications. In this paper, we present a novel run-time mapping approach, where identification of an efficient mapping for a use-case is done by the online execution trace analysis of the active applications. The trace analysis facilitates for fast identification of the mapping while optimizing for the system resource usage and throughput of the active applications, leading to reduced energy consumption as well. By rapidly identifying the efficient mapping at run-time, the proposed approach overcomes the mappings' exploration time bottleneck for large-size problems and their storage overhead problem when compared to the traditional approaches. Our experiments show that on average the exploration time to identify the mapping is reduced 14\texttimes{} when compared to stateof-the-art approaches and storage overhead is reduced by 92\%. Additionally, energy and resource savings are achieved along with identification of high-quality mapping.},
  file = {/Users/jonathanrainer/Zotero/storage/D2ED9PFG/Singh et al. - 2016 - Resource and Throughput Aware Execution Trace Anal.pdf},
  journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  language = {en},
  number = {1}
}

@inproceedings{smaragdakisEELRUSimpleEffective1999,
  title = {{{EELRU}}: {{Simple}} and {{Effective Adaptive Page Replacement}}},
  shorttitle = {{{EELRU}}},
  booktitle = {Proceedings of the 1999 {{ACM SIGMETRICS International Conference}} on {{Measurement}} and {{Modeling}} of {{Computer Systems}}},
  author = {Smaragdakis, Yannis and Kaplan, Scott and Wilson, Paul},
  year = {1999},
  pages = {122--133},
  publisher = {{ACM}},
  address = {{Atlanta, Georgia, USA}},
  doi = {10.1145/301453.301486},
  file = {/Users/jonathanrainer/Zotero/storage/9Y4K9D4P/Smaragdakis et al. - 1999 - EELRU Simple and Effective Adaptive Page Replacem.pdf},
  isbn = {978-1-58113-083-6},
  series = {{{SIGMETRICS}} '99}
}

@inproceedings{smaragdakisGeneralAdaptiveReplacement2004,
  title = {General Adaptive Replacement Policies},
  author = {Smaragdakis, Yannis},
  year = {2004},
  pages = {108},
  publisher = {{ACM Press}},
  doi = {10.1145/1029873.1029887},
  abstract = {We propose a general scheme for creating adaptive replacement policies with good performance and strong theoretical guarantees. Specifically, we show how to combine any two existing replacement policies so that the resulting policy provably can never perform worse than either of the original policies by more than a small factor. To show that our scheme performs very well with real application data, we derive a virtual memory replacement policy that adapts between LRU, loop detection, LFU, and MRU-like replacement. The resulting policy often performs better than all of the policies it adapts over, as well as two other hand-tuned adaptive policies from the recent literature.},
  file = {/Users/jonathanrainer/Zotero/storage/8HFTAD4R/Smaragdakis - 2004 - General adaptive replacement policies.pdf},
  isbn = {978-1-58113-945-7},
  language = {en}
}

@article{smithCacheMemories1982,
  title = {Cache {{Memories}}},
  author = {Smith, Alan Jay},
  year = {1982},
  month = sep,
  volume = {14},
  pages = {473--530},
  issn = {0360-0300},
  doi = {10.1145/356887.356892},
  file = {/Users/jonathanrainer/Zotero/storage/JALV7R5I/Smith - 1982 - Cache Memories.pdf},
  journal = {ACM Comput. Surv.},
  number = {3}
}

@article{soCacheOperationsMRU1988,
  title = {Cache Operations by {{MRU}} Change},
  author = {So, K. and Rechtschaffen, R. N.},
  year = {1988},
  month = jun,
  volume = {37},
  pages = {700--709},
  issn = {0018-9340},
  doi = {10.1109/12.2208},
  abstract = {The performance of set associative caches is analyzed. The method used is to group the cache lines into regions according to their positions in the replacement stacks of a cache, and then to observe how the memory access of a CPU is distributed over these regions. Results from the preserved CPU traces show that the memory accesses are heavily concentrated on the most recently used (MRU) region in the cache. The concept of MRU change is introduced; the idea is to use the event that the CPU accesses a non-MRU line to approximate the time the CPU is changing its working set. The concept is shown to be useful in many aspects of cache design and performance evaluation, such as comparison of various replacement algorithms, improvement of prefetch algorithms, and speedup of cache simulation.{$<>$}},
  file = {/Users/jonathanrainer/Zotero/storage/STZIW7R2/So and Rechtschaffen - 1988 - Cache operations by MRU change.pdf;/Users/jonathanrainer/Zotero/storage/N4EDTW82/2208.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Algorithm design and analysis,cache simulation,Cache storage,Central Processing Unit,Computational modeling,content-addressable storage,CPU,Memory,memory access,Microcomputers,most recently used,MRU change,performance,performance evaluation,prefetch algorithms,Prefetching,replacement algorithms,set associative caches,storage management,Very large scale integration,virtual storage},
  number = {6}
}

@inproceedings{sokolinskyLFUKEffectiveBuffer2004,
  title = {{{LFU}}-{{K}}: {{An Effective Buffer Management Replacement Algorithm}}},
  shorttitle = {{{LFU}}-{{K}}},
  booktitle = {Database {{Systems}} for {{Advanced Applications}}},
  author = {Sokolinsky, Leonid B.},
  editor = {Lee, YoonJoon and Li, Jianzhong and Whang, Kyu-Young and Lee, Doheon},
  year = {2004},
  pages = {670--681},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24571-1_60},
  abstract = {This paper introduces a new approach to database disk buffering, called the LFU- K method. The LFU- K page replacement algorithm is an improvement to the Least Frequently Used (LFU) algorithm. The paper proposes a theoretical-probability model for formal description of LFU- K algorithm. Using this model we evaluate estimations for the LFU- K parameters. This paper also describes an implementation of LFU-2 policy. As we demonstrate by trace-driven simulation experiments, the LFU-2 algorithm provides significant improvement over conventional buffering algorithms for the shared-nothing database systems.},
  file = {/Users/jonathanrainer/Zotero/storage/54AUTECS/Sokolinsky - 2004 - LFU-K An Effective Buffer Management Replacement .pdf},
  isbn = {978-3-540-24571-1},
  keywords = {Buffer Management,Buffer Size,Periodic Distribution,Placement Algorithm,Reference Probability},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{solihinUsingUserlevelMemory2002a,
  title = {Using a User-Level Memory Thread for Correlation Prefetching},
  author = {Solihin, Yan and Lee, Jaejin and Torrellas, Josep},
  year = {2002},
  month = may,
  volume = {30},
  pages = {171--182},
  issn = {0163-5964},
  doi = {10.1145/545214.545235},
  abstract = {This paper introduces the idea of using a User-Level Memory Thread (ULMT) for correlation prefetching. In this approach, a user thread runs on a general-purpose processor in main memory, either in the memory controller chip or in a DRAM chip. The thread performs correlation prefetching in software, sending the prefetched data into the L2 cache of the main processor. This approach requires minimal hardware beyond the memory processor: the correlation table is a software data structure that resides in main memory, while the main processor only needs a few modifications to its L2 cache so that it can accept incoming prefetches. In addition, the approach has wide usability, as it can effectively prefetch even for irregular applications. Finally, it is very flexible, as the prefetching algorithm can be customized by the user on an application basis. Our simulation results show that, through a new design of the correlation table and prefetching algorithm, our scheme delivers good results. Specifically, nine mostly-irregular applications show an average speedup of 1.32. Furthermore, our scheme works well in combination with a conventional processor-side sequential prefetcher, in which case the average speedup increases to 1.46. Finally, by exploiting the customization of the prefetching algorithm, we increase the average speedup to 1.53.},
  file = {/Users/jonathanrainer/Zotero/storage/RNCEFWVH/Solihin et al. - 2002 - Using a user-level memory thread for correlation p.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  keywords = {caches,computer architecture,correlation prefetching,data prefetching,intelligent memory,memory hierarchies,processing-in-memory,threads},
  number = {2}
}

@inproceedings{soryaniPerformanceEvaluationCache2007,
  title = {Performance {{Evaluation}} of {{Cache Memory Organizations}} in {{Embedded Systems}}},
  booktitle = {Fourth {{International Conference}} on {{Information Technology}} ({{ITNG}}'07)},
  author = {Soryani, M. and Sharifi, M. and Rezvani, M. H.},
  year = {2007},
  month = apr,
  pages = {1045--1050},
  doi = {10.1109/ITNG.2007.150},
  abstract = {The tremendous rise in microprocessor technology has offered high speed processors and has increased the processor-memory speed gap dramatically. On the other hand, real-time embedded systems often have a hard deadline to complete their instructions. Consequently, the design of cache memory hierarchy is a critical issue in embedded systems. This paper describes a simulation-based performance evaluation of typical cache design issues in embedded systems such as using split caches for data and instruction versus unified cache for data and instruction, cache size and associativity and replacement policy. The evaluation is done using SimpleScalar simulation tools based on its Alpha version. We select some benchmarks for this study based on some previous researches about the clustering of SPEC CPU2000 benchmark suite. The contribution of this work is identifying important parameters for cache design in general-purpose embedded systems. Our results show that the Pseudo LRU techniques for cache replacement, such as MRU can approximate LRU with much lower complexity for a wide variety of cache sizes and degree of associativities},
  file = {/Users/jonathanrainer/Zotero/storage/HMGSFECC/Soryani et al. - 2007 - Performance Evaluation of Cache Memory Organizatio.pdf;/Users/jonathanrainer/Zotero/storage/8RWY5RDL/4151842.html},
  keywords = {Cache memory,cache memory hierarchy design,cache memory organization,cache storage,Computer architecture,Costs,Embedded computing,Embedded system,embedded systems,Hardware,High performance computing,integrated circuit design,Microprocessors,Optimized production technology,performance evaluation,Real time systems,real-time embedded systems,split caches,unified cache}
}

@inproceedings{sreedharanCacheReplacementPolicy2017,
  title = {A Cache Replacement Policy Based on Re-Reference Count},
  booktitle = {2017 {{International Conference}} on {{Inventive Communication}} and {{Computational Technologies}} ({{ICICCT}})},
  author = {Sreedharan, S. and Asokan, S.},
  year = {2017},
  month = mar,
  pages = {129--134},
  doi = {10.1109/ICICCT.2017.7975173},
  abstract = {The cache replacement policy is a major factor which determines the effectiveness of memory hierarchy. The replacement policy affects both the hit rate and the access latency of the cache. It decides the cache block to be replaced to give room for the incoming block. The replacement policy has to be chosen in such a way that the cache misses are reduced. Last level cache misses causes hundreds of stall cycles due to the need for main memory access. So last level cache misses are given more priority over L1 cache misses. The traditional cache replacement policy used is Least Recently Used (LRU) policy. LRU policy favors workloads having cyclic access pattern which fit in cache, but it exhibit thrashing behavior for memory-intensive workloads that does not fit in the available cache. Hence, many replacement policies were proposed to improve the miss rate for last level caches while maintaining low hardware overhead and minimum design changes. Here a novel replacement policy which is a variation of LRU Insertion policy (LIP) based on re-reference count is proposed. The promotion policy in LIP is modified to implement the new policy which is based on the re-reference count. The proposed replacement policy was implemented and performance comparisons of the replacement policies were done on Gem5 simulator using cpu2006 benchmarks. Under this policy, the memory intensive workload mcf attains 64\% improvement in L2 cache miss rate over LRU policy.},
  file = {/Users/jonathanrainer/Zotero/storage/WDMF4RQQ/Sreedharan and Asokan - 2017 - A cache replacement policy based on re-reference c.pdf;/Users/jonathanrainer/Zotero/storage/FJV9V2XT/7975173.html},
  keywords = {Algorithm design and analysis,Arrays,cache access latency,cache block,Cache Hit Rate,cache replacement policy,cache storage,Computer science,Conferences,cpu2006 benchmarks,cyclic access pattern,Electronics packaging,Gem5 simulator,hit rate,L1 cache,last-level cache miss rate,least recently used policy,Lips,low-hardware overhead,LRU,LRU insertion policy,LRU policy,memory hierarchy,memory intensive workload,Memory management,Promotion Policy,re-reference count,Re-reference count,thrashing behavior}
}

@inproceedings{srivastava190MHzCMOS4Kbyte1995,
  title = {190-{{MHz CMOS}} 4-{{Kbyte}} Pipelined Caches},
  booktitle = {Proceedings of {{ISCAS}}'95 - {{International Symposium}} on {{Circuits}} and {{Systems}}},
  author = {Srivastava, A. and {Yong-Seon Koh} and Sano, B. and Despain, A.M.},
  year = {1995},
  month = apr,
  volume = {2},
  pages = {1053-1056 vol.2},
  issn = {null},
  doi = {10.1109/ISCAS.1995.519948},
  abstract = {In this paper we describe the design and implementation of a 190-MHz pipelined 4-Kbyte instruction and data cache. The caches are designed in 1.0-/spl mu/m CMOS and measure 0.78/spl times/0.47 cm/sup 2/. This paper describes the microarchitecture, cache timing, circuit implementation, and layout of both the instruction and the data cache. The key features of these caches are pipelined execution and the use of dynamic single-phase clock logic. We discuss the interface of this cache with the processor core and the off-chip controller. This paper also describes the pipelined structure of the cache and the miss detection and handling logic.},
  file = {/Users/jonathanrainer/Zotero/storage/EJ6F9GQQ/Srivastava et al. - 1995 - 190-MHz CMOS 4-Kbyte pipelined caches.pdf;/Users/jonathanrainer/Zotero/storage/ZG6ZEJXF/519948.html},
  keywords = {1.0 micron,190 MHz,4 KB,cache storage,cache timing,circuit implementation,Clocks,CMOS,CMOS logic circuits,CMOS memory circuits,Computer aided instruction,Driver circuits,dynamic single-phase clock logic,handling logic,microarchitecture,Microarchitecture,miss detection,Multiplexing,parallel architectures,pipeline processing,Pipeline processing,pipelined caches,pipelined execution,Registers,Signal processing,timing,Timing}
}

@inproceedings{stankovicDRAMControllerComplete2005,
  title = {{{DRAM Controller}} with a {{Complete Predictor}}: {{Preliminary Results}}},
  shorttitle = {{{DRAM Controller}} with a {{Complete Predictor}}},
  booktitle = {TEL{{SIKS}} 2005 - 2005 Uth {{International Conference}} on {{Telecommunication}} in {{ModernSatellite}}, {{Cable}} and {{Broadcasting Services}}},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  year = {2005},
  month = sep,
  volume = {2},
  pages = {593--596},
  doi = {10.1109/\%0021SKS.2005.1572183},
  abstract = {In the arsenal of solutions for computer memory system performance improvement, predictors have gained an increasing role in the past years. They enable hiding the latencies when accessing cache or main memory. Recently the technique of using temporal parameters of cache memory accesses and tag patterns observing has been applied by some authors for prediction of data prefetching. In this paper a possibility of applying analog techniques on controlling DRAM rows opening/closing, is being researched. Obtained results confirm such a possibility, in a form of a complete predictor, which predicts not only when to close the currently open row but also which is the next row to be opened. Using such a predictor can decrease the average DRAM latency, which is very important in many areas, including telecommunications},
  file = {/Users/jonathanrainer/Zotero/storage/QT2ZCLBE/Stankovic and Milenkovic - 2005 - DRAM Controller with a Complete Predictor Prelimi.pdf;/Users/jonathanrainer/Zotero/storage/9CM8S23P/1572183.html},
  keywords = {analog techniques,Cache memory,computer memory system,data prefetching,Delay,Digital images,DRAM,DRAM chips,DRAM controller,latency,policy,predictor,Prefetching,Random access memory,System performance,Table lookup,Telecommunication buffers,Telecommunication control}
}

@inproceedings{stankovicTechniquesPerformanceImprovements2001,
  title = {Some Techniques for Performance Improvements of {{DRAMs}} in Multimedia Applications},
  booktitle = {5th {{International Conference}} on {{Telecommunications}} in {{Modern Satellite}}, {{Cable}} and {{Broadcasting Service}}. TEL{{SIKS}} 2001. {{Proceedings}} of {{Papers}} ({{Cat}}. {{No}}.{{01EX517}})},
  author = {Stankovic, V. V. and Milenkovic, N. Z.},
  year = {2001},
  month = sep,
  volume = {2},
  pages = {794-797 vol.2},
  doi = {10.1109/\%0021SKS.2001.955891},
  abstract = {DRAM memory performance is critical, factor in many multimedia applications. Some techniques, which improve DRAM memory performance, are proposed in this paper. These are, first, combined strategies of opening and closing DRAM pages, and second, address remapping in DRAM memory referencing. Simulations we have done showed some improvements in latency of memory references. Implied requirements on DRAM controller are also discussed.},
  file = {/Users/jonathanrainer/Zotero/storage/BVRE73SK/Stankovic and Milenkovic - 2001 - Some techniques for performance improvements of DR.pdf;/Users/jonathanrainer/Zotero/storage/JXMQTIFM/955891.html},
  keywords = {address remapping,Bandwidth,Clocks,closing pages,combined strategies,Data structures,DC generators,Delay,Digital signal processing,DRAM chips,DRAM controller,DRAM memory performance,Image processing,latency,memory architecture,memory referencing,Microprocessors,multimedia applications,multimedia systems,multiple concurrent transactions,opening pages,performance improvement techniques,Random access memory,row buffers,SDRAM,SimpleScalar Tool Set,storage allocation,synchronous DRAM}
}

@article{stiliadisSelectiveVictimCaching1997,
  title = {Selective Victim Caching: A Method to Improve the Performance of Direct-Mapped Caches},
  shorttitle = {Selective Victim Caching},
  author = {Stiliadis, D. and Varma, A.},
  year = {1997},
  month = may,
  volume = {46},
  pages = {603--610},
  issn = {2326-3814},
  doi = {10.1109/12.589235},
  abstract = {Although direct-mapped caches suffer from higher miss ratios as compared to set-associative caches, they are attractive for today's high-speed pipelined processors that require very low access times. Victim caching was proposed by Jouppi (1990) as an approach to improve the miss rate of direct-mapped caches without affecting their access time. This approach augments the direct-mapped main cache with a small fully associate cache, called victim cache, that stores cache blocks evicted from the main cache as a result of replacements. We propose and evaluate an improvement of this scheme, called selective victim caching. In this scheme, incoming blocks into the first-level cache are placed selectively in the main cache or a small victim cache by the use of a prediction scheme based on their past history of use. In addition, interchanges of blocks between the main cache and the victim cache are also performed selectively. We show that the scheme results in significant improvements in miss rate as well as the average memory access time, for both small and large caches (4 Kbytes-128 Kbytes). For example, simulations with ten instruction traces from the SPEC '92 benchmark suite showed an average improvement of approximately 21 percent in miss rate over simple victim caching for a 16-Kbyte cache with a block size of 32 bytes; the number of blocks interchanged between the main and victim caches reduced by approximately 70 percent. Implementation alternatives for the scheme in an on-chip processor cache are also described.},
  file = {/Users/jonathanrainer/Zotero/storage/LLAP76RJ/Stiliadis and Varma - 1997 - Selective victim caching a method to improve the .pdf;/Users/jonathanrainer/Zotero/storage/XCILSPXG/589235.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Cache memory,cache storage,Central Processing Unit,Clocks,direct-mapped caches,History,miss rate,performance,performance evaluation,Pipeline processing,Random access memory,Reduced instruction set computing,selective victim caching,victim caching},
  number = {5}
}

@article{stuecheliCoordinatingDRAMLastLevelCache2011,
  title = {Coordinating {{DRAM}} and {{Last}}-{{Level}}-{{Cache Policies}} with the {{Virtual Write Queue}}},
  author = {Stuecheli, J. and Kaseridis, D. and Daly, D. and Hunter, H. and John, L.},
  year = {2011},
  month = jan,
  volume = {31},
  pages = {90--98},
  issn = {0272-1732},
  doi = {10.1109/MM.2010.102},
  abstract = {To alleviate bottlenecks in this era of many-core architectures, the authors propose a virtual write queue to expand the memory controller's scheduling window through visibility of cache behavior. Awareness of the physical main memory layout and a focus on writes can shorten both read and write average latency, reduce memory power consumption, and improve overall system performance.},
  file = {/Users/jonathanrainer/Zotero/storage/D93G3SAW/Stuecheli et al. - 2011 - Coordinating DRAM and Last-Level-Cache Policies wi.pdf;/Users/jonathanrainer/Zotero/storage/2T3837LK/5661752.html},
  journal = {IEEE Micro},
  keywords = {Bandwidth,cache,Cache memory,cache replacement,cache storage,cache write-back,DRAM,DRAM chips,DRAM page-mode,DRAM parameters,last level cache policy,last-level cache,many core architecture,Memory,memory bandwidth,memory controller scheduling,memory scheduling,multiprocessing systems,performance evaluation,processor scheduling,Processor scheduling,Program processors,Queueing analysis,Random access memory,system performance,virtual write queue},
  number = {1}
}

@inproceedings{subhaArchitectureVictimCache2016,
  title = {An Architecture for Victim Cache},
  booktitle = {2016 2nd {{International Conference}} on {{Advances}} in {{Electrical}}, {{Electronics}}, {{Information}}, {{Communication}} and {{Bio}}-{{Informatics}} ({{AEEICB}})},
  author = {Subha, S.},
  year = {2016},
  month = feb,
  pages = {255--258},
  issn = {null},
  doi = {10.1109/AEEICB.2016.7538284},
  abstract = {Victim caches enable the direct mapped and fully associative modules during cache operation. This paper proposes model to reduce power consumption in victim caches with improved performance. The model introduces the setnumber variable per fully associative cache block indicating the set to which the block belongs in the direct mapped cache module of victim cache. An address is checked for the setnumber match in fully associative cache before address matching. The number of block comparisons in fully associative cache is reduced. Power saving of 69\% with performance improvement of 5\% is observed for simulations on SPEC2K benchmarks.},
  file = {/Users/jonathanrainer/Zotero/storage/QB28YR6X/Subha - 2016 - An architecture for victim cache.pdf;/Users/jonathanrainer/Zotero/storage/CPECTWFS/7538284.html},
  keywords = {address matching,Analytical models,associative cache block,Benchmark testing,Biological system modeling,cache average memory access time,cache operation,cache power saving,cache storage,direct mapped cache module,fully associative cache,fully associative modules,Mathematical analysis,power aware computing,power consumption,Power demand,power saving,Registers,SPEC2K benchmarks,victim cache}
}

@inproceedings{subramanianAdaptiveCachesEffective2006a,
  title = {Adaptive {{Caches}}: {{Effective Shaping}} of {{Cache Behavior}} to {{Workloads}}},
  shorttitle = {Adaptive {{Caches}}},
  booktitle = {Proceedings of the 39th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Subramanian, Ranjith and Smaragdakis, Yannis and Loh, Gabriel H.},
  year = {2006},
  pages = {385--396},
  publisher = {{IEEE Computer Society}},
  address = {{Washington, DC, USA}},
  doi = {10.1109/MICRO.2006.7},
  abstract = {We present and evaluate the idea of adaptive processor cache management. Specifically, we describe a novel and general scheme by which we can combine any two cache management algorithms (e.g., LRU, LFU, FIFO, Random) and adaptively switch between them, closely tracking the locality characteristics of a given program. The scheme is inspired by recent work in virtual memory management at the operating system level, which has shown that it is possible to adapt over two replacement policies to provide an aggregate policy that always performs within a constant factor of the better component policy. A hardware implementation of adaptivity requires very simple logic but duplicate tag structures. To reduce the overhead, we use partial tags, which achieve good performance with a small hardware cost. In particular, adapting between LRU and LFU replacement policies on an 8-way 512KB L2 cache yields a 12.7\% improvement in average CPI on applications that exhibit a non-negligible L2 miss ratio. Our approach increases total cache storage by 4.0\%, but it still provides slightly better performance than a conventional 10-way setassociative 640KB cache which requires 25\% more storage.},
  file = {/Users/jonathanrainer/Zotero/storage/7W7HCXBR/Subramanian et al. - 2006 - Adaptive Caches Effective Shaping of Cache Behavi.pdf},
  isbn = {978-0-7695-2732-1},
  series = {{{MICRO}} 39}
}

@inproceedings{subramanianClosedOpenDRAM2018,
  title = {Closed yet {{Open DRAM}}: {{Achieving Low Latency}} and {{High Performance}} in {{DRAM Memory Systems}}},
  shorttitle = {Closed yet {{Open DRAM}}},
  booktitle = {2018 55th {{ACM}}/{{ESDA}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Subramanian, L. and Vaidyanathan, K. and Nori, A. and Subramoney, S. and Karnik, T. and Wang, H.},
  year = {2018},
  month = jun,
  pages = {1--6},
  doi = {10.1109/DAC.2018.8465817},
  abstract = {DRAM memory access is a critical performance bottleneck. To access one cache block, an entire row needs to be sensed and amplified, data restored into the bitcells and the bitlines precharged, incurring high latency. Isolating the bitlines and sense amplifiers after activation enables reads and precharges to happen in parallel. However, there are challenges in achieving this isolation. We tackle these challenges and propose an effective scheme, simultaneous read and precharge (SRP), to isolate the sense amplifiers and bitlines and serve reads and precharges in parallel. Our detailed architecture and circuit simulations demonstrate that our simultaneous read and precharge (SRP) mechanism is able to achieve an 8.6\% performance benefit over baseline, while reducing sense amplifier idle power by 30\%, as compared to prior work, over a wide range of workloads.},
  file = {/Users/jonathanrainer/Zotero/storage/9XHCYX32/Subramanian et al. - 2018 - Closed yet Open DRAM Achieving Low Latency and Hi.pdf;/Users/jonathanrainer/Zotero/storage/H6EUQWEM/8465817.html},
  keywords = {access one cache block,amplifier idle power,amplifiers,cache storage,circuit simulation,circuit simulations,critical performance bottleneck,DRAM chips,DRAM memory access,DRAM memory systems,high performance,History,isolation,memory architecture,Memory management,open DRAM,Organizations,Parallel processing,Random access memory,sense amplifiers,simultaneous read and precharge mechanism,SRAM chips,SRP,Timing,Transistors}
}

@article{tadaCacheReplacementPolicy2019,
  title = {A {{Cache Replacement Policy}} with {{Considering Global Fluctuations}} of {{Priority Values}}},
  author = {Tada, Jubee},
  year = {2019},
  month = jul,
  volume = {9},
  pages = {161--170},
  issn = {2185-2847},
  abstract = {In the high-associativity caches, the hardware overheads of the cache replacement policy become a problem. To avoid this problem, the Adaptive Demotion Policy (ADP) is proposed. The ADP focuses on the priority value demotion at a cache miss, and it can achieve a higher performance compared with conventional cache replacement policies. The ADP can be implemented with small hardware resources, and the priority value update logic can be implemented with a small hardware cost. The ADP can suit for various applications by the appropriate selection of its insertion, promotion and selection policies. If the dynamic selection of the suitable policies for the running application is possible, the performance of the cache replacement policy will be increased. In order to achieve the dynamic selection of the suitable policies, this paper focuses on the global fluctuations of the priority values. At first, the cache is partitioned into several partitions. At every cache access, the total of priority values in each partition is calculated. At every set interval, the fluctuations of total priority values in all the partitions are checked, and the information is used to detect the behavior of the application. This paper adapts this mechanism to the ADP, and the adapted cache replacement policy is called the ADP-G. The performance evaluation shows that the ADP-G achieves the MPKI reductions and the IPC improvements, compared to the LRU policy, the RRIP policy and the ADP.},
  file = {/Users/jonathanrainer/Zotero/storage/CNYU8Q5L/Tada - A Cache Replacement Policy with Considering Global.pdf},
  journal = {International Journal of Networking and Computing},
  language = {en},
  number = {2}
}

@inproceedings{taozhangCustomizedDesignDRAM2010,
  title = {A Customized Design of {{DRAM}} Controller for On-Chip {{3D DRAM}} Stacking},
  booktitle = {{{IEEE Custom Integrated Circuits Conference}} 2010},
  author = {{Tao Zhang} and {Kui Wang} and {Yi Feng} and {Xiaodi Song} and {Lian Duan} and Xie, Y. and {Xu Cheng} and {Youn-Long Lin}},
  year = {2010},
  month = sep,
  pages = {1--4},
  doi = {10.1109/CICC.2010.5617465},
  abstract = {To address the ``memory wall'' challenge, on-chip memory stacking has been proposed as a promising solution. The stacking memory adopts three-dimensional (3D) IC technology, which leverages through-silicon-vias (TSVs) to connect layers, to dramatically reduce the access latency and improve the bandwidth without the constraint of I/O pins. To demonstrate the feasibility of 3D memory stacking, this paper introduces a customized 3D Double-Data-Rate (DDR) SDRAM controller design, which communicates with DRAM layers by TSVs. In addition, we propose a parallel access policy to further improve the performance. The 3D DDR controller is integrated in a 3D stacking System-on-Chip (SoC) architecture, where a high-bandwidth 3D DRAM chip is stacked on the top. The 3D SoC is divided into two logic layers with each having an area of 2.5 \texttimes{} 5.0mm2, with a 3-layer 2Gb DRAM stacking. The whole chip has been fabricated in Chartered 130nm low-power process and Tezzaron's 3D bonding technology. The simulation result shows that the on-chip DRAM controller can run as fast as 133MHz and provide 4.25GB/s data bandwidth in a single channel and 8.5GB/s with parallel access policy.},
  file = {/Users/jonathanrainer/Zotero/storage/ILDYCDTB/Tao Zhang et al. - 2010 - A customized design of DRAM controller for on-chip.pdf;/Users/jonathanrainer/Zotero/storage/TXIARCL7/5617465.html},
  keywords = {3D bonding technology,3D DDR controller,3D DRAM chip,3D IC technology,3D memory stacking,3D SoC,3D stacking system-on-chip architecture,access latency,Bandwidth,Computer architecture,customized 3D double-data-rate,customized design,data bandwidth,DRAM chips,DRAM layers,logic design,logic layers,memory wall challenge,on-chip 3D DRAM stacking,on-chip memory stacking,parallel access policy,Random access memory,Registers,SDRAM controller design,SoC architecture,Stacking,stacking memory,System-on-a-chip,system-on-chip,Three dimensional displays,through-silicon-vias,TSV}
}

@inproceedings{thakkar3DWizNovelHigh2014,
  title = {{{3D}}-{{Wiz}}: {{A}} Novel High Bandwidth, Optically Interfaced {{3D DRAM}} Architecture with Reduced Random Access Time},
  shorttitle = {{{3D}}-{{Wiz}}},
  booktitle = {2014 {{IEEE}} 32nd {{International Conference}} on {{Computer Design}} ({{ICCD}})},
  author = {Thakkar, I. G. and Pasricha, S.},
  year = {2014},
  month = oct,
  pages = {1--7},
  doi = {10.1109/ICCD.2014.6974654},
  abstract = {This paper introduces 3D-Wiz, which is a high bandwidth, low latency, optically interfaced 3D DRAM architecture with fine grained data organization and activation. 3D-Wiz integrates sub-bank level 3D partitioning of the data array to enable fine-grained activation and greater memory parallelism. A novel method of routing the internal memory bus using TSVs and fan-out buffers enables 3D-Wiz to use smaller dimension subarrays without significant area overhead. This in turn reduces the random access latency and activation-precharge energy. 3D-Wiz demonstrates access latency of 19.5ns and row cycle time of 25ns. It yields per access activation energy and precharge energy of 0.78nJ and 0.62nJ respectively with 42.5\% area efficiency. 3D-Wiz yields the best latency and energy consumption values per access among other well-known 3D DRAM architectures. Experimental results with PARSEC benchmarks indicate that 3D-Wiz achieves 38.8\% improvement in performance, 81.1\% reduction in power consumption, and 77.1\% reduction in energy-delay product (EDP) on average over 3D DRAM architectures from prior work.},
  file = {/Users/jonathanrainer/Zotero/storage/J6D4QVDS/Thakkar and Pasricha - 2014 - 3D-Wiz A novel high bandwidth, optically interfac.pdf;/Users/jonathanrainer/Zotero/storage/IIEGCK6U/6974654.html},
  keywords = {3D-Wiz,access activation energy,activation-precharge energy,Arrays,Bandwidth,dimension subarrays,DRAM chips,EDP,efficiency 42.5 percent,energy consumption value,energy-delay product,fan-out buffers,fine-grained data organization,high-bandwidth 3D DRAM architecture,internal memory bus routing,low-latency 3D DRAM architecture,memory parallelism,optically-interfaced 3D DRAM architecture,PARSEC benchmarks,Photonics,precharge energy,random access latency,Random access memory,reduced random access time,row cycle time,subbank level 3D partitioning,Three-dimensional displays,three-dimensional integrated circuits,Through-silicon vias,time 19.5 ns,time 25 ns,TSV}
}

@article{tianEffectivenessbasedAdaptiveCache2014,
  title = {An Effectiveness-Based Adaptive Cache Replacement Policy},
  author = {Tian, Geng and Liebelt, Michael},
  year = {2014},
  month = feb,
  volume = {38},
  pages = {98--111},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2013.11.011},
  abstract = {Belady's optimal cache replacement policy is an algorithm to work out the theoretical minimum number of cache misses, but the rationale behind it was too simple. In this work, we revisit the essential function of caches to develop an underlying analytical model. We argue that frequency and recency are the only two affordable attributes of cache history that can be leveraged to predict a good replacement. Based on those two properties, we propose a novel replacement policy, the Effectiveness-Based Replacement policy (EBR) and a refinement, Dynamic EBR (D-EBR), which combines measures of recency and frequency to form a rank sequence inside each set and evict blocks with lowest rank. To evaluate our design, we simulated all 30 applications from SPEC CPU2006 for uni-core system and a set of combinations for 4-core systems, for different cache sizes. The results show that EBR achieves an average miss rate reduction of 12.4\%. With the help of D-EBR, we can tune the weight ratio between `frequency' and `recency' dynamically. D-EBR can nearly double the miss reduction achieved by EBR alone. In terms of hardware overhead, EBR requires half the hardware overhead of real LRU and even compared with Pseudo LRU the overhead is modest.},
  file = {/Users/jonathanrainer/Zotero/storage/EWQW8W3A/Tian and Liebelt - 2014 - An effectiveness-based adaptive cache replacement .pdf;/Users/jonathanrainer/Zotero/storage/FEIKKXWD/S014193311300197X.html},
  journal = {Microprocessors and Microsystems},
  keywords = {Cache memory,Configurable cache,Micro architecture,Performance,Scan resistance,Thrashing},
  number = {1}
}

@inproceedings{tien-fuchenEffectiveProgrammablePrefetch1995,
  title = {An Effective Programmable Prefetch Engine for On-Chip Caches},
  booktitle = {Proceedings of the 28th {{Annual International Symposium}} on {{Microarchitecture}}},
  author = {Chen, Tien-Fu},
  year = {1995},
  month = nov,
  pages = {237--242},
  issn = {1072-4451},
  doi = {10.1109/MICRO.1995.476831},
  abstract = {Prefetching has been shown to be one of several effective approaches that can tolerate large memory latencies. In this paper, we consider a prefetch engine called Hare, which handles prefetches at run time and is built in addition to the data pipelining in the on-chip data cache for high-performance processors. The key design is that it is programmable so that techniques of software prefetching can be also employed in exploiting the benefits of prefetching. The engine always launches prefetches ahead of current execution, which is controlled by the program counter. We evaluate the proposed scheme by trace-driven simulation and consider area and cycle time factors for the evaluation of cost-effectiveness. Our performance results show that the prefetch engine can significantly reduce data access penalty with only little prefetching overhead.},
  file = {/Users/jonathanrainer/Zotero/storage/ARIDNNMI/Tien-Fu Chen - 1995 - An effective programmable prefetch engine for on-c.pdf;/Users/jonathanrainer/Zotero/storage/9BMWF7J7/476831.html},
  keywords = {cache storage,computer architecture,Computer architecture,Computer science,Counting circuits,data cache,Delay,Engines,Hardware,Hare,high-performance processors,memory latencies,on-chip caches,performance evaluation,Pipeline processing,prefetch engine,Prefetching,Process design,programmable,software prefetching,storage management,Time factors,trace-driven simulation}
}

@inproceedings{tsaiJengaSoftwaredefinedCache2017,
  title = {Jenga: {{Software}}-Defined Cache Hierarchies},
  shorttitle = {Jenga},
  booktitle = {2017 {{ACM}}/{{IEEE}} 44th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Tsai, P. and Beckmann, N. and Sanchez, D.},
  year = {2017},
  month = jun,
  pages = {652--665},
  doi = {10.1145/3079856.3080214},
  abstract = {Caches are traditionally organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, since working sets settle at the smallest (i.e., fastest and most energy-efficient) level they fit in. However, rigid hierarchies also add overheads, because each level adds latency and energy even when it does not fit the working set. These overheads are expensive on emerging systems with heterogeneous memories, where the differences in latency and energy across levels are small. Significant gains are possible by specializing the hierarchy to applications. We propose Jenga, a reconfigurable cache hierarchy that dynamically and transparently specializes itself to applications. Jenga builds virtual cache hierarchies out of heterogeneous, distributed cache banks using simple hardware mechanisms and an OS runtime. In contrast to prior techniques that trade energy and bandwidth for performance (e.g., dynamic bypassing or prefetching), Jenga eliminates accesses to unwanted cache levels. Jenga thus improves both performance and energy efficiency. On a 36-core chip with a 1 GB DRAM cache, Jenga improves energy-delay product over a combination of state-of-the-art techniques by 23\% on average and by up to 85\%.},
  file = {/Users/jonathanrainer/Zotero/storage/QZDZ8SK8/Tsai et al. - 2017 - Jenga Software-defined cache hierarchies.pdf;/Users/jonathanrainer/Zotero/storage/ZTP33ABZ/8192509.html},
  keywords = {Bandwidth,Cache,cache storage,DRAM cache,DRAM chips,energy-delay product,Hardware,heterogeneous distributed cache banks,heterogeneous memories,Heterogeneous memories,Hierarchy,Jenga,latency energy,memory architecture,memory size 1.0 GByte,Multicore processing,multiprocessing systems,NUCA,Organizations,Partitioning,Random access memory,reconfigurable cache hierarchy,rigid hierarchy,simple hardware mechanisms,slower memories,Software,storage management,System-on-chip,unwanted cache levels,virtual cache hierarchies}
}

@incollection{tsaoMultiFactorPagingExperiment1972,
  title = {A {{Multi}}-{{Factor Paging Experiment}}: {{I}}. {{The Experiment}} and {{Conclusions}}},
  shorttitle = {A {{MULTI}}-{{FACTOR PAGING EXPERIMENT}}},
  booktitle = {Statistical {{Computer Performance Evaluation}}},
  author = {Tsao, R. F. and Comeau, L. W. and Margolin, B. H.},
  editor = {Freiberger, Walter},
  year = {1972},
  month = jan,
  pages = {103--134},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-266950-7.50012-8},
  abstract = {This paper reports on a statistically designed and analyzed experiment performed to study the effects of four factors, memory size, problem program, load sequence of system program subroutines and replacement algorithm, upon the paging process. The usefulness of the number of page swaps as a measure for comparing replacement algorithms is evaluated and alternative measures are proposed. The experiment and the conclusions drawn are presented in this paper, while the statistical methodology used to derive these conclusions is presented in a companion paper.},
  file = {/Users/jonathanrainer/Zotero/storage/9YCTKAWW/B9780122669507500128.html},
  isbn = {978-0-12-266950-7},
  language = {en}
}

@inproceedings{tuckScalableCacheMiss2006,
  title = {Scalable {{Cache Miss Handling}} for {{High Memory}}-{{Level Parallelism}}},
  booktitle = {2006 39th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}}'06)},
  author = {Tuck, James and Ceze, Luis and Torrellas, Josep},
  year = {2006},
  month = dec,
  pages = {409--422},
  issn = {2379-3155},
  doi = {10.1109/MICRO.2006.44},
  abstract = {Recently-proposed processor microarchitectures for high memory level parallelism (MLP) promise substantial performance gains. Unfortunately, current cache hierarchies have miss-handling architectures (MHAs) that are too limited to support the required MLP - they need to be redesigned to support 1-2 orders of magnitude more outstanding misses. Yet, designing scalable MHAs is challenging: designs must minimize cache lock-up time and deliver high bandwidth while keeping the area consumption reasonable. This paper presents a novel scalable MHA design for high-MLP processors. Our design introduces two main innovations. First, it is hierarchical, with a small MSHR file per cache bank, and a larger MSHR file shared by all banks. Second, it uses a Bloom filter to reduce searches in the larger MSHR file. The result is a high-performance, area-efficient design. Compared to a state-of-the-art MHA on a high-MLP processor, our design speeds-up some SPECint, SPECfp, and multiprogrammed workloads by a geometric mean of 32\%, 50\%, and 95\%, respectively. Moreover, compared to two extrapolations of current MHA designs, namely a large monolithic MSHR file and a large banked MSHR file, all consuming the same area, our design speeds-up the workloads by a geometric mean of 1-18\% and 10-21\%, respectively. Finally, our design performs very close to an unlimited-size, ideal MHA},
  file = {/Users/jonathanrainer/Zotero/storage/FAKNB4CI/Tuck et al. - 2006 - Scalable Cache Miss Handling for High Memory-Level.pdf;/Users/jonathanrainer/Zotero/storage/BR7INA26/4041864.html},
  keywords = {Bandwidth,Banking,Bloom filter,cache storage,computer architecture,Extrapolation,Filters,Hardware,high memory-level parallelism,large monolithic MSHR file,microarchitecture,Microarchitecture,microprocessor chips,miss-handling architecture,MSHR file per cache bank,multiprogramming,multiprogramming workload,Out of order,Performance gain,Process design,Technological innovation}
}

@inproceedings{turnerSegmentedFIFOPage1981,
  title = {Segmented {{FIFO}} Page Replacement},
  author = {Turner, Rollins and Levy, Henry},
  year = {1981},
  pages = {48--51},
  publisher = {{ACM Press}},
  doi = {10.1145/800189.805473},
  file = {/Users/jonathanrainer/Zotero/storage/9QZS46BA/Turner and Levy - 1981 - Segmented FIFO page replacement.pdf},
  isbn = {978-0-89791-051-4},
  language = {en}
}

@inproceedings{tzu-chiehlinQualityawareMemoryController2003,
  title = {Quality-Aware Memory Controller for Multimedia Platform {{SoC}}},
  booktitle = {2003 {{IEEE Workshop}} on {{Signal Processing Systems}} ({{IEEE Cat}}. {{No}}.{{03TH8682}})},
  author = {{Tzu-Chieh Lin} and {Kun-Bin Lee} and {Chein-Wei Jen}},
  year = {2003},
  month = aug,
  pages = {328--333},
  doi = {10.1109/SIPS.2003.1235691},
  abstract = {The ongoing advancements in VLSI technology allow SoC design to integrate heterogeneous control and computing functions into a single chip. On the other hand, the pressures of area and cost lead to the requirement for a single, shared off-chip DRAM memory subsystem. To satisfy different memory access requirements (for latency and bandwidth) of these heterogeneous functions to this kind of DRAM memory subsystem, a quality-aware memory controller is important. The paper presents an efficient memory controller that contains a quality-aware scheduler and a configurable DRAM memory interface socket to achieve high DRAM utilization while still meeting different requirements for bandwidth and latency. Simulation results show that the latency of the latency-sensitive data flow can be reduced to 50\%, and the memory bandwidths can be precisely allocated to bandwidth-sensitive data flows with a high degree of control.},
  file = {/Users/jonathanrainer/Zotero/storage/VN6NF4GS/Tzu-Chieh Lin et al. - 2003 - Quality-aware memory controller for multimedia pla.pdf;/Users/jonathanrainer/Zotero/storage/93XMK69I/1235691.html},
  keywords = {Bandwidth,bandwidth allocation,bandwidth-sensitive data flows,Communication system control,Control systems,Costs,Delay,Design engineering,interface socket,latency-sensitive data flow,memory access,memory bandwidth allocation,multimedia platform SoC,off-chip DRAM memory subsystem,Processor scheduling,Quality of service,quality-aware memory controller,quality-aware scheduler,Random access memory,random-access storage,scheduling,storage management,System-on-a-chip,system-on-chip,VLSI technology}
}

@techreport{UltraSPARCT2Supplement2007,
  title = {{{UltraSPARC T2}}\texttrademark{} {{Supplement}} to the {{UltraSPARC Architecture}} 2007},
  year = {2007},
  month = sep,
  pages = {940},
  address = {{Santa Clara, CA}},
  institution = {{Sun Microsystems}},
  file = {/Users/jonathanrainer/Zotero/storage/W8TMVKT9/Microsystems and Clara - Hyperprivileged, Privileged, and Nonprivileged.pdf},
  language = {en},
  number = {Draft D1.4.3},
  type = {Technical {{Report}}}
}

@article{uzelacHardwareBasedLoadValue2013,
  title = {Hardware-{{Based Load Value Trace Filtering}} for {{On}}-the-{{Fly Debugging}}},
  author = {Uzelac, Vladimir and Milenkovi{\'c}, Aleksandar},
  year = {2013},
  month = may,
  volume = {12},
  pages = {97:1--97:18},
  issn = {1539-9087},
  doi = {10.1145/2465787.2465799},
  abstract = {Capturing program and data traces during program execution unobtrusively on-the-fly is crucial in debugging and testing of cyber-physical systems. However, tracing a complete program unobtrusively is often cost-prohibitive, requiring large on-chip trace buffers and wide trace ports. This article describes a new hardware-based load data value filtering technique called Cache First-access Tracking. Coupled with an effective variable encoding scheme, this technique achieves a significant reduction of load data value traces, from 5.86 to 56.39 times depending on the data cache size, thus enabling cost-effective, unobtrusive on-the-fly tracing and debugging.},
  file = {/Users/jonathanrainer/Zotero/storage/3VWT5UG7/Uzelac and Milenković - 2013 - Hardware-Based Load Value Trace Filtering for On-t.pdf},
  journal = {ACM Trans. Embed. Comput. Syst.},
  keywords = {Debugging,load value filtering,program tracing,software debugger,trace compression,trace module,variable encoding},
  number = {2s}
}

@inproceedings{vakaliLRUbasedAlgorithmsWeb2000,
  title = {{{LRU}}-Based Algorithms for {{Web Cache Replacement}}},
  booktitle = {Electronic {{Commerce}} and {{Web Technologies}}},
  author = {Vakali, A. I.},
  editor = {Bauknecht, Kurt and Madria, Sanjay Kumar and Pernul, G{\"u}nther},
  year = {2000},
  pages = {409--418},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44463-7_36},
  abstract = {Caching has been introduced and applied in prototype and commercial Web-based information systems in order to reduce the overall bandwidth and increase system's fault tolerance. This paper presents a track of Web cache replacement algorithms based on the Least Recently Used (LRU) idea. We propose an extension to the conventional LRU algorithm by considering the number of references to Web objects as a critical parameter for the cache content replacement. The proposed algorithms are validated and experimented under Web cache traces provided by a major Squid proxy cache server installation environment. Cache and bytes hit rates are reported showing that the proposed cache replacement algorithms improve cache content.},
  file = {/Users/jonathanrainer/Zotero/storage/926BNS5V/Vakali - 2000 - LRU-based algorithms for Web Cache Replacement.pdf},
  isbn = {978-3-540-44463-3},
  keywords = {Cache consistency,Cache replacement algorithms,Web caching and proxies,Web-based information systems},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{vanderwielCompilerassistedDataPrefetch1999,
  title = {A Compiler-Assisted Data Prefetch Controller},
  booktitle = {Proceedings 1999 {{IEEE International Conference}} on {{Computer Design}}: {{VLSI}} in {{Computers}} and {{Processors}} ({{Cat}}. {{No}}.{{99CB37040}})},
  author = {Vander Wiel, S.P. and Lilja, D.J.},
  year = {1999},
  month = oct,
  pages = {372--377},
  issn = {1063-6404},
  doi = {10.1109/ICCD.1999.808569},
  abstract = {Data prefetching has been proposed as a means of hiding the memory access latencies of data referencing patterns that defeat caching strategies. Prefetching techniques that either use special cache logic to issue prefetches or that rely on the processor to issue prefetch requests typically involve some compromise between accuracy and instruction overhead. A data prefetch controller (DPC) is proposed that combines low instruction overhead with the flexibility and accuracy of a compiler-directed prefetch mechanism. At run-time, the processor and prefetch controller each execute separate, but cooperating instruction streams. Simulations in which both programs are generated from a single application source file using a commercial compiler show that the prefetch controller can significantly improve the cache utilization and execution time of several SPECfp95 benchmarks. Performance comparisons also indicate that the DPC outperforms software prefetching techniques and prefetching via a hardware reference prediction table.},
  file = {/Users/jonathanrainer/Zotero/storage/2ZCWC6VI/Vander Wiel and Lilja - 1999 - A compiler-assisted data prefetch controller.pdf;/Users/jonathanrainer/Zotero/storage/J52TXJVL/808569.html},
  keywords = {Cache memory,cache storage,cache utilization,caching strategies,circuit layout CAD,circuit simulation,commercial compiler,compiler-assisted data prefetch controller,compiler-directed prefetch mechanism,cooperating instruction streams,data prefetching,data referencing patterns,Delay,Dispatching,DPC,Etching,execution time,Hardware,hardware reference prediction table,instruction overhead,instruction sets,Logic,low instruction overhead,memory access latencies,prefetch controller,prefetch requests,Prefetching,prefetching techniques,Program processors,Read only memory,Runtime,single application source file,software prefetching techniques,SPECfp95 benchmarks,special cache logic}
}

@article{vanderwielDataPrefetchMechanisms2000,
  title = {Data Prefetch Mechanisms},
  author = {Vanderwiel, Steven P. and Lilja, David J.},
  year = {2000},
  month = jun,
  volume = {32},
  pages = {174--199},
  issn = {0360-0300},
  doi = {10.1145/358923.358939},
  abstract = {The expanding gap between microprocessor and DRAM performance has necessitated the use of increasingly aggressive techniques designed to reduce or hide the latency of main memory access. Although large cache hierarchies have proven to be effective in reducing this latency for the most frequently used data, it is still not uncommon for many programs to spend more than half their run times stalled on memory requests. Data prefetching has been proposed as a technique for hiding the access latency of data referencing patterns that defeat caching strategies. Rather than waiting for a cache miss to initiate a memory fetch, data prefetching anticipates such misses and issues a fetch to the memory system in advance of the actual memory reference. To be effective, prefetching must be implemented in such a way that prefetches are timely, useful, and introduce little overhead. Secondary effects such as cache pollution and increased memory bandwidth requirements must also be taken into consideration. Despite these obstacles, prefetching has the potential to significantly improve overall program execution time by overlapping computation with memory accesses. Prefetching strategies are diverse, and no single strategy has yet been proposed that provides optimal performance. The following survey examines several alternative approaches, and discusses the design tradeoffs involved when implementing a data prefetch strategy.},
  file = {/Users/jonathanrainer/Zotero/storage/MJSENFKY/Vanderwiel and Lilja - 2000 - Data prefetch mechanisms.pdf},
  journal = {ACM Computing Surveys},
  keywords = {memory latency,prefetching},
  number = {2}
}

@article{vanderwielWhenCachesAren1997,
  title = {When Caches Aren't Enough: Data Prefetching Techniques},
  shorttitle = {When Caches Aren't Enough},
  author = {Vander Wiel, S.P. and Lilja, D.J.},
  year = {1997},
  month = jul,
  volume = {30},
  pages = {23--30},
  issn = {1558-0814},
  doi = {10.1109/2.596622},
  abstract = {With data prefetching, memory systems call data into the cache before the processor needs it, thereby reducing memory-access latency. Using the most suitable techniques is critical to maximizing data prefetching's effectiveness. The authors review three popular prefetching techniques: software-initiated prefetching, sequential hardware-initiated prefetching, and prefetching via reference prediction tables.},
  file = {/Users/jonathanrainer/Zotero/storage/4M79UV2X/Vander Wiel and Lilja - 1997 - When caches aren't enough data prefetching techni.pdf;/Users/jonathanrainer/Zotero/storage/VRQVTN6D/596622.html},
  journal = {Computer},
  keywords = {Application software,cache,Cache memory,cache storage,data prefetching techniques,Delay,DRAM chips,Fabrication,Hardware,memory architecture,memory systems,memory-access latency,Prefetching,Processor scheduling,Random access memory,Read-write memory,reference prediction tables,sequential hardware-initiated prefetching,software-initiated prefetching},
  number = {7}
}

@inproceedings{veidenbaumDecoupledAccessDRAM1997,
  title = {Decoupled Access {{DRAM}} Architecture},
  booktitle = {Proceedings {{Innovative Architecture}} for {{Future Generation High}}-{{Performance Processors}} and {{Systems}}},
  author = {Veidenbaum, A. V. and Gallivan, K. A.},
  year = {1997},
  month = oct,
  pages = {94--103},
  doi = {10.1109/IWIA.1997.670415},
  abstract = {This paper discusses an approach to reducing memory latency in future systems. It focuses on systems where a single chip DRAM/processor will not be feasible even in 10 years, e.g. systems requiring a large memory and/or many CPU's. In such systems a solution needs to be found to DRAM latency and bandwidth as well as to inter-chip communication. Utilizing the projected advances in chip I/O bandwidth we propose to implement a decoupled access-execute processor where the access processor is placed in memory. A program is compiled to run as a computational process and several access processes with the latter executing in the DRAM processors. Instruction set extensions are discussed to support this paradigm. Using multi-level branch prediction the access processor stays ahead of the execute processor and keeps the latter supplied with data. The system reduces latency by moving address computation to memory and thus avoiding sending address to memory by the computational processor. This and the fetch-ahead capabilities of the access processor are combined with multiple DRAM "streaming" to improve performance. DRAM caching is assumed to be used to assist in this as well.},
  file = {/Users/jonathanrainer/Zotero/storage/NA2WU962/00670415.pdf;/Users/jonathanrainer/Zotero/storage/XGK3SY5D/670415.html},
  keywords = {access processor,Access protocols,Bandwidth,chip I/O bandwidth,Circuit noise,Circuit synthesis,Clocks,Computer architecture,Computer science,decoupled access-execute processor,Delay,DRAM caching,execute processor,memory architecture,multi-level branch prediction,multiple DRAM,parallel architectures,Random access memory,reducing memory latency,single chip DRAM,Very large scale integration}
}

@inproceedings{wangBuildingLowLatency2016,
  title = {Building a {{Low Latency}}, {{Highly Associative DRAM Cache}} with the {{Buffered Way Predictor}}},
  booktitle = {2016 28th {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}} ({{SBAC}}-{{PAD}})},
  author = {Wang, Z. and Jim{\'e}nez, D. A. and Zhang, T. and Loh, G. H. and Xie, Y.},
  year = {2016},
  month = oct,
  pages = {109--117},
  doi = {10.1109/SBAC-PAD.2016.22},
  abstract = {The emerging die-stacked DRAM technology allows computer architects to design a last-level cache (LLC) with high memory bandwidth and large capacity. There are four key requirements for DRAM cache design: minimizing on-chip tag storage overhead, optimizing access latency, improving hit rate, and reducing off-chip traffic. These requirements seem mutually incompatible. For example, to reduce the tag storage overhead, the recent proposed LH-cache co-locates tags and data in the same DRAM cache row, and the Alloy Cache proposed to alloy data and tags in the same cache line in a direct-mapped design. However, these ideas either require significant tag lookup latency or sacrifice hit rate for hit latency. To optimize all four key requirements, we propose the Buffered Way Predictor (BWP). The BWP predicts the way ID of a DRAM cache request with high accuracy and coverage, allowing data and tag to be fetched back to back. Thus, the read latency for the data can be completely hidden so that DRAM cache hitting requests have low access latency. The BWP technique is designed for highly associative block-based DRAM caches and achieves a low miss rate and low off-chip traffic. Our evaluation with multi-programmed workloads and a 128MB DRAM cache shows that a 128KB BWP achieves a 76.2\% hit rate. The BWP improves performance by 8.8\% and 12.3\% compared to LH-cache and Alloy Cache, respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/LE4LJECF/Wang et al. - 2016 - Building a Low Latency, Highly Associative DRAM Ca.pdf;/Users/jonathanrainer/Zotero/storage/4VD5KSS2/7789330.html},
  keywords = {access latency optimization,alloy cache,alloy data,Arrays,Bandwidth,buffered way predictor,BWP technique,cache storage,Compounds,content-addressable storage,die-stacked DRAM technology,direct-mapped design,DRAM cache hitting requests,DRAM chips,integrated circuit design,last-level cache,LH-cache,LLC,low latency highly associative DRAM cache design,memory bandwidth,Memory management,Metals,off-chip traffic reduction,on-chip tag storage overhead minimization,Random access memory,read latency,sacrifice hit rate,storage capacity 128 Mbit,System-on-chip,tag lookup latency,tag storage overhead reduction}
}

@inproceedings{wangRealTimeCache2013,
  title = {Real {{Time Cache Performance Analyzing}} for {{Multi}}-Core {{Parallel Programs}}},
  booktitle = {2013 {{International Conference}} on {{Cloud}} and {{Service Computing}}},
  author = {Wang, R. and Gao, Y. and Zhang, G.},
  year = {2013},
  month = nov,
  pages = {16--23},
  doi = {10.1109/CSC.2013.11},
  abstract = {Modern processors mostly use cache to hide the memory access latency, so cache performance is very important to application program. A detailed cache performance analysis will provide programmers a clear view of their program behaviors, which can help them to identify the performance bottleneck and to optimize the source code. As the chip industry turn to integrate multiple cores into one chip, multi-core/many-core processor becomes the new approach to maintain the Moor's Law. Therefore, Parallel programs turn to be more important even in the personal computers. In parallel programs, the interaction between tasks is the source of bugs and errors and is hard to handling for most of programmers. The detailed cache behaviors will greatly helpful to the programmer to find the errors and optimize the programs. However, the existing cache performance analysis tools, due to the limitations of the hardware performance counters they depend on to get data, cannot get as much data as we expected. Those tools cannot reveal the program routines characteristics on shared cache and the source of cache misses with limited metrics on cache misses. In this paper, we propose a method to obtain and analysis real time cache performance with binary instrumentation and cache emulation. We instrument the parallel program while it is running, and get the trace data about memory access. Then we transport the trace data to an carefully configured cache emulation module to get the detailed cache behavior information. The emulation module can not only get more information than hardware performance counter but also can be configured to simulate different target hardware environment. Additionally, we use the performance data to form a group of cache performance metrics which can intuitively help programmers to optimize their codes. The accuracy of this method is demonstrated by comparing the summary result with the hardware performance counter. Finally, we design an cache performance analysis tool named CC-Analyzer for parallel programs. Comparing with the existing technologies, CC-Analyzer is able to analyze the cause of cache misses and gather much more performance statistics when the parallel program is running on different cache architectures.},
  file = {/Users/jonathanrainer/Zotero/storage/DWRQN76V/Wang et al. - 2013 - Real Time Cache Performance Analyzing for Multi-co.pdf;/Users/jonathanrainer/Zotero/storage/6AZKHWY5/6693173.html},
  keywords = {application program,binary instrumentation,cache architectures,cache behavior information,cache misses,cache performance analysis tools,cache performance metrics,cache storage,CC-Analyzer,chip industry,configured cache emulation module,data tracing,detailed cache performance analysis,Emulation,Hardware,hardware performance counter,Instruments,limited metrics,many-core processor,Measurement,memory access latency,modern processors,Moor's law,multicore parallel programs,multicore processor,multiple cores,multiprocessing systems,parallel programming,Performance analysis,performance statistics,personal computers,program behaviors,Program processors,program routines characteristics,Radiation detectors,real time cache performance,shared cache,source code}
}

@inproceedings{wangReducingDRAMLatency2018,
  title = {Reducing {{DRAM Latency}} via {{Charge}}-{{Level}}-{{Aware Look}}-{{Ahead Partial Restoration}}},
  booktitle = {2018 51st {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Wang, Y. and Tavakkol, A. and Orosa, L. and Ghose, S. and Ghiasi, N. Mansouri and Patel, M. and Kim, J. S. and Hassan, H. and Sadrosadati, M. and Mutlu, O.},
  year = {2018},
  month = oct,
  pages = {298--311},
  doi = {10.1109/MICRO.2018.00032},
  abstract = {Long DRAM access latency is a major bottleneck for system performance. In order to access data in DRAM, a memory controller (1) activates (i.e., opens) a row of DRAM cells in a cell array, (2) restores the charge in the activated cells back to their full level, (3) performs read and write operations to the activated row, and (4) precharges the cell array to prepare for the next activation. The restoration operation is responsible for a large portion (up to 43.6\%) of the total DRAM access latency. We find two frequent cases where the restoration operations performed by DRAM do not need to fully restore the charge level of the activated DRAM cells, which we can exploit to reduce the restoration latency. First, DRAM rows are periodically refreshed (i.e., brought back to full charge) to avoid data loss due to charge leakage from the cell. The charge level of a DRAM row that will be refreshed soon needs to be only partially restored, providing just enough charge so that the refresh can correctly detect the cells' data values. Second, the charge level of a DRAM row that will be activated again soon can be only partially restored, providing just enough charge for the activation to correctly detect the data value. However, partial restoration needs to be done carefully: for a row that will be activated again soon, restoring to only the minimum possible charge level can undermine the benefits of complementary mechanisms that reduce the activation time of highly-charged rows. To enable effective latency reduction for both activation and restoration, we propose charge-level-aware look-ahead partial restoration (CAL). CAL consists of two key components. First, CAL accurately predicts the next access time, which is the time between the current restoration operation and the next activation of the same row. Second, CAL uses the predicted next access time and the next refresh time to reduce the restoration time, ensuring that the amount of partial charge restoration is enough to maintain the benefits of reducing the activation time of a highly-charged row. We implement CAL fully in the memory controller, without any changes to the DRAM module. Across a wide variety of applications, we find that CAL improves the average performance of an 8-core system by 14.7\%, and reduces average DRAM energy consumption by 11.3\%.},
  file = {/Users/jonathanrainer/Zotero/storage/AGBUL9LY/8574549.html},
  keywords = {activation-latency,Arrays,CAL,Capacitors,charge-level-aware look-ahead partial restoration,data access time,DRAM cells array activation,DRAM chips,DRAM energy consumption,DRAM latency reduction,DRAM-latency,memory controller,partial-restoration,restoration-latency,System performance,Timing}
}

@article{warrierSkipCacheApplicationAware2015,
  title = {{{SkipCache}}: Application Aware Cache Management for Chip Multi-Processors},
  shorttitle = {{{SkipCache}}},
  author = {Warrier, Tripti S. and Raghavendra, Kanakagiri and Mutyam, Madhu},
  year = {2015},
  volume = {9},
  pages = {293--299},
  issn = {1751-861X},
  doi = {10.1049/iet-cdt.2014.0150},
  abstract = {With the advent of multiple cores on a single chip, it is common for the systems to have multi-level caches. Multiple levels of cache reduce the pressure on the memory bandwidth by allowing applications to store their frequently accessed data in them. The levels of cache nearer to the core filter the locality in the application access, which can result in high miss rates at farther levels. This piece of study revolves around one question: are all levels of cache needed by all applications during all phases of their execution? The study observes the effect of 2-level and 3-level cache hierarchies on the performance of different applications. On the basis of this study, this study proposes an application aware cache management policy called `SkipCache', which allows an application to choose a 2-level or 3-level cache hierarchy during run-time. SkipCache dynamically tracks the applications at shared last-level cache (LLC) to identify the applications that do not obtain advantage by using the LLC. Such applications can completely skip the LLC so that other co-scheduled cache friendly applications can efficiently use it. Evaluation of SkipCache in a 4-core chip multi-processor with multi-programmed workloads shows significant performance improvement. SkipCache is orthogonal to other cache management techniques and can be used along with other optimisation techniques to improve the system performance.},
  file = {/Users/jonathanrainer/Zotero/storage/HDUEUVSE/7303989.html},
  journal = {IET Computers Digital Techniques},
  keywords = {2-level cache hierarchy,3-level cache hierarchy,4-core chip multi-processor,application aware cache management,cache storage,chip multi-processors,memory bandwidth,microprocessor chips,multiprocessing systems,optimisation,optimisation technique,shared last-level cache,SkipCache},
  number = {6}
}

@inproceedings{waslyHidingMemoryLatency2014,
  title = {Hiding Memory Latency Using Fixed Priority Scheduling},
  booktitle = {2014 {{IEEE}} 19th {{Real}}-{{Time}} and {{Embedded Technology}} and {{Applications Symposium}} ({{RTAS}})},
  author = {Wasly, S. and Pellizzoni, R.},
  year = {2014},
  month = apr,
  pages = {75--86},
  doi = {10.1109/RTAS.2014.6925992},
  abstract = {Modern embedded platforms contain a variety of physical resources, such as caches, interconnects, main memory, etc., which the processor must access during the execution of a task. We argue that processor task execution and accesses to physical resources should be co-scheduled in real-time systems to predictably hide resource access latency. In particular, in this work we focus on co-scheduling task execution and accesses to main memory to hide DRAM access latency. Since modern systems implement DMA controllers that can be operated independently of processor execution, this allows us to hide memory transfer latency by scheduling DMA transfer in parallel with processor execution. The main contribution of this paper is a dynamic scheduling algorithm for a set of sporadic real-time tasks that efficiently co-schedules processor and DMA execution to hide memory transfer latency. The proposed algorithm can be applied to either uniprocessor or partitioned multiprocessor systems. We demonstrate that we improve processor utilization significantly compared to existing scratchpad and cache management systems.},
  file = {/Users/jonathanrainer/Zotero/storage/QEZCWNCP/Wasly and Pellizzoni - 2014 - Hiding memory latency using fixed priority schedul.pdf;/Users/jonathanrainer/Zotero/storage/HQ8M63W6/6925992.html},
  keywords = {Algorithm design and analysis,cache management systems,cache storage,coscheduling task execution,DMA controllers,DMA execution,DMA transfer scheduling,DRAM access latency,DRAM chips,embedded platforms,fixed priority scheduling,interconnects,Interference,main memory,memory latency,Memory management,memory transfer latency,multiprocessing systems,partitioned multiprocessor systems,physical resources,processor scheduling,processor task execution,real-time systems,Real-time systems,Schedules,Scheduling algorithms,scratchpad management systems,Time factors,uniprocessor}
}

@misc{watermanRISCVInstructionSet2019,
  title = {The {{RISC}}-{{V Instruction Set Manual}}},
  editor = {Waterman, Andrew and Asanovic, Krste},
  year = {2019},
  month = jun,
  publisher = {{RISC-V Foundation}},
  file = {/Users/jonathanrainer/Zotero/storage/H9R784BP/Waterman et al. - Volume I Unprivileged ISA.pdf},
  language = {en}
}

@inproceedings{wei-chetsengOptimalSchedulingMinimize2010,
  title = {Optimal Scheduling to Minimize Non-Volatile Memory Access Time with Hardware Cache},
  booktitle = {2010 18th {{IEEE}}/{{IFIP International Conference}} on {{VLSI}} and {{System}}-on-{{Chip}}},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  year = {2010},
  month = sep,
  pages = {131--136},
  doi = {10.1109/VLSISOC.2010.5642609},
  abstract = {In power and size sensitive embedded systems, flash memory and phase change memory are replacing DRAM as the main memory. Unfortunately, these technologies are limited by their endurance and long write latencies. To minimize the main memory access time, we optimally schedule tasks by an ILP formulation that can be generally applied to other main memory technologies, including DRAM. We also present a heuristic, Wander Scheduling, to solve larger instances in a reasonable amount of time. Our experimental results show that when compared with list scheduling, Wander Scheduling can reduce memory access times by an average of 40.73\% and increase the lifetime of flash and phase change memory by 82.56\%.},
  file = {/Users/jonathanrainer/Zotero/storage/SH8ZWFF9/05642609.pdf;/Users/jonathanrainer/Zotero/storage/HJP9WQ5Y/5642609.html},
  keywords = {Ash,cache storage,DRAM,DRAM chips,embedded system,flash memories,flash memory,hardware cache,Law,Nonvolatile memory,nonvolatile memory access time minimization,optimal scheduling,Optimal scheduling,phase change memories,phase change memory,processor scheduling,Schedules,Scheduling,wander scheduling}
}

@inproceedings{wei-chetsengPRRLowoverheadCache2012,
  title = {{{PRR}}: {{A}} Low-Overhead Cache Replacement Algorithm for Embedded Processors},
  shorttitle = {{{PRR}}},
  booktitle = {17th {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {{Wei-Che Tseng} and {Chun Jason Xue} and {Qingfeng Zhuge} and {Jingtong Hu} and Sha, E. H.-},
  year = {2012},
  month = jan,
  pages = {35--40},
  doi = {10.1109/ASPDAC.2012.6164972},
  abstract = {In embedded systems power consumption and area tightly constrain the cache capacity and management logic. Many good cache replacement policies have been proposed in the past, but none approach the performance of the least recently used (LRU) algorithm without incurring high overheads. In fact, many embedded designers consider even pseudo-LRU too complex for their embedded systems processors. In this paper, we propose a new level 1 (L1) data cache replacement algorithm, Protected Round-Robin (PRR) that is simple enough to be incorporated into embedded processors while providing miss rates that are very similar to the miss rates of LRU. Our experiments showed that on average the miss rates of PRR are only 0.22\% higher than the miss rates of LRU on a 32KB, 4-way L1 data cache with 32 byte long cache lines. PRR has miss rates that are on average 4.72\% and 4.66\% lower than random and round-robin replacement algorithms, respectively.},
  file = {/Users/jonathanrainer/Zotero/storage/EMTYUQ72/Wei-Che Tseng et al. - 2012 - PRR A low-overhead cache replacement algorithm fo.pdf;/Users/jonathanrainer/Zotero/storage/SB2XYXES/6164972.html},
  keywords = {Algorithm design and analysis,Benchmark testing,cache capacity,cache replacement policies,cache storage,Complexity theory,data cache replacement algorithm,embedded systems,Embedded systems,embedded systems power consumption,embedded systems processor,least recently used algorithm,low-overhead cache replacement algorithm,LRU algorithm,management logic,miss rates,Multicore processing,Program processors,protected round-robin,pseudoLRU,random algorithm,Round robin,round-robin replacement algorithm}
}

@inproceedings{weiExploitingProgramSemantics2015,
  title = {Exploiting {{Program Semantics}} to {{Place Data}} in {{Hybrid Memory}}},
  booktitle = {2015 {{International Conference}} on {{Parallel Architecture}} and {{Compilation}} ({{PACT}})},
  author = {Wei, W. and Jiang, D. and McKee, S. A. and Xiong, J. and Chen, M.},
  year = {2015},
  month = oct,
  pages = {163--173},
  doi = {10.1109/PACT.2015.10},
  abstract = {Large-memory applications like data analytics and graph processing benefit from extended memory hierarchies, and hybrid DRAM/NVM (non-volatile memory) systems represent an attractive means by which to increase capacity at reasonable performance/energy tradeoffs. Compared to DRAM, NVMs generally have longer latencies and higher energies for writes, which makes careful data placement essential for efficient system operation. Data placement strategies that resort to monitoring all data accesses and migrating objects to dynamically adjust data locations incur high monitoring overhead and unnecessary memory copies due to mispredicted migrations. We find that program semantics (specifically, global access characteristics) can effectively guide initial data placement with respect to memory types, which, in turn, makes run-time migration more efficient. We study a combined offline/online placement scheme that uses access profiling information to place objects statically and then selectively monitors run-time behaviors to optimize placements dynamically. We present a software/hardware cooperative framework, 2PP, and evaluate it with respect to state-of-the-art migratory placement, finding that it improves performance by an average of 12.1\%. Furthermore, 2PP improves energy efficiency by up to 51.8\%, and by an average of 18.4\%. It does so by reducing run-time monitoring and migration overheads.},
  file = {/Users/jonathanrainer/Zotero/storage/ZJ69QVFR/Wei et al. - 2015 - Exploiting Program Semantics to Place Data in Hybr.pdf;/Users/jonathanrainer/Zotero/storage/9NUV2RPH/7429303.html},
  keywords = {2PP,access profiling information,computer centres,data center,Data mining,data placement strategy,DRAM,DRAM chips,energy efficiency,hybird memory systems,hybrid memory,Monitoring,Nonvolatile memory,nonvolatile memory system,NVM system,offline/online placement scheme,Organizations,Phase change materials,program semantics,Random access memory,Semantics,software/hardware cooperative framework,storage management}
}

@patent{wen-tzerImplementationPseudoLRUAlgorithm2006,
  title = {Implementation of a Pseudo-{{LRU}} Algorithm in a Partitioned Cache},
  author = {{Wen-Tzer}, Thomas Chen and Peichun, Peter Liu and Kevin, C. Stelzer},
  year = {2006},
  month = jun,
  abstract = {The present invention provides for a plurality of partitioned ways of an associative cache. A pseudo-least recently used binary tree is provided, as is a way partition binary tree, and signals are derived from the way partition binary tree as a function of a mapped partition. Signals from the way partition binary tree and the pseudo-least recently used binary tree are combined. A cache line replacement signal is employable to select one way of a partition as a function of the pseudo-least recently used binary tree and the signals derived from the way partition binary tree.},
  assignee = {International Business Machines Corp},
  file = {/Users/jonathanrainer/Zotero/storage/3STJMZZQ/US7069390.pdf},
  number = {7069390B2}
}

@article{whithamTimePredictableOutofOrderExecution2010,
  title = {Time-{{Predictable Out}}-of-{{Order Execution}} for {{Hard Real}}-{{Time Systems}}},
  author = {Whitham, Jack and Audsley, Neil},
  year = {2010},
  month = sep,
  volume = {59},
  pages = {1210--1223},
  issn = {0018-9340},
  doi = {10.1109/TC.2010.109},
  abstract = {Superscalar out-of-order CPU designs can achieve higher performance than simpler in-order designs through exploitation of instruction-level parallelism in software. However, these CPU designs are often considered to be unsuitable for hard real-time systems because of the difficulty of guaranteeing the worst-case execution time (WCET) of software. This paper proposes and evaluates modifications for a superscalar out-of-order CPU core to allow instruction-level parallelism to be exploited without sacrificing time predictability and support for WCET analysis. Experiments using the M5 O3 CPU simulator show that WCETs can be two-four times smaller than those obtained using an idealized in-order CPU design, as instruction-level parallelism is exploited without compromising timing safety.},
  file = {/Users/jonathanrainer/Zotero/storage/56ZI6LUR/Whitham and Audsley - 2010 - Time-Predictable Out-of-Order Execution for Hard R.pdf},
  journal = {IEEE Transactions on Computers},
  language = {en},
  number = {9}
}

@article{wilkesMemoryGapFuture2001,
  title = {The Memory Gap and the Future of High Performance Memories},
  author = {Wilkes, Maurice V.},
  year = {2001},
  month = mar,
  volume = {29},
  pages = {2--7},
  issn = {01635964},
  doi = {10.1145/373574.373576},
  file = {/Users/jonathanrainer/Zotero/storage/85ND6GEF/Wilkes - 2001 - The memory gap and the future of high performance .pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {1}
}

@article{wilkesSlaveMemoriesDynamic1965,
  title = {Slave {{Memories}} and {{Dynamic Storage Allocation}}},
  author = {Wilkes, M. V.},
  year = {1965},
  month = apr,
  volume = {EC-14},
  pages = {270--271},
  issn = {0367-7508},
  doi = {10.1109/PGEC.1965.264263},
  file = {/Users/jonathanrainer/Zotero/storage/SERKATC8/Wilkes - 1965 - Slave Memories and Dynamic Storage Allocation.pdf},
  journal = {IEEE Transactions on Electronic Computers},
  language = {en},
  number = {2}
}

@article{wilsonHighBandwidthOnchip2001,
  title = {High Bandwidth On-Chip Cache Design},
  author = {Wilson, K.M. and Olukotun, K.},
  year = {2001},
  month = apr,
  volume = {50},
  pages = {292--307},
  issn = {2326-3814},
  doi = {10.1109/12.919276},
  abstract = {In this paper, we evaluate the performance of high bandwidth cache organizations employing multiple cache ports, multiple cycle hit times, and cache port efficiency enhancements, such as load all and line buffer, to find the organization that provides the best processor performance. Using a dynamic superscalar processor running realistic benchmarks that include operating system references, we use execution time to measure processor performance. When the cache is limited to a single cache port without enhancements, we find that two cache ports increase processor performance by 25 percent. With the addition of line buffer and load all to a single pelted cache, the processor achieves 91 percent of the performance of the same processor containing a cache with two ports. When the processor is not limited to a single cache port, the results show that a large dual-ported multicycle pipelined SRAM cache with a line buffer maximizes processor performance. A large pipelined cache provides both a low miss rate and a high CPU clock frequency. Dual-porting the cache and using a line buffer provide the bandwidth needed by a dynamic superscalar processor. The line buffer makes the pipelined dual-ported cache the best option by increasing cache port bandwidth and hiding cache latency.},
  file = {/Users/jonathanrainer/Zotero/storage/PW8SY4BF/Wilson and Olukotun - 2001 - High bandwidth on-chip cache design.pdf;/Users/jonathanrainer/Zotero/storage/K638LKKZ/919276.html},
  journal = {IEEE Transactions on Computers},
  keywords = {Bandwidth,Buffer storage,cache design,cache storage,Clocks,Delay,dual-ported multicycle pipelined SRAM cache,Frequency,high bandwidth cache,line buffer,memory architecture,Microprocessors,Operating systems,performance,pipelined cache,Random access memory,System-on-a-chip,Time measurement},
  number = {4}
}

@inproceedings{wongModifiedLRUPolicies2000,
  title = {Modified {{LRU}} Policies for Improving Second-Level Cache Behavior},
  booktitle = {Proceedings {{Sixth International Symposium}} on {{High}}-{{Performance Computer Architecture}}. {{HPCA}}-6 ({{Cat}}. {{No}}.{{PR00550}})},
  author = {Wong, W.A. and Baer, J.-L.},
  year = {2000},
  month = jan,
  pages = {49--60},
  issn = {null},
  doi = {10.1109/HPCA.2000.824338},
  abstract = {Main memory accesses continue to be a significant bottleneck for applications whose working sets do not fit in second-level caches. With the trend of greater associativity in second-level caches, implementing effective replacement algorithms might become more important than reducing conflict misses. After showing that an opportunity exists to close part of the gap between the OPT and the LRU algorithms, we present a replacement algorithm based on the detection of temporal locality in lines residing in the L2 cache. Rather than always replacing the LRU line, the victim is chosen by considering both its priority in the LRU stack and whether it exhibits temporal locality or not. We consider two strategies which use this replacement algorithm: a profile-based scheme where temporal locality is detected by processing a trace from a training set of the application and an on-line scheme, where temporal locality is detected with the assistance of a small locality table. Both schemes improve on the second-level cache miss rate over a pure LRU algorithm, by as much as 12\% in the profiling case and 20\% in the dynamic case.},
  file = {/Users/jonathanrainer/Zotero/storage/528YVZBJ/Wong and Baer - 2000 - Modified LRU policies for improving second-level c.pdf;/Users/jonathanrainer/Zotero/storage/EXTBC33N/824338.html},
  keywords = {associativity,cache storage,content-addressable storage,Delay,effective replacement algorithms,Hardware,modified LRU policies,Optimized production technology,performance evaluation,profile-based scheme,Radio access networks,replacement algorithm,second-level cache behavior,second-level caches,temporal locality}
}

@article{woodMinimizationDemandPaging1983,
  title = {Minimization of Demand Paging for the {{LRU}} Stack Model of Program Behavior},
  author = {Wood, Christopher and Fernandez, Eduardo B. and Lang, Tomas},
  year = {1983},
  month = feb,
  volume = {16},
  pages = {99--104},
  issn = {0020-0190},
  doi = {10.1016/0020-0190(83)90034-0},
  abstract = {The minimization of page faults in a demand paging environment where program behavior is described by the LRU stack model is studied. Previous work on this subject considered a specific type of stack probability distribution. As there exist practical situations which do not satisfy this assumption, we extend the analysis to arbitrary distributions. The minimization is stated as an optimization problem under constraints, a method to obtain a class of optimal solution is presented, and a fixed-space replacement algorithm to implement these solutions is proposed. The parameters of this replacement algorithm can be varied to adapt to specific stack probability distributions and to the number of allocated pages in memory. This paper also determines a necessary and sufficient condition for the optimality of the LRU algorithm.},
  file = {/Users/jonathanrainer/Zotero/storage/DI2CPZ73/Wood et al. - 1983 - Minimization of demand paging for the LRU stack mo.pdf;/Users/jonathanrainer/Zotero/storage/TYYCZD5R/0020019083900340.html},
  journal = {Information Processing Letters},
  keywords = {demand paging,LRU stack model,memory management,Page replacement algorithms,performance optimization,virtual memory},
  language = {en},
  number = {2}
}

@inproceedings{wuHybridCacheArchitecture2009,
  title = {Hybrid Cache Architecture with Disparate Memory Technologies},
  booktitle = {Proceedings of the 36th Annual International Symposium on {{Computer}} Architecture},
  author = {Wu, Xiaoxia and Li, Jian and Zhang, Lixin and Speight, Evan and Rajamony, Ram and Xie, Yuan},
  year = {2009},
  month = jun,
  pages = {34--45},
  publisher = {{Association for Computing Machinery}},
  address = {{Austin, TX, USA}},
  doi = {10.1145/1555754.1555761},
  abstract = {Caching techniques have been an efficient mechanism for mitigating the effects of the processor-memory speed gap. Traditional multi-level SRAM-based cache hierarchies, especially in the context of chip multiprocessors (CMPs), present many challenges in area requirements, core-to-cache balance, power consumption, and design complexity. New advancements in technology enable caches to be built from other technologies, such as Embedded DRAM (EDRAM), Magnetic RAM (MRAM), and Phase-change RAM (PRAM), in both 2D chips or 3D stacked chips. Caches fabricated in these technologies offer dramatically different power and performance characteristics when compared with SRAM-based caches, particularly in the areas of access latency, cell density, and overall power consumption. In this paper, we propose to take advantage of the best characteristics that each technology offers, through the use of Hybrid Cache Architecture (HCA) designs. We discuss and evaluate two types of hybrid cache architectures: inter cache Level HCA (LHCA), in which the levels in a cache hierarchy can be made of disparate memory technologies; and intra cache level or cache Region based HCA (RHCA), where a single level of cache can be partitioned into multiple regions, each of a different memory technology. We have studied a number of different HCA architectures and explored the potential of hardware support for intra-cache data movement and power consumption management within HCA caches. Utilizing a full-system simulator that has been validated against real hardware, we demonstrate that an LHCA design can provide a geometric mean 7\% IPC improvement over a baseline 3-level SRAM cache design under the same area constraint across a collection of 25 workloads. A more aggressive RHCA-based design provides 12\% IPC improvement over the baseline. Finally, a 2-layer 3D cache stack (3DHCA) of high density memory technology within the same chip footprint gives 18\% IPC improvement over the baseline. Furthermore, up to 70\% reduction in power consumption over a baseline SRAM-only design is achieved.},
  file = {/Users/jonathanrainer/Zotero/storage/E9PWEE84/Wu et al. - 2009 - Hybrid cache architecture with disparate memory te.pdf},
  isbn = {978-1-60558-526-0},
  keywords = {hybrid cache architecture,three-dimensional ic},
  series = {{{ISCA}} '09}
}

@article{wulfHittingMemoryWall1995,
  title = {Hitting the Memory Wall: Implications of the Obvious},
  shorttitle = {Hitting the Memory Wall},
  author = {Wulf, Wm. A. and McKee, Sally A.},
  year = {1995},
  month = mar,
  volume = {23},
  pages = {20--24},
  issn = {01635964},
  doi = {10.1145/216585.216588},
  file = {/Users/jonathanrainer/Zotero/storage/KCVD5Z8F/Wulf and McKee - 1995 - Hitting the memory wall implications of the obvio.pdf},
  journal = {ACM SIGARCH Computer Architecture News},
  language = {en},
  number = {1}
}

@article{wuLoopPartitionTechnique1996,
  title = {A Loop Partition Technique for Reducing Cache Bank Conflict in Multithreaded Architecture},
  author = {Wu, Chao-Chin and Chen, Cheng},
  year = {1996},
  volume = {143},
  pages = {30},
  issn = {13502387},
  doi = {10.1049/ip-cdt:19960007},
  file = {/Users/jonathanrainer/Zotero/storage/EXXNTV5W/00487922.pdf},
  journal = {IEE Proceedings - Computers and Digital Techniques},
  language = {en},
  number = {1}
}

@inproceedings{xiangLessReusedFilter2009,
  title = {Less {{Reused Filter}}: {{Improving L2 Cache Performance}} via {{Filtering Less Reused Lines}}},
  shorttitle = {Less {{Reused Filter}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  author = {Xiang, Lingxiang and Chen, Tianzhou and Shi, Qingsong and Hu, Wei},
  year = {2009},
  pages = {68--79},
  publisher = {{ACM}},
  address = {{Yorktown Heights, NY, USA}},
  doi = {10.1145/1542275.1542290},
  abstract = {The L2 cache is commonly managed using LRU policy. For workloads that have a working set larger than L2 cache, LRU behaves poorly, resulting in a great number of less reused lines that are never reused or reused for few times. In this case, the cache performance can be improved through retaining a portion of working set in cache for a period long enough. Previous schemes approach this by bypassing never reused lines. Nevertheless, severely constrained by the number of never reused lines, sometimes they deliver no benefit due to the lack of never reused lines. This paper proposes a new filtering mechanism that filters out the less reused lines rather than just never reused lines. The extended scope of bypassing provides more opportunities to fit the working set into cache. This paper also proposes a Less Reused Filter (LRF), a separate structure that precedes L2 cache, to implement the above mechanism. LRF employs a reuse frequency predictor to accurately identify the less reused lines from incoming lines. Meanwhile, based on our observation that most less reused lines have a short life span, LRF places the filtered lines into a small filter buffer to fully utilize them, avoiding extra misses. Our evaluation, for 24 SPEC 2000 benchmarks, shows that augmenting a 512KB LRU-managed L2 cache with a LRF having 32KB filter buffer reduces the average MPKI by 27.5\%, narrowing the gap between LRU and OPT by 74.4\%.},
  file = {/Users/jonathanrainer/Zotero/storage/DYWMAUSB/Xiang et al. - 2009 - Less Reused Filter Improving L2 Cache Performance.pdf},
  isbn = {978-1-60558-498-0},
  keywords = {cache filtering,less reused line},
  series = {{{ICS}} '09}
}

@inproceedings{yabeNextGenerationChanneledDRAM2000,
  title = {A next Generation Channeled-{{DRAM}} Architecture with Direct Background-Operation and Delayed Channel-Replacement Techniques},
  booktitle = {2000 {{Symposium}} on {{VLSI Circuits}}. {{Digest}} of {{Technical Papers}} ({{Cat}}. {{No}}.{{00CH37103}})},
  author = {Yabe, Y. and Nakamura, N. and Aimoto, Y. and Motomura, M. and Matsui, Y. and Adakura, Y.},
  year = {2000},
  month = jun,
  pages = {108--111},
  doi = {10.1109/VLSIC.2000.852864},
  abstract = {As processor performance is reaching the level of executing a single instruction in 1 ns, long memory latencies have become a critical problem, because a single memory access could stall the execution of hundreds of instructions. A recently announced channeled-DRAM approaches this problem by integrating a small low-latency buffer, called "channels", in front of a DRAM core in order to reduce the effective memory latency. Since the channels can provide intrinsically faster access than that of a bare DRAM core when they hit, key considerations in this architecture become (1) how to achieve high channel hit rates and (2) how to reduce the channel-miss latencies. Since channeled-DRAMs rely on an external memory controller to handle all the channel management, design of the memory controller heavily dominates the first issue. In this paper, we propose two novel techniques for reducing the channel-miss latencies: direct background operation and delayed channel replacement. We examined these techniques in a future 256-Mb DRAM with a 200-MHz double-data-rate (DDR) synchronous interface. Both SPICE simulation results (that show channel-miss latency reduction) and system-level simulation results (that reveal system-level performance improvement) are presented.},
  file = {/Users/jonathanrainer/Zotero/storage/TZKGEQH3/Yabe et al. - 2000 - A next generation channeled-DRAM architecture with.pdf;/Users/jonathanrainer/Zotero/storage/VG4NHJ46/852864.html},
  keywords = {1 ns,200 MHz,256 Mbit,Bandwidth,channel hit rates,channel management,channel-miss latencies,channeled-DRAM architecture,Delay,delayed channel replacement,Delays,direct background operation,double-data-rate synchronous interface,DRAM chips,DRAM core,external memory controller,Graphics,Laboratories,Large scale integration,low-latency buffer,memory access,Memory architecture,memory latency,Memory management,National electric code,Prefetching,processor performance,Random access memory,Silicon,SPICE simulation,system-level performance improvement,system-level simulation}
}

@inproceedings{yamauchiHierarchicalMultibankDRAM1997,
  title = {The Hierarchical Multi-Bank {{DRAM}}: A High-Performance Architecture for Memory Integrated with Processors},
  shorttitle = {The Hierarchical Multi-Bank {{DRAM}}},
  booktitle = {Proceedings {{Seventeenth Conference}} on {{Advanced Research}} in {{VLSI}}},
  author = {Yamauchi, T. and Hammond, L. and Olukotun, K.},
  year = {1997},
  month = sep,
  pages = {303--319},
  doi = {10.1109/ARVLSI.1997.634862},
  abstract = {A microprocessor integrated with DRAM on the same die has the potential to improve system performance by reducing the memory latency and improving the memory bandwidth. However a high performance microprocessor will typically send more accesses than the DRAM can handle due to the long cycle time of the embedded DRAM, especially in applications with significant memory requirements. A multi-bank DRAM can hide the long cycle time by allowing the DRAM to process multiple accesses in parallel, but it will incur a significant area penalty and will therefore restrict the density of the embedded DRAM main memory. In this paper we propose a hierarchical multi-bank DRAM architecture to achieve high system performance with a minimal area penalty. In this architecture, the independent memory banks are each divided into many semi-independent subbanks that share I/O and decoder resources. A hierarchical multi-bank DRAM with 4 main banks each composed of 32 subbanks occupies approximately the same area as a conventional 4 bank DRAM while performing like a 32 bank one-up to 65\% better than a conventional 4 bank DRAM when integrated with a single-chip multiprocessor.},
  file = {/Users/jonathanrainer/Zotero/storage/9JSHHHNP/Yamauchi et al. - 1997 - The hierarchical multi-bank DRAM a high-performan.pdf;/Users/jonathanrainer/Zotero/storage/P6T3597Y/634862.html},
  keywords = {Bandwidth,Computer architecture,decoder resources,Delay,DRAM chips,embedded DRAM,hierarchical multi-bank DRAM,High performance computing,high-performance architecture,Laboratories,memory architecture,Memory architecture,microprocessor,microprocessor chips,Microprocessors,multiple accesses,processor integrated memory,Random access memory,System performance,Ultra large scale integration}
}

@inproceedings{yangOverlappingDependentLoads2006,
  title = {Overlapping Dependent Loads with Addressless Preload},
  booktitle = {2006 {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Yang, Zhen and Shi, Xudong and Su, Feiqi and Peir, Jih-Kwon},
  year = {2006},
  month = sep,
  pages = {275--284},
  issn = {null},
  abstract = {Modern out-of-order processors with non-blocking caches exploit Memory-Level Parallelism (MLP) by overlapping cache misses in a wide instruction window. The exploitation of MLP, however, can be limited due to long-latency operations in producing the base address of a cache miss load. When the parent instruction is also a cache miss load, a serialization of the two loads must be enforced to satisfy the load-load data dependence. In this paper, we propose a mechanism that dynamically captures the load-load data dependences at runtime. A special Preload is issued in place of the dependent load without waiting for the parent load, thus effectively overlapping the two loads. The Preload provides necessary information for the memory controller to calculate the correct memory address upon the availability of the parent's data to eliminate any interconnect delay between the two loads. Performance evaluations based on SPEC2000 and Olden applications show that significant speedups up to 40\% with an average of 16\% are achievable using the Preload. In conjunction with other aggressive MLP exploitation methods, such as runahead execution, the Preload can make more significant improvement with an average of 22\%.},
  file = {/Users/jonathanrainer/Zotero/storage/BIE44DJZ/Yang et al. - 2006 - Overlapping dependent loads with addressless prelo.pdf;/Users/jonathanrainer/Zotero/storage/5MBXDTLD/7847604.html},
  keywords = {Data Prefetching,Delays,Instruction and Issue Window,Load modeling,Memory-Level Parallelism,Parallel processing,Performance evaluation,Pointer-Chasing Loads,Prefetching,Random access memory}
}

@inproceedings{yuIMPIndirectMemory2015,
  title = {{{IMP}}: {{Indirect}} Memory Prefetcher},
  shorttitle = {{{IMP}}},
  booktitle = {2015 48th {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Yu, X. and Hughes, C. J. and Satish, N. and Devadas, S.},
  year = {2015},
  month = dec,
  pages = {178--190},
  doi = {10.1145/2830772.2830807},
  abstract = {Machine learning, graph analytics and sparse linear algebra-based applications are dominated by irregular memory accesses resulting from following edges in a graph or non-zero elements in a sparse matrix. These accesses have little temporal or spatial locality, and thus incur long memory stalls and large bandwidth requirements. A traditional streaming or striding prefetcher cannot capture these irregular access patterns. A majority of these irregular accesses come from indirect patterns of the form A[B[j]]. We propose an efficient hardware indirect memory prefetcher (IMP) to capture this access pattern and hide latency. We also propose a partial cacheline accessing mechanism for these prefetches to reduce the network and DRAM bandwidth pressure from the lack of spatial locality. Evaluated on 7 applications, IMP shows 56\% speedup on average (up to 2.3\texttimes{}) compared to a baseline 64 core system with streaming prefetchers. This is within 23\% of an idealized system. With partial cacheline accessing, we see another 9.4\% speedup on average (up to 46.6\%).},
  file = {/Users/jonathanrainer/Zotero/storage/W7P2BQS8/Yu et al. - 2015 - IMP Indirect memory prefetcher.pdf;/Users/jonathanrainer/Zotero/storage/C24P7438/7856597.html},
  keywords = {Arrays,Bandwidth,DRAM bandwidth pressure,DRAM chips,graph analytics,graph theory,Hardware,IMP,Indexes,indirect memory prefetcher,irregular memory accesses,learning (artificial intelligence),linear algebra,machine learning,Multicore processing,non-zero elements,Prefetching,sparse linear algebra,Sparse matrices,sparse matrix,spatial locality,temporal locality}
}

@inproceedings{zebchukReexaminingCacheReplacement2008,
  title = {Re-Examining Cache Replacement Policies},
  author = {Zebchuk, Jason and Makineni, Srihari and Newell, Don},
  year = {2008},
  month = oct,
  pages = {671--678},
  publisher = {{IEEE}},
  doi = {10.1109/ICCD.2008.4751933},
  abstract = {The replacement policies commonly used in modern processors perform an average of 57\% worse than an optimal replacement policy for commercial applications using large, shared caches in a chip-multiprocessor (CMP). Recent proposals that improve the performance of smaller, uniprocessor caches with SPEC CPU workloads do not achieve similar benefits with commercial workloads and larger caches , even though these caches still perform worse than optimal. The recently proposed Shepherd Cache replacement policy reduces miss-ratios by 7.3\% on average, but it relies on an impractical LRU policy and requires 5.3\% overhead relative to the total cache capacity. We propose two new, practical, low-overhead replacement policies that mimic Shepherd Cache with significantly less meta-data overhead. First, we propose a Lightweight Shepherd Cache design that reduces miss-ratios by 8\% on average and up to 19\%, while requiring only 1.9\% meta-data overhead . We also propose an Extra-Lightweight Shepherd Cache design that reduces overhead to only 0.5\% when combined with a practical Clock replacement policy while reducing miss-ratios by an average of 5.4\% and up to 14\%.},
  file = {/Users/jonathanrainer/Zotero/storage/PZ5VNPMM/Zebchuk et al. - 2008 - Re-examining cache replacement policies.pdf},
  isbn = {978-1-4244-2657-7},
  language = {en}
}

@article{zhangCachedDRAMILP2001,
  title = {Cached {{DRAM}} for {{ILP}} Processor Memory Access Latency Reduction},
  author = {Zhang, Z. and Zhu, Z. and Zhang, X.},
  year = {2001},
  month = jul,
  volume = {21},
  pages = {22--32},
  issn = {0272-1732},
  doi = {10.1109/40.946676},
  abstract = {Cached DRAM adds a small cache onto a DRAM chip to reduce average DRAM access latency. The authors compare cached DRAM with other advanced DRAM techniques for reducing memory access latency in instruction-level-parallelism processors.},
  file = {/Users/jonathanrainer/Zotero/storage/VH8GNL7D/Zhang et al. - 2001 - Cached DRAM for ILP processor memory access latenc.pdf;/Users/jonathanrainer/Zotero/storage/3MRLLL4U/946676.html},
  journal = {IEEE Micro},
  keywords = {Bandwidth,cache storage,cached DRAM,Capacitors,Computer aided instruction,Concurrent computing,Delay,DRAM access latency,DRAM chips,ILP,Impedance,instruction-level-parallelism processors,Interleaved codes,latency reduction,memory architecture,parallel architectures,Parallel processing,processor memory access latency,Random access memory,SDRAM},
  number = {4}
}

@inproceedings{zhangCacheReplacementPolicy2010,
  title = {A {{Cache Replacement Policy Using Adaptive Insertion}} and {{Re}}-Reference {{Prediction}}},
  booktitle = {2010 22nd {{International Symposium}} on {{Computer Architecture}} and {{High Performance Computing}}},
  author = {Zhang, X. and Li, C. and Wang, H. and Wang, D.},
  year = {2010},
  month = oct,
  pages = {95--102},
  doi = {10.1109/SBAC-PAD.2010.21},
  abstract = {Previous research shows that LRU replacement policy is not efficient when applications exhibit a distant re-reference interval. Recently proposed RRIP policy improves performance for such workloads. However, RRIP lacks of access recency information, which may confuse the replacement policy to make accurate prediction. Consequently, RRIP is not robust for recency-friendly workloads. This paper proposes an Adaptive Insertion and Re-reference Prediction (AI-RRP) policy which evicts data based on both re-reference prediction value and the access recency information. To make the replacement policy more adaptive across different workloads and different phases during execution, Dynamic AI-RRP (DAI-RRP) is proposed which adjusts the insertion position and prediction value for different access patterns. Simulation results show DAI-RRP reduces CPI over LRU and Dynamic RRIP by an average of 8.3\% and 4.1\% respectively on a single-core processor with a 1MB 16-way set last-level cache (LLC). Evaluations on quad-core CMP with a 4MB shared LLC show that DAI-RRP outperforms LRU and Dynamic RRIP (DRRIP) on the weighted speedup metric by an average of 13.2\% and 26.7\% respectively. Furthermore, compred to LRU, DAI-RRP requires similar hardware, or even less hardware for high-associativity cache.},
  file = {/Users/jonathanrainer/Zotero/storage/WIVN9I7B/Zhang et al. - 2010 - A Cache Replacement Policy Using Adaptive Insertio.pdf;/Users/jonathanrainer/Zotero/storage/X9QNV5M9/5644963.html},
  keywords = {access pattern,Adaptive Insertion,adaptive insertion and re-reference prediction,Cache Replacement,cache replacement policy,cache storage,Electronics packaging,Hardware,last level cache,Proposals,Radiation detectors,Registers,Robustness,Set Dueling,Shared Cache,Simulation,single core processor}
}

@inproceedings{zhangDivideandconquerBubbleReplacement2009,
  title = {Divide-and-Conquer: {{A Bubble Replacement}} for {{Low Level Caches}}},
  shorttitle = {Divide-and-Conquer},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Supercomputing}}},
  author = {Zhang, Chuanjun and Xue, Bing},
  year = {2009},
  pages = {80--89},
  publisher = {{ACM}},
  address = {{Yorktown Heights, NY, USA}},
  doi = {10.1145/1542275.1542291},
  abstract = {The widely used LRU replacement policy suffers from the following problems. First, LRU does not exploit fre-quency information of cache accesses. Second, LRU may experience cache thrashing when access to cache exhibits cyclic patterns and the cache capacity is less than the working set. Finally, LRU is expensive to implement in hardware. We propose a bubble replacement for low-level caches, where cache blocks in one set are arranged in a queue for replacement determination. An incoming block enters the queue from the bottom and exchanges its posi-tion with the block above when the block hits, therefore, both recency and frequency information of a program are exploited. A victim block can be chosen from either the bottom or the top block of the queue, which is controlled by a single-bit set-hit flag per set. Choosing the bottom block as the victim makes the bubble replacement resistant to less frequently used blocks from polluting the cache while choosing the top block as the victim makes the bub-ble replacement adaptable to changes in the working set. We also propose to divide the blocks in a cache set into groups where each group implements the bubble replace-ment (we name it the DC-Bubble) to resolve the problems of the bubble replacement. The victim block is chosen ran-domly from the bottom block of each group. The proposed DC-Bubble reduces the average MPKI of the baseline 1MB 16-way L2 cache by 14\%, bridges 47\% of the gap between LRU and the OPT, reduces the storage require-ment by 61\% and simplifies the circuit design compared to LRU.},
  file = {/Users/jonathanrainer/Zotero/storage/NXVXF6HB/Zhang and Xue - 2009 - Divide-and-conquer A Bubble Replacement for Low L.pdf},
  isbn = {978-1-60558-498-0},
  keywords = {cache replacement policy,divide-and-conquer,high-performance computing},
  series = {{{ICS}} '09}
}

@inproceedings{zhangSpeedingIrregularApplications1995,
  title = {Speeding up Irregular Applications in Shared-Memory Multiprocessors: Memory Binding and Group Prefetching},
  shorttitle = {Speeding up Irregular Applications in Shared-Memory Multiprocessors},
  booktitle = {Proceedings of the 22nd Annual International Symposium on {{Computer}} Architecture},
  author = {Zhang, Zheng and Torrellas, Josep},
  year = {1995},
  month = may,
  pages = {188--199},
  publisher = {{Association for Computing Machinery}},
  address = {{S. Margherita Ligure, Italy}},
  doi = {10.1145/223982.224423},
  abstract = {While many parallel applications exhibit good spatial locality, other important codes in areas like graph problem-solving or CAD do not. Often, these irregular codes contain small records accessed via pointers. Consequently, while the former applications benefit from long cache lines, the latter prefer short lines. One good solution is to combine short lines with prefetching. In this way, each application can exploit the amount of spatial locality that it has. However, prefetching, if provided, should also work for the irregular codes. This paper presents a new prefetching scheme that, while usable by regular applications, is specifically targeted to irregular ones: Memory Binding and Group Prefetching.The idea is to hardware-bind and prefetch together groups of data that the programmer suggests are strongly related to each other. Examples are the different fields in a record or two records linked by a permanent pointer. This prefetching scheme, combined with short cache lines, results in a memory hierarchy design that can be exploited by both regular and irregular applications. Overall, it is better to use a system with short lines (16-32 bytes) and our prefetching than a system with long lines (128 bytes) with or without our prefetching. The former system runs 6 out of 7 Splash-class applications faster. In particular, some of the most irregular applications run 25-40\% faster.},
  file = {/Users/jonathanrainer/Zotero/storage/PB7585FC/Zhang and Torrellas - 1995 - Speeding up irregular applications in shared-memor.pdf},
  isbn = {978-0-89791-698-1},
  series = {{{ISCA}} '95}
}

@inproceedings{zhangTagbasedCacheReplacement2010,
  title = {A Tag-Based Cache Replacement},
  booktitle = {2010 {{IEEE International Conference}} on {{Computer Design}}},
  author = {Zhang, C. and Xue, B.},
  year = {2010},
  month = oct,
  pages = {92--97},
  doi = {10.1109/ICCD.2010.5647602},
  abstract = {Conventional cache replacement policies use access information of each cache block for replacement decisions. We observe that there are many identical tags across different cache sets because programs exhibit spatial locality. The number of different tags in cache memory is significantly less than the total number of cache blocks in a cache. We propose a tag-based replacement that uses access frequency and recency of tags instead of cache blocks for the replacement decision. The tag-based replacement reduces the average miss rate of the baseline 1MB L2 cache by 15\% over conventional LRU with 95\% status bits reduction over conventional LRU. The performance improvement of a processor using the tag-based replacement is up to 40\% with an average of 4.5\% over LRU.},
  file = {/Users/jonathanrainer/Zotero/storage/W9SHQL3V/Zhang and Xue - 2010 - A tag-based cache replacement.pdf;/Users/jonathanrainer/Zotero/storage/28FVPR5F/5647602.html},
  keywords = {access frequency,Art,Benchmark testing,cache memory,Cache memory,cache storage,Frequency conversion,Hardware,Indexes,LRU,processor performance improvement,Radiation detectors,tag-based cache replacement}
}

@inproceedings{zhangWebCachingFramework1999,
  title = {Web Caching Framework: Analytical Models and Beyond},
  shorttitle = {Web Caching Framework},
  booktitle = {Proceedings 1999 {{IEEE Workshop}} on {{Internet Applications}} ({{Cat}}. {{No}}.{{PR00197}})},
  author = {Zhang, J. and Izmailov, R. and Reininger, D. and Ott, M.},
  year = {1999},
  month = jul,
  pages = {132--141},
  issn = {null},
  doi = {10.1109/WIAPP.1999.788030},
  abstract = {Many Web caching algorithms have been proposed in recent years. However the lack of analytical support and systematic evaluation environment significantly affect the applicability of these algorithms. We introduce a framework within which Web caching algorithms can be consistently analyzed and empirically examined. The framework consists of two complementary parts. The statistical model and the simulation environment. The analytical model covers both the Web trace characteristics and the caching algorithm behaviors. The simulation system, referred to as WebCASE (Web Caching Algorithm Simulation Environment), consists of an extensible simulation core and a front end graphical interface showing the running algorithm behaviors. By using this framework, we are able to better understand the performance discrepancies exhibited by different algorithms and develop more efficient new algorithms. These new algorithms take into consideration practical issues and make noticeable performance improvements over existing algorithms.},
  file = {/Users/jonathanrainer/Zotero/storage/KEIGEMGK/Zhang et al. - 1999 - Web caching framework analytical models and beyon.pdf;/Users/jonathanrainer/Zotero/storage/2NTFETQZ/788030.html},
  keywords = {Algorithm design and analysis,analytical models,Analytical models,cache storage,caching algorithm behaviors,digital simulation,Electrical capacitance tomography,Explosives,extensible simulation core,file servers,front end graphical interface,graphical user interfaces,information resources,Internet,Laboratories,Measurement,National electric code,Performance analysis,performance discrepancies,performance improvements,running algorithm behaviors,search engines,simulation environment,simulation system,statistical model,Web Caching Algorithm Simulation Environment,Web caching algorithms,Web caching framework,Web server,Web sites,Web trace characteristics,WebCASE}
}

@inproceedings{zhaoExploringDRAMCache2007,
  title = {Exploring {{DRAM}} Cache Architectures for {{CMP}} Server Platforms},
  booktitle = {2007 25th {{International Conference}} on {{Computer Design}}},
  author = {Zhao, Li and Iyer, Ravi and Illikkal, Ramesh and Newell, Don},
  year = {2007},
  month = oct,
  pages = {55--62},
  issn = {1063-6404},
  doi = {10.1109/ICCD.2007.4601880},
  abstract = {As dual-core and quad-core processors arrive in the marketplace, the momentum behind CMP architectures continues to grow strong. As more and more cores/threads are placed on-die, the pressure on the memory subsystem is rapidly increasing. To address this issue, we explore DRAM cache architectures for CMP platforms. In this paper, we investigate the impact of introducing a low latency, large capacity and high bandwidth DRAM-based cache between the last level SRAM cache and memory subsystem. We first show the potential benefits of large DRAM caches for key commercial server workloads. As the primary hurdle to achieving these benefits with DRAM caches is the tag space overheads associated with them, we identify the most efficient DRAM cache organization and investigate various options. Our results show that the combination of 8-bit partial tags and 2-way sectoring achieves the highest performance (20\% to 70\%) with the lowest tag space ({$<$}25\%) overhead.},
  file = {/Users/jonathanrainer/Zotero/storage/K4XYXNDL/Zhao et al. - 2007 - Exploring DRAM cache architectures for CMP server .pdf;/Users/jonathanrainer/Zotero/storage/LSWCH73H/4601880.html},
  keywords = {cache storage,CMP server platform,DRAM cache architecture,DRAM chips,logic design,memory architecture,microprocessor chips,multiprocessing systems}
}

@inproceedings{zhenlinwangUsingCompilerImprove2002,
  title = {Using the Compiler to Improve Cache Replacement Decisions},
  booktitle = {Proceedings.{{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  author = {{Zhenlin Wang} and McKinley, K. S. and Rosenberg, A. L. and Weems, C. C.},
  year = {2002},
  month = sep,
  pages = {199--208},
  doi = {10.1109/PACT.2002.1106018},
  abstract = {Memory performance is increasingly determining microprocessor performance and technology trends are exacerbating this problem. Most architectures use set-associative caches with LRU replacement policies to combine fast access with relatively low miss rates. To improve replacement decisions in set-associative caches, we develop a new set of compiler algorithms that predict which data will and will not be reused and provide these hints to the architecture. We prove that the hints either match or improve hit rates over LRU. We describe a practical one-bit cache-line tag implementation of our algorithm, called evict-me. On a cache replacement, the architecture will replace a line for which the evict-me bit is set, or if none is set, it will use the LRU bits. We implement our compiler analysis and its output in the Scale compiler. On a variety of scientific programs, using the evict-me algorithm in both the level 1 and 2 caches improves simulated cycle times by up to 34\% over the LRU policy by increasing hit rates. In addition, a combination of simple hardware prefetching and evict-me works together to further improve performance.},
  file = {/Users/jonathanrainer/Zotero/storage/ZR6QY6PF/Zhenlin Wang et al. - 2002 - Using the compiler to improve cache replacement de.pdf;/Users/jonathanrainer/Zotero/storage/DFHI92ZY/1106018.html},
  keywords = {Bridges,cache replacement decisions,cache storage,Clocks,Computer architecture,Computer science,content-addressable storage,Delay,evict-me,Hardware,hardware prefetching,hit rates,LRU replacement policies,memory performance,Microarchitecture,microprocessor performance,Microprocessors,one-bit cache-line tag implementation,performance evaluation,Prediction algorithms,Prefetching,program compiler,program compilers,Scale compiler,scientific programs,set-associative cache,storage management}
}

@inproceedings{zhiganghuTimekeepingMemorySystem2002,
  title = {Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior},
  shorttitle = {Timekeeping in the Memory System},
  booktitle = {Proceedings 29th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Zhigang Hu and Kaxiras, S. and Martonosi, M.},
  year = {2002},
  month = may,
  pages = {209--220},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2002.1003579},
  abstract = {This paper offers afresh perspective on the problem of predicting and optimizing memory behavior. We show quantitatively the extent to which detailed timing characteristics of past memory reference events are strongly predictive of future program reference behavior. We propose a family of timekeeping techniques that optimize behavior based on observations about particular cache time durations, such as the cache access interval or the cache dead time. Timekeeping techniques can be used to build small, simple, and high-accuracy (often 90\% or more) predictors for identifying conflict misses, for predicting dead blocks, and even for estimating the time at which the next reference to a cache frame will occur and the address that will be accessed. Based on these predictors, we demonstrate two new and complementary time-based hardware structures: (1) a time-based victim cache that improves performance by only storing conflict miss lines with likely reuse, and (2) a time-based prefetching technique that hones in on the right address to prefetch, and the right time to schedule the prefetch.},
  file = {/Users/jonathanrainer/Zotero/storage/CBSRLBIE/Zhigang Hu et al. - 2002 - Timekeeping in the memory system predicting and o.pdf;/Users/jonathanrainer/Zotero/storage/NUANHIAP/1003579.html},
  keywords = {cache access interval,cache dead time,Cache memory,cache storage,cache time durations,Communication system software,future program reference behavior,Hardware,memory architecture,memory referencing behavior,memory system,past memory reference,Performance analysis,Power dissipation,Prefetching,Proposals,Software performance,Software systems,storage management,time-based prefetching,time-based victim cache,timekeeping techniques,Timing}
}

@inproceedings{zhouMultiQueueReplacementAlgorithm2001,
  title = {The {{Multi}}-{{Queue Replacement Algorithm}} for {{Second Level Buffer Caches}}},
  booktitle = {Proceedings of the {{General Track}}: 2001 {{USENIX Annual Technical Conference}}},
  author = {Zhou, Yuanyuan and Philbin, James and Li, Kai},
  year = {2001},
  month = jun,
  pages = {91--104},
  publisher = {{USENIX Association}},
  address = {{USA}},
  file = {/Users/jonathanrainer/Zotero/storage/PVY2Q3VI/zhou.pdf},
  isbn = {978-1-880446-09-6}
}

@inproceedings{zhuangHardwarebasedCachePollution2003,
  title = {A Hardware-Based Cache Pollution Filtering Mechanism for Aggressive Prefetches},
  booktitle = {2003 {{International Conference}} on {{Parallel Processing}}, 2003. {{Proceedings}}.},
  author = {Zhuang, X. and Lee, H.-H.S.},
  year = {2003},
  month = oct,
  pages = {286--293},
  issn = {0190-3918},
  doi = {10.1109/ICPP.2003.1240591},
  abstract = {Aggressive hardware-based and software-based prefetch algorithms for hiding memory access latencies were proposed to bridge the gap of the expanding speed disparity between processors and memory subsystems. As smaller L1 caches prevail in deep submicron processor designs in order to maintain short cache access cycles, cache pollution caused by ineffective prefetches is becoming a major challenge. When too aggressive prefetching are applied, ineffective prefetches not only can offset the benefits of benign prefetches due to pollution but also throttle bus bandwidth, leading to overall performance degradation. A hardware based cache pollution filtering mechanism is proposed to differentiate good and bad prefetches dynamically using a history table. Two schemes-peraddress (PA) based and program counter (PC) based-for triggering prefetches are proposed and evaluated. Our cache pollution filters work in tandem with both hardware and software prefetchers. As shown in the analysis of our simulated results, the cache pollution filters can significantly reduce the number of ineffective prefetches by over 90\%, alleviating the excessive memory bandwidth induced by them. The IPC is improved by up to 9\% as a result of reduced cache pollution and less competition for the limited number of cache ports},
  file = {/Users/jonathanrainer/Zotero/storage/S42F45C9/Zhuang and Lee - 2003 - A hardware-based cache pollution filtering mechani.pdf;/Users/jonathanrainer/Zotero/storage/XTM9RY6F/1240591.html},
  keywords = {aggressive prefetch,Bandwidth,Bridges,cache storage,Degradation,Delay,Filtering,Filters,Hardware,hardware based cache pollution filtering mechanism,instruction sets,memory access latency,memory bandwidth,memory processor,memory subsystem,peraddress scheme,Pollution,Prefetching,Process design,program counter scheme,storage allocation,submicron processor design}
}

@inproceedings{zhuSurveyComputerSystem,
  title = {A {{Survey}} on {{Computer System Memory Management}}},
  booktitle = {{{WorldComp}} 2012 {{Proceedings}}},
  author = {Zhu, Qi},
  pages = {7},
  address = {{Las Vegas, Nevada}},
  abstract = {Computer memory is central to the operation of a modern computer system; it stores data or program instructions on a temporary or permanent basis for use in a computer. In this paper, various memory management and optimization techniques to improve computer performance are reviewed, such as the hardware design of the memory organization, the memory management algorithms and optimization techniques, and some hardware and software memory optimization techniques.},
  file = {/Users/jonathanrainer/Zotero/storage/ZI42W2KX/Zhu - A Survey on Computer System Memory Management.pdf},
  language = {en}
}


