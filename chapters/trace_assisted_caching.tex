\chapter{Trace Assisted Caching}
\label{chap:trace-assisted-caching}

Having now considered the literature we move on to a theoretical explanation of the new technique this thesis proposes. We will start by exploring the motivation for this new technique; give a theoretical, processor agnostic, description of how to works; and follow that with a section that explains how this technique will bring benefit to hardware that implements this system.

\section{Motivation}

If we return to the central question of this thesis, we are trying to understand how memory latency can be reduced to reduce overall program runtime. However the results of the literature review in the previous chapter appear to show that we are blocked along all the avenues considered as a route to this goal. For example, cache policies definitely show some ability to reduce latency by making smarter decisions about which elements should be replaced and when that should happen. However there is an upper limit on their ability to reduce latency, namely the \texttt{OPT} ceiling, and much of this gap has been closed with the research we've seen. As a result more work in this area is going to follow the law of diminishing returns, leading a lot of researchers to abandon work on cache policies because they are 'good enough` \cite{podlipnigSurveyWebCache2003}. Moreover, \texttt{OPT} cannot be implemented because perfect clairvoyance is impossible in general and even if it could be the effectiveness of \texttt{OPT} is still not 100\%. So even if we could implement a version of \texttt{OPT} that worked there are serious questions as to whether we'd want to or whether there were more effective ways to reduce latency in other areas.

\subsection{Defining the Key Problems}

If we look at the other key areas of the literature we see the same story over and over again. Either the economics of the decision don't balance, i.e. we need a very large hardware budget to achieve a comparatively small drop in latency; or techniques don't have enough information available to them to make good decisions. So if we boil down these problems what is the root cause? I believe it stems from two key problems, the first is that these techniques are making decisions based upon limited information. In the case of caches for example, they can only react to information once a miss has happened, and the only option they have is to perform a costly memory access. With more information they could react earlier and have more scope to perform other actions, not just performing a memory access. Even with pre-fetching, which is more pro-active, the problem recurs. Simpler pre-fetch schemes like \gls{obl} or stride are similarly limited in the information they have, and even in more advanced schemes they lack the knowledge of dynamic information in the program, like effective addresses, to make very high quality decisions.

The second key problem is an assumption that is made on the part of computer architects is a belief in the synchronicity of the memory and computation parts of a computer. This leads to systems that exhibit limited potential for memory/computation parallelism because in their model of execution all instructions take the same amount of time to execute which is simply not the case. Some work, particularly in the realm of pre-fetching has tried to address this \cite{veidenbaumDecoupledAccessDRAM1997, vanderwielCompilerassistedDataPrefetch1999, mutluRunaheadExecutionAlternative2003}, by proposing systems whereby the memory system can work independently of the computation system. However even these papers do not go as far as they could in pursuing this independence because of the problem of a lack of information. In particular, if you cannot predict the effective addresses of memory accesses you are forced to incur lots of stalls in your memory execution, while you wait for non-obvious effective addresses to be calculated. This will increase the level of synchronicity between memory and computation which you are trying to avoid. Otherwise, you accept that you are predicting these memory accesses and have to allow for the fact that you may get that wrong and have to roll-back any changes made. 

\subsection{Tracing: A Solution}

These two problems together form the block that stops most of the techniques we've seen being more effective. A lack of information and the assumption of synchronicity place a restraint on the level of pre-emptive action that techniques can undertake. But how might these problems be solved? What if we assumed a certain level of asynchronicity between memory and computation systems? What information would we need to not hit the problems that previous approaches have? The key is access to the effective memory addresses before the memory operation has started. If this information were available it would be immediately obvious to a memory system when memory instructions could be re-ordered, because there would be no doubt as to whether two instructions referenced the same location in memory Ã  la \ref{fig:static-analysis-problem}. It would also be possible to pre-fetch much more effectively because the pre-fetcher would know far ahead of time what data was required and could plan accordingly. 

% % List out the objections to using dynamic information, that it doesn't exist, predicting it is very difficult and storing it all would make processors very slow.

So we know what data we need but how might we access it? A central objection comes to fore already, which says that information like effective addresses simply do not exist before they are required, so they are already available as early as they could be. Not only that but 


\subsection{A Vision of our Destination}

\section{A Schematic Design}

\subsection{Trace Recorder}

\subsection{Intelligent Cache \& Memory System}

\section{Justification of Success}