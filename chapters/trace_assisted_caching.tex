\chapter{Trace Assisted Caching}
\label{chap:trace-assisted-caching}

Having now considered the literature we move on to a theoretical explanation of the new technique this thesis proposes. We will start by exploring the motivation for this new technique; give a theoretical, processor agnostic, description of how to works; and follow that with a section that explains how this technique will bring benefit to hardware that implements this system.

\section{Motivation}

If we return to the central question of this thesis, we are trying to understand how memory latency can be reduced to reduce overall program runtime. However the results of the literature review in the previous chapter appear to show that we are blocked along all the avenues considered as a route to this goal. For example, cache policies definitely show some ability to reduce latency by making smarter decisions about which elements should be replaced and when that should happen. However there is an upper limit on their ability to reduce latency, namely the \texttt{OPT} ceiling, and much of this gap has been closed with the research we've seen. As a result more work in this area is going to follow the law of diminishing returns, leading a lot of researchers to abandon work on cache policies because they are 'good enough` \cite{podlipnigSurveyWebCache2003}. Moreover, \texttt{OPT} cannot be implemented because perfect clairvoyance is impossible in general and even if it could be the effectiveness of \texttt{OPT} is still not 100\%. So even if we could implement a version of \texttt{OPT} that worked there are serious questions as to whether we'd want to or whether there were more effective ways to reduce latency in other areas.

\subsection{Defining the Key Problems}

If we look at the other key areas of the literature we see the same story over and over again. Either the economics of the decision don't balance, i.e. we need a very large hardware budget to achieve a comparatively small drop in latency; or techniques don't have enough information available to them to make good decisions. So if we boil down these problems what is the root cause? I believe it stems from two key problems, the first is that these techniques are making decisions based upon limited information. In the case of caches for example, they can only react to information once a miss has happened, and the only option they have is to perform a costly memory access. With more information they could react earlier and have more scope to perform other actions, not just performing a memory access. Even with pre-fetching, which is more pro-active, the problem recurs. Simpler pre-fetch schemes like \gls{obl} or stride are similarly limited in the information they have, and even in more advanced schemes they lack the knowledge of dynamic information in the program, like effective addresses, to make very high quality decisions.

The second key problem is an assumption that is made on the part of computer architects is a belief in the synchronicity of the memory and computation parts of a computer. This leads to systems that exhibit limited potential for memory/computation parallelism because in their model of execution all instructions take the same amount of time to execute which is simply not the case. Some work, particularly in the realm of pre-fetching has tried to address this \cite{veidenbaumDecoupledAccessDRAM1997, vanderwielCompilerassistedDataPrefetch1999, mutluRunaheadExecutionAlternative2003}, by proposing systems whereby the memory system can work independently of the computation system. However even these papers do not go as far as they could in pursuing this independence because of the problem of a lack of information. In particular, if you cannot predict the effective addresses of memory accesses you are forced to incur lots of stalls in your memory execution, while you wait for non-obvious effective addresses to be calculated. This will increase the level of synchronicity between memory and computation which you are trying to avoid. Otherwise, you accept that you are predicting these memory accesses and have to allow for the fact that you may get that wrong and have to roll-back any changes made. 

\subsection{Tracing: A Solution}

These two problems together form the block that stops most of the techniques we've seen being more effective. A lack of information and the assumption of synchronicity place a restraint on the level of pre-emptive action that techniques can undertake. But how might these problems be solved? What if we assumed a certain level of asynchronicity between memory and computation systems? What information would we need to not hit the problems that previous approaches have? The key is access to the effective memory addresses before the memory operation has started. If this information were available it would be immediately obvious to a memory system when memory instructions could be re-ordered, because there would be no doubt as to whether two instructions referenced the same location in memory. It would also be possible to pre-fetch much more effectively because the pre-fetcher would know far ahead of time what data was required and could plan accordingly. 

So we know what data we need but how might we access it? A central objection comes to fore already, which says that information like effective addresses simply do not exist before they are required, so they are already available as early as they could be. Consequently we are going need some way to record these effective addresses, in an efficient way, and then make those recordings available to the processor as it progresses through the memory instructions present in the program. These `recordings' would be complete program traces, these would record the exact set of steps taken by the program through its execution and would consequently contain the vital link between instructions executed by the processor and the effective addresses they targeted. But this is only one part of what would be required, we will also need to add more intelligence to the L1 cache such that it can be responsible for querying these new traces and pro-actively responding to their contents if it's possible to do so. This would allow the cache and memory system to run-ahead of the processor, up to a point, executing high latency memory instructions much before they are required. This process is trace-assisted caching, using tracing to allow the cache to run-ahead of the processor to bring forwards memory operations, reducing run-time by increasing the overlap of high and low latency instructions.

\section{A Schematic Design}

Now we have seen a theoretical description of how trace assisted caching works let us consider a schematic design for the system. Chapter \ref{chap:experimental-design} will focus on the implementation but let us consider, at a conceptual level, the key pieces of an implementation and their function before being bogged down in the exact details of how a system like this is implemented. So first let us imagine a single-core \gls{cpu} with a Harvard Architecture, with an L1 data cache. This cache can be direct-mapped or set-associative and implements a replacement policy of some kind. This can be seen in the diagram below:

%TODO Add Diagram of Architecture before any changes

As was alluded to in the previous section there are two key pieces that form the architecture for Trace Assisted Caching: the trace recorder and the intelligent cache. Each will be described in turn. We'll also describe any changes that would need to be made to the processor to support these new pieces of hardware.

\subsection{Trace Recorder}

The trace recorder would sit attached to the processor and would monitor the internal control signals emitted by the processor. These control signals would not only be those that manipulated memory but would initially be every pipeline phase of every instruction the processor executed. This would require changes to the processor, exposing previously enclosed signals, but because we have to perform this recording asynchronously from the processor, so as not to block its execution unnecessarily, this is the only option available to us.

%TODO Add Diagram of Architecture Plus Trace Recorder

The amount of data a trace like this would generate is potentially gargantuan, multiplying the amount which already exists in instruction traces by the length of the pipeline, but there are several reasons why this level of detail is required. The first is that the goal of this recording is to link together instructions in the program with effective addresses they generate. If you record at a more granular level, it can be very difficult to associate memory accesses with the instructions that generated them. Recording at the level of pipeline stages and aggregating to instructions later means there is no ambiguity. This does also mean that we have to take into account branching behaviour and stalls in our recorder, and this will be explored further in Chapter \ref{chap:experimental-design}, but this would give us a complete trace, at the level of pipeline stages, for any program that runs on the processor. In addition tracing at the level of pipeline stages means that you are recording a serialised data stream, meaning there is no need to do any de-multiplexing or complex analysis to associate actions with instructions, at the level of pipeline stages there can only be one instruction per stage at a time. 

Now we have this potentially gargantuan amount of data we have to filter it down in some way. Even for a small processor running a short program this could lead to gigabytes of data being generated and this is simply not viable to store or query. The first thing we can do is to throw away the details of each individual pipeline stage, these are not needed to instruct the memory system directly, we simply use them as a way to ensure the correct ordering of the memory instructions. The second piece of filtering is to remove any memory instructions that do not have memory implications, this could be as much as 60\% of the instructions captured. By filtering and aggregating using these two criteria we can produce a trace that is an ordered list of program counter values linked to effective memory addresses. An example of which is shown below:

%TODO Show example trace

We now need to turn our attention to a second problem which is how should this be stored? We cannot use up data memory for this because it could affect the retrieval of actual program data, so we need a separate memory store to keep these traces accessible to the intelligent cache. This takes the form of a trace repository that is stored in memory so it can be queried by the cache on each clock cycle when the program is run for a second time. It's also possible to seed this trace repository with data from a previous run if you didn't want to have to run the program once in a `training phase'. Issues such as where this repository is to be stored, how it might be fetched from efficiently and how it might be built up iteratively over multiple runs will be dealt with in future sections. 

So if we consider an execution of a program on a processor that is architected this way. The program would execute as normal, though the cache itself would by bypassed during the recording phase, this is so that we are recording the `worst case' behaviour, rather than that caused by the cache. While the \gls{cpu} executes the control signals it generates are tracked and recorded by the trace recorder. Filtered as the recording occurs and sent to the trace repository. Once the program reaches its end, as marked by it entering the processor's trap state, the trace recorder will stop recording and the data will held in the repository is then available for the second and further subsequent runs of the program. However storing all this data is of no use if it cannot provide any benefit so on a subsequent run of 

\subsection{Intelligent Cache \& Memory System}

\section{Justification of Success}