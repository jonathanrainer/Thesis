\chapter{Trace Assisted Caching}
\label{chap:trace-assisted-caching}

Having now considered the literature we move on to a theoretical explanation of Trace Assisted Caching, the new technique this thesis proposes. We will start by exploring the motivation for this new technique; follow that with a theoretical, processor agnostic, description of how it works; and then conclude with a section that explains how this technique will bring benefit to hardware that implements this system.

\section{Motivation}

If we return to the central question of this thesis, we are trying to understand how memory latency can be reduced to reduce overall program runtime. However the results of the literature review in the previous chapter appear to show that we are blocked along all the routes we've considered. The naive method of simply engineering a faster, cheaper memory type is never going to be economical. Cache policies definitely show some potential, however there is an upper limit on their ability to reduce latency, which the performance ceiling provided by \texttt{OPT}. Some of the gap between this ceiling and the state of the art has been closed with the research we've seen, but as a result more work in this area is going to follow the law of diminishing returns. The low hanging fruit has already been picked, leading a lot of researchers to abandon work on cache policies because they are now 'good enough` \cite{podlipnigSurveyWebCache2003} and the extra effort expended is not commensurate with the gains. To add to that we can't simply implement \texttt{OPT} efficiently because it relies on clairvoyance to achieve its high performance, but even if we could there is a question as to whether we should. \texttt{OPT}, whilst the most effective cache policy from the point of view of miss rate, does not boast a 100\% hit rate, because it is essentially reactionary. All the cache policies we've considered up to this point are in this vein, they have no way of taking pre-emptive action to ensure a higher hit rate. All these reasons necessitate us looking beyond cache policies if we want to reduce runtimes in a non-piecemeal fashion
                                                                                                                                                                                                                                                                                                                                                                                                                                           
\subsection{Defining the Key Problems}                                                                                                                                                                                                                                                                                                                                                                                                     
                                                                                                                                                                                                                                                                                                                                                                                                                                           
If we look at the other key areas of the literature we see the same story repeatedly. So where does the root cause lie? It stems from two key problems: the first is that techniques are making decisions based upon limited information and have a limited choice of reactions when a point of decisions arises. In the case of caches for example, they can only react to information once a miss has happened, and the only option they have is to perform a costly memory access. With more information they could react earlier and have more scope to perform other actions, that when used judicially should lead to lower program runtimes. Even with pre-fetching, which is more pro-active, the problem recurs. Simpler pre-fetch schemes like \gls{obl} or stride are limited in the information they have, and even in more advanced schemes they lack the knowledge dynamic information from the program itself, like effective addresses, to make very high quality decisions.

The second key problem is an assumption that is made by computer architects that whilst true in the past is no longer the case. Namely that memory operations and computational operations take a similar length of time. This assumption is what gives us pipeline stalls and the need for caching and memory hierarchies in the first place and leads to a co-assumption that because they take the same length of time they should operate synchronously. Some work, particularly in the realm of pre-fetching has tried to address this \cite{veidenbaumDecoupledAccessDRAM1997, vanderwielCompilerassistedDataPrefetch1999, mutluRunaheadExecutionAlternative2003}, by proposing systems whereby the memory system can work independently of the computation system. However even these papers do not go as far as they could in, because of a lack of information. In particular, if you cannot predict effective addresses, you are forced to incur lots of stalls in your memory execution, while you wait for non-obvious effective addresses to be calculated. This will increase the level of synchronicity between memory and computation which you are trying to avoid. Otherwise, you accept that you are predicting these memory accesses and have to allow for for incorrect predictions and roll-backs. These two points together form the crux of Trace-Assisted Caching. If we had more information and tried to step away from the synchronous, one-size-fits all instruction model that we are so often presented with, how much performance benefit could be gained? But where might this information come from? The answer lies in full system tracing. 

\subsection{Tracing: A Solution}

These two problems together form the block that stops most of the techniques we've seen being more effective. A lack of information and the assumption of synchronicity place a restraint on the level of pre-emptive action that techniques can undertake. But how might these problems be solved? What if we assumed a certain level of asynchronicity between memory and computation systems? What information would we need to not hit the problems that previous approaches have? The key is access to the effective memory addresses before the memory operation has started. It would then be possible to pre-fetch much more effectively because the pre-fetcher would know far ahead of time what data was required and could plan accordingly. It might also be possible to use this information to give insight to a memory system when memory instructions could be re-ordered, because there would be no doubt as to whether two instructions referenced the same location in memory. That is not covered explicitly in this thesis due to the complexity of an implementation but some thoughts are expressed as how this might be integrated in Chapter \ref{chap:conclusion}.

So we know what data we need but how might we access it? A central objection comes to fore already, which says that information like effective addresses simply do not exist before they are required, so they are already available as early as they could be. Consequently we are going need some way to record these effective addresses, in an efficient way, and then make those recordings available to the processor as it progresses through the memory instructions present in the program. These `recordings' would be complete program traces, these would record the exact set of steps taken by the program through its execution and would consequently contain the vital link between instructions executed by the processor and the effective addresses they targeted. But this is only one part of what would be required, we will also need to add more intelligence to the L1 cache such that it can be responsible for querying these new traces and pro-actively responding to their contents if it's possible to do so. This would allow the cache and memory system to run-ahead of the processor, up to a point, executing high latency memory instructions much before they are required. This process is trace-assisted caching, using tracing to allow the cache to run-ahead of the processor to bring forwards memory operations, reducing run-time by increasing the overlap of high and low latency instructions.

\section{A Schematic Design}

Now we have seen a theoretical description of how trace assisted caching works let us consider a schematic design for the system. Chapter \ref{chap:experimental-design} will focus on the implementation but let us consider how the key pieces this scheme would work before continuing to the actual implementation. So first let us imagine a single-core \gls{cpu} with a Harvard Architecture, with an L1 data cache. This cache can be direct-mapped or set-associative and implements a replacement policy of some kind. This can be seen in Figure \ref{fig:base-architecture}

\begin{figure}[htbp]
	\input{diagrams/trace_assisted_caching/base_architecture.tex}
	\caption[Basic Processor Architecture]{The basic design begins with a processor with a Harvard Architecture. This is so that the effect of any caching on the data side can be isolated and quantified more easily.}
	\label{fig:base-architecture}
\end{figure}

As was alluded to in the previous section there are two key pieces that form the architecture for Trace Assisted Caching: the trace recorder and the intelligent cache. Each will be described in turn. We'll also describe any changes that would need to be made to the processor to support these new pieces of hardware.

\subsection{Trace Recorder}

The trace recorder would sit attached to the processor and would monitor the internal control signals emitted by the processor. These control signals would not only be those that manipulated memory but would initially be every pipeline phase of every instruction the processor executed. This would require changes to the processor, externalising previously internal signals to allow us to track the execution of instructions, a diagram of this can be seen in Figure \ref{fig:base-architecture-with-recorder}. It would be simpler to rely perhaps on dedicated, pre-existing tracing hardware such as CoreSight but this doesn't provide the level of introspection to track every pipeline stage. Further we do not want this system to introduce a performance penalty to the running system itself. Therefore anything that means the processor has to perform more actions than it would on a normal execution, like outputting to a trace port or adding extra instructions must be avoided. 


\begin{figure}[htbp]
	\input{diagrams/trace_assisted_caching/base_architecture_with_recorder.tex}
	\caption[Basic Architecture with Trace Recorder]{The trace recorder takes in information from both the instruction stream and the data stream to produce a stream of pipeline stages bundled into instructions. This is stored in the trace repository after filtering to remove all non-memory instructions.}
	\label{fig:base-architecture-with-recorder}
\end{figure}

The amount of data a trace like this would generate is potentially gargantuan, multiplying the amount which already exists in instruction traces by the length of the pipeline, but there are several reasons why this level of detail is required. The first is that the goal to link together instructions in the program with the effective addresses they generate. If you record at a coarser granularity, it can be very difficult to associate memory accesses with the instructions that generated them. This is particularly true in very complex programs unless you implement an \gls{iss} that can be co-simulated with the actual processor. Recording at the level of pipeline stages and aggregating to instructions later means there is no ambiguity. This also means that we have to take into account branching behaviour and stalls in our recorder, and this will be explored further in Chapter \ref{chap:experimental-design}. This would give us a complete trace, at the level of pipeline stages, for any program that runs on the processor. In addition tracing at the level of pipeline stages means that you are recording a serialised data stream, so there is no need to do any de-multiplexing or complex analysis to associate memory operations with instructions, at the level of pipeline stages there can only be one instruction per stage at a time. 

Now we have this data it's imperative that we filter it down in some way. Even for a small processor running a short program gigabytes of data could be generated and this is simply not viable to store or query efficiently, especially in the context of an embedded processor. The first thing we can do is to throw away the details of each individual pipeline stage, these are not needed to instruct the memory system directly, we simply use them as a way to ensure the correct ordering of the memory instructions and the correct association of effective addresses. The second piece of filtering is to remove any instructions that do not have memory implications, this could be as much as 60\% of the instructions captured. We are fortunate when using a processor like the \texttt{RI5CY} that only \texttt{LOAD} and \texttt{STORE} can access memory and these are easily identified by their opcodes, but in other architectures this may not be the case. By filtering and aggregating using these two criteria we can produce a trace that is an ordered list of instructions linked to effective memory addresses. An example of which is shown in Figure \ref{fig:base-architecture}.

\begin{figure}[htbp]
	\input{diagrams/trace_assisted_caching/trace_with_original.tex}
	\caption[Program Fragment with Trace]{On the left can be seen a fragment of the disassembled version of the benchmark \texttt{fac.c} which is part of the MÃ¤lardalen Benchmark Suite \cite{gustafssonMalardalenWCETBenchmarks2010}. On the right can be seen the trace that relates to this part of the file, with effective addresses next to each memory instruction. The colours indicate which trace element maps to each instruction in the disassembled file and it can be seen that the trace follows the execution of the program, following jumps and branches as they arise.}
	\label{fig:trace-with-original}
\end{figure}

We now need to turn our attention to a second problem, how should this data be stored? We cannot use up the same data memory as the running processor, because it could affect the retrieval of actual program data, so we need a separate memory store to keep these traces accessible to the cache. This takes the form of a trace repository that is stored in a physically separate memory so it can be queried by the cache on each clock cycle when the program is run for a second time. It's also possible to seed this trace repository with data from a previous run if you didn't want to have to run the program once in a `training phase'. Issues such as where this repository is to be stored, how it might be fetched from efficiently and how it might be built up iteratively over multiple runs will be dealt with in future sections. 

So if we consider an execution of a program on a processor that is architected this way. The program would execute as normal, though the cache itself would by bypassed during the recording phase, this is so that we are recording the `worst case' behaviour, rather than that caused by the cache. While the \gls{cpu} executes, the control signals it generates are tracked and recorded by the trace recorder, filtered as the recording occurs, and sent to the trace repository. Once the program reaches its end, as marked by it entering the processor's trap state, the trace recorder will stop recording and the data will held in the repository is then available for the second and further runs of the program.

\subsection{Intelligent Cache \& Memory System}

Now that we have a highly filtered version of the trace stored we can start to use it to allow the cache and memory system to work asynchronously from the processor. This works in the following way: on every clock cycle instead of sitting waiting for the processor to issue a command to it the cache polls the trace repository. The trace repository then returns to the cache the next trace in the stream that it had recorded in the previous run of the program. Now the cache has potentially two requests it could service, a pre-emptive request or an on demand request. In all cases the on-demand request would take priority because we don't want to introduce further latency by delaying a request from the processor. If we did allow this it might be possible for the program to exhibit behaviour worse than the training run and this is something we wish to avoid at all costs.

If there is no on-demand request the cache seeks to service the pre-emptive and there are four scenarios that could occur, depending on the content of the instruction to be executed, these 4 scenarios are illustrated in Figure \ref{fig:scenarios}.

\begin{enumerate}[label=\textbf{Scenario \arabic*:}, align=left, leftmargin=*,labelindent=16pt]
	\item If the pre-emptive instruction is a load and there is space within the cache then simply execute the instruction.
	\item If the pre-emptive instruction is a store and there is space in the cache then you cannot execute it because you don't know what data is to be stored there. However you can reserve the space in the cache to make sure it isn't overwritten by another action.
	\item If it's a load but there is no space in the cache then perform a write-back and then do the same as in Scenario 1.
	\item If it's a store and there's no space again perform the write-back and then reserve the space.
\end{enumerate}

\tikzstyle{cell} = [rectangle,draw=black, fill=green!30]
\tikzstyle{space} = [minimum height=1.5em,matrix of nodes,row sep=-\pgflinewidth,column sep=-\pgflinewidth,text depth=0.5ex,text height=2ex,nodes in empty cells]

\begin{figure}[htbp]
	\begin{center}
		\begin{subfigure}[b]{\textwidth}
			\begin{center}
				\input{diagrams/trace_assisted_caching/scenario_1.tex}
				\caption{Scenario 1 - A load where the cache has capacity, the load is simply executed by the cache and stored in the correct location.}
			\end{center}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\begin{center}
				\input{diagrams/trace_assisted_caching/scenario_2.tex}
				\caption{Scenario 2 - A store where the cache has capacity, the store cannot be actioned because the data to store is not known in advance but the cache element is reserved.}
			\end{center}
		\end{subfigure}
		\begin{subfigure}[b]{\textwidth}
			\begin{center}
				\input{diagrams/trace_assisted_caching/scenario_3.tex}
				\caption{Scenario 3 - If the cache doesn't have capacity for a load then perform the write-back to memory first and then act in accordance with Scenario 1}
			\end{center}
		\end{subfigure}
	\caption[Illustration of Pre-emptive Scenarios]{Illustrations of the four scenarios described. The R, O and D columns above the cache in each diagram stand for Reserved, Occupied and Data respectively. The bold text indicates elements that change throughout the scenario. The cache may also have other elements, such as validity bits, but these are omitted for the sake of clarity and are mostly implementation details. Scenario 4 is printed on the next page}
	\label{fig:scenarios}
	\end{center}
\end{figure}
\begin{figure}\ContinuedFloat
	\begin{center}
    	\begin{subfigure}[b]{\textwidth}
    		\begin{center}
    			\input{diagrams/trace_assisted_caching/scenario_4.tex}
    			\caption{Scenario 4 - If the cache doesn't have capacity for a store then again perform a write-back and proceed as Scenario 2}
    		\end{center}
    	\end{subfigure}
	\end{center}
\end{figure}

All this means that when the processor catches up to the pre-emptive action it will either, have the data already in the case of scenario 1 or 3 or will be guaranteed to not have to perform a write-back in the case of scenario's 2 or 4. As a result latency will decrease because all these operations will have been done ahead of time. 

There are some limitations to this approach of course, most of which relate to how far ahead of the processor the memory system can go. For one thing no attempt is made to re-order the memory operations in the way that an \gls{ooo} processor would. This is mostly to simplify the implementation and to avoid the complex problems of ensuring consistency over multiple interleavings of the instructions. As a result of this there are situations where no more pre-emptive actions can be taken until the processor has performed some critical action. For example should a processor issue pre-emptive instructions such that it tries to perform a \texttt{LOAD} to a location that is being reserved for a \texttt{STORE} then it will not take any action until the processor has completed the \texttt{STORE}. This is because the cache cannot know in advance what data will be part of the \texttt{STORE} operation, this is shown in Figure \ref{fig:problem-scenario}. In this situation the processor will keep polling to discover if the operation has happened and can then act accordingly. There is a potential for future work to consider continuing to execute independent instructions at this point but that was not considered in this work. 

\begin{figure} [htbp]
	\begin{center}
		\input{diagrams/trace_assisted_caching/problem_scenario.tex}
		\caption[Illustration of a Block on Pre-emptive Actions]{In this situation the latest trace entry is attempting a \texttt{LOAD} into a location that is reserved for a store. At this point the pre-emptive execution cannot continue because the \texttt{STORE} has not yet happened. The only option is for the pre-emptive instruction to poll the cache until the \texttt{STORE} has happened and then processing can continue.}
		\label{fig:problem-scenario}
	\end{center}
\end{figure}

A final point to address is that this scheme is agnostic to the implementation details of the cache that it works with. In this thesis both an 8-way associative and a direct-mapped cache were used but there is nothing special about these choices. It would be very easy to implement higher associativity caches or larger caches with very few changes to the trace-assisted mechanism. This also brings up the possibility of varying the replacement policy for the cache to see if that, working in concert with this system had any benefit but again that was not considered in this work. 

This is trace assisted caching, recording the traces of programs, filtering them and then allowing the cache to run-ahead of the processor, leveraging the trace information to pre-emptively perform memory operations. But how do we know this is going to be a success? And how do we know that we're not going to introduce performance regressions from this technique? The next section answers these questions.

\section{Justification of Success}

Now we understand how trace assisted caching works at a high level we need to dig a little deeper into how we expect to benefit from this system. It's potentially a high investment in extra hardware, as a trace recorder is required as well as the changes to the cache and the trace repository, so there needs to be a commensurate benefit in runtime reduction to justify the expense. The best way to explain where the benefit derives from is by example and this explained below.

\subsection{Performance Increase}

Let us imagine a processor with a 4-stage pipeline, namely Fetch (\texttt{IF}), Decode (\texttt{ID}), Execute (\texttt{EX}) and Write-back (\texttt{WB}). Let us further suppose that there are no data forwarding mechanisms and for the sake of argument let us assume that fetches from main memory take 5 clock cycles from the submission of the request to the data being available. Now if that processor is executing the listing fragment below:

\lstinputlisting[label={listing:pipeline-test},language=risc-v, style=printing, caption={This code simply executes a few computational instructions that have interdependencies.}, captionpos=b]{diagrams/trace_assisted_caching/justification.S}

Then we can consider what the pipeline might look like at each clock cycle, this can be seen in Figure \ref{fig:pipeline} starting from the execution of the \texttt{ADDI} line.

\begin{sidewaysfigure}
	\begin{center}
		\input{diagrams/trace_assisted_cache/pipeline.tex}
		\caption[Pipeline Diagram - No Trace Assisted Caching]{The pipeline execution continues as normal until clock cycle 4 where the \texttt{LW} instruction enters its Execution phase. Because loading a value from main memory takes 5 clock cycles this extra latency is added not only to the \texttt{LW} but also to the \texttt{MUL} and \texttt{MV} instructions. Overall this leads to a program runtime of 14 clock cycles when the optimal time, assuming no pipeline stalls would be 8. To further clarify the memory system row contains \texttt{PR} and \texttt{AV} these stand for Processing and Available resp.}
		\label{fig:pipeline}
	\end{center}
\end{sidewaysfigure}

We see in this diagram that the pipeline fills up to its maximum depth but then as soon as the load instruction reaches the \texttt{EX} phase the pipeline is forced to stall while the memory access is dealt with, not only adding latency to its own execution but also to all the other instructions blocked behind it. Using trace assisted caching however we enter into a new situation as in Figure \ref{fig:ideal-pipeline}.

\begin{figure}
	\begin{center}
		\input{diagrams/trace_assisted_cache/ideal_pipeline.tex}
		\caption[Pipeline Diagram - With Trace Assisted Caching]{In this example we see the power of trace assisted caching. If we assume the pre-emptive action to take is the one at line 3 in Listing \ref{listing:pipeline-test} then because the effective address can be known at clock cycle 0 the memory system can begin this load much earlier. As a consequence, the stalls associated with performing the memory operation are eliminated from the program's runtime reducing the overall runtime by 4 clock cycles. A decrease of nearly 30\%.}
		\label{fig:ideal-pipeline}
	\end{center}
\end{figure}


So we can see that because the processor has access to the effective address much earlier the cache miss can be pre-empted by the cache and dealt with much earlier, which leads to commensurate benefits throughout the whole program.In addition this technique is additive to any other technique that might be utilised as all it's doing is making information available to the cache earlier so it can make better decisions. So this is how performance is to be increased, by overlapping the execution of the long latency memory operations with others in the pipeline the latency can be more effectively hidden. But the next question is how can we ensure this technique is of net benefit to the system overall. There are many examples in this area of research of techniques that seem to be helpful actually introducing timing anomalies or adding undesirable behaviour in certain situations so how might that be avoided? This will be explored in the next section.

\subsection{Protections Against Performance Degradation}

We have seen in the previous section how we can effectively provide the cache with much more information than it has ever previously had. But as is well known in this area of research, how do we prevent unfortunate performance degredations from entering the program because of this technique? How can we be sure that this technique will not introduce a performance penalty in some cases for all that it gains us. Let us consider the three scenarios that might arise when we try to execute a pre-emptive instruction before the processor requires the data it references. There are three scenarios, detailed below:

\begin{enumerate}[label=\textbf{Situation \arabic*}, align=left, leftmargin=*,labelindent=16pt]
	\item The memory instruction is predicted far enough ahead of time such that the data is ready in the cache before the processor requests it. This is the situation depicted in Figure \ref{fig:ideal-pipeline}
	\item The memory instruction is predicted ahead of its execution but not far enough to be ready when the processor requests it. In this situation there will still be some benefit but the pipeline will still stall
	\item There is not time to perform the memory instruction ahead of it being needed so the processor reverts to the original behaviour. Here no benefit will be experienced.
\end{enumerate}

In all of these situations the effect on the length of time taken for the memory operation ranges from nothing, where it is as effective as it in the first run of the program, to a memory instruction that has virtually no latency. Because we are not attempting to re-order the memory operations we are not allowing the processor to overtake the pre-emptive memory system so the only thing it will experience is much faster memory accesses in a transparent way. There is of course the chance that no benefit is gained from any of these enhancements, which admittedly questions to the case for the economics of this technique, but at the very least this technique is not actively harmful to the program running on it in terms of its runtime. 

A further point that must be considered is what about programs that are badly designed with regards to this technique? This doesn't mean non-functional programs, but rather programs that repeatedly cause situations as in Figure \ref{fig:problem-scenario}, or programs that are heavily computational and so access memory very little. In these cases, we'll see the kinds of behaviour described in Scenario 3 repeatedly and the technique will have little to no impact on latency. This technique relies on exposing more potential for concurrent memory and processor execution but this has to be present in the first place in order to be exposed. If there is no scope to allow this interleaving in the first place, there is no improvement to be had. It is hoped that one of the outcomes of this research will be the ability to quantify exactly which types programs will benefit most from this technique so it can be appropriately targeted rather than wasting time trying to speed up fundamentally ill-suited programs. 

It would be remiss of us, when considering performance degradation, to not think about what happens in the case of a miss-prediction. An example of this might be if there are memory instructions that rely on data that must be input to the program at the start. Consequently it would be impossible to predict those addresses before the program starts and running the program repeatedly won't generate any useful patterns because the addresses are input dependent. In this work we do not deal explicitly with this problem, however we will address some of the ideas around resolving this in our section on Future Work in Chapter \ref{chap:conclusion}. For the purposes of this thesis we are going to assume that repeated runs of a program work with the same input data each time for the purposes of simplicity, however as we'll explore later this may be able to be relaxed in future work.

This chapter has described a schematic design for what a Trace-Assisted cache would look like and has considered some of the theoretical hurdles that will have to be considered in order to implement such a scheme. The next chapter deals with the implementation of this scheme in the context of an \gls{fpga} running a soft-core \texttt{RISC-V} processor, and also details the experiment that will be performed in order to measure the benefit this caching system can bestow upon certain programs. 