
\chapter{Literature Review}

This chapter presents a review of the literature surrounding memory latency reduction. It first focuses on more traditional methods, considering caching in its totality and then branching out to look beyond the cache to other methods of controlling latency, such as prefetching or scheduling. In the second section the focus moves to newer techniques that involve tracing and then finishes with a summary and evaluation of the position the literature presents.

To begin let us consider a standard model of a process interacting with a memory system and including a cache. This can be seen below:

%TODO: Include diagram of standard model

%TODO: Create diagram to illustrate second point

Our ultimate goal in this research project is to reduce the overall latency of memory transactions. So if we consider this standard model from that perspective and consider a memory transaction from the processor that misses in the cache where are the potential sources of latency? The processor itself provides negligible latency, there is of course usually a delay between the instruction being fetched and decoded, but this would be incurred by any instruction. Once the control signals for the memory transaction have been created they will enter the cache, here latency can certainly be added as there is a cache search to perform and potentially many other processes dependent upon the exact architecture of the cache. Assuming the request misses in the cache the request is then forwarded to the memory controller which also can induce latency, and then is forward the memory system itself which introduces the largest amount of latency. Consequently we have 3 sites that could be focussed on to reduce latency however in this review only the cache and the memory organisation subsystem will be considered, not the memory technology itself. The reason for this is that as \citet{pattersonComputerOrganizationDesign2018} describe, the gulf between processor speeds and memory access times is ever widening. So while it would be possible to consider many papers that describe new formulations of memory hardware this is of little benefit to us because no published work is proposing that they have bridged this gap entirely or have even closed it by a significant amount. Consequently any changes to memory technology are akin to a constant factor speedup for an algorithm as opposed to a change of complexity and thus the gulf between access times will always be there and will always need to be bridged. As a a result, though we may consider memory technology changes in the context of cache architectures wholesale changes in the architecture of main memory are not in scope for this project or this review.

\section{Cache Intrinsic Techniques}

\label{sec:cache}

After their first introduction to super-computers in the 1960's \cite{pattersonComputerOrganizationDesign2018} caches have become a standard part of almost any memory architecture. Iterative improvements, thanks to Moore's law, have pushed the performance of caches to higher and higher levels but this is not the whole story. Despite advances in technology it is still the case that the cache replacement policy is the most significant factor when determining how effective a cache will be at reducing memory latency \cite{hennessyComputerArchitectureQuantitative2019}.

\subsection{Cache Replacement Policy}
\label{sec:replacement_policy}

In it's simplest form a cache replacement policy is simply a way of deciding what happens when a cache gets full. For direct-mapped caches they are not important because there is only one location for a piece of data to inhabit. Consequently, if it is full there is no choice as to which element is replaced. However in set-associative caches the choice of replacement policy is absolutely crucial \cite{hennessyComputerArchitectureQuantitative2019} and as such there has been much research into which policies yield the best outcomes across a variety of metrics.

When discussing various policies that are useful in reducing latency it's good to have in mind two policies that are often referred to in the literature but are not actually implemented. The first of these is known as \texttt{OPT} \cite{jeongOptimalReplacementsCaches1999} a theoretical optimal replacement policy which, by some unspecified means, can perfectly predict which cache block will be needed furthest in the future. Consequently when the decision is made to replace a cache block it will make an optimal decision. Due to the requirement to have perfect fore-knowledge of memory requirements, some authors \cite{jeongOptimalReplacementsCaches1999, pandaSurveyReplacementStrategies2016} are of the belief that it is not possible to implement. We will see in future sections that this may be an incorrect assumption but for the purposes of this section \texttt{OPT} will be used as a comparison to measure the performance of other cache replacement polcies as compared to the theoretical gold standard.

The second theoretical replacement policy is that of random replacement or \texttt{RAND}. Under this scheme when a decision on replacement has to be made, the choice is made completely at random without reference to any other information \cite{beladyStudyReplacementAlgorithms1966}. Again this method is not usually implemented by cache designers when optimising for latency reduction \cite{karedlaCachingStrategiesImprove1994} but its utility is as the lower end of a continuum, bounded by \texttt{OPT} at the other end. So any cache replacement policy that is to be useful, has to justify the extra resources it will use by being better than \texttt{RAND} but will probably be worse than \texttt{OPT} because no policy can ever have perfect fore-knowledge.

\subsubsection{Arrival-Time Based Techniques}

One of the more simple cache replacement policies is to decide which cache line is to be replaced based on when the item entered the cache, or its arrival time. When implementing a technique like this you have two essential options: either you remove the most recently arrived cache line, or you remove the one that arrived furthest back in the past. The first of these two options is almost never used because it violates the principles of temporal locality, i.e. if something has been referenced recently it has a higher probabilty of being referenced in the immeadiate future, so the second is used. To implement this most designers choose a \gls{fifo} queue, sometimes referred to in the literature as a Round-Robin queue. A lot of implementers choose this technique because it has a very low hardware requirement \cite{pandaSurveyReplacementStrategies2016} which leads to a low cost, esepcially useful in embedded system design. On the other hand it's not highly performative \cite{al-zoubiPerformanceEvaluationCache2004, tsaoMultiFactorPagingExperiment1972}, often performing similarly to \texttt{RAND} despite the slight increase in hardware. Not only that but \gls{fifo} is susceptible to Belady's Anomaly \cite{beladyAnomalySpacetimeCharacteristics1969} so there's no guarantee that larger caches will produce lower miss rates. 

Due to its low performance research into using \gls{fifo} queues has been limited. Often \gls{fifo} is used a baseline to compare other policies to \cite{faresPerformanceEvaluationTraditional2012} rather than being implemented in its own right as it's simple to simulate. Some early development of topic include \citet{turnerSegmentedFIFOPage1981}'s work on \gls{sfifo} paritions main memory into two sections, the sizes of which are controlled by $P$ which sets the proportion of memory dedicated to the second section. This creates a pseduo-multi-level cache and achieves performance very close to \gls{lru} but at lower hardware cost. In the same vein \citet{devilleLowcostUsagebasedReplacement1990} augments \gls{fifo} with a usage counter per set to do the partitioning in a more granular way, this also yields performance comparable to \gls{lru}. In more recent times \citet{wei-chetsengPRRLowoverheadCache2012} have experimented with combining a \gls{fifo} policy with cache-line locking and this produces similar performance to a \gls{lru} policy but uses a lot less hardware to do so also. There are more complex policies that would do even better, but when these are not possible, \gls{fifo} is a viable option, especially when combined with other techniques to supplement its short-comings. All in all \gls{fifo} is a good baseline to build from, and a viable option to implement if resources are limited, however we can use more information to make better decisions if we consider frequency of access. 

\subsubsection{Frequency Based Techniques}

A slightly more sophisticated approach to cache replacement is to count the number of times a cache block or line has been accessed, and then to evict the one with the lowest frequency of access, this approach is known as \gls{lfu} . In terms of implementation the most common form is to turn the cache into a priority queue where keys are calculated according to a variety of different formula \cite{podlipnigSurveyWebCache2003}. In addition implementations choose between perfect \gls{lfu}, where every object is uniquely tracked across replacements, and in-cache \gls{lfu} where counts are only tracked when items are in the cache, this is by far the most common otpion \cite{podlipnigSurveyWebCache2003}.

Desipite it's simplicity in concept, \gls{lfu} has several problems that often make it unsuitable for many applications without modification. The first is the problem of cache pollution \cite{karedlaCachingStrategiesImprove1994} where a cache block has a high number of accesses very early on and then is never referenced again. This causes the block to remain in the cache much longer than it should usefully do, giving less space for other more necessary blocks. The second problem is that often you can end up with many different cache blocks having the same frequency count so you need some kind of tiebreaking arbitration \cite{podlipnigSurveyWebCache2003}. This then means you  have to allocate more resources to the cache replacement algorithm just to resolve ties. Moreover the hardware to keep track of all the frequency counts, potentially across multiple replacements, gives a very high hardware overhead and increased energy consumption, something many embedded systems cannot afford \cite{pandaSurveyReplacementStrategies2016}. 

Some have attempted however to address some of the shortcomings of \gls{lfu} with a variety of augmentations, most of which relate to adding ageing parameters to counter cache pollution. \citet{arlittEvaluatingContentManagement2000} suggest calculating the keys ($K_i$) in the priority queue that powers LFU with a formula $K_i = C_i * F_i + L$ where $C_i$ is the cost of bringing an object into the cache, $F_i$ is the frequency that LFU tracks and $L$ is equal to the $K_f$ where $f$ is the most recently evicted cache element. He dubs this policy, \gls{lfuda}. Others like \citet{robinsonDataCacheManagement1990} choose to age slightly differently by protecting new entries to the cache and also aging the entire cache by reducing all reference counts $C$ to $\ceil[\big]{\frac{C}{2}}$ whenever the average reference count exceeds a predefined maximum value. Both of these techniques produce results comparable than \gls{lru} ,which will be discussed in the next section, but the hardware cost is much higher due to the number of counters. In addition both of these approaches are concerned with the size of objects in the cache, something that is not a concern in this work due to the uniformity of CPU cache blocks.

A further augmentation of \gls{lfu} from \citet{kellyVariableQosShared1999} is to allow weighting parameters that come from the memory system to indicate how `useful' the caching of that element is. The problem, as \citeauthor{kellyVariableQosShared1999} admits, is the difficult of obtaining those weights and also the problem that this approach still requires the implementation of \gls{lru} as well to resolve ties. With that in mind, and with the marginal gains that \gls{lfu} can make over \gls{lru} it seems peverse to employ a system that requires hardware for both. A final interesting approach to frequency type statistics is from \citet{mekhielMultiLevelCacheMost29} who proposes a two level cache with the \gls{mfu} elements going in the equivalent of an L1 cache and the \gls{lfu} elements going in an L2 cache. This allows frequently accessed data to be easily available to \gls{cpu} and not easily evicted. This still suffers from the same cache pollution problems as other \gls{lfu} methods and still requires large amounts of hardware.

A slightly different approach is to take inspiration from probability theory as LFU-K \cite{sokolinskyLFUKEffectiveBuffer2004} does. Itself a development of the LRU-K algorithm, it uses probability theory to predcit the number of occurences of an element (page, cache block etc.) in a reference string. The development from \gls{lfu} is that it adds extra terms into the estimation formula to account for the changing probability of referencing an element as time goes on. In this work \citet{sokolinskyLFUKEffectiveBuffer2004} demonstrate that in terms of reducing cache miss rate, LFU-K outperforms \gls{lfu} and \gls{lru}. However the effectiveness of this technique is intrinsically linked to the estimation of two parameters $m$ and $h$ neither of which is a trivial task. In addition the hardware requirements are still large, larger than \gls{lfu} due to the extra costs in calculating the probability functions. 

\gls{lfu} is an improvement from simple \gls{fifo} policies but suffers from the problem of cache pollution and a very high hardware requirement, particularly in the perfect case due to the number of counters necessary \cite{podlipnigSurveyWebCache2003}. Considering recency rather than frequency has long been considered a better metric to approximate how far in the future a piece of data will be needed and so the next section covers techniques that consider recency rather than frequency. 

\subsubsection{Recency Based Techniques}

Recency, as a general class of algorithms orders the elements in a cache by the time they were last referenced. This leads to two very general categories of recency-based algorithms, \gls{mrre} and \gls{lrre}. \gls{mrre} algorithms are much less common than \gls{lrre} and in general are less performant due to their poor temporal locality \cite{pandaSurveyReplacementStrategies2016}, and as such will not be focussed on in this thesis. \gls{lrre} techniques in general attempt to evict the least recently referenced element in the cache, using this as a heurisitc to predict the element that will be needed furthest away in the future. The most popular \gls{lrre} algorithm is \gls{lru} \cite{pitkowSimpleRobustCaching1994, karedlaCachingStrategiesImprove1994, smithCacheMemories1982}, it makes use of temoral locality and, given a few simplifying assumptions, is very easy to implement. It also scores highly when one considers the tradeoff between resources and the resulting performance increase. Most often \gls{lru} is implemented with counters or as a square matrix, an example of which can be seen in the work of \citet{acklandSinglechip6billion16b2000}. 

However \gls{lru} is not without its problems. It performs very badly in a shared data environment, or when using virtual memory \cite{bansalCARClockAdaptive2004} and is also susceptible to cache-thrashing \cite{denningThrashingItsCauses1968}, when the working set of the program, defined by \citet{denningWorkingSetModel1968} as \textquote{the set of most recently referenced pages}, exceeds the size of the cache. This is especially problematic in large loops \cite{linPredictingLastTouchReferences2002}. There's also the problem of dead blocks, identified by \citet{liuCacheBurstsNew2008a}, this causes problems because a lot of blocks read into memory are never referenced again but take time to be labelled as \gls{lru} and so evicted. This ties up space that could be used by other elements, consequently dead-block prediction and identification is a large area of work in improving \gls{lru} based algorithms. Finally, as \citet{linPredictingLastTouchReferences2002} describe, \gls{lru} does not have the desirable property that as associativity in the cache increases the miss rate goes down, in fact they state the oppositie is often true, this is highly problematic as cache associativity and size will only increase as time goes on.

The development of \gls{lru} is split into two streams, the first is an attempt to approximate true \gls{lru} because it has been long been acknowledged that implementing complete \gls{lru} is very expensive because a total order must be provided over all the elements present \cite{soCacheOperationsMRU1988}. Work from \citet{corbatoPagingExperimentMultics1969} and \citet{eastonUseBitScanning1979} describe an approximation called \texttt{CLOCK} which uses a lot less hardware than true \gls{lru} solutions. Conceptually the elements are arranged as on a clock face and each have an associated use-bit, and there is also a global $k$-bit shift register. When a replacement is needed a cursor sweeps round the 'clock' looking for a replacement based on the contents of the use bits as they are shifted into the shift register. \citet{corbatoPagingExperimentMultics1969} makes the case that if $k=0$ this approximates \gls{fifo} that we saw earlier and as $k \rightarrow \infty$ the miss rate tends to match \gls{lru}, though this is qualified in \citet{eastonUseBitScanning1979} as there are a number of cases where this doesn't happen. 

\citet{soCacheOperationsMRU1988} catalogue several different approaches to approximating \gls{lru} using tree-based methods. The first of these is a relaxation the constraint that the cache must keep a total order of elements stored within it. They call this the \gls{plru} algorithm (though some other authors refer to this as \gls{plrut}), under their model \gls{lru} is modelled a stack that requires $\log_2(n!)$ bits to repesent all the configurations of $n$ cache lines. Under \gls{plru} this simplified so that cache lines are arrange into groups of two. This significantly reduces the number of bits required to represent this to $n-1$, and the upshot is that \gls{lru} ends up occuring within each of these subsets rather than at the global level of the cache set. This can cause issues because the net effect of approximating \gls{lru} like this is not exactly equivalent to \gls{lru} so there are performance variations. This is further simiplified by the same authors down to a single bit per cache set, that partitions the group in half into \gls{lru} elements at one end and \gls{mru} elements at the other (a good visual representation is providied in \cite{damienStudyDifferentCache2007}), the rationale being that the exact element to be replaced is not important as long as it's in the \gls{lru} part of the cache set. Both policies are good approximations of \gls{lru} at low levels of associativity but the simplifications made begin to bite as associativity increases and the 1-bit method performs as badly as random at the highest levels of associativity. \gls{plru} on the other hand tracks \gls{lru}'s performance well \cite{al-zoubiPerformanceEvaluationCache2004} and is widely used in data caches for this reason \cite{damienStudyDifferentCache2007}.

The idea of using tree-based methods to approximation \gls{lru} is picked up by \citet{ghasemzadehModifiedPseudoLRU2006} with two new approximating algorithms \gls{bplru} and \gls{mplru}. Under \gls{bplru} a tree that stores history bits is used that is updated on each cache hit so that an approximation of \gls{lru} can be performed at very low cost. However \gls{bplru} lacks the correct amount of hysteresis to not make mistakes when compared to true \gls{lru} because information is lost at the higher levels of the tree. This is addressed in \gls{mplru} where higher levels of the tree are classified into \gls{tbai} and \gls{mbai}. \gls{mbai} nodes save their previous history bit when it is updated allowing them more information to closely approximate \gls{lru}. The performance is very close to true \gls{lru} and signfiicantly better than \gls{fifo} or \texttt{RAND}, and though an overhead exists it's much less than a full \gls{lru} implementation making this a very tempting candidate when resources are scarce.

\citet{malamy1994methods} propose a very simple method of approximation using \gls{mru} bits in a cache along with a locking mechanism to save very frequently accessed data. Each element is assigned an \gls{mru} bit which is initially 0 and when a line is replaced the \gls{mru} bit is set to 1, when all lines are set to 1 all the bits are reset to 0 except the one that has just been inserted. This is shown visually in the work of \citet{damienStudyDifferentCache2007}. This method outperforms \gls{lru} for some data patterns and for others approximates the performance curve very well \cite{al-zoubiPerformanceEvaluationCache2004}, making this a very popular approximation due to its cheapness to implement and relative performance. The \gls{nru} algorithm, detailed in a technical report on the UltraSPARC Architecture \cite{UltraSPARCT2Supplement2007} takes a similar approach adding a \emph{used} bit $u$ and an \emph{allocated} bit $a$ per line. It then searches from a pre-defined pointer to find an element where $u$ and $a$ are both cleared to perform the replacement. This approximates \gls{lru} but dispenses with ordering so as to greatly simplify the number of bits necessary to do so.

\citet{ghasemzadehPseudoFIFOArchitectureLRU2005} takes a different approach and looks at the underlying \gls{lru} hardware rather than the process. In this work the reference implementation is taken as a matrix to represent the various \gls{lru} stack configurations, and this requires $\frac{n(n-1)}{2}$ bits where $n$ is level of associativity in the cache. Under the implementation presented by \citet{ghasemzadehPseudoFIFOArchitectureLRU2005} they reduce this to $n\log_2\left(\frac{n}{2}\right)$, which asymptotically grows less fast than the reference. They also show their technique consistently outperforms \gls{lru} on miss rate.

The second stream of the development of \gls{lru} is an attempt to extend the general idea of the policy to counteract its shortcomings and this takes several general forms. The first of these is to try and use statistical inference or measured history of accesses to divine the future behaviour of a program and act accordingly. This is the approach taken by \citet{oneilLRUKPageReplacement1993} where the \gls{lru-k} algorithm is described \footnote{The work of \citet{oneilLRUKPageReplacement1993} is cited as the inspiration for \citet{sokolinskyLFUKEffectiveBuffer2004}'s work we discussed earlier.}, using Bayesian methods to estimate the interarrival times of memory references from the collected set of past references. \citet{vakaliLRUbasedAlgorithmsWeb2000} continues this idea with \gls{hlru} which defines a function $hist(x,h)$ which returns the $h$th past reference to the cache object $x$, and then uses the maximum value of $hist$ among the cached objects to decide a replacement. \citet{wongModifiedLRUPolicies2000} dsecribes a suite of algorithms that are improvements of each other \gls{prl} and \gls{orl}. The essential idea of these algorithms is to favour lines that exhibit temporal behaviour by marking them as such using special \gls{isa} instructions. \gls{prl} does the identification offline using profiling but \gls{orl} does profiling online at run-time, keeping a table of hits to non-\gls{mru} lines and using this to set temporal bits. All of these policies show increased performance over \gls{lru} to varying degrees, but the problem with all of them is the extra hardware and book-keeping required. In addition none of these algorithms address some of the underlying flaws in \gls{lru} such as its suceptability to flooding \cite{glassAdaptivePageReplacement1997} as unless the history collection cover a whole loop the behaviour would not be picked up. The next set of approaches address these concerns.

Flooding is a phenomemnon particularly associated with \gls{lru} which happens when applications try to access a large address space in a sequential fashion. As more addresses are accessed and the cache capacity is exhausted, old elements (those at the beginning of the space) are evicted from the cache, and then when the loop begins again old elements are pushed out. This means that the cache policy has no positive impact on the execution at all. To address this \citet{glassAdaptivePageReplacement1997} proposes the \texttt{SEQ} algorithm which records long sequences of requests for sequential pages and applies \gls{mru} replacement to those sequences. Otherwise it defaults to simple \gls{lru}. This has a high overhead to implement compared to \gls{lru} which is even more of a problem as \gls{lru} is costly to implement anyway and there are more than just sequential accesses that can cause flooding. \citet{smaragdakisEELRUSimpleEffective1999} further develops these ideas to create the \gls{eelru} algorithm where the definition of 'sequential' is weakened to make the algorithm more ammenable to data structures that are not contiguous in memory. It does this via an adaptive feature where the behaviour of the algorithm changes between standard \gls{lru} and what \citeauthor{smaragdakisEELRUSimpleEffective1999} refers to as the \gls{wfl} algorithm, which can evict pages before they become the \gls{lru} page. \gls{eelru} merges these together by calculating probablistically whether \gls{wfl} will have more hits than measured from \gls{lru} over multiple sets of parameters for \gls{wfl}. A further approach to the problem of flooding is cache partitioning, as proposed by \citeauthor{kimLowoverheadHighperformanceUnified2000} in their \gls{ubm} technique. Here the cache is divided into three regions, a sequential region, a looping region and an other region. When a reference is requested it is classifed into one of the three categories and then different replacement algorithms are run on each region accordingly. All of these techniques are at least comparable to \gls{lru} and many perform at least as well as \gls{lru} in a variety of situations but they are not silver bullets. Several of them have very high hardware requirements, particlarly \citet{kimLowoverheadHighperformanceUnified2000} which is targetted as a software technique in its current presentation. This stems from the fact that analysis has to be performed online so a lot of state has to be stored as in all these cases the analysis takes into account history. Other authors suggest using static analysis to cut down this amount of state needed to be stored and that's what we'll consider next.

Static analysis and the use of compiler techniques to enhance caching shift augment compilers and programs with cache hints adding to the amount of information avaiable to a cache when a replacement decision needs to be made. \citet{jainSoftwareassistedCacheReplacement2001} tackles this by augmenting the \gls{isa} of a processor with \texttt{KILL}, \texttt{KEEP} and \texttt{COND-KILL} instructions that are used instead of normal \texttt{LOAD} and \texttt{STORE} instructions when a variable is considered dead. The \texttt{KILL} instruction is used for short-lived variables or ones that are only accessed once and \texttt{KEEP} for long-lived variables and this method shows increased hit rate of \gls{lru} over multiple levels of associativity. \citet{zhenlinwangUsingCompilerImprove2002} simplifies this further through the use of an evict-me bit which is set when a reference is accessed that is \textquote{sufficiently far away} or has no reuse, this is done by issuing a different instruction so the compiler is responsible for setting the evict-me bit. This technique also has enhanced performance over plain \texttt{LRU}. The problem with techniques of this kind are that they are difficult to implement for existing systems because they require changes to the \gls{isa}, which if you are using \gls{cots} components is impossible, but also because they have limited perception and so have a ceiling on their utility. A theme that will be returned to later is the inability of static techniques to comprehend dynamic behaviour, and that is exactly the case here. As a result, it may be the case that there are more references that could be marked for eviction if only the behaviour could be percieved. 

A consistent assumption of all the techniques seen so far is that cache misses all have a uniform cost to them. However research in the recent past has shown this simply not the case, as \citet{qureshiCaseMLPAwareCache2006} pithily puts it \textquote{not all misses are created equal}. Consequently several authors have attempted to integrate cost functions into \gls{lru} algorithms to improve performance so as to prioritise low-cost misses. \citet{jeongCostsensitiveCacheReplacement2003} is an early example of this, where each block is not only associated with a last reference time but also with a cost of replacement. This way the algorithm can make a choice between evicting the \gls{lru} element or evicting non-\gls{lru} element with a lower cost. Overall several iterations of the algorithm a 15-18\% increase on plain \gls{lru} is recorded with minimal extra hardware requirement. This approach is developed further in \citet{kharbutliLACSLocalityAwareCostSensitive2014a} with the introduction of the \gls{lacs} algorithm. Instead of using a 2-cost model, where a miss is cheap or expensive, this technique records latency when a miss happens for a particular element. These costs are incremented and decremented as other events occur in the cache as well such as a hit to an element or a different element not being accessed for a long period of time. This results in large drops in the miss rate but depends a lot on the working set size relative to the size of the cache. \citet{dasLatencyAwareBlock2017} takes a similar approach but uses latencies calculated from a \gls{noc} rather than a tradtional hardware arrangement. The big problem with these approaches is how the cost function is arrived at and what variables it considers. The examples previously presented use different criteria but the potential list of criteria is endless so it's difficult to distil all these competing pieces of information into the set that is most salient for reducing latency. Also none of these authors contend with the problem of \emph{calculating} the cost as a program is running \citet{jeongCostsensitiveCacheReplacement2003} uses information that can easily be cribbed from memory addresses and \citet{kharbutliLACSLocalityAwareCostSensitive2014a} uses simple counters.

One of the consistent problems of \gls{lru} is that in reality it's only a small subset of cache elements that actually get referenced after they are read into the cache \cite{qureshiAdaptiveInsertionPolicies2007}. This means that a lot of elements sit in \gls{lru} caches for a long time, simply taking up space that could be used by other elements. The root cause is that under vanilla \gls{lru} all elements are inserted in the \gls{mru} position (usually in a stack but sometimes in other data structures) and then progress to the \gls{lru} position over time. \citeauthor{qureshiAdaptiveInsertionPolicies2007} \cite{qureshiAdaptiveInsertionPolicies2007, qureshiSetDuelingControlledAdaptiveInsertion2008} asks whether changing this might lead to better cache utilisation and so proposes a suite of new insertion policies to achieve this. The eventual policy arrived at is known as \gls{dip} which combines standard \gls{lru} with a policy known as \gls{bip}, the algorithm switching between these two policies when one performs better than the other. In order to track the utility of switching a technique known as set-duelling is used, where part of the cache is dedicated to each policy and the miss-rate for each part is calculcated before it's decided which algorithm should be used. This closes the gap between \gls{lru} and \texttt{OPT} in the situations studied by two-thirds. \citet{sreedharanCacheReplacementPolicy2017} takes a similar approach but uses a reference count attached to each cache block. If it is high the insertions happen at the \gls{mru} position, otherwise it's \gls{lru}. \citet{guTheoryPotentialLRUMRU2011} takes a slightly different approach they call collabortive caching. Here the cache is split in half so you have \gls{mru} and \gls{lru} instructions so data can be inserted in different places. The problem with the last two solutions is very much around how you would gain the information to decided when to use each instruction on the fly. In \citet{sreedharanCacheReplacementPolicy2017, qureshiAdaptiveInsertionPolicies2007} there are mechanisms to decide however none of that exists in \citet{guTheoryPotentialLRUMRU2011} as it's a more theoretical paper. All these policies improve performance and some close the gap to \texttt{OPT} significantly but there are still times where caching policies like these will make mistakes due to the relative paucity of information they have so our final set of solutions addresses this problem through mistake correction.

\citet{kampeSelfCorrectingLRUReplacement2004} suggest that the wide gap between \texttt{OPT} and \gls{lru} is indicative of \gls{lru} making too many mistakes and so their policy of self-correcting \gls{lru} adds a feedback loop to the \gls{lru} policy and starts with the goal that no mistake should occur more than once. To do this they employ a shadow directory to track when blocks are evicted too early and a mistake history table to persist the information even after blocks are removed from the shadow cache. There's also an \gls{mru} victim cache that means blocks that bypass the cache and ones that are evicted from the \gls{mru} position so that mispredictions can be quickly recovered from. All these improvements together give a 24\% improvement in miss rates during the experiments made, but this is at the cost of quite a lot of extra hardware and book keeping to manage this extra state.

Having now toured many different manifestations of \gls{lru} policies we see a pattern emerging, which is that \gls{lru} when compared to \texttt{OPT} will always make mistakes and mispredictions so will never track \texttt{OPT}'s performance perfectly. There are many techniques to close the gap somewhat but no technique has done it flawlessly up to now. This implies that recency is not the only piece of information needed to make the best caching decisions and so the next set of policies we'll consider combine recency and frequency to close the information gap to \texttt{OPT} and attempt to match its performance.

\subsubsection{Methods Combining Recency and Frequency}

Having considered recency and frequency in isolation it makes sense to ask, can the two sources of information be usefully combined? Many authors have attempted to bridge this gap and their solutions fall into a few key categories. the first of these is to start from a basis of using \gls{lru} but to augment that with cache partitioning based on frequency counts. Cache partitioning involves logically subdividing the cache into multiple regions, where each region has a different probability of replacement. Consequently some elements become protected more so than they would under a vanilla \gls{lru} or \gls{lfu} policy. This is exactly the approach taken by \citet{robinsonDataCacheManagement1990} where the cache is partitioned into \texttt{new}, \texttt{middle} and \texttt{old}. Elements start in \texttt{new} when they are first referenced and slowly move towards \texttt{old} as the time since their last reference increases. When it's time for replacement the element with the lowest reference count in the \texttt{old} section is selected, where \gls{lru} is used to break ties. \citet{karedlaCachingStrategiesImprove1994} take a similar approach but only divide the cache into two section and abstract the frequency count to either 1 or more than 2. \citet{osawaGenerationalReplacementSchemes1997} meanwhile uses generational caching to split the cache into $N$ generations with cache elements moving towards generation $N$ on every hit. Also presented by \citeauthor{osawaGenerationalReplacementSchemes1997} is the addition of a small history list which means that if an entry is found there on insertion it can be inserted into generation 2 instead of 1 as it is more recent than something the cache has never seen. 

\citet{juanImprovedMulticoreShared2012} takes a similar approach to \citeauthor{osawaGenerationalReplacementSchemes1997} in using $N$ partitions of the cache but they apply it to \gls{cmp}s and so use the cache organisation to give each core a part of the cache, while allowing stealing between cores if that is of benefit. The problem with a lot of these schemes is whilst they are often very good, \citet{robinsonDataCacheManagement1990} boasts of closing 34\% of the gap between \texttt{OPT} and \gls{lru} for example, they rely on  \blockcquote{bansalCARClockAdaptive2004}{user-specificed magic parameters} to set the size of the generations for highest effect. Implementing these in reality would require a lot of performance tuning or simply guessing to get the correct size of generations or paritions because as \citeauthor{osawaGenerationalReplacementSchemes1997} points out, if you get this wrong lots of accesses in a short period mean elements can get protected when actually there may only be a short burst of accesses that require that element. As a result cache partitioning is often used as an auxillary tool to enhance other algorithms as opposed to being used on its own.

One of the more popular techniques to integrated recency and frequency is to introduce new structures into the cache to re-organise the data. These structures are mostly logical in nature, reorganising the cache from the perspecitve of the replacement algorithm but they can be very effective. The first of these is \texttt{2Q} \cite{johnson2QLowOverhead1994} which divides the cache into two queues known as $A_m$ and $A_1$. $A_1$ is further subdivided in two ${A_1}_{in}$ and ${A_1}_{out}$. The essential principle is to admit \blockcquote{johnson2QLowOverhead1994}{only hot pages to the main buffer [$A_m$]} so $A_1$ essentially acts as a filter so re-referenced pages can go into $A_m$. If a page is not rereferenced while in $A_1$ it is unlikely to be hot and so is evicted. This has a low overhead compared to LRU and boasts a moderate improvement (5-10\%). \citet{menaudImprovingEffectivenessWeb2000} uses a similar concept with counters to control the ordering of queues but is innappropriate for our purposes because it relies on a secondary process to differentiate 'hot' and 'cold' accesses which is infeasible in CPU caches as opposed to web caches.  \citet{megiddoARCSelfTuningLow2003} brings something new to the discussion with \texttt{ARC} a self-tuning algorithm that uses a cache directory to list out elements that have been accessed once recently ($L_1$) and twice or more recently ($L_2$). The algorithm then attempts to keep $p$ pages from $L_1$ and $c-p$ pages from $L_2$ in the cache where $c$ is the size of the cache and $p$ is altered as hits and misses occur on different elements. This was further advanced by \citet{bansalCARClockAdaptive2004} by combining the algorithm with \texttt{CLOCK}, turining $L_1$ and $L_2$ into clocks in order to remove some of the disadvantages of \gls{lru} not addressed by \texttt{ARC}. 

In work by \citet{liCRFPNovelAdaptive2008} only one queue is used as a cache directory and each cache block is associated with an $R$ and $F$ value to track recency and frequency respectively. This cache directory ($Q_{out}$) maintains two counters $O$ and $H$ and when an entry is found in $Q_{out}$ before insertion into the cache $H$ is incremented, $O$ is incremented otherwise. The ratio between these counters decide whether the $R$ or the $F$ value is used when a replacement is required. \citet{zhangDivideandconquerBubbleReplacement2009} takes the biggest departure, splitting each 'way' in an $n$-way cache into $k$ groups where elements bubble up these groups on a hit and removals are taken from the set of elements at the bottom of each of the groups. All of these policies achieve improvements over \gls{lru} and track closer to \texttt{OPT}, closing the gap by 47\% in some cases \cite{zhangDivideandconquerBubbleReplacement2009}. The problem comes from the extra hardware overhead they incur, most of these papers only speculate on the theoretical properties of their cache policies and not how they would actually be implemented with the exception of \texttt{ARC} and \texttt{CAR}/\texttt{CART}. In those two cases they are good candidates as some of the most applicable replacement policies so far seen, proving the intuition that lies behind this whole thesis, that providing more information to a process will allow it to make better decisions.

Rather than introducing new structures into their cache architecture some authors attempt to integrate frequency and recency together by defining a new objective function to use for replacement. In strict \gls{lru} the objective function can be formulated in natural language as ``which element has been accessed least recently?'' but some authors attempt to change that to make better replacement decisions. \citet{reddyIntelligentWebCaching1998} for example creates an objective function that applies a weighting to the contribution of recency and frequency, using a parameter $\alpha$ to control the weighting. \citeauthor{dongheeleeImplementationPerformanceEvaluation1997} \cite{dongheeleeLRFUSpectrumPolicies2001} and developed later by \citet{cuiNewHybridApproach2003} takes this a stage further and subsumes this all into a simple function controlled by a parameter $\lambda$ which is shown to subsume all weightings of frequency and recency. \citet{abdelfattahLeastRecentlyFive2012} takes a similar approach but calculates all the weights relative to every other element in the cache and then sums them using constants for weights, concluding that weighting frequency five times as much as recency gives best performance, this is further optimised by \citet{anandkumarHybridCacheReplacement2014}. \citet{dasArbitrationCacheReplacements2016} takes another similar approach but simply takes the product of frequency and recency allowing each to weight the other.

All the previous papers on objective functions exist as ways to weight the contribution to replacement of frequency and recency but other papers try to define entirely new metrics that move beyond this. \citet{tianEffectivenessbasedAdaptiveCache2014} considers effectiveness as \blockcquote{tianEffectivenessbasedAdaptiveCache2014}{the rate of re-use of the block over [a] future time period}, which is realised as $\dfrac{r*R_{count}}{f*E_{count}}$ where $r$ and $f$ are the recency and frequency weights, the $R_{count}$ is a count of re-references and $E_{count}$ is a count of how long has elapsed since the last re-reference. These new objective functions have a wide spectrum of success, from improvements of 9\% over \gls{lru} to outperforming \texttt{ARC} which itself outperforms \gls{lru} by a considerable factor. The problem with implementing many of them would be the explosion of counters and calculation units that would be required even in relatively simple cases with small caches. In addition to that the problem of magic parameters recurs, most of the proposed solutions rely on having a good sense of what the workload for the cache will look like a-priori which is simply impossible for a standard implementation, the adaptive examples go some way to alleviating this problem but will require more resources dependent on the exact method of adaptation used.

A final category of integrations of recency and frequency are algorithms that simply augment \gls{lru} with information about frequency, adding it as a second criterion by which to select a replacement candidate. This is certainly true of \citet{changLRUWWWProxy1999} uses a policy \texttt{LRU}* where cache hits increment a counter on each cache element. When replacements every item checked for replacement has its hit counter decreased by 1 and a replacement is only made when a counter hits 0 for a particular element.  This is further developed in \citet{alghazoSFLRUCacheReplacement2004} whereby for each replacement the \gls{lru} and second least-recently used are compared, with frequency counters used to decide if the \gls{lru} element should be saved. \citet{dybdahlLRUbasedReplacementAlgorithm2006} rounds out this set of enhacements by increasing the frequency differently for reads and writes and implementing cache bypassing for particularly high frequency counter values, to immeadiately promote elements to high levels of the cache hierarchy if necessary. In terms of utility these schemes have similar problems to redefining the objective function in that the proliferation of counters may make them a problem in CPU caches as opposed to simulations. The problem of `magical parameters' also persists with many of these algorithms having multiple tuning parameters that would need to be estimated prior to use.


\subsubsection{Beyond Recency and Frequency Combinations}

In recent years there have been attempts to move beyond collecting frequency and recency information to create a replacement policy. These fall roughly into three categories, the first being tracking new metrics and using those to inform the replacement policy, switching policies on the fly (based either on miss-rate or on oher metrics) and reorganising the cache. Taking new metrics first, the earliest example of this is the work of  \citet{rizzoReplacementPoliciesProxy2000} which calculates a value ($V$) for each element in the cache as $V = \dfrac{C}{B}*P_r$, where $C$ is the cost of retrieval, $B$ is the benefit of remvoal and $P_r$ is the probability of re-reference. Each of these values has other factors that feed into its calculation which allows the policy to shape itself around multiple factors very easily. In \citet{kharbutliCounterbasedCacheReplacement2005} two new metrics are proposed the \gls{aip} and \gls{lvp}. \gls{aip} increments a counter for a cache line whenever an access is made to another line in the same cache set. \gls{lvp} on the other hand counts the number of access to a line in a single generation (time from insertion to eviction from the cache). In either case once the counter reaches a particualr threshold the line is evicted. \citet{keramidasCacheReplacementBased2007} takes a slightly different approach and attempts to predict the \blockcquote{keramidasCacheReplacementBased2007}{reuse distance}, or the number of access to the L1 cache between address references, of particular addresses and then uses this prediction in tandem with LRU to overcome some of its shortcomings. All these techniques suffer from similar problems in that many of them rely on `magic parameters' which have to be set apriori and also that they have high hardware requirements, either for calculation of probabilities or to store data to be analysed, rendering them difficult to apply in an embedded context. The work of \citet{duongSCOREScoreBasedMemory2010} and to some extent \citet{tadaCacheReplacementPolicy2019} takes a slightly different approach in that they abstract specific metrics into an overall `score' for each cache element, selecting the lowest score each time to evict each time its required. Scores are set initially upon entry into the cache and are then altered as different events occur, with \citet{duongSCOREScoreBasedMemory2010} having tunable parameters for how events affect overall scores. These are much more applicable to an embedded context and shows very good gains over standard algorithms when given more information. In general these policies show that increasing the information available to your caching policy will increase its ability to make better decisions, something that we will return to in our analysis of the literature and ideas to move forward.

The second type of policy in this area is an attempt to take the best of all worlds in terms of all the `simpler' policies we've studied so far by changing the policy that the cache applies dynamically based upon observable conditions within the cache itself. The genesis of this is ACME \cite{ariACMEAdaptiveCaching2002, gramacyAdaptiveCachingRefetching2003, riaz-ud-dinAcmeDBAdaptiveCaching2006} which uses small virtual caches to test the effect of a suite of policies on the miss rate of the cache and then computes a set of weights that minimises the miss rate via machine learning. \citet{subramanianAdaptiveCachesEffective2006a} develops a similar idea but gives a more realistic implementation than the original presentation of ACME. This is some of the best of what cache policies can achieve, in that this approach sees reductions in miss rates across a wide variety of benchmarks and cache configurations. Not only that but since this approach is adaptible it requires no `magic parameters' as setup and will adapt to any program given enough time. A sub-variant of this idea is presented by \citet{jongmoochoiDesignImplementationPerformance2002} as well as other authors \cite{smaragdakisGeneralAdaptiveReplacement2004, aguilarGeneralAdaptiveCache2004, aguilarCoherenceReplacementProtocolWeb2006, changAdaptiveBufferCache2016, changPARCNovelOS2018} with the key difference between them being the metrics they choose to use to perform the adaptation, with \citeauthor{jongmoochoiDesignImplementationPerformance2002} using estimated forward distances, \citeauthor{changAdaptiveBufferCache2016} using classification of PC access into sequential and looping access and  \citeauthor{aguilarGeneralAdaptiveCache2004} using a variety of metrics that are all observable by the cache directly. All this put together gives some of the best results in this category of implementation but there are still problems, for one the virtual caches required or the extra metric tracking can take up a lot of extra hardware, particularly if a machine learning element is included, but also all of these policies are still under-performing if compared to \texttt{OPT} from \citet{beladyStudyReplacementAlgorithms1966}, there is still a signifcant gap that needs to be closed in order to really attack the latency that caches introduce and sadly despite their promise, these techniques still do not achieve that.

A third attempt to move beyond recency and frequency combinations is to re-organise the cache somehwat in light of information beyond recency and frequency. The first of these is \citet{chaudhuriPseudoLIFOFoundationNew2009a} who changes the insertion policy in order to make sure dead blocks are not kept in the cache longer than necessary while the working set of the cache remains untouched. \citet{manikantanNUcacheEfficientMulticore2011} takes a slightly different approach by dividing up the ways in the cache to prioritise a set of delinquent program counter values that contribute large numbers of misses to the overall count. This set of PCs is detected and changed adaptively as the program executes. Finally \citet{khanDecoupledDynamicCache2012} builds on earlier work seen in \citet{johnson2QLowOverhead1994} to dynamically alter the size of the once referenced and more than once referenced cache partitions. All of these obtain some speed up (particularly when looking at processors with multiple cores) and most are adaptive but the hardware overhead is still very prohibitive when considering a CPU cache rather than a web or software cache.

Having now seen many different cache policies one thing is consistent, none of the policies, ever matches the theoretical performance of \texttt{OPT} for the same cache size. While this makes sense, because it's impossible to argue that a perfect knowledge of the future is better than an imperfect knowledge of the present, it leaves us with three choices as to how to further reduce latency in our memory systems. Either we work on reducing the gap between \texttt{OPT} and the current best performing cache replacement algorithms, we look for different avenues to reduce latency or we fundamentally change how we do memory accesses to allow us to inject more information into the caches so they can make better decisions. The first of these choice is unattractive because the gap in some cases is quite small but would require enormous effort to close, in addition this research has been de-prioritised in recent times \cite{podlipnigSurveyWebCache2003} as many of the caching algorithms presented are `good enough'. The third option is potentially worth exploring but it would be remiss of us to not explore other areas of the design of the cache first before potentially throwing the baby out with the bathwater. The next section we will explore several cache architecture techniques to reduce latency and then we shall conclude to see what this body of evidence tells us about how effective caching, as a whole, is as a tool to control latencies.

\subsection{Augmenting Cache Architectures}
Having considered cache policies, we can now turn our attention to Cache Architectures and how effectively designing them can lead to reductions in latency. In the coming section we will consider: Associativity, Multi-Banking, Multi-Level Caches, Non-Blocking Caches, Pipelined Caches and Victim Caches. 

At the outset of this section it's important to restate the purpose of this literature review, which is to explicitly search for evidence of a reduction in latency via the use of the above techniques. As a consequence several papers in each of the areas just described will be omitted as they are primarily concerned with saving energy in the cache rather than reducing memory access latency. It's also worth pointing out that, ``trace caches''\cite{rotenbergTraceCacheMicroarchitecture1999, ramirezTraceCacheRedundancy2000} , though very useful for reducing latency in instruction caches won't be included either because this thesis focuses on reducing data cache latency and trace caches specifically target instruction caches.

\subsubsection{Increasing Associativity}

Put simply, the level of associativity a cache presents is the number of alternative places a cache block could be placed for a given cache index \cite{pachecoParallelHardwareParallel2011}. For example if you have a 128 entry cache that is divided into 8 equally sized sets then the cache has an associativity of 16 as there are 16 different places you could place an element for a given set. Associativity forms a continuum that ranges from an associativity of 1, which we refer to as a Direct Mapped cache, to an associativity of $n$ where $n$ is the size of the cache. This latter form of cache is known as fully associative. There is however a tradeoff at play because as associativity increases, the miss rate for a cache goes down. This is because there are multiple places for each element inside the cache, cutting down on conflict misses\footnote{Conflict misses occur when a new block is forced to displace one currently in the cache because they map to the same cache address}. However because these places have to be searched when you want to query the cache the access time for elements in the cache goes up \cite{kesslerInexpensiveImplementationsSetAssociativity1989}. Consequently many researchers have tried to find ways to balance this tradeoff, with the ultimate goal of producing a cache with a high level of associativity and a correspondingly low access time. 

There are three general approaches to solving this problem, the first is to accept that to have the benefits of associativity searches are inevitable and so focus on making them as fast as possible. \citet{kesslerInexpensiveImplementationsSetAssociativity1989} is one of the first to attempt attempt this approach and formulated a scheme whereby the tags for elements in the cache were stored in associative sets but were ordered by \gls{mru}, hoping that searches could be ended early and thus saving time on average. As an alternatie he also introduces partial comparison for tags, so the first $k = \left\lfloor\frac{t}{a}\right\rfloor$ bits where $t$ is the length of the tag in bits and $a$ is the level of associativity. This means you can easily throw out tags that are definitely not in the cache and so, on average, increase the access time as often a search can be avoided. Overall though this approach gives associativity for a relatively low cost there are many factors that govern its performance that are outside its control and as a result it fails to be a good candidate for performing latency reduction. \citet{calderPredictiveSequentialAssociative1996} takes a similar approach but uses a steering bit to direct the search towards one or the other bank within the cache. When this is correct it negates the need for a second costly search and so reduces the average case access time. \citet{chenxizhangMulticolumnImplementationsCache1997} describes this approach as optimal for a 2-way set associative cache but then continues to ask what would happen if this were expanded further. Their approach is to provide multiple entry points into each set, known as ``major locations'' and to then track what they call ``selected locations'' which are locations that have been loaded from main memory as a result of other misses to this major location. Consequently you can construct a simple search algorithm, that checks the major location first (which is kept updated with the \gls{mru} element) and then, if that fails, checks the selected locations until either a load from main memory is required or the data has been found. This way the number of searches required is reduced as the likelihood is the check to the \gls{mru} element will succeed. The problem with these techniques is though they improve access time on average they introduce variable latency and complexity for very small gains. Most of their performance measures are trying to achieve the same level of performance as direct-mapped or similarly sized associative caches rather than trying to bear down on or hide the underlying latency these are very much attempts to mitigate the problem and, as will become clear in later sections there is much more to be done.

An alternative approach to that is to try and remove the problem of search time entirely by making it unecessary. This is the approach favoured by \citet{agarwalColumnassociativeCachesTechnique1993} who implements a pseudo-associativity by having two hashing functions to reduce conflict misses. The index from the address is put through the first hash function and if this results in a clash it is then put through the second. This eliminates the need for searches and via some optimisations means you can easily bypass the second lookup via the use of rehash bits. \citet{hallnorFullyAssociativeSoftwaremanaged2000} also uses hashes to create an Indirect Index Cache. This cache both eliminates searches and breaks the link between the placement of tags in the tag array and location of cache data in the data array. This is done by hashing tags directly from addresses and then looking up the result in a hash table which contains the index required to find the associated data. Unfortunately from the results presented the performance is only competitive with current caches, there are no large gains to be made in reducing latency. \citeauthor{seznecSkewedassociativeCaches1993} \cite{seznecSkewedassociativeCaches1993, seznecCaseTwowaySkewedassociative1993, bodinSkewedAssociativityImproves1997} also use hasing functions to eliminate searching by using a seperate hashing function for each way in the cache. \citet{djordjalianMinimallyskewedassociativeCaches2002} builds on \citeauthor{seznecCaseTwowaySkewedassociative1993} but builds to a higher level of associativity by grouping 4-ways into subsets of 2 and then re-using \citeauthor{seznecCaseTwowaySkewedassociative1993}'s technique on each of the sublevels. This leads to small reductions in miss rates but nothing ground-breaking. \citet{sanchezZCacheDecouplingWays2010} takes this even further and uses mutliple hashing functions to simulate an arbitrary level of associativity with the Z-Cache, but with much improved performance due to search reduction. The problem with a lot of these schemes is that they require a lot of extra hardware to implement, especially in terms of functional units if you wish to have a complicated hash function. In addition the extra latency added from calculation of the hash function, while it may be less than searching a whole cache, is not trivial, so in a lot of cases it may be that one source of latency is simply being traded for another. 

Some authors attempt to maximise the benefits of associativity by only applying it when the program will benefit. For example, if a working set fits perfectly in a direct mapped cache then all the extra cycles spent on managing a fully associative cache are wasted. \citet{batsonReactiveassociativeCaches2001} takes this idea and fuses together the ideas of direct mapping and set associativity by trying to map everything directly and falling back to a set associative mapping only if that fails. They also incorporate a predictor for the set associative side to reduce probing delay. \citet{alyVariablewaySetAssociative2003} takes the idea of different mapping functions in parallel further and varies the associativity on the fly based on offline profiling of the program. This is good as far as it goes but the speed up gained is only 2\%, most of the other gains being in power reduction, in addition the applicability is limited because it requires a-priori knowledge of the code that's going to run, making this only applicable for a limited class of embedded systems rather than all embedded processors. That said \citet{qureshiVWayCacheDemandbased2005} takes the idea of variable associativity but rather than using lookup tables to remap memory throughout the cache the size of the tag store is increased so there is no longer a 1-1 correspondence between a tag and a location in the data store. This means associativity can increase and decrease as the workload demands. The V-Way cache is used as a component in \citet{deepikaHybridwayCacheMobile2011}'s work where the primary way for an index is directly mapped to the data but the V-Cache is used for the other minor ways, this does lead to a large drop in miss rates but at the cost of more hardware and higher power utilisation. \citet{dasVictimRetentionReducing2014} work, extended in \citet{dasDynamicAssociativityManagement2013}, takes a slightly different approach and partitions the ways in an associative \gls{llc} into normal and reserve portions. Recently evicted blocks can then be shared between reserve portions to effectively increase the associativity of certain sets in the cache as the need arises. In the extension work \cite{dasDynamicAssociativityManagement2013} the sharing is limited to ``fellow sets'', or sets that are ``near'' to each other and miss rate reductions of 30\% are recorded when compared against their baseline.

\subsubsection{Multi-Banked Caches}

Multi-banked caches focus on architecting the cache into multiple portions that are all placed on a common interconnect. This interconnect also allows them to interface with several load and store units, increasing the potential for cache parellelism and, in the best case, spreading the data more evenly throughout the cache \cite{riversHighbandwidthDataCache1997}. When a memory read or write is required the address is routed in two phases, first to the correct bank and then to the correct line. This allows for accesses that have very similar low order bits to be spread out amongst multiple banks to allow faster recall. \citet{riversHighbandwidthDataCache1997} combines this technique with memory re-ordering by the compiler to make this technique even more effective. One of the problems with the interconnect approach is that bank conflicts can introduce non-determinims in the latencies seen when accessing data \citet{neefsTechniqueHighBandwidth2000} solves this by extending \citeauthor{riversHighbandwidthDataCache1997} work adding a predictor so that on a correct prediction the latency is constant and known in advance. That being said the latency reductions are quite low and in the optimal case rely on a clairvoyant prediction mechanism which is difficult to implement with the current cache paradigm.

\subsubsection{Multi-Level Caches}

One of the larger areas of research in increasing cache performance has been the use of multi-level caches. Their development follows a roughly chronological progression and is the pre-eminent mechanism for increasing cache effectiveness used in modern processor. The technique works by having multiple caches that increase in size and `distance' from the processor. These are usually arranged into levels (with the level closest to the processor being the Level 1 or L1 cache, the next being L2 etc.) Their development begins with the acceptance, after analysis in \citet{przybylskiPerformanceTradeoffsCache1988}, that there are limits to the effectivness of a single cache, no matter how large it may be. \citet{przybylskiCharacteristicsPerformanceOptimalMultilevel1989} characterises the optimal cachce as having a short cycle time (serves hits quickly) and a low miss ratio and so suggests expanding to 2 levels of caching to reduce the miss penalty in the L1 cache without having a corresponding increase in cycle time for for a much larger L1 cache \cite{jouppiTradeoffsTwolevelOnchip1994}. \citet{azimiTwoLevelCache1992} refines this notion and shows that while this technique is very good at reducing latency the key is placement of cache objects within the cache hierarchy in order that more latency isn't introduced with second level cache misses. Moreover \citet{ju-hotangPerformanceDesignChoices1994} discusses the benefits of making the L2 cache much larger than the L1 to reduce lengthy miss pauses but also acknowledges the degree to which the workload itself determines the most effective cache hierarchy. In effect there is no silver bullet for every concievable program. 

Expanding beyond two levels of caching, \citet{zhaoExploringDRAMCache2007} considers 4 levels of caching, with the \gls{llc} being composed of \gls{dram}, the same memory technology that main memory is usually constructed from. The big advantage to including an \gls{llc} like this is that although you lose the capacity of main memory due to chip space constraints you still get lower latencies because the data has not got to travel off-chip. The problem with a large \gls{dram} \gls{llc} is storing a large number of tags to address it. There are several options but the optimal is to only use partial tags and accept a string of false positives while also using a sectored cache on chip. \citet{wuHybridCacheArchitecture2009} takes this idea even further and proposes that every level of the cache should be constructed from a different memory technology to gain the most from the in-built advantages of each. A variety of cache designs are proposed, and even 3D stacking is introduced which in the end achieves an 18\% IPC increase as compared to a standard 3 level cache. Though this doesn't all address all concerns, problems of endurance of each of the memory technologies are not considered and there's an acknowledgement that much better performance could be gained if the construction of the cache were tuned to the specific requirements of the program to run. \citet{hameedAdaptiveCacheManagement2013a, hameedReducingLatencySRAM2014} takes a similar view but proposes a hybrid \gls{llc} composed of \gls{dram} and \gls{sram} to act as it's own independent cache. A MissMap \cite{lohEfficientlyEnablingConventional2011} is used to decide on whether there is a hit in the SRAM or not and set-duelling is used to decide on a policy to refill the L3-\gls{dram} or not, a large source of latency. All this culminates in the work of \citet{tsaiJengaSoftwaredefinedCache2017} where it's proposed that software should define the cache hierarchy from a pool of generic resources. This is very much the optimal utilisation of this technique, as some workloads will not benefit from deep hierarchies and actually a large L1 cache would be preferable, for example. 

Little work has been done on replacement policies that act across all levels of the cache hierarchy, attempts have often been made to keep caches, even if they exist as part of a hierarchy, self contained and simple and without an overall managing process so most of the work on replacement policies simply adapts work seen in Section \ref{sec:replacement_policy}. \citet{kelwadeReputationBasedCache2017} considers several policies including \texttt{PROMOTE}, \texttt{DEMOTE} and a hybrid method. The first two policies use the number of promotions and demotions in the hierarchy to decide which block should be replaced but the latter combines those two measures to give better performance. Cache replacement policy over multiple levels of caching is certainly an area ripe for further research particularly perhaps used asa supplement to existing techniques to identify hot and cold data and thus cache data more effectively.

\subsubsection{Non-Blocking Caches}

\label{sec:non-blocking}

Non-blocking caches attempt to decouple the cache from synchronously accessing external memory at all. First proposed by \citet{kroftLockupfreeInstructionFetch1981} in \citeyear{kroftLockupfreeInstructionFetch1981}, this approach works by effectively buffering the memory operations that miss in the cache in \gls{mshr}s allowing stalls for data dependencies on reads to be ignored until they become essential and, via the use of write buffers, allows writes to memory to be dealt with asynchronously. In \citet{chenReducingMemoryLatency1992}'s study of this technique it was found that write penalties could be eliminated entirely, as long as reads could bypass writes in a write buffer. There are limits to the technique however, \citet{belaynehDiscussionNonblockingLockupfree1996} discusses that in general the more resources spent on these caches (wider buffers etc.) the better they become but eventually other unrelated delays dominate the latency calculations so there is a sense of diminishing returns. \citet{tuckScalableCacheMiss2006} develops this idea further and proposes a hierarchial \gls{mshr} structure to support many more outstanding misses. Each cache bak contains its own \gls{mshr} along with a bloom filter that links to a large \gls{mshr} file. This leads to a large increase in performance but the limitation is then on the compiler and the program author. If their code consists of relatively few data hazards this technique will work very effectively but there is no guarantee of this. Closer hardware and software co-operation is needed to consistently produce code that can exercise these structures to their best effect. That being said it's important to recognise that striving for the cache and memory system to operate asynchronously from each other is a very good idea and is something that will be returned to in later sections as we explore how tracing might be brought to bear in this situation.

\subsubsection{Pipelines}

The final architecture optimisation that can be made is to consider pipelined caches \cite{olukotunMultilevelOptimizationPipelined1997, olukotunPerformanceOptimizationPipelined1992}. This works in a similar way to the multiple stage pipelines that are seen in CPUs, so any memory access required because of a cache miss or similar can be split into several smaller phases that can be interleaved. This allows caches to effectively hide some of the latency they experience in accessing memory and challenges the dichotomy that as cache size increases so does the access time. \citet{srivastava190MHzCMOS4Kbyte1995, agarwalExploringHighBandwidth2003, martinDesignAsynchronousMIPS1997} show examples of this. Unfortunately this technique is rather ignored however it's considered again by \citet{hongAVICAAccesstimeVariation2013} who uses asymmetric pipelining to resolve the problem that \gls{sram} cell access can be somewhat unpredictable in their latencies. By extending the cell access phase in the pipeline this can be accounted for and using pseduo-multi-banking the bandwidth lost can be regained.

\subsubsection{Victim Caches}

The final architectural technique employed to reduce latency is victim caches. First proposed by \citet{jouppiImprovingDirectmappedCache1990} in \citeyear{jouppiImprovingDirectmappedCache1990}, they grew out of the idea of miss caching whereby when a cache miss occurs a small (2-5 entry) fully associative cache is also loaded with the data that is being loaded into the main cache. As a result if in future the entry disappears from the main cache because a conflict miss\footnote{A miss generated in a direct mapped cache because two cache lines want to occupy the same space} occurs the data will be retained in the miss cache so retrieving it will be faster than requesting it from main memory. The victim cache takes this one step further by removing the duplication present in miss caching. So in a scheme with victim caching, anything that is evicted from the main cache is stored in the victim cache instead of duplicating the main cache. This leads to a stark drop in the number of conflict misses and improves latency as a result. This idea is further developed by \cite{stiliadisSelectiveVictimCaching1997} who adds a more selective element to the victim cache. Their formulation acts very much like \citeauthor{jouppiImprovingDirectmappedCache1990} however if an access misses in the main cache but hits in the victim cache a predcition algorithm is invoked to decide if the main cache and victim cache should be swapped. Similarly if the access misses in both caches the prediction algorithm is again used to decide which element should be evicted to make space for the new element coming from memory. This approach improves on \citeauthor{jouppiImprovingDirectmappedCache1990}'s work but is deficient when turned towards data caches because the elements retrived are inherently less predictable than instructions due to the basic block structure of programs. \citet{hormdeeAsynchronousVictimCache2002} develops the idea into an asynchronous formulation but achieves little in terms of latency reductions.

The next step change in victim caches comes from \citet{khanUsingDeadBlocks2010} who proposes to virtualise the victim caches through the re-use of dead blocks. The idea is that if a block is dead it's simply waiting to be evicted from the cache so it's taking up valuable capacity that could be used by other useful elements. \citeauthor{khanUsingDeadBlocks2010} uses this to suggest that these dead-blocks could be re-apprioriated as a victim cache so all the benefits of retaining elements are kept but without any extra hardware to store them. This does lead to significant decrease in \gls{mpki} but is entirely dependent upon the effectiveness of the dead block predictor as a miss-identification can lead to extra latency being incurred to re-load the evicted block. A selection of papers after this improve further on the concept, \citet{asaduzzamanEffectiveLockingfreeCaching2014} proposes a Smart Victim Cache which includes a level of cache locking to keep good data in the cache longer, \citet{navarroAdaptiveVictimCache2014} proposes an adaptive victim cache that adapts it's size and level of associativity as the program progresses, however doesn't provide any concrete methods to achieve this. Finally \citet{subhaArchitectureVictimCache2016} proposes to change the victim cache from being fully associative to using a small register to cut down searches of the cache. Overall victim caches are a very useful concept and have been extended to cover a wide range of use-cases. There needs to be more research into the adaptive models, considered by \citet{navarroAdaptiveVictimCache2014} as the extra resource they require needs to be justified and may well not always be utilised to its fullest capacity.

\section{Cache Extrinsic Techniques}

If we now return to our overview diagram we can now consider methods to reduce latency that are not focussed on the cache. First we'll consider prefetching, so latency is reduced by trying to execute memory operations in advance of the need for them, that will be followed by a section on Memory Scheduling and how re-ordering memory accesses can be beneficial. Finally there will be a short section on Code Transformations to aid latency reduction.

\subsection{Prefetching}

Prefetching, at its heart, is very simple. It means attempting to issue memory instructions before they are needed by the processor so that the long latency main memory access can be hidden and when eventually the data is called for it's already there. Obviously this is idealised and realising a perfect version of this scheme is impossible without clairvoyancy many authors have constructed schemes that perform a function similar to this with varying degrees of success. Prefetching methods are loosely split into software and hardware types but some combine elements of both.

\subsubsection{Software-based Prefetching}

Software-based Prefetching tends to involve either the compiler or the programmer giving explicit instructions to the memory control system to perform a prefetch action. This is exactly the approach taken by \citet{callahanSoftwarePrefetching1991} in that the compiler decides when a prefetch instruction should be executed. \citeauthor{callahanSoftwarePrefetching1991} focuses this on loops and makes the assumption that the memory latency of a load in the loop will be covered by one iteration of the loop. With this in mind prefetches are inserted into the loop body so that on the next iteration of the loop the data necessary will always be present. However this is quite a primitive approach to the placement of prefetch instructions so in work by \citet{mowryDesignEvaluationCompiler1992} they refine the algorithm in an attempt to pre-fetch for only those accesses that are likely to be cache misses and so eliminating prefetches, that would have been made under the previous scheme, for objects already in the cache.  \citet{zhangSpeedingIrregularApplications1995} takes the idea of explicitly inserting instructions further by marking in the code groups of records that should be pre-fetched together. This allows the compiler to bind these, seemingly disparate, pieces of data together so when they are accessed in the program they can be prefetched exploiting non-obvious (to the compiler) spatial locality. Another approach to software prefetching is proposed by \citet{lipastiSPAIDSoftwarePrefetching1995} in the form of \gls{spaid}, this is a heuristic that inserts prefetch instructions for the data referenced by pointers when they're used as arguments to a procedure. \citet{lukCompilerbasedPrefetchingRecursive1996} takes a similar approach but considers pointers in recursive data structures, proposing several different algorithms for prefecthing them. The problem with both of these previous schemes is that one is only proposed and the other is implemented by hand, adding lots of complexity to the task of a programmer but also they leave somewhat open the question of how the recursive structure or pointer candidates are discovered, which is the key to the efficacy of approaches like this.

There are several problems with software prefetching that render it a dead-end in research terms. The first is that the addition of prefetch instructions can as much as double the size of the executable code \cite{leeWhenPrefetchingWorks2012}. This a problem for two reasons, first in embedded CPUs it's uneconomical, there simply isn't space to double the code size to gain a small increase in performance, but also for general purpose CPUs there's no guarantee that all those extra instructions will actually be effective. The placement of software prefetch instructions must be made at the 'Goldilocks' \footnote{Not too close but not too far way} point before the actual memory request is made. If the prefetch is made too close the memory request it won't have time to complete before it's required so the processor will still stall, but if it's made too far away from the access then there's a chance it could be displaced from the cache by another memory operation which would still cause a cache miss. Finally to even calculate this 'Golilocks' point would require knowledge of the system that is either very hard to attain or may not even be knowable statically such as the latencies of all the memory components involved, or the state of each component for a particular code point. As a result of this, this section will focus on hardware-based prefetching.

\subsubsection{Hardware-based Prefetching}

In comparison to software-based prefetching hardware based prefetching does not rely on the programmer or compiler to insert prefetch instructions instead the prefetching happens entirely transparently to the programmer and is handled by structures in hardware that dictate when and how prefetches occur. The earliest example of this is from \citet{smithCacheMemories1982} where he describes \gls{obl} which, as it sounds, simply fetches the block next to the current one, this can be triggered either on every miss, on every memory access or using a tagging system so it prefetches on a miss or when a prefetch is good, i.e. it prevents a miss. \citet{jouppiImprovingDirectmappedCache1990} develops this idea with a hardware structure known as a stream buffer, this uses \citeauthor{smithCacheMemories1982}'s ideas but places the prefetched data into an auxillary structure rather than straight into the cache. This avoids the issue of having prefetched data replaced by another cache operation. This is also effective at reducing runtime of programs as \citet{farkasHowUsefulAre1995} demonstrates with 26\% runtime reductions on average. \citet{fuDataPrefetchingMultiprocessor1991} develops the ideas of \gls{obl} by defining it as a special case of sequential prefetching, where you instruct the memory unit to get the next $p$ consecutive blocks after the one you want. However as they point out some data is stored in a pattern which is still regular but has large gaps so they introduce stride prefetching, whereby $p$ blocks are still prefetched but each block is separated by a given number of bytes. \citet{baerEffectiveOnchipPreloading1991a} develops further the idea of stride prefetching but makes the stride dynamic by predicting it based on tracking good and bad prefetches and correcting the stride length accordingly. A similar approach is taken by \citet{fuStrideDirectedPrefetching1992} with the use of a stride prediction table, though this is limited to prediction in loops only. \citet{dahlgrenFixedAdaptiveSequential1993} takes on the idea of adaptation but applys it to sequential prefetching instead, monitoring the number of successful preteches and slowly increasing the degree of prefetching\footnote{how many elements are prefetched in each interation}, while \citet{palacharlaEvaluatingStreamBuffers1994} synthesises several ideas together to extend stream buffers with filtering and non-unit strides. \citet{chenEffectiveHardwarebasedData1995} meanwhile extended the work from \citeyear{baerEffectiveOnchipPreloading1991a}\cite{baerEffectiveOnchipPreloading1991a} to create a correlated reference prediction table that solves the problem of misprediction when nested loops are invovled. All of these approaches show reductions in execution time for the benchmarks they use, but they are still focussed on predictable regular data patterns like loops or contiguous memory sections. The next step in the development of this method is account for irregular data patterns.

On of the first pieces of work to attempt to account for irregular data patterns is by \citet{alexanderDistributedPrefetchbufferCache1996}. In this approach stride is taken as a baseline but instead of trying to predict individual addresses to prefetch this work attempts to predict blocks of addresses instead. This takes advantage of the fact that memory references tend to cluster and so if you can predict a block of addresses effectively you will decrease latency through spatial locality. \citet{linReducingDRAMLatencies2001} tries to use spatial locality as well by fetching around demand misses but also couples the prefetch to the memory controller so that prefetches are only issued when the memory system is idle. Though this method sees large speedups in some benchmarks the authors admit more work is still to be done to shape the prefetching to the application. \citet{solihinUsingUserlevelMemory2002} attempts to solve the problem of prefetching complex structures by using pair-based correlation prefetching. Under this scheme chains of misses are detected by the system and stored so that action can be taken on multiple misses at once, once they've been observed the first time. \citet{cookseyStatelessContentdirectedData2002a} develops the ideas of address identification and pointer chasing by attemtping to identify virtual addresses as they are returned from memory systems so they can be prefetched. This is combined with a standard stride prefetcher to give 12.6\% speedup on average throughout a suite of benchmarks. Finally \citet{yuIMPIndirectMemory2015} also uses correlation prefetching for the specific case of accesses in high performance computing of the form \texttt{A[B[i]]} where \texttt{A} and \texttt{B} are arrays. Normally this would be challenging because the accesses to \texttt{A} are irregular but because of the use of \texttt{B} it's possible to predict them much more accurately. Even though this techniques advance on the simpler methods previously described there effect is still limtied. Prefetching the kind of complicated, disparate memory access that often occur in general programs is difficult to do through correlation as there simply isn't enough information to predict some accesses. Many authors recognised this, and as \gls{ooo} processors developed, the next step in hardware based prefetching was to seperate out the prefetch controller to allow it to be asynchronous from the processor, effectively letting the processor define locality rather than the placement of objects in memory.

One of the first steps down the path to asynchronicity was \citet{veidenbaumDecoupledAccessDRAM1997} who took the drastic step of suggesting two entirely separate processors with a compiler that could produce computational code and memory access code. They reasoned that this would give the ultimate overlap as the memory processor could run ahead of the computation processor. However there are problems with synchronicity but also the effectiveness of the compiler, if there are not enough instructions that don't have memory operands then no progress will be made at all. \citet{rothDependenceBasedPrefetching1998} is slightly more conservative and proposes a structure based on a correlation table and a prefetch buffer. This is combined with a method of pointer chasing that allows the prefetcher to remain ahead of the processor but there are issues with address identification and also there are problems with letting the prefetcher run too far ahead due to the `Goldilocks point' mentioned in the previous section. \citet{vanderwielCompilerassistedDataPrefetch1999} takes a similar approach to \citeauthor{veidenbaumDecoupledAccessDRAM1997} in letting the compiler control what prefetches are issued but offloading the task of issuing them to a separate data-prefetch-controller. Work by \citet{collinsDynamicSpeculativePrecomputation2001} and developed in \citeyear{collinsPointerCacheAssisted2002}\cite{collinsPointerCacheAssisted2002} takes the idea of separate execution but optimises it by pre-computing a subset of the program to pre-fetch dynamically and then executing it in a separate thread context. This is augmented in the later paper by adding a pointer cache to break load dependencies where memory instructions wait for pointers that are also stored in memory. \citet{mutluRunaheadExecutionAlternative2003, mutluRunaheadExecutionEffective2003} takes this concept the furthest with the run-ahead processor. Under this scheme when you hit a long latency instruction you checkpoint the program state, as you would in a context switch, and let the processor continue to run. This allows independent memory instructions to execute without needing to wait. None of the operations are actually committed to the processors registers as once the long latency instruction stops processing is resumed and all interim results are blown away but this means that, in the ideal case, a lot of prefetching has happened in the runahead period. All these techniques show promise but they rely on a lot of assumptions particularly about the amount of independent instructions that a processor has between long latency instructions. Unfortunately some programs are very prone to not exhibiting this behaviour and so although this techniques are useful there are limitations to what they can achieve without substantial care on the programmers part or an incredibly sophisticated compiler with knowledge of the internal hardware of the processor.

Some authors set out to improve the process of pre-fetching itself without attaching these improvements to any particular technique. \citet{laiDeadblockPredictionDeadblock2001} combines prefetching with a dead-block predictor so that rather than potentially displacing useful data on pre-fetch blocks that are dead are removed instead. \citet{frittsMultilevelMemoryPrefetching2002a} suggests prefetching at multiple levels of the cache hierarchy but stops short of proposing a mechanism for doing this. \citet{zhuangHardwarebasedCachePollution2003} discusses pollution filters which iteratively classify prefetches as good or bad if they are referenced after being pre-fetched. This allows further more adaptive schemes to alter prefetch parameters should many prefetches start to be classified as bad. Finally \citet{linDRAMLevelPrefetchingFullyBuffered2007a} develops prefetching hardware using advanced memory buffers to utilise wasted bandwidth between these buffers and memory without eating into bandwidth needed for communication. 

All these methods lead to increases in the performance of the system under test but still suffer from the inherent problems seen in previous iterations of hardware based prefetching. To move further beyond those limitations adaptive schemes are introduced so that the prefetching mechanism itself is not a-priori dictating the form of memory accesses but is instead being driven by the compiler. This development is very similar to the development of cache policies that we saw in Section \ref{sec:replacement_policy}. \citet{nesbitACDCAdaptive2004} is one of the first to try this approach, building on their work on global history buffers \cite{nesbitDataCachePrefetching2004} they use a system of calculating address deltas to find the next address to pre-fetch. The adaptive part comes by monitoring this algorithm and tuning the zone size and prefetch degree or turning off pre-fetching altogether when its degrading performance. This idea is developed further by \citet{aroraCompositeDataPrefetcher2014} who takes the monitoring and optimisation idea and switches between multiple prefetching algorithms as the prefetching degree changes in a predictable way. This is taken to its ultimate conclusion by \citet{pandaExpertPrefetchPrediction2016} by using multiple expert predictors and weighted majority voting to decide the best element to prefetch.

\subsection{Scheduling}

Scheduling based techniques are often used to re-order accesses to memory to decrease latency. All are designed in some way to make the schedule of memory accesses more suited to the underlying memory technology and this is done in a number of ways. \citet{kiniwaLookaheadSchedulingRequests1998} is one of first sets of authors to pursue this route, taking a similar approach to the work on non-blocking caches that we saw in \ref{sec:non-blocking}. Under this scheme all memory requests are placed into a queue and this can re-ordered to maximise the length of time elements spend in the cache. Unfortunately this method requires some knowledge of future events, so is not a completely online process, and has a significant overhead to implement. \citet{yangOverlappingDependentLoads2006} looks at resolving load-load dependencies faster, places where processors load an address from memory and then immediately access it via a data forwarding mechanism to allow more overlapping, this has somewhat limited utility but is the only place that explicitly considers data hazards in this context. \citet{luoDesignRealizationOptimized2010} tries a similar approach but uses genetic algorithms to generate a scheduling algorithm offline. This gains a modest speedup but requires the software under discussion to display predictable patterns of execution. The same is true of \citet{wei-chetsengOptimalSchedulingMinimize2010, kegleyPredictiveCacheModeling2011} but rather than genetic algorithms they propose a slightly different optimisation approach by making tasks `wander' around the schedule in the former case and assigning a \gls{smack} score to various schedules in the latter. \citet{qaziOptimizationAccessLatency2016} describes a method of re-organising loop operations to fill up empty slots and so make them more efficient however the reduction in latency is quite small and its unclear how to apply the technique outside of the predictable context of loop execution. \citet{modgilImprovingPerformanceChip2018} takes a more drastic approach and allows the memory controller to adapt its policy on when to act on write commands based on the level of memory traffic it observes, certainly a step in the right directions but the performance increases are highly correlated with the memory configuration itself, showing this policy is much less general than it would ideally be.

Approaching this from the point-of-view of a scheduling algorithm requires more information than the system can provide in the current paradigm. Other author's have instead chosen to exploit properties of the underlying memory technologies instead, accepting there needs to be a higher level of synergy between the memory controller and the physical electronics of the memory it controls. \citet{rixnerMemoryAccessScheduling2000a} increases the complexity of the memory controller, adding a queues of pending references and an addresses arbitrator to produce a stream of DRAM instructions that (in more aggressive cases) take advantage of row and column locality to reduce the number of actual access the \gls{dram} must make. \citet{shaoBurstSchedulingAccess2007} takes the idea further in the xontext of \gls{ooo} processors and groups memory transactions into bursts in the context of \gls{sdram}, due to its non-uniform access latency. These methods are all reasonable and achieve somewhere in teh region of 30\% perofrmance increase \cite{rixnerMemoryAccessScheduling2000a} and 21\% execution time decrease \cite{shaoBurstSchedulingAccess2007} but the real problem with reducing, particularly DRAM latency is that the memory has pre-set timing parameters to ensure the data is valid when its accessed. These parameters change at a glacially slow pace and are now very long compared to processor cycle times, but \citet{hassanChargeCacheReducingDRAM2016} makes the observation that you can reduce the timing parameters if the data has been accessed recently, so they propose a small cache or recently accessed rows that dictates the values of the timing parameters, allowing a performance increase for row local data. \citet{shinDRAMLatencyOptimizationInspired2016} pushes this idea even further but rather than caching the rows they track the time between refreshes, exploiting electrical properties of the \gls{dram} to restore data that had previously been present. \citet{kimSolarDRAMReducingDRAM2018} brings a similar approach, describing a new kind of \gls{dram} called Solar-\gls{dram}. This new memory type has different regions characterised as strong or weak allowing different timing parameters to be used for different regions. The idea here would be to place key data in the strong regions so as to speed up latency when required. 

Rather than approach this from the point of view of the memory technology other authors have chosen to add new hardware to the memory controller itself, in general to integrate the controller more with the cache and memory systems. \citet{stuecheliCoordinatingDRAMLastLevelCache2011} proposes a virtual write queue and scheduled write-backs so the memory controller directs the cache when to transfer lines as opposed to the other way round and the writes are `harvested' from the \gls{mru} part of the \gls{llc}, this section becoming the virtual write queue. \citet{waslyHidingMemoryLatency2014} on the other hand splits the memory controller out of the processor completely and uses the existing \gls{dma} module to completely memory behaviour as opposed to computation. This has a lot of similarities to \citet{veidenbaumDecoupledAccessDRAM1997} though prefetching is not considered.

All these methods exhibit performance increases against the benchmarks they run but there are fundamental problems with attacking this problem from the point of view of scheduling, no matter which general approach is taken. The big problem is that in many cases there simply isn't enough information available online to make the best decisions. This problem stems from a number of sources: the lack of integration between the cache, memory controller and memory hardware, the simplicity of memory controllers due to the lack of available space on a processor die and the lack of knowledge in a compiler of the underlying memory system. This last point has been addressed in the literature and so for this last section we will consider how programs can be transformed and data can best be laid out in memory to reduce latency. 

\subsection{Program Transformation \& Data Layout}

The idea that there are better and worse ways of laying out data in memory is nothing new. As so many processes, i.e. prefetching and caching rely on the idea that data used together will be `physically' close together it makes sense to use processes to increase those metrics if at all possible. This is the approach taken by \citet{pandaMemoryDataOrganization1997} who considers clustering of variables in the code, and the insertion of dummy instructions to make sure that pieces of data accessed together are assigned to different cache lines so they do not displace each other. Others consider loop fusion, a technique girst demonstrated by \citet{gaoCollectiveLoopFusion1993} in \citeyear{gaoCollectiveLoopFusion1993}. Under this technique multiple loops can be merged together into a single loop to reduce program runtime as only one loop will occur rather than multiple loops. A myriad of techniques can be added on top of this to increase the efficacy of the technique \citet{chendingMemoryBandwidthBottleneck2000} proposes the use of hypergraphs to discover the data sharing between loops, an improvement on the original technique as it means more than two loops can be fused if there is sufficient data sharing. \citet{gomezOptimizingMemoryBandwidth2004} pushes this even further and proposes loop morphing which, via loop splitting and fusion, can make even non-conformable loops fuse under some circumstances. \citet{marchalOptimizingMemoryBandwidth2004} in turn describes a technique where loop fusion is done incrementally and over several passes to gain the maximum speed up. On the data side there has been comparatively little work done from the point of view of latency reduction. \citet{qaziOptimizationAccessLatency2016} describes a technique where you can search the array accesses for potential conflicts and place them in different parts of memory, akin to \citet{pandaMemoryDataOrganization1997}. 

The issue with nearly all these techniques is they rely on static analysis and have very little consideration for dynamic behaviour and in fact this is true of very many of the techniques we have seen in this section. What is necessary to improve a lot of these techniques is the ability to consider dynamic behaviour alongside the static to get a real picture of what operations would actually decrease latency rather than accepting an lack of information on this point. Tracing and the use of traces is on way to achieve this and the next section will explore in detail how it has been used as a technique to add more information to help other process reduce latency in a serious way. 

\section{Incorporating Tracing to Reduce Latency}

\label{sec:tracing}

In the third portion of this literature review we consider tracing. There has been very little work in online trace processing and it's application to the problem of latency reduction, which we'll explore in the conclusion and future sections. However some work has been done to utilise tracing in various offline processes, some of which are detailed in the next few sections. To bring this consideration of the literature to a close we'll also consider some examples of work done on using tracing as a method of on-chip debugging as this forms the basis for the implementation that will be presented in future chapters.

\subsection{Tracing as a Control Loop}



\subsection{Tracing for In-Silicon Debugging}



\section{Conclusion}

Having considered a vast swathe of the literature in the last three sections we are forced to ask where this leaves us. Our goal of reducing latency appears to be blocked along the many paths we have considered. Solutions which attempt to improve the cache, via cache policy, are bounded above by \texttt{OPT}, which is itself not 100\% accurate due to compulsory misses and a finite cache capacity. Solutions that target the cache architecture are very well in their way but do not address the basic problem that they are trying to make decisions with a paltry amount of information because of the need to decouple all these systems from each other. Certain papers propose integrating the cache and memory controllers for example \cite{stuecheliCoordinatingDRAMLastLevelCache2011} but there is not the integration across all components of the system, from the compiler to the memory hardware, that would give the information necessary to make better decisions.

Moving outside the cache prefetching seems like a sensible way to solve these problems, and could be added on to almost any other solution but it's failures come down again to the lack of information available to the pre-fetching unit. As a result most pre-fetching comes down to recognising patterns or attempting to derive the future from the past which whilst good is not foolproof, especially as the diversity of application increases. Scheduling and program transformation also look promising but the inability to incorporate dynamic information limits their utility to what can be predicted statically which is a relatively small amount of information. 

\subsection{Potential for the application of Tracing}

So it would appear that there is little more that could be done, other than tinker at the edges with cache policies and hope that the processor-memory gap shrinks of its own accord, but what if there was a way to address the lack of information that is at the heart of the problems we've seen? I propose that tracing is a way to do this, as we've seen in Section \ref{sec:tracing}

