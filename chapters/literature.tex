
\chapter{Literature Review}

This chapter presents a review of the literature surrounding memory latency reduction. It first focuses on more traditional methods, considering caching in its totality and then branching out to look beyond the cache to other methods of controlling latency, such as memory technology changes or scheduling. In the second section the focus moves to newer techniques that involve tracing and then finishes with a summary and evaluation of the position the literature presents.

\section{Cache Intrinsic Techniques}

After their first introduction to super-computers in the 1960's \cite{pattersonComputerOrganizationDesign2018} caches have become a standard part of almost any memory architecture. Iterative improvements, thanks to Moore's law, have pushed the performance of caches to higher and higher levels but this is not the whole story. Despite advances in technology it is still the case that the cache replacement policy is the most significant factor when determining how effective a cache will be at reducing memory latency \cite{hennessyComputerArchitectureQuantitative2019}.

\subsection{Cache Replacement Policy}

In it's simplest form a cache replacement policy is simply a way of deciding what happens when a cache gets full. For direct-mapped caches they are not important because there is only one location for a piece of data to inhabit. Consequently, if it is full there is no choice as to which element is replaced. However in set-associative caches the choice of replacement policy is absolutely crucial \cite{hennessyComputerArchitectureQuantitative2019} and as such there has been much research into which policies yield the best outcomes across a variety of metrics.

When discussing various policies that are useful in reducing latency it's good to have in mind two policies that are often referred to in the literature but are not actually implemented. The first of these is known as \texttt{OPT} \cite{jeongOptimalReplacementsCaches1999} a theoretical optimal replacement policy which, by some unspecified means, can perfectly predict which cache block will be needed furthest in the future. Consequently when the decision is made to replace a cache block it will make an optimal decision. Due to the requirement to have perfect fore-knowledge of memory requirements, some authors \cite{jeongOptimalReplacementsCaches1999, pandaSurveyReplacementStrategies2016} are of the belief that it is not possible to implement. We will see in future sections that this may be an incorrect assumption but for the purposes of this section \texttt{OPT} will be used as a comparison to measure the performance of other cache replacement polcies as compared to the theoretical gold standard.

The second theoretical replacement policy is that of random replacement or \texttt{RAND}. Under this scheme when a decision on replacement has to be made, the choice is made completely at random without reference to any other information \cite{beladyStudyReplacementAlgorithms1966}. Again this method is not usually implemented by cache designers when optimising for latency reduction \cite{karedlaCachingStrategiesImprove1994} but its utility is as the lower end of a continuum, bounded by \texttt{OPT} at the other end. So any cache replacement policy that is to be useful, has to justify the extra resources it will use by being better than \texttt{RAND} but will probably be worse than \texttt{OPT} because no policy can ever have perfect fore-knowledge.

\subsubsection{Arrival-Time Based Techniques}

One of the more simple cache replacement policies is to decide which cache line is to be replaced based on when the item entered the cache, or its arrival time. When implementing a technique like this you have two essential options: either you remove the most recently arrived cache line, or you remove the one that arrived furthest back in the past. The first of these two options is almost never used because it violates the principles of temporal locality, i.e. if something has been referenced recently it has a higher probabilty of being referenced in the immeadiate future, so the second is used. To implement this most designers choose a \gls{fifo} queue, sometimes referred to in the literature as a Round-Robin queue. A lot of implementers choose this technique because it has a very low hardware requirement \cite{pandaSurveyReplacementStrategies2016} which leads to a low cost, esepcially useful in embedded system design. On the other hand it's not highly performative \cite{al-zoubiPerformanceEvaluationCache2004, tsaoMultiFactorPagingExperiment1972}, often performing similarly to \texttt{RAND} despite the slight increase in hardware. Not only that but \gls{fifo} is susceptible to Belady's Anomaly \cite{beladyAnomalySpacetimeCharacteristics1969} so there's no guarantee that larger caches will produce lower miss rates. 

Due to its low performance research into using \gls{fifo} queues has been limited. Often \gls{fifo} is used a baseline to compare other policies to \cite{faresPerformanceEvaluationTraditional2012} rather than being implemented in its own right as it's simple to simulate. Some early development of topic include \citet{turnerSegmentedFIFOPage1981}'s work on \gls{sfifo} paritions main memory into two sections, the sizes of which are controlled by $P$ which sets the proportion of memory dedicated to the second section. This creates a pseduo-multi-level cache and achieves performance very close to \gls{lru} but at lower hardware cost. In the same vein \citet{devilleLowcostUsagebasedReplacement1990} augments \gls{fifo} with a usage counter per set to do the partitioning in a more granular way, this also yields performance comparable to \gls{lru}. In more recent times \citet{wei-chetsengPRRLowoverheadCache2012} have experimented with combining a \gls{fifo} policy with cache-line locking and this produces similar performance to a \gls{lru} policy but uses a lot less hardware to do so also. There are more complex policies that would do even better, but when these are not possible, \gls{fifo} is a viable option, especially when combined with other techniques to supplement its short-comings. All in all \gls{fifo} is a good baseline to build from, and a viable option to implement if resources are limited, however we can use more information to make better decisions if we consider frequency of access. 

\subsubsection{Frequency Based Techniques}

A slightly more sophisticated approach to cache replacement is to count the number of times a cache block or line has been accessed, and then to evict the one with the lowest frequency of access, this approach is known as \gls{lfu} . In terms of implementation the most common form is to turn the cache into a priority queue where keys are calculated according to a variety of different formula \cite{podlipnigSurveyWebCache2003}. In addition implementations choose between perfect \gls{lfu}, where every object is uniquely tracked across replacements, and in-cache \gls{lfu} where counts are only tracked when items are in the cache, this is by far the most common otpion \cite{podlipnigSurveyWebCache2003}.

Desipite it's simplicity in concept, \gls{lfu} has several problems that often make it unsuitable for many applications without modification. The first is the problem of cache pollution \cite{karedlaCachingStrategiesImprove1994} where a cache block has a high number of accesses very early on and then is never referenced again. This causes the block to remain in the cache much longer than it should usefully do, giving less space for other more necessary blocks. The second problem is that often you can end up with many different cache blocks having the same frequency count so you need some kind of tiebreaking arbitration \cite{podlipnigSurveyWebCache2003}. This then means you  have to allocate more resources to the cache replacement algorithm just to resolve ties. Moreover the hardware to keep track of all the frequency counts, potentially across multiple replacements, gives a very high hardware overhead and increased energy consumption, something many embedded systems cannot afford \cite{pandaSurveyReplacementStrategies2016}. 

Some have attempted however to address some of the shortcomings of \gls{lfu} with a variety of augmentations, most of which relate to adding ageing parameters to counter cache pollution. \citet{arlittEvaluatingContentManagement2000} suggest calculating the keys ($K_i$) in the priority queue that powers LFU with a formula $K_i = C_i * F_i + L$ where $C_i$ is the cost of bringing an object into the cache, $F_i$ is the frequency that LFU tracks and $L$ is equal to the $K_f$ where $f$ is the most recently evicted cache element. He dubs this policy, \gls{lfuda}. Others like \citet{robinsonDataCacheManagement1990} choose to age slightly differently by protecting new entries to the cache and also aging the entire cache by reducing all reference counts $C$ to $\ceil[\big]{\frac{C}{2}}$ whenever the average reference count exceeds a predefined maximum value. Both of these techniques produce results comparable than \gls{lru} ,which will be discussed in the next section, but the hardware cost is much higher due to the number of counters. In addition both of these approaches are concerned with the size of objects in the cache, something that is not a concern in this work due to the uniformity of CPU cache blocks.

A further augmentation of \gls{lfu} from \citet{kellyVariableQosShared1999} is to allow weighting parameters that come from the memory system to indicate how `useful' the caching of that element is. The problem, as \citeauthor{kellyVariableQosShared1999} admits, is the difficult of obtaining those weights and also the problem that this approach still requires the implementation of \gls{lru} as well to resolve ties. With that in mind, and with the marginal gains that \gls{lfu} can make over \gls{lru} it seems peverse to employ a system that requires hardware for both. A final interesting approach to frequency type statistics is from \citet{mekhielMultiLevelCacheMost29} who proposes a two level cache with the \gls{mfu} elements going in the equivalent of an L1 cache and the \gls{lfu} elements going in an L2 cache. This allows frequently accessed data to be easily available to \gls{cpu} and not easily evicted. This still suffers from the same cache pollution problems as other \gls{lfu} methods and still requires large amounts of hardware.

A slightly different approach is to take inspiration from probability theory as LFU-K \cite{sokolinskyLFUKEffectiveBuffer2004} does. Itself a development of the LRU-K algorithm, it uses probability theory to predcit the number of occurences of an element (page, cache block etc.) in a reference string. The development from \gls{lfu} is that it adds extra terms into the estimation formula to account for the changing probability of referencing an element as time goes on. In this work \citet{sokolinskyLFUKEffectiveBuffer2004} demonstrate that in terms of reducing cache miss rate, LFU-K outperforms \gls{lfu} and \gls{lru}. However the effectiveness of this technique is intrinsically linked to the estimation of two parameters $m$ and $h$ neither of which is a trivial task. In addition the hardware requirements are still large, larger than \gls{lfu} due to the extra costs in calculating the probability functions. 

\gls{lfu} is an improvement from simple \gls{fifo} policies but suffers from the problem of cache pollution and a very high hardware requirement, particularly in the perfect case due to the number of counters necessary \cite{podlipnigSurveyWebCache2003}. Considering recency rather than frequency has long been considered a better metric to approximate how far in the future a piece of data will be needed and so the next section covers techniques that consider recency rather than frequency. 

\subsubsection{Recency Based Techniques}

Recency, as a general class of algorithms orders the elements in a cache by the time they were last referenced. This leads to two very general categories of recency-based algorithms, \gls{mrre} and \gls{lrre}. \gls{mrre} algorithms are much less common than \gls{lrre} and in general are less performant due to their poor temporal locality \cite{pandaSurveyReplacementStrategies2016}, and as such will not be focussed on in this thesis. \gls{lrre} techniques in general attempt to evict the least recently referenced element in the cache, using this as a heurisitc to predict the element that will be needed furthest away in the future. The most popular \gls{lrre} algorithm is \gls{lru} \cite{pitkowSimpleRobustCaching1994, karedlaCachingStrategiesImprove1994, smithCacheMemories1982}, it makes use of temoral locality and, given a few simplifying assumptions, is very easy to implement. It also scores highly when one considers the tradeoff between resources and the resulting performance increase. Most often \gls{lru} is implemented with counters or as a square matrix, an example of which can be seen in the work of \citet{acklandSinglechip6billion16b2000}. 

However \gls{lru} is not without its problems. It performs very badly in a shared data environment, or when using virtual memory \cite{bansalCARClockAdaptive2004} and is also susceptible to cache-thrashing \cite{denningThrashingItsCauses1968}, when the working set of the program, defined by \citet{denningWorkingSetModel1968} as \textquote{the set of most recently referenced pages}, exceeds the size of the cache. This is especially problematic in large loops \cite{linPredictingLastTouchReferences2002}. There's also the problem of dead blocks, identified by \citet{liuCacheBurstsNew2008a}, this causes problems because a lot of blocks read into memory are never referenced again but take time to be labelled as \gls{lru} and so evicted. This ties up space that could be used by other elements, consequently dead-block prediction and identification is a large area of work in improving \gls{lru} based algorithms. Finally, as \citet{linPredictingLastTouchReferences2002} describe, \gls{lru} does not have the desirable property that as associativity in the cache increases the miss rate goes down, in fact they state the oppositie is often true, this is highly problematic as cache associativity and size will only increase as time goes on.

The development of \gls{lru} is split into two streams, the first is an attempt to approximate true \gls{lru} because it has been long been acknowledged that implementing complete \gls{lru} is very expensive because a total order must be provided over all the elements present \cite{soCacheOperationsMRU1988}. Work from \citet{corbatoPagingExperimentMultics1969} and \citet{eastonUseBitScanning1979} describe an approximation called \texttt{CLOCK} which uses a lot less hardware than true \gls{lru} solutions. Conceptually the elements are arranged as on a clock face and each have an associated use-bit, and there is also a global $k$-bit shift register. When a replacement is needed a cursor sweeps round the 'clock' looking for a replacement based on the contents of the use bits as they are shifted into the shift register. \citet{corbatoPagingExperimentMultics1969} makes the case that if $k=0$ this approximates \gls{fifo} that we saw earlier and as $k \rightarrow \infty$ the miss rate tends to match \gls{lru}, though this is qualified in \citet{eastonUseBitScanning1979} as there are a number of cases where this doesn't happen. 

\citet{soCacheOperationsMRU1988} catalogue several different approaches to approximating \gls{lru} using tree-based methods. The first of these is a relaxation the constraint that the cache must keep a total order of elements stored within it. They call this the \gls{plru} algorithm (though some other authors refer to this as \gls{plrut}), under their model \gls{lru} is modelled a stack that requires $\log_2(n!)$ bits to repesent all the configurations of $n$ cache lines. Under \gls{plru} this simplified so that cache lines are arrange into groups of two. This significantly reduces the number of bits required to represent this to $n-1$, and the upshot is that \gls{lru} ends up occuring within each of these subsets rather than at the global level of the cache set. This can cause issues because the net effect of approximating \gls{lru} like this is not exactly equivalent to \gls{lru} so there are performance variations. This is further simiplified by the same authors down to a single bit per cache set, that partitions the group in half into \gls{lru} elements at one end and \gls{mru} elements at the other (a good visual representation is providied in \cite{damienStudyDifferentCache2007}), the rationale being that the exact element to be replaced is not important as long as it's in the \gls{lru} part of the cache set. Both policies are good approximations of \gls{lru} at low levels of associativity but the simplifications made begin to bite as associativity increases and the 1-bit method performs as badly as random at the highest levels of associativity. \gls{plru} on the other hand tracks \gls{lru}'s performance well \cite{al-zoubiPerformanceEvaluationCache2004} and is widely used in data caches for this reason \cite{damienStudyDifferentCache2007}.

The idea of using tree-based methods to approximation \gls{lru} is picked up by \citet{ghasemzadehModifiedPseudoLRU2006} with two new approximating algorithms \gls{bplru} and \gls{mplru}. Under \gls{bplru} a tree that stores history bits is used that is updated on each cache hit so that an approximation of \gls{lru} can be performed at very low cost. However \gls{bplru} lacks the correct amount of hysteresis to not make mistakes when compared to true \gls{lru} because information is lost at the higher levels of the tree. This is addressed in \gls{mplru} where higher levels of the tree are classified into \gls{tbai} and \gls{mbai}. \gls{mbai} nodes save their previous history bit when it is updated allowing them more information to closely approximate \gls{lru}. The performance is very close to true \gls{lru} and signfiicantly better than \gls{fifo} or \texttt{RAND}, and though an overhead exists it's much less than a full \gls{lru} implementation making this a very tempting candidate when resources are scarce.

\citet{malamy1994methods} propose a very simple method of approximation using \gls{mru} bits in a cache along with a locking mechanism to save very frequently accessed data. Each element is assigned an \gls{mru} bit which is initially 0 and when a line is replaced the \gls{mru} bit is set to 1, when all lines are set to 1 all the bits are reset to 0 except the one that has just been inserted. This is shown visually in the work of \citet{damienStudyDifferentCache2007}. This method outperforms \gls{lru} for some data patterns and for others approximates the performance curve very well \cite{al-zoubiPerformanceEvaluationCache2004}, making this a very popular approximation due to its cheapness to implement and relative performance. The \gls{nru} algorithm, detailed in a technical report on the UltraSPARC Architecture \cite{UltraSPARCT2Supplement2007} takes a similar approach adding a \emph{used} bit $u$ and an \emph{allocated} bit $a$ per line. It then searches from a pre-defined pointer to find an element where $u$ and $a$ are both cleared to perform the replacement. This approximates \gls{lru} but dispenses with ordering so as to greatly simplify the number of bits necessary to do so.

\citet{ghasemzadehPseudoFIFOArchitectureLRU2005} takes a different approach and looks at the underlying \gls{lru} hardware rather than the process. In this work the reference implementation is taken as a matrix to represent the various \gls{lru} stack configurations, and this requires $\frac{n(n-1)}{2}$ bits where $n$ is level of associativity in the cache. Under the implementation presented by \citet{ghasemzadehPseudoFIFOArchitectureLRU2005} they reduce this to $n\log_2\left(\frac{n}{2}\right)$, which asymptotically grows less fast than the reference. They also show their technique consistently outperforms \gls{lru} on miss rate.

The second stream of the development of \gls{lru} is an attempt to extend the general idea of the policy to counteract its shortcomings and this takes several general forms. The first of these is to try and use statistical inference or measured history of accesses to divine the future behaviour of a program and act accordingly. This is the approach taken by \citet{oneilLRUKPageReplacement1993} where the \gls{lru-k} algorithm is described \footnote{The work of \citet{oneilLRUKPageReplacement1993} is cited as the inspiration for \citet{sokolinskyLFUKEffectiveBuffer2004}'s work we discussed earlier.}, using Bayesian methods to estimate the interarrival times of memory references from the collected set of past references. \citet{vakaliLRUbasedAlgorithmsWeb2000} continues this idea with \gls{hlru} which defines a function $hist(x,h)$ which returns the $h$th past reference to the cache object $x$, and then uses the maximum value of $hist$ among the cached objects to decide a replacement. \citet{wongModifiedLRUPolicies2000} dsecribes a suite of algorithms that are improvements of each other \gls{prl} and \gls{orl}. The essential idea of these algorithms is to favour lines that exhibit temporal behaviour by marking them as such using special \gls{isa} instructions. \gls{prl} does the identification offline using profiling but \gls{orl} does profiling online at run-time, keeping a table of hits to non-\gls{mru} lines and using this to set temporal bits. All of these policies show increased performance over \gls{lru} to varying degrees, but the problem with all of them is the extra hardware and book-keeping required. In addition none of these algorithms address some of the underlying flaws in \gls{lru} such as its suceptability to flooding \cite{glassAdaptivePageReplacement1997} as unless the history collection cover a whole loop the behaviour would not be picked up. The next set of approaches address these concerns.

Flooding is a phenomemnon particularly associated with \gls{lru} which happens when applications try to access a large address space in a sequential fashion. As more addresses are accessed and the cache capacity is exhausted, old elements (those at the beginning of the space) are evicted from the cache, and then when the loop begins again old elements are pushed out. This means that the cache policy has no positive impact on the execution at all. To address this \citet{glassAdaptivePageReplacement1997} proposes the \texttt{SEQ} algorithm which records long sequences of requests for sequential pages and applies \gls{mru} replacement to those sequences. Otherwise it defaults to simple \gls{lru}. This has a high overhead to implement compared to \gls{lru} which is even more of a problem as \gls{lru} is costly to implement anyway and there are more than just sequential accesses that can cause flooding. \citet{smaragdakisEELRUSimpleEffective1999} further develops these ideas to create the \gls{eelru} algorithm where the definition of 'sequential' is weakened to make the algorithm more ammenable to data structures that are not contiguous in memory. It does this via an adaptive feature where the behaviour of the algorithm changes between standard \gls{lru} and what \citeauthor{smaragdakisEELRUSimpleEffective1999} refers to as the \gls{wfl} algorithm, which can evict pages before they become the \gls{lru} page. \gls{eelru} merges these together by calculating probablistically whether \gls{wfl} will have more hits than measured from \gls{lru} over multiple sets of parameters for \gls{wfl}. A further approach to the problem of flooding is cache partitioning, as proposed by \citeauthor{kimLowoverheadHighperformanceUnified2000} in their \gls{ubm} technique. Here the cache is divided into three regions, a sequential region, a looping region and an other region. When a reference is requested it is classifed into one of the three categories and then different replacement algorithms are run on each region accordingly. All of these techniques are at least comparable to \gls{lru} and many perform at least as well as \gls{lru} in a variety of situations but they are not silver bullets. Several of them have very high hardware requirements, particlarly \citet{kimLowoverheadHighperformanceUnified2000} which is targetted as a software technique in its current presentation. This stems from the fact that analysis has to be performed online so a lot of state has to be stored as in all these cases the analysis takes into account history. Other authors suggest using static analysis to cut down this amount of state needed to be stored and that's what we'll consider next.

Static analysis and the use of compiler techniques to enhance caching shift augment compilers and programs with cache hints adding to the amount of information avaiable to a cache when a replacement decision needs to be made. \citet{jainSoftwareassistedCacheReplacement2001} tackles this by augmenting the \gls{isa} of a processor with \texttt{KILL}, \texttt{KEEP} and \texttt{COND-KILL} instructions that are used instead of normal \texttt{LOAD} and \texttt{STORE} instructions when a variable is considered dead. The \texttt{KILL} instruction is used for short-lived variables or ones that are only accessed once and \texttt{KEEP} for long-lived variables and this method shows increased hit rate of \gls{lru} over multiple levels of associativity. \citet{zhenlinwangUsingCompilerImprove2002} simplifies this further through the use of an evict-me bit which is set when a reference is accessed that is \textquote{sufficiently far away} or has no reuse, this is done by issuing a different instruction so the compiler is responsible for setting the evict-me bit. This technique also has enhanced performance over plain \texttt{LRU}. The problem with techniques of this kind are that they are difficult to implement for existing systems because they require changes to the \gls{isa}, which if you are using \gls{cots} components is impossible, but also because they have limited perception and so have a ceiling on their utility. A theme that will be returned to later is the inability of static techniques to comprehend dynamic behaviour, and that is exactly the case here. As a result, it may be the case that there are more references that could be marked for eviction if only the behaviour could be percieved. 

A consistent assumption of all the techniques seen so far is that cache misses all have a uniform cost to them. However research in the recent past has shown this simply not the case, as \citet{qureshiCaseMLPAwareCache2006} pithily puts it \textquote{not all misses are created equal}. Consequently several authors have attempted to integrate cost functions into \gls{lru} algorithms to improve performance so as to prioritise low-cost misses. \citet{jeongCostsensitiveCacheReplacement2003} is an early example of this, where each block is not only associated with a last reference time but also with a cost of replacement. This way the algorithm can make a choice between evicting the \gls{lru} element or evicting non-\gls{lru} element with a lower cost. Overall several iterations of the algorithm a 15-18\% increase on plain \gls{lru} is recorded with minimal extra hardware requirement. This approach is developed further in \citet{kharbutliLACSLocalityAwareCostSensitive2014a} with the introduction of the \gls{lacs} algorithm. Instead of using a 2-cost model, where a miss is cheap or expensive, this technique records latency when a miss happens for a particular element. These costs are incremented and decremented as other events occur in the cache as well such as a hit to an element or a different element not being accessed for a long period of time. This results in large drops in the miss rate but depends a lot on the working set size relative to the size of the cache. \citet{dasLatencyAwareBlock2017} takes a similar approach but uses latencies calculated from a \gls{noc} rather than a tradtional hardware arrangement. The big problem with these approaches is how the cost function is arrived at and what variables it considers. The examples previously presented use different criteria but the potential list of criteria is endless so it's difficult to distil all these competing pieces of information into the set that is most salient for reducing latency. Also none of these authors contend with the problem of \emph{calculating} the cost as a program is running \citet{jeongCostsensitiveCacheReplacement2003} uses information that can easily be cribbed from memory addresses and \citet{kharbutliLACSLocalityAwareCostSensitive2014a} uses simple counters.

One of the consistent problems of \gls{lru} is that in reality it's only a small subset of cache elements that actually get referenced after they are read into the cache \cite{qureshiAdaptiveInsertionPolicies2007}. This means that a lot of elements sit in \gls{lru} caches for a long time, simply taking up space that could be used by other elements. The root cause is that under vanilla \gls{lru} all elements are inserted in the \gls{mru} position (usually in a stack but sometimes in other data structures) and then progress to the \gls{lru} position over time. \citeauthor{qureshiAdaptiveInsertionPolicies2007} \cite{qureshiAdaptiveInsertionPolicies2007, qureshiSetDuelingControlledAdaptiveInsertion2008} asks whether changing this might lead to better cache utilisation and so proposes a suite of new insertion policies to achieve this. The eventual policy arrived at is known as \gls{dip} which combines standard \gls{lru} with a policy known as \gls{bip}, the algorithm switching between these two policies when one performs better than the other. In order to track the utility of switching a technique known as set-duelling is used, where part of the cache is dedicated to each policy and the miss-rate for each part is calculcated before it's decided which algorithm should be used. This closes the gap between \gls{lru} and \texttt{OPT} in the situations studied by two-thirds. \citet{sreedharanCacheReplacementPolicy2017} takes a similar approach but uses a reference count attached to each cache block. If it is high the insertions happen at the \gls{mru} position, otherwise it's \gls{lru}. \citet{guTheoryPotentialLRUMRU2011} takes a slightly different approach they call collabortive caching. Here the cache is split in half so you have \gls{mru} and \gls{lru} instructions so data can be inserted in different places. The problem with the last two solutions is very much around how you would gain the information to decided when to use each instruction on the fly. In \citet{sreedharanCacheReplacementPolicy2017, qureshiAdaptiveInsertionPolicies2007} there are mechanisms to decide however none of that exists in \citet{guTheoryPotentialLRUMRU2011} as it's a more theoretical paper. All these policies improve performance and some close the gap to \texttt{OPT} significantly but there are still times where caching policies like these will make mistakes due to the relative paucity of information they have so our final set of solutions addresses this problem through mistake correction.

\citet{kampeSelfCorrectingLRUReplacement2004} suggest that the wide gap between \texttt{OPT} and \gls{lru} is indicative of \gls{lru} making too many mistakes and so their policy of self-correcting \gls{lru} adds a feedback loop to the \gls{lru} policy and starts with the goal that no mistake should occur more than once. To do this they employ a shadow directory to track when blocks are evicted too early and a mistake history table to persist the information even after blocks are removed from the shadow cache. There's also an \gls{mru} victim cache that means blocks that bypass the cache and ones that are evicted from the \gls{mru} position so that mispredictions can be quickly recovered from. All these improvements together give a 24\% improvement in miss rates during the experiments made, but this is at the cost of quite a lot of extra hardware and book keeping to manage this extra state.

Having now toured many different manifestations of \gls{lru} policies we see a pattern emerging, which is that \gls{lru} when compared to \texttt{OPT} will always make mistakes and mispredictions so will never track \texttt{OPT}'s performance perfectly. There are many techniques to close the gap somewhat but no technique has done it flawlessly up to now. This implies that recency is not the only piece of information needed to make the best caching decisions and so the next set of policies we'll consider combine recency and frequency to close the information gap to \texttt{OPT} and attempt to match its performance.

\subsubsection{Methods Combining Recency and Frequency}



\subsubsection{Beyond Recency and Frequency Combinations}

\subsection{Improving Cache Technology}

\subsection{Multi-Level Caches}

\subsection{Prefecting \& Pre-loading}

\section{Cache Extrinsic Techniques}

\subsection{Hardware Changes}

\subsection{Scheduling}

\subsection{Program Rewriting}

\chapter{Utilising Tracing to Reduce Memory Latency}

