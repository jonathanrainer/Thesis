
\chapter{Literature Review}

This chapter presents a review of the literature surrounding memory latency reduction. It first focuses on more traditional methods, considering caching in its totality and then branching out to look beyond the cache to other methods of controlling latency, such as memory technology changes or scheduling. In the second section the focus moves to newer techniques that involve tracing and then finishes with a summary and evaluation of the position the literature presents.

\section{Cache Intrinsic Techniques}

After their first introduction to super-computers in the 1960's \cite{pattersonComputerOrganizationDesign2018} caches have become a standard part of almost any memory architecture. Iterative improvements, thanks to Moore's law, have pushed the performance of caches to higher and higher levels but this is not the whole story. Despite advances in technology it is still the case that the cache replacement policy is the most significant factor when determining how effective a cache will be at reducing memory latency \cite{hennessyComputerArchitectureQuantitative2019}.

\subsection{Cache Replacement Policy}

In it's simplest form a cache replacement policy is simply a way of deciding what happens when a cache gets full. For direct-mapped caches they are not important because there is only one location for a piece of data to inhabit. Consequently, if it is full there is no choice as to which element is replaced. However in set-associative caches the choice of replacement policy is absolutely crucial \cite{hennessyComputerArchitectureQuantitative2019} and as such there has been much research into which policies yield the best outcomes across a variety of metrics.

When discussing various policies that are useful in reducing latency it's good to have in mind two policies that are often referred to in the literature but are not actually implemented. The first of these is known as \texttt{OPT} \cite{jeongOptimalReplacementsCaches1999} a theoretical optimal replacement policy which, by some unspecified means, can perfectly predict which cache block will be needed furthest in the future. Consequently when the decision is made to replace a cache block it will make an optimal decision. Due to the requirement to have perfect fore-knowledge of memory requirements, some authors \cite{jeongOptimalReplacementsCaches1999, pandaSurveyReplacementStrategies2016} are of the belief that it is not possible to implement. We will see in future sections that this may be an incorrect assumption but for the purposes of this section \texttt{OPT} will be used as a comparison to measure the performance of other cache replacement polcies as compared to the theoretical gold standard.

The second theoretical replacement policy is that of random replacement or \texttt{RAND}. Under this scheme when a decision on replacement has to be made, the choice is made completely at random without reference to any other information \cite{beladyStudyReplacementAlgorithms1966}. Again this method is not usually implemented by cache designers when optimising for latency reduction \cite{karedlaCachingStrategiesImprove1994} but its utility is as the lower end of a continuum, bounded by \texttt{OPT} at the other end. So any cache replacement policy that is to be useful, has to justify the extra resources it will use by being better than \texttt{RAND} but will probably be worse than \texttt{OPT} because no policy can ever have perfect fore-knowledge.

\subsubsection{Arrival-Time Based Techniques}

One of the more simple cache replacement policies is to decide which cache line is to be replaced based on when the item entered the cache, or its arrival time. When implementing a technique like this you have two essential options: either you remove the most recently arrived cache line, or you remove the one that arrived furthest back in the past. The first of these two options is almost never used because it violates the principles of temporal locality, i.e. if something has been referenced recently it has a higher probabilty of being referenced in the immeadiate future, so the second is used. To implement this most designers choose a \gls{fifo} queue, sometimes referred to in the literature as a Round-Robin queue. A lot of implementers choose this technique because it has a very low hardware requirement \cite{pandaSurveyReplacementStrategies2016} which leads to a low cost, esepcially useful in embedded system design. On the other hand it's not highly performative \cite{al-zoubiPerformanceEvaluationCache2004, tsaoMultiFactorPagingExperiment1972}, often performing similarly to \texttt{RAND} despite the slight increase in hardware. Not only that but \gls{fifo} is susceptible to Belady's Anomaly \cite{beladyAnomalySpacetimeCharacteristics1969} so there's no guarantee that larger caches will produce lower miss rates. 

Due to its low performance research into using \gls{fifo} queues has been limited. Often \gls{fifo} is used a baseline to compare other policies to \cite{faresPerformanceEvaluationTraditional2012} rather than being implemented in its own right as it's simple to simulate. Some early development of topic include \citet{turnerSegmentedFIFOPage1981}'s work on \gls{sfifo} paritions main memory into two sections, the sizes of which are controlled by $P$ which sets the proportion of memory dedicated to the second section. This creates a pseduo-multi-level cache and achieves performance very close to \gls{lru} but at lower hardware cost. In the same vein \citet{devilleLowcostUsagebasedReplacement1990} augments \gls{fifo} with a usage counter per set to do the partitioning in a more granular way, this also yields performance comparable to \gls{lru}. In more recent times \citet{wei-chetsengPRRLowoverheadCache2012} have experimented with combining a \gls{fifo} policy with cache-line locking and this produces similar performance to a \gls{lru} policy but uses a lot less hardware to do so also. There are more complex policies that would do even better, but when these are not possible, \gls{fifo} is a viable option, especially when combined with other techniques to supplement its short-comings. All in all \gls{fifo} is a good baseline to build from, and a viable option to implement if resources are limited, however we can use more information to make better decisions if we consider frequency of access. 

\subsubsection{Frequency Based Techniques}

A slightly more sophisticated approach to cache replacement is to count the number of times a cache block or line has been accessed, and then to evict the one with the lowest frequency of access, this approach is known as \gls{lfu} . In terms of implementation the most common form is to turn the cache into a priority queue where keys are calculated according to a variety of different formula \cite{podlipnigSurveyWebCache2003}. In addition implementations choose between perfect \gls{lfu}, where every object is uniquely tracked across replacements, and in-cache \gls{lfu} where counts are only tracked when items are in the cache, this is by far the most common otpion \cite{podlipnigSurveyWebCache2003}.

Desipite it's simplicity in concept, \gls{lfu} has several problems that often make it unsuitable for many applications without modification. The first is the problem of cache pollution \cite{karedlaCachingStrategiesImprove1994} where a cache block has a high number of accesses very early on and then is never referenced again. This causes the block to remain in the cache much longer than it should usefully do, giving less space for other more necessary blocks. The second problem is that often you can end up with many different cache blocks having the same frequency count so you need some kind of tiebreaking arbitration \cite{podlipnigSurveyWebCache2003}. This then means you  have to allocate more resources to the cache replacement algorithm just to resolve ties. Moreover the hardware to keep track of all the frequency counts, potentially across multiple replacements, gives a very high hardware overhead and increased energy consumption, something many embedded systems cannot afford \cite{pandaSurveyReplacementStrategies2016}. 

Some have attempted however to address some of the shortcomings of \gls{lfu} with a variety of augmentations, most of which relate to adding ageing parameters to counter cache pollution. \citet{arlittEvaluatingContentManagement2000} suggest calculating the keys ($K_i$) in the priority queue that powers LFU with a formula $K_i = C_i * F_i + L$ where $C_i$ is the cost of bringing an object into the cache, $F_i$ is the frequency that LFU tracks and $L$ is equal to the $K_f$ where $f$ is the most recently evicted cache element. He dubs this policy, \gls{lfuda}. Others like \citet{robinsonDataCacheManagement1990} choose to age slightly differently by protecting new entries to the cache and also aging the entire cache by reducing all reference counts $C$ to $\ceil[\big]{\frac{C}{2}}$ whenever the average reference count exceeds a predefined maximum value. Both of these techniques produce results comparable than \gls{lru} ,which will be discussed in the next section, but the hardware cost is much higher due to the number of counters. In addition both of these approaches are concerned with the size of objects in the cache, something that is not a concern in this work due to the uniformity of CPU cache blocks.

A further augmentation of \gls{lfu} from \citet{kellyVariableQosShared1999} is to allow weighting parameters that come from the memory system to indicate how `useful' the caching of that element is. The problem, as \citeauthor{kellyVariableQosShared1999} admits, is the difficult of obtaining those weights and also the problem that this approach still requires the implementation of \gls{lru} as well to resolve ties. With that in mind, and with the marginal gains that \gls{lfu} can make over \gls{lru} it seems peverse to employ a system that requires hardware for both. A final interesting approach to frequency type statistics is from \citet{mekhielMultiLevelCacheMost29} who proposes a two level cache with the \gls{mfu} elements going in the equivalent of an L1 cache and the \gls{lfu} elements going in an L2 cache. This allows frequently accessed data to be easily available to \gls{cpu} and not easily evicted. This still suffers from the same cache pollution problems as other \gls{lfu} methods and still requires large amounts of hardware.

A slightly different approach is to take inspiration from probability theory as LFU-K \cite{sokolinskyLFUKEffectiveBuffer2004} does. Itself a development of the LRU-K algorithm, it uses probability theory to predcit the number of occurences of an element (page, cache block etc.) in a reference string. The development from \gls{lfu} is that it adds extra terms into the estimation formula to account for the changing probability of referencing an element as time goes on. In this work \citet{sokolinskyLFUKEffectiveBuffer2004} demonstrate that in terms of reducing cache miss rate, LFU-K outperforms \gls{lfu} and \gls{lru}. However the effectiveness of this technique is intrinsically linked to the estimation of two parameters $m$ and $h$ neither of which is a trivial task. In addition the hardware requirements are still large, larger than \gls{lfu} due to the extra costs in calculating the probability functions. 

\gls{lfu} is an improvement from simple \gls{fifo} policies but suffers from the problem of cache pollution and a very high hardware requirement, particularly in the perfect case due to the number of counters necessary \cite{podlipnigSurveyWebCache2003}. Considering recency rather than frequency has long been considered a better metric to approximate how far in the future a piece of data will be needed and so the next section covers techniques that consider recency rather than frequency. 

\subsubsection{Recency Based Techniques}

Recency, as a general class of algorithms orders the elements in a cache by the time they were last referenced. This leads to two very general categories of recency-based algorithms, \gls{mrre} and \gls{lrre}. \gls{mrre} algorithms are much less common than \gls{lrre} and in general are less performant due to their poor temporal locality \cite{pandaSurveyReplacementStrategies2016}, and as such will not be focussed on in this thesis. \gls{lrre} techniques in general attempt to evict the least recently referenced element in the cache, using this as a heurisitc to predict the element that will be needed furthest away in the future. The most popular \gls{lrre} algorithm is \gls{lru} \cite{pitkowSimpleRobustCaching1994, karedlaCachingStrategiesImprove1994, smithCacheMemories1982}, it makes use of temoral locality and, given a few simplifying assumptions, is very easy to implement. It also scores highly when one considers the tradeoff between resources and the resulting performance increase. Most often \gls{lru} is implemented with counters or as a square matrix, an example of which can be seen in the work of \citet{acklandSinglechip6billion16b2000}. 

However \gls{lru} is not without its problems. It performs very badly in a shared data environment, or when using virtual memory \cite{bansalCARClockAdaptive2004} and is also susceptible to cache-thrashing \cite{denningThrashingItsCauses1968}, when the working set of the program, defined by \citet{denningWorkingSetModel1968} as \textquote{the set of most recently referenced pages}, exceeds the size of the cache. This is especially problematic in large loops \cite{linPredictingLastTouchReferences2002}. There's also the problem of dead blocks, identified by \citet{liuCacheBurstsNew2008a}, this causes problems because a lot of blocks read into memory are never referenced again but take time to be labelled as \gls{lru} and so evicted. This ties up space that could be used by other elements, consequently dead-block prediction and identification is a large area of work in improving \gls{lru} based algorithms. Finally, as \citet{linPredictingLastTouchReferences2002} describe, \gls{lru} does not have the desirable property that as associativity in the cache increases the miss rate goes down, in fact they state the oppositie is often true, this is highly problematic as cache associativity and size will only increase as time goes on.

The development of \gls{lru} is split into two streams, the first is an attempt to approximate true \gls{lru} because it has been long been acknowledged that implementing complete \gls{lru} is very expensive because a total order must be provided over all the elements present \cite{soCacheOperationsMRU1988}. Work from \citet{corbatoPagingExperimentMultics1969} and \citet{eastonUseBitScanning1979} describe an approximation called \texttt{CLOCK} which uses a lot less hardware than true \gls{lru} solutions. Conceptually the elements are arranged as on a clock face and each have an associated use-bit, and there is also a global $k$-bit shift register. When a replacement is needed a cursor sweeps round the 'clock' looking for a replacement based on the contents of the use bits as they are shifted into the shift register. \citet{corbatoPagingExperimentMultics1969} makes the case that if $k=0$ this approximates \gls{fifo} that we saw earlier and as $k \rightarrow \infty$ the miss rate tends to match \gls{lru}, though this is qualified in \citet{eastonUseBitScanning1979} as there are a number of cases where this doesn't happen. 

\citet{soCacheOperationsMRU1988} catalogue several different approaches to approximating \gls{lru} using tree-based methods. The first of these is a relaxation the constraint that the cache must keep a total order of elements stored within it. They call this the \gls{plru} algorithm (though some other authors refer to this as \gls{plrut}), under their model \gls{lru} is modelled a stack that requires $\log_2(n!)$ bits to repesent all the configurations of $n$ cache lines. Under \gls{plru} this simplified so that cache lines are arrange into groups of two. This significantly reduces the number of bits required to represent this to $n-1$, and the upshot is that \gls{lru} ends up occuring within each of these subsets rather than at the global level of the cache set. This can cause issues because the net effect of approximating \gls{lru} like this is not exactly equivalent to \gls{lru} so there are performance variations. This is further simiplified by the same authors down to a single bit per cache set, that partitions the group in half into \gls{lru} elements at one end and \gls{mru} elements at the other (a good visual representation is providied in \cite{damienStudyDifferentCache2007}), the rationale being that the exact element to be replaced is not important as long as it's in the \gls{lru} part of the cache set. Both policies are good approximations of \gls{lru} at low levels of associativity but the simplifications made begin to bite as associativity increases and the 1-bit method performs as badly as random at the highest levels of associativity. \gls{plru} on the other hand tracks \gls{lru}'s performance well \cite{al-zoubiPerformanceEvaluationCache2004} and is widely used in data caches for this reason \cite{damienStudyDifferentCache2007}.

The idea of using tree-based methods to approximation \gls{lru} is picked up by \citet{ghasemzadehModifiedPseudoLRU2006} with two new approximating algorithms \gls{bplru} and \gls{mplru}. Under \gls{bplru} a tree that stores history bits is used that is updated on each cache hit so that an approximation of \gls{lru} can be performed at very low cost. However \gls{bplru} lacks the correct amount of hysteresis to not make mistakes when compared to true \gls{lru} because information is lost at the higher levels of the tree. This is addressed in \gls{mplru} where higher levels of the tree are classified into \gls{tbai} and \gls{mbai}. \gls{mbai} nodes save their previous history bit when it is updated allowing them more information to closely approximate \gls{lru}. The performance is very close to true \gls{lru} and signfiicantly better than \gls{fifo} or \texttt{RAND}, and though an overhead exists it's much less than a full \gls{lru} implementation making this a very tempting candidate when resources are scarce.

\citet{malamy1994methods} propose a very simple method of approximation using \gls{mru} bits in a cache along with a locking mechanism to save very frequently accessed data. Each element is assigned an \gls{mru} bit which is initially 0 and when a line is replaced the \gls{mru} bit is set to 1, when all lines are set to 1 all the bits are reset to 0 except the one that has just been inserted. This is shown visually in the work of \citet{damienStudyDifferentCache2007}. This method outperforms \gls{lru} for some data patterns and for others approximates the performance curve very well \cite{al-zoubiPerformanceEvaluationCache2004}, making this a very popular approximation due to its cheapness to implement and relative performance. The \gls{nru} algorithm, detailed in a technical report on the UltraSPARC Architecture \cite{UltraSPARCT2Supplement2007} takes a similar approach adding a \emph{used} bit $u$ and an \emph{allocated} bit $a$ per line. It then searches from a pre-defined pointer to find an element where $u$ and $a$ are both cleared to perform the replacement. This approximates \gls{lru} but dispenses with ordering so as to greatly simplify the number of bits necessary to do so.

\citet{ghasemzadehPseudoFIFOArchitectureLRU2005} takes a different approach and looks at the underlying \gls{lru} hardware rather than the process. In this work the reference implementation is taken as a matrix to represent the various \gls{lru} stack configurations, and this requires $\frac{n(n-1)}{2}$ bits where $n$ is level of associativity in the cache. Under the implementation presented by \citet{ghasemzadehPseudoFIFOArchitectureLRU2005} they reduce this to $n\log_2\left(\frac{n}{2}\right)$, which asymptotically grows less fast than the reference. They also show their technique consistently outperforms \gls{lru} on miss rate.

The second stream of the development of \gls{lru} is an attempt to extend the general idea of the policy to counteract its shortcomings and this takes several general forms. The first of these is to try and use statistical inference or measured history of accesses to divine the future behaviour of a program and act accordingly. This is the approach taken by \citet{oneilLRUKPageReplacement1993} where the \gls{lru-k} algorithm is described \footnote{The work of \citet{oneilLRUKPageReplacement1993} is cited as the inspiration for \citet{sokolinskyLFUKEffectiveBuffer2004}'s work we discussed earlier.}, using Bayesian methods to estimate the interarrival times of memory references from the collected set of past references. \citet{vakaliLRUbasedAlgorithmsWeb2000} continues this idea with \gls{hlru} which defines a function $hist(x,h)$ which returns the $h$th past reference to the cache object $x$, and then uses the maximum value of $hist$ among the cached objects to decide a replacement. \citet{wongModifiedLRUPolicies2000} dsecribes a suite of algorithms that are improvements of each other \gls{prl} and \gls{orl}. The essential idea of these algorithms is to favour lines that exhibit temporal behaviour by marking them as such using special \gls{isa} instructions. \gls{prl} does the identification offline using profiling but \gls{orl} does profiling online at run-time, keeping a table of hits to non-\gls{mru} lines and using this to set temporal bits. All of these policies show increased performance over \gls{lru} to varying degrees, but the problem with all of them is the extra hardware and book-keeping required. In addition none of these algorithms address some of the underlying flaws in \gls{lru} such as its suceptability to flooding \cite{glassAdaptivePageReplacement1997} as unless the history collection cover a whole loop the behaviour would not be picked up. The next set of approaches address these concerns.

Flooding is a phenomemnon particularly associated with \gls{lru} which happens when applications try to access a large address space in a sequential fashion. As more addresses are accessed and the cache capacity is exhausted, old elements (those at the beginning of the space) are evicted from the cache, and then when the loop begins again old elements are pushed out. This means that the cache policy has no positive impact on the execution at all. To address this \citet{glassAdaptivePageReplacement1997} proposes the \texttt{SEQ} algorithm which records long sequences of requests for sequential pages and applies \gls{mru} replacement to those sequences. Otherwise it defaults to simple \gls{lru}. This has a high overhead to implement compared to \gls{lru} which is even more of a problem as \gls{lru} is costly to implement anyway and there are more than just sequential accesses that can cause flooding. \citet{smaragdakisEELRUSimpleEffective1999} further develops these ideas to create the \gls{eelru} algorithm where the definition of 'sequential' is weakened to make the algorithm more ammenable to data structures that are not contiguous in memory. It does this via an adaptive feature where the behaviour of the algorithm changes between standard \gls{lru} and what \citeauthor{smaragdakisEELRUSimpleEffective1999} refers to as the \gls{wfl} algorithm, which can evict pages before they become the \gls{lru} page. \gls{eelru} merges these together by calculating probablistically whether \gls{wfl} will have more hits than measured from \gls{lru} over multiple sets of parameters for \gls{wfl}. A further approach to the problem of flooding is cache partitioning, as proposed by \citeauthor{kimLowoverheadHighperformanceUnified2000} in their \gls{ubm} technique. Here the cache is divided into three regions, a sequential region, a looping region and an other region. When a reference is requested it is classifed into one of the three categories and then different replacement algorithms are run on each region accordingly. All of these techniques are at least comparable to \gls{lru} and many perform at least as well as \gls{lru} in a variety of situations but they are not silver bullets. Several of them have very high hardware requirements, particlarly \citet{kimLowoverheadHighperformanceUnified2000} which is targetted as a software technique in its current presentation. This stems from the fact that analysis has to be performed online so a lot of state has to be stored as in all these cases the analysis takes into account history. Other authors suggest using static analysis to cut down this amount of state needed to be stored and that's what we'll consider next.

Static analysis and the use of compiler techniques to enhance caching shift augment compilers and programs with cache hints adding to the amount of information avaiable to a cache when a replacement decision needs to be made. \citet{jainSoftwareassistedCacheReplacement2001} tackles this by augmenting the \gls{isa} of a processor with \texttt{KILL}, \texttt{KEEP} and \texttt{COND-KILL} instructions that are used instead of normal \texttt{LOAD} and \texttt{STORE} instructions when a variable is considered dead. The \texttt{KILL} instruction is used for short-lived variables or ones that are only accessed once and \texttt{KEEP} for long-lived variables and this method shows increased hit rate of \gls{lru} over multiple levels of associativity. \citet{zhenlinwangUsingCompilerImprove2002} simplifies this further through the use of an evict-me bit which is set when a reference is accessed that is \textquote{sufficiently far away} or has no reuse, this is done by issuing a different instruction so the compiler is responsible for setting the evict-me bit. This technique also has enhanced performance over plain \texttt{LRU}. The problem with techniques of this kind are that they are difficult to implement for existing systems because they require changes to the \gls{isa}, which if you are using \gls{cots} components is impossible, but also because they have limited perception and so have a ceiling on their utility. A theme that will be returned to later is the inability of static techniques to comprehend dynamic behaviour, and that is exactly the case here. As a result, it may be the case that there are more references that could be marked for eviction if only the behaviour could be percieved. 

A consistent assumption of all the techniques seen so far is that cache misses all have a uniform cost to them. However research in the recent past has shown this simply not the case, as \citet{qureshiCaseMLPAwareCache2006} pithily puts it \textquote{not all misses are created equal}. Consequently several authors have attempted to integrate cost functions into \gls{lru} algorithms to improve performance so as to prioritise low-cost misses. \citet{jeongCostsensitiveCacheReplacement2003} is an early example of this, where each block is not only associated with a last reference time but also with a cost of replacement. This way the algorithm can make a choice between evicting the \gls{lru} element or evicting non-\gls{lru} element with a lower cost. Overall several iterations of the algorithm a 15-18\% increase on plain \gls{lru} is recorded with minimal extra hardware requirement. This approach is developed further in \citet{kharbutliLACSLocalityAwareCostSensitive2014a} with the introduction of the \gls{lacs} algorithm. Instead of using a 2-cost model, where a miss is cheap or expensive, this technique records latency when a miss happens for a particular element. These costs are incremented and decremented as other events occur in the cache as well such as a hit to an element or a different element not being accessed for a long period of time. This results in large drops in the miss rate but depends a lot on the working set size relative to the size of the cache. \citet{dasLatencyAwareBlock2017} takes a similar approach but uses latencies calculated from a \gls{noc} rather than a tradtional hardware arrangement. The big problem with these approaches is how the cost function is arrived at and what variables it considers. The examples previously presented use different criteria but the potential list of criteria is endless so it's difficult to distil all these competing pieces of information into the set that is most salient for reducing latency. Also none of these authors contend with the problem of \emph{calculating} the cost as a program is running \citet{jeongCostsensitiveCacheReplacement2003} uses information that can easily be cribbed from memory addresses and \citet{kharbutliLACSLocalityAwareCostSensitive2014a} uses simple counters.

One of the consistent problems of \gls{lru} is that in reality it's only a small subset of cache elements that actually get referenced after they are read into the cache \cite{qureshiAdaptiveInsertionPolicies2007}. This means that a lot of elements sit in \gls{lru} caches for a long time, simply taking up space that could be used by other elements. The root cause is that under vanilla \gls{lru} all elements are inserted in the \gls{mru} position (usually in a stack but sometimes in other data structures) and then progress to the \gls{lru} position over time. \citeauthor{qureshiAdaptiveInsertionPolicies2007} \cite{qureshiAdaptiveInsertionPolicies2007, qureshiSetDuelingControlledAdaptiveInsertion2008} asks whether changing this might lead to better cache utilisation and so proposes a suite of new insertion policies to achieve this. The eventual policy arrived at is known as \gls{dip} which combines standard \gls{lru} with a policy known as \gls{bip}, the algorithm switching between these two policies when one performs better than the other. In order to track the utility of switching a technique known as set-duelling is used, where part of the cache is dedicated to each policy and the miss-rate for each part is calculcated before it's decided which algorithm should be used. This closes the gap between \gls{lru} and \texttt{OPT} in the situations studied by two-thirds. \citet{sreedharanCacheReplacementPolicy2017} takes a similar approach but uses a reference count attached to each cache block. If it is high the insertions happen at the \gls{mru} position, otherwise it's \gls{lru}. \citet{guTheoryPotentialLRUMRU2011} takes a slightly different approach they call collabortive caching. Here the cache is split in half so you have \gls{mru} and \gls{lru} instructions so data can be inserted in different places. The problem with the last two solutions is very much around how you would gain the information to decided when to use each instruction on the fly. In \citet{sreedharanCacheReplacementPolicy2017, qureshiAdaptiveInsertionPolicies2007} there are mechanisms to decide however none of that exists in \citet{guTheoryPotentialLRUMRU2011} as it's a more theoretical paper. All these policies improve performance and some close the gap to \texttt{OPT} significantly but there are still times where caching policies like these will make mistakes due to the relative paucity of information they have so our final set of solutions addresses this problem through mistake correction.

\citet{kampeSelfCorrectingLRUReplacement2004} suggest that the wide gap between \texttt{OPT} and \gls{lru} is indicative of \gls{lru} making too many mistakes and so their policy of self-correcting \gls{lru} adds a feedback loop to the \gls{lru} policy and starts with the goal that no mistake should occur more than once. To do this they employ a shadow directory to track when blocks are evicted too early and a mistake history table to persist the information even after blocks are removed from the shadow cache. There's also an \gls{mru} victim cache that means blocks that bypass the cache and ones that are evicted from the \gls{mru} position so that mispredictions can be quickly recovered from. All these improvements together give a 24\% improvement in miss rates during the experiments made, but this is at the cost of quite a lot of extra hardware and book keeping to manage this extra state.

Having now toured many different manifestations of \gls{lru} policies we see a pattern emerging, which is that \gls{lru} when compared to \texttt{OPT} will always make mistakes and mispredictions so will never track \texttt{OPT}'s performance perfectly. There are many techniques to close the gap somewhat but no technique has done it flawlessly up to now. This implies that recency is not the only piece of information needed to make the best caching decisions and so the next set of policies we'll consider combine recency and frequency to close the information gap to \texttt{OPT} and attempt to match its performance.

\subsubsection{Methods Combining Recency and Frequency}

Having considered recency and frequency in isolation it makes sense to ask, can the two sources of information be usefully combined? Many authors have attempted to bridge this gap and their solutions fall into a few key categories. the first of these is to start from a basis of using \gls{lru} but to augment that with cache partitioning based on frequency counts. Cache partitioning involves logically subdividing the cache into multiple regions, where each region has a different probability of replacement. Consequently some elements become protected more so than they would under a vanilla \gls{lru} or \gls{lfu} policy. This is exactly the approach taken by \citet{robinsonDataCacheManagement1990} where the cache is partitioned into \texttt{new}, \texttt{middle} and \texttt{old}. Elements start in \texttt{new} when they are first referenced and slowly move towards \texttt{old} as the time since their last reference increases. When it's time for replacement the element with the lowest reference count in the \texttt{old} section is selected, where \gls{lru} is used to break ties. \citet{karedlaCachingStrategiesImprove1994} take a similar approach but only divide the cache into two section and abstract the frequency count to either 1 or more than 2. \citet{osawaGenerationalReplacementSchemes1997} meanwhile uses generational caching to split the cache into $N$ generations with cache elements moving towards generation $N$ on every hit. Also presented by \citeauthor{osawaGenerationalReplacementSchemes1997} is the addition of a small history list which means that if an entry is found there on insertion it can be inserted into generation 2 instead of 1 as it is more recent than something the cache has never seen. 

\citet{juanImprovedMulticoreShared2012} takes a similar approach to \citeauthor{osawaGenerationalReplacementSchemes1997} in using $N$ partitions of the cache but they apply it to \gls{cmp}s and so use the cache organisation to give each core a part of the cache, while allowing stealing between cores if that is of benefit. The problem with a lot of these schemes is whilst they are often very good, \citet{robinsonDataCacheManagement1990} boasts of closing 34\% of the gap between \texttt{OPT} and \gls{lru} for example, they rely on  \blockcquote{bansalCARClockAdaptive2004}{user-specificed magic parameters} to set the size of the generations for highest effect. Implementing these in reality would require a lot of performance tuning or simply guessing to get the correct size of generations or paritions because as \citeauthor{osawaGenerationalReplacementSchemes1997} points out, if you get this wrong lots of accesses in a short period mean elements can get protected when actually there may only be a short burst of accesses that require that element. As a result cache partitioning is often used as an auxillary tool to enhance other algorithms as opposed to being used on its own.

One of the more popular techniques to integrated recency and frequency is to introduce new structures into the cache to re-organise the data. These structures are mostly logical in nature, reorganising the cache from the perspecitve of the replacement algorithm but they can be very effective. The first of these is \texttt{2Q} \cite{johnson2QLowOverhead1994} which divides the cache into two queues known as $A_m$ and $A_1$. $A_1$ is further subdivided in two ${A_1}_{in}$ and ${A_1}_{out}$. The essential principle is to admit \blockcquote{johnson2QLowOverhead1994}{only hot pages to the main buffer [$A_m$]} so $A_1$ essentially acts as a filter so re-referenced pages can go into $A_m$. If a page is not rereferenced while in $A_1$ it is unlikely to be hot and so is evicted. This has a low overhead compared to LRU and boasts a moderate improvement (5-10\%). \citet{menaudImprovingEffectivenessWeb2000} uses a similar concept with counters to control the ordering of queues but is innappropriate for our purposes because it relies on a secondary process to differentiate 'hot' and 'cold' accesses which is infeasible in CPU caches as opposed to web caches.  \citet{megiddoARCSelfTuningLow2003} brings something new to the discussion with \texttt{ARC} a self-tuning algorithm that uses a cache directory to list out elements that have been accessed once recently ($L_1$) and twice or more recently ($L_2$). The algorithm then attempts to keep $p$ pages from $L_1$ and $c-p$ pages from $L_2$ in the cache where $c$ is the size of the cache and $p$ is altered as hits and misses occur on different elements. This was further advanced by \citet{bansalCARClockAdaptive2004} by combining the algorithm with \texttt{CLOCK}, turining $L_1$ and $L_2$ into clocks in order to remove some of the disadvantages of \gls{lru} not addressed by \texttt{ARC}. 

In work by \citet{liCRFPNovelAdaptive2008} only one queue is used as a cache directory and each cache block is associated with an $R$ and $F$ value to track recency and frequency respectively. This cache directory ($Q_{out}$) maintains two counters $O$ and $H$ and when an entry is found in $Q_{out}$ before insertion into the cache $H$ is incremented, $O$ is incremented otherwise. The ratio between these counters decide whether the $R$ or the $F$ value is used when a replacement is required. \citet{zhangDivideandconquerBubbleReplacement2009} takes the biggest departure, splitting each 'way' in an $n$-way cache into $k$ groups where elements bubble up these groups on a hit and removals are taken from the set of elements at the bottom of each of the groups. All of these policies achieve improvements over \gls{lru} and track closer to \texttt{OPT}, closing the gap by 47\% in some cases \cite{zhangDivideandconquerBubbleReplacement2009}. The problem comes from the extra hardware overhead they incur, most of these papers only speculate on the theoretical properties of their cache policies and not how they would actually be implemented with the exception of \texttt{ARC} and \texttt{CAR}/\texttt{CART}. In those two cases they are good candidates as some of the most applicable replacement policies so far seen, proving the intuition that lies behind this whole thesis, that providing more information to a process will allow it to make better decisions.

Rather than introducing new structures into their cache architecture some authors attempt to integrate frequency and recency together by defining a new objective function to use for replacement. In strict \gls{lru} the objective function can be formulated in natural language as ``which element has been accessed least recently?'' but some authors attempt to change that to make better replacement decisions. \citet{reddyIntelligentWebCaching1998} for example creates an objective function that applies a weighting to the contribution of recency and frequency, using a parameter $\alpha$ to control the weighting. \citeauthor{dongheeleeImplementationPerformanceEvaluation1997} \cite{dongheeleeLRFUSpectrumPolicies2001} and developed later by \citet{cuiNewHybridApproach2003} takes this a stage further and subsumes this all into a simple function controlled by a parameter $\lambda$ which is shown to subsume all weightings of frequency and recency. \citet{abdelfattahLeastRecentlyFive2012} takes a similar approach but calculates all the weights relative to every other element in the cache and then sums them using constants for weights, concluding that weighting frequency five times as much as recency gives best performance, this is further optimised by \citet{anandkumarHybridCacheReplacement2014}. \citet{dasArbitrationCacheReplacements2016} takes another similar approach but simply takes the product of frequency and recency allowing each to weight the other.

All the previous papers on objective functions exist as ways to weight the contribution to replacement of frequency and recency but other papers try to define entirely new metrics that move beyond this. \citet{tianEffectivenessbasedAdaptiveCache2014} considers effectiveness as \blockcquote{tianEffectivenessbasedAdaptiveCache2014}{the rate of re-use of the block over [a] future time period}, which is realised as $\dfrac{r*R_{count}}{f*E_{count}}$ where $r$ and $f$ are the recency and frequency weights, the $R_{count}$ is a count of re-references and $E_{count}$ is a count of how long has elapsed since the last re-reference. These new objective functions have a wide spectrum of success, from improvements of 9\% over \gls{lru} to outperforming \texttt{ARC} which itself outperforms \gls{lru} by a considerable factor. The problem with implementing many of them would be the explosion of counters and calculation units that would be required even in relatively simple cases with small caches. In addition to that the problem of magic parameters recurs, most of the proposed solutions rely on having a good sense of what the workload for the cache will look like a-priori which is simply impossible for a standard implementation, the adaptive examples go some way to alleviating this problem but will require more resources dependent on the exact method of adaptation used.

A final category of integrations of recency and frequency are algorithms that simply augment \gls{lru} with information about frequency, adding it as a second criterion by which to select a replacement candidate. This is certainly true of \citet{changLRUWWWProxy1999} uses a policy \texttt{LRU}* where cache hits increment a counter on each cache element. When replacements every item checked for replacement has its hit counter decreased by 1 and a replacement is only made when a counter hits 0 for a particular element.  This is further developed in \citet{alghazoSFLRUCacheReplacement2004} whereby for each replacement the \gls{lru} and second least-recently used are compared, with frequency counters used to decide if the \gls{lru} element should be saved. \citet{dybdahlLRUbasedReplacementAlgorithm2006} rounds out this set of enhacements by increasing the frequency differently for reads and writes and implementing cache bypassing for particularly high frequency counter values, to immeadiately promote elements to high levels of the cache hierarchy if necessary. In terms of utility these schemes have similar problems to redefining the objective function in that the proliferation of counters may make them a problem in CPU caches as opposed to simulations. The problem of `magical parameters' also persists with many of these algorithms having multiple tuning parameters that would need to be estimated prior to use.


\subsubsection{Beyond Recency and Frequency Combinations}

In recent years there have been attempts to move beyond collecting frequency and recency information to create a replacement policy. These fall roughly into three categories, the first being tracking new metrics and using those to inform the replacement policy, switching policies on the fly (based either on miss-rate or on oher metrics) and reorganising the cache. Taking new metrics first, the earliest example of this is the work of  \citet{rizzoReplacementPoliciesProxy2000} which calculates a value ($V$) for each element in the cache as $V = \dfrac{C}{B}*P_r$, where $C$ is the cost of retrieval, $B$ is the benefit of remvoal and $P_r$ is the probability of re-reference. Each of these values has other factors that feed into its calculation which allows the policy to shape itself around multiple factors very easily. In \citet{kharbutliCounterbasedCacheReplacement2005} two new metrics are proposed the \gls{aip} and \gls{lvp}. \gls{aip} increments a counter for a cache line whenever an access is made to another line in the same cache set. \gls{lvp} on the other hand counts the number of access to a line in a single generation (time from insertion to eviction from the cache). In either case once the counter reaches a particualr threshold the line is evicted. \citet{keramidasCacheReplacementBased2007} takes a slightly different approach and attempts to predict the \blockcquote{keramidasCacheReplacementBased2007}{reuse distance}, or the number of access to the L1 cache between address references, of particular addresses and then uses this prediction in tandem with LRU to overcome some of its shortcomings. All these techniques suffer from similar problems in that many of them rely on `magic parameters' which have to be set apriori and also that they have high hardware requirements, either for calculation of probabilities or to store data to be analysed, rendering them difficult to apply in an embedded context. The work of \citet{duongSCOREScoreBasedMemory2010} and to some extent \citet{tadaCacheReplacementPolicy2019} takes a slightly different approach in that they abstract specific metrics into an overall `score' for each cache element, selecting the lowest score each time to evict each time its required. Scores are set initially upon entry into the cache and are then altered as different events occur, with \citet{duongSCOREScoreBasedMemory2010} having tunable parameters for how events affect overall scores. These are much more applicable to an embedded context and shows very good gains over standard algorithms when given more information. In general these policies show that increasing the information available to your caching policy will increase its ability to make better decisions, something that we will return to in our analysis of the literature and ideas to move forward.

The second type of policy in this area is an attempt to take the best of all worlds in terms of all the `simpler' policies we've studied so far by changing the policy that the cache applies dynamically based upon observable conditions within the cache itself. The genesis of this is ACME \cite{ariACMEAdaptiveCaching2002, gramacyAdaptiveCachingRefetching2003, riaz-ud-dinAcmeDBAdaptiveCaching2006} which uses small virtual caches to test the effect of a suite of policies on the miss rate of the cache and then computes a set of weights that minimises the miss rate via machine learning. \citet{subramanianAdaptiveCachesEffective2006a} develops a similar idea but gives a more realistic implementation than the original presentation of ACME. This is some of the best of what cache policies can achieve, in that this approach sees reductions in miss rates across a wide variety of benchmarks and cache configurations. Not only that but since this approach is adaptible it requires no `magic parameters' as setup and will adapt to any program given enough time. A sub-variant of this idea is presented by \citet{jongmoochoiDesignImplementationPerformance2002} as well as other authors \cite{smaragdakisGeneralAdaptiveReplacement2004, aguilarGeneralAdaptiveCache2004, aguilarCoherenceReplacementProtocolWeb2006, changAdaptiveBufferCache2016, changPARCNovelOS2018} with the key difference between them being the metrics they choose to use to perform the adaptation, with \citeauthor{jongmoochoiDesignImplementationPerformance2002} using estimated forward distances, \citeauthor{changAdaptiveBufferCache2016} using classification of PC access into sequential and looping access and  \citeauthor{aguilarGeneralAdaptiveCache2004} using a variety of metrics that are all observable by the cache directly. All this put together gives some of the best results in this category of implementation but there are still problems, for one the virtual caches required or the extra metric tracking can take up a lot of extra hardware, particularly if a machine learning element is included, but also all of these policies are still under-performing if compared to \texttt{OPT} from \citet{beladyStudyReplacementAlgorithms1966}, there is still a signifcant gap that needs to be closed in order to really attack the latency that caches introduce and sadly despite their promise, these techniques still do not achieve that.

A third attempt to move beyond recency and frequency combinations is to re-organise the cache somehwat in light of information beyond recency and frequency. The first of these is \citet{chaudhuriPseudoLIFOFoundationNew2009a} who changes the insertion policy in order to make sure dead blocks are not kept in the cache longer than necessary while the working set of the cache remains untouched. \citet{manikantanNUcacheEfficientMulticore2011} takes a slightly different approach by dividing up the ways in the cache to prioritise a set of delinquent program counter values that contribute large numbers of misses to the overall count. This set of PCs is detected and changed adaptively as the program executes. Finally \citet{khanDecoupledDynamicCache2012} builds on earlier work seen in \citet{johnson2QLowOverhead1994} to dynamically alter the size of the once referenced and more than once referenced cache partitions. All of these obtain some speed up and most are adaptive but the hardware overhead is still very prohibitive when considering a CPU cache rather than a web or operating system cache.

Having considered a broad spectrum of cache policies a pattern starts to emerge as to their performance as I alluded to earlier. It's clear from this broad review of the literature that no cache policy, no matter how complicated can outperform \texttt{OPT}. From our consideration of the literature it seems that \texttt{OPT} is a fundamental limit on the latency reduction that can be gained from changing the replacement policy, in that sense we have reached the limit of what is possible because there is no sensible way of creating a clairvoyant cache algorithm in the current architectural paradigm. As such if we want to move into new realms where average case latency is further reduced we need to look beyond the cache policy to other techniques, so the next section will consider whether a change in how we design caches will give us the latency reductions that we seek.


\subsection{Improving Cache Technology}

\subsection{Multi-Level Caches}

\subsection{Prefecting \& Pre-loading}

\section{Cache Extrinsic Techniques}

All the techniques we have seen up to this point have assumed

\subsection{Hardware Changes}

\subsection{Scheduling}

\subsection{Program Rewriting}

\section{Incorporating Tracing to Reduce Latency}

\subsection{Tracing as a Control Loop}

\subsection{Other Uses of Tracing}

\section{Conclusion}

\subsection{Problems with Cache-based Solutions}

\subsection{Problems with Cache Extrinsic Solutions}

\subsection{Potential for the application of Tracing}

